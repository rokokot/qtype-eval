SLURM_JOB_ID: 64442817
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: qtype_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Sat May  3 16:22:09 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main probing experiments (non-control)...
=======================
PROBING LAYER 1
=======================
Experiment probe_layer1_question_type_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/ar/ar/results.json for layer 1
Experiment probe_layer1_complexity_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/ar/ar/results.json for layer 1
Experiment probe_layer1_question_type_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/en/en/results.json for layer 1
Experiment probe_layer1_complexity_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/en/en/results.json for layer 1
Experiment probe_layer1_question_type_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/fi/fi/results.json for layer 1
Experiment probe_layer1_complexity_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/fi/fi/results.json for layer 1
Experiment probe_layer1_question_type_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/id/id/results.json for layer 1
Experiment probe_layer1_complexity_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/id/id/results.json for layer 1
Experiment probe_layer1_question_type_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/ja/ja/results.json for layer 1
Experiment probe_layer1_complexity_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/ja/ja/results.json for layer 1
Experiment probe_layer1_question_type_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/ko/ko/results.json for layer 1
Experiment probe_layer1_complexity_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/ko/ko/results.json for layer 1
Experiment probe_layer1_question_type_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/ru/ru/results.json for layer 1
Experiment probe_layer1_complexity_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/ru/ru/results.json for layer 1
=======================
PROBING LAYER 2
=======================
Experiment probe_layer2_question_type_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/ar/ar/results.json for layer 2
Experiment probe_layer2_complexity_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/ar/ar/results.json for layer 2
Experiment probe_layer2_question_type_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/en/en/results.json for layer 2
Experiment probe_layer2_complexity_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/en/en/results.json for layer 2
Experiment probe_layer2_question_type_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_complexity_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_question_type_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/id/id/results.json for layer 2
Experiment probe_layer2_complexity_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_question_type_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_question_type_ja"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/layer2/ja"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-03 16:23:07,199][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/ja
experiment_name: probe_layer2_question_type_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-03 16:23:07,199][__main__][INFO] - Normalized task: question_type
[2025-05-03 16:23:07,199][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-03 16:23:07,199][__main__][INFO] - Determined Task Type: classification
[2025-05-03 16:23:07,204][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ja']
[2025-05-03 16:23:07,204][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-03 16:23:13,544][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-03 16:23:15,884][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-03 16:23:15,885][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:23:16,338][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:23:16,705][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:23:17,005][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-03 16:23:17,014][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:23:17,014][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-03 16:23:17,015][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:23:17,052][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:23:17,108][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:23:17,122][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-03 16:23:17,123][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:23:17,123][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-03 16:23:17,124][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:23:17,279][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:23:17,444][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:23:17,467][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-03 16:23:17,468][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:23:17,468][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-03 16:23:17,469][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-03 16:23:17,470][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:23:17,471][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:23:17,471][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:23:17,471][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:23:17,471][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-05-03 16:23:17,471][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-05-03 16:23:17,471][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-03 16:23:17,471][src.data.datasets][INFO] - Sample label: 1
[2025-05-03 16:23:17,471][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:23:17,471][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:23:17,472][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:23:17,472][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:23:17,472][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-05-03 16:23:17,472][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-05-03 16:23:17,472][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-03 16:23:17,472][src.data.datasets][INFO] - Sample label: 0
[2025-05-03 16:23:17,472][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:23:17,472][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:23:17,472][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:23:17,472][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:23:17,473][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-05-03 16:23:17,473][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-05-03 16:23:17,473][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-03 16:23:17,473][src.data.datasets][INFO] - Sample label: 1
[2025-05-03 16:23:17,473][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-03 16:23:17,473][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-03 16:23:17,473][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-03 16:23:17,473][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-03 16:23:17,474][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-03 16:23:28,429][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-03 16:23:28,432][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-03 16:23:28,432][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-03 16:23:28,432][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-03 16:23:28,438][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-03 16:23:28,438][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-03 16:23:28,438][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-03 16:23:28,438][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-03 16:23:28,438][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-03 16:23:28,439][__main__][INFO] - Total parameters: 394,568,839
[2025-05-03 16:23:28,439][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-03 16:23:28,440][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7109Epoch 1/15: [                              ] 2/75 batches, loss: 0.7290Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7260Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7104Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7026Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7039Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7032Epoch 1/15: [===                           ] 8/75 batches, loss: 0.7013Epoch 1/15: [===                           ] 9/75 batches, loss: 0.7009Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6991Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6981Epoch 1/15: [====                          ] 12/75 batches, loss: 0.6968Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.6963Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.6961Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6954Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6944Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6950Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6945Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6942Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6934Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6926Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6915Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6918Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6906Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6908Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6906Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6894Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6909Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6905Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6900Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6900Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6889Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6878Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6881Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6874Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6857Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6839Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6823Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6825Epoch 1/15: [================              ] 40/75 batches, loss: 0.6826Epoch 1/15: [================              ] 41/75 batches, loss: 0.6815Epoch 1/15: [================              ] 42/75 batches, loss: 0.6817Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6808Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6806Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6798Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6799Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6784Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6766Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6758Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6756Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6751Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6746Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6738Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6728Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6716Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6697Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6705Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6704Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6698Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6692Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6667Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6656Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6657Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6649Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6622Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6623Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6623Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6622Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6610Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6603Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6588Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6585Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6582Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6566Epoch 1/15: [==============================] 75/75 batches, loss: 0.6571
[2025-05-03 16:23:36,474][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6571
[2025-05-03 16:23:36,768][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.5954, Metrics: {'accuracy': 0.782608695652174, 'f1': 0.75, 'precision': 0.9375, 'recall': 0.625}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.5370Epoch 2/15: [                              ] 2/75 batches, loss: 0.5573Epoch 2/15: [=                             ] 3/75 batches, loss: 0.5694Epoch 2/15: [=                             ] 4/75 batches, loss: 0.5607Epoch 2/15: [==                            ] 5/75 batches, loss: 0.5604Epoch 2/15: [==                            ] 6/75 batches, loss: 0.5749Epoch 2/15: [==                            ] 7/75 batches, loss: 0.5774Epoch 2/15: [===                           ] 8/75 batches, loss: 0.5788Epoch 2/15: [===                           ] 9/75 batches, loss: 0.5646Epoch 2/15: [====                          ] 10/75 batches, loss: 0.5656Epoch 2/15: [====                          ] 11/75 batches, loss: 0.5660Epoch 2/15: [====                          ] 12/75 batches, loss: 0.5686Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.5664Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.5709Epoch 2/15: [======                        ] 15/75 batches, loss: 0.5718Epoch 2/15: [======                        ] 16/75 batches, loss: 0.5667Epoch 2/15: [======                        ] 17/75 batches, loss: 0.5663Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.5676Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.5640Epoch 2/15: [========                      ] 20/75 batches, loss: 0.5665Epoch 2/15: [========                      ] 21/75 batches, loss: 0.5669Epoch 2/15: [========                      ] 22/75 batches, loss: 0.5655Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.5657Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.5676Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.5660Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.5661Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.5644Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.5673Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.5638Epoch 2/15: [============                  ] 30/75 batches, loss: 0.5616Epoch 2/15: [============                  ] 31/75 batches, loss: 0.5609Epoch 2/15: [============                  ] 32/75 batches, loss: 0.5629Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.5631Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.5613Epoch 2/15: [==============                ] 35/75 batches, loss: 0.5622Epoch 2/15: [==============                ] 36/75 batches, loss: 0.5617Epoch 2/15: [==============                ] 37/75 batches, loss: 0.5627Epoch 2/15: [===============               ] 38/75 batches, loss: 0.5622Epoch 2/15: [===============               ] 39/75 batches, loss: 0.5614Epoch 2/15: [================              ] 40/75 batches, loss: 0.5617Epoch 2/15: [================              ] 41/75 batches, loss: 0.5621Epoch 2/15: [================              ] 42/75 batches, loss: 0.5643Epoch 2/15: [=================             ] 43/75 batches, loss: 0.5628Epoch 2/15: [=================             ] 44/75 batches, loss: 0.5627Epoch 2/15: [==================            ] 45/75 batches, loss: 0.5640Epoch 2/15: [==================            ] 46/75 batches, loss: 0.5617Epoch 2/15: [==================            ] 47/75 batches, loss: 0.5619Epoch 2/15: [===================           ] 48/75 batches, loss: 0.5622Epoch 2/15: [===================           ] 49/75 batches, loss: 0.5629Epoch 2/15: [====================          ] 50/75 batches, loss: 0.5627Epoch 2/15: [====================          ] 51/75 batches, loss: 0.5616Epoch 2/15: [====================          ] 52/75 batches, loss: 0.5615Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.5618Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.5628Epoch 2/15: [======================        ] 55/75 batches, loss: 0.5623Epoch 2/15: [======================        ] 56/75 batches, loss: 0.5620Epoch 2/15: [======================        ] 57/75 batches, loss: 0.5608Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.5612Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.5608Epoch 2/15: [========================      ] 60/75 batches, loss: 0.5612Epoch 2/15: [========================      ] 61/75 batches, loss: 0.5621Epoch 2/15: [========================      ] 62/75 batches, loss: 0.5629Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.5632Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.5627Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.5620Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.5615Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.5608Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.5611Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.5606Epoch 2/15: [============================  ] 70/75 batches, loss: 0.5607Epoch 2/15: [============================  ] 71/75 batches, loss: 0.5602Epoch 2/15: [============================  ] 72/75 batches, loss: 0.5599Epoch 2/15: [============================= ] 73/75 batches, loss: 0.5605Epoch 2/15: [============================= ] 74/75 batches, loss: 0.5592Epoch 2/15: [==============================] 75/75 batches, loss: 0.5587
[2025-05-03 16:23:39,515][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.5587
[2025-05-03 16:23:39,890][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.5540, Metrics: {'accuracy': 0.8913043478260869, 'f1': 0.9019607843137255, 'precision': 0.8518518518518519, 'recall': 0.9583333333333334}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.5489Epoch 3/15: [                              ] 2/75 batches, loss: 0.5384Epoch 3/15: [=                             ] 3/75 batches, loss: 0.5235Epoch 3/15: [=                             ] 4/75 batches, loss: 0.5320Epoch 3/15: [==                            ] 5/75 batches, loss: 0.5247Epoch 3/15: [==                            ] 6/75 batches, loss: 0.5218Epoch 3/15: [==                            ] 7/75 batches, loss: 0.5211Epoch 3/15: [===                           ] 8/75 batches, loss: 0.5160Epoch 3/15: [===                           ] 9/75 batches, loss: 0.5163Epoch 3/15: [====                          ] 10/75 batches, loss: 0.5094Epoch 3/15: [====                          ] 11/75 batches, loss: 0.5108Epoch 3/15: [====                          ] 12/75 batches, loss: 0.5122Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.5095Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.5079Epoch 3/15: [======                        ] 15/75 batches, loss: 0.5072Epoch 3/15: [======                        ] 16/75 batches, loss: 0.5058Epoch 3/15: [======                        ] 17/75 batches, loss: 0.5074Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.5062Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.5058Epoch 3/15: [========                      ] 20/75 batches, loss: 0.5131Epoch 3/15: [========                      ] 21/75 batches, loss: 0.5107Epoch 3/15: [========                      ] 22/75 batches, loss: 0.5097Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.5102Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.5091Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.5134Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.5145Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.5149Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.5155Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.5149Epoch 3/15: [============                  ] 30/75 batches, loss: 0.5179Epoch 3/15: [============                  ] 31/75 batches, loss: 0.5172Epoch 3/15: [============                  ] 32/75 batches, loss: 0.5170Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.5152Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.5173Epoch 3/15: [==============                ] 35/75 batches, loss: 0.5175Epoch 3/15: [==============                ] 36/75 batches, loss: 0.5195Epoch 3/15: [==============                ] 37/75 batches, loss: 0.5205Epoch 3/15: [===============               ] 38/75 batches, loss: 0.5208Epoch 3/15: [===============               ] 39/75 batches, loss: 0.5219Epoch 3/15: [================              ] 40/75 batches, loss: 0.5223Epoch 3/15: [================              ] 41/75 batches, loss: 0.5238Epoch 3/15: [================              ] 42/75 batches, loss: 0.5252Epoch 3/15: [=================             ] 43/75 batches, loss: 0.5263Epoch 3/15: [=================             ] 44/75 batches, loss: 0.5271Epoch 3/15: [==================            ] 45/75 batches, loss: 0.5283Epoch 3/15: [==================            ] 46/75 batches, loss: 0.5301Epoch 3/15: [==================            ] 47/75 batches, loss: 0.5289Epoch 3/15: [===================           ] 48/75 batches, loss: 0.5304Epoch 3/15: [===================           ] 49/75 batches, loss: 0.5322Epoch 3/15: [====================          ] 50/75 batches, loss: 0.5322Epoch 3/15: [====================          ] 51/75 batches, loss: 0.5318Epoch 3/15: [====================          ] 52/75 batches, loss: 0.5320Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.5307Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.5311Epoch 3/15: [======================        ] 55/75 batches, loss: 0.5317Epoch 3/15: [======================        ] 56/75 batches, loss: 0.5309Epoch 3/15: [======================        ] 57/75 batches, loss: 0.5298Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.5296Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.5307Epoch 3/15: [========================      ] 60/75 batches, loss: 0.5303Epoch 3/15: [========================      ] 61/75 batches, loss: 0.5309Epoch 3/15: [========================      ] 62/75 batches, loss: 0.5304Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.5306Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.5304Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.5311Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.5320Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.5321Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.5332Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.5322Epoch 3/15: [============================  ] 70/75 batches, loss: 0.5318Epoch 3/15: [============================  ] 71/75 batches, loss: 0.5315Epoch 3/15: [============================  ] 72/75 batches, loss: 0.5317Epoch 3/15: [============================= ] 73/75 batches, loss: 0.5314Epoch 3/15: [============================= ] 74/75 batches, loss: 0.5319Epoch 3/15: [==============================] 75/75 batches, loss: 0.5341
[2025-05-03 16:23:42,699][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.5341
[2025-05-03 16:23:43,064][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.5337, Metrics: {'accuracy': 0.9347826086956522, 'f1': 0.9361702127659575, 'precision': 0.9565217391304348, 'recall': 0.9166666666666666}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6137Epoch 4/15: [                              ] 2/75 batches, loss: 0.5871Epoch 4/15: [=                             ] 3/75 batches, loss: 0.5793Epoch 4/15: [=                             ] 4/75 batches, loss: 0.5566Epoch 4/15: [==                            ] 5/75 batches, loss: 0.5613Epoch 4/15: [==                            ] 6/75 batches, loss: 0.5610Epoch 4/15: [==                            ] 7/75 batches, loss: 0.5566Epoch 4/15: [===                           ] 8/75 batches, loss: 0.5530Epoch 4/15: [===                           ] 9/75 batches, loss: 0.5497Epoch 4/15: [====                          ] 10/75 batches, loss: 0.5459Epoch 4/15: [====                          ] 11/75 batches, loss: 0.5359Epoch 4/15: [====                          ] 12/75 batches, loss: 0.5308Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.5241Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.5178Epoch 4/15: [======                        ] 15/75 batches, loss: 0.5219Epoch 4/15: [======                        ] 16/75 batches, loss: 0.5247Epoch 4/15: [======                        ] 17/75 batches, loss: 0.5246Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.5257Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.5262Epoch 4/15: [========                      ] 20/75 batches, loss: 0.5275Epoch 4/15: [========                      ] 21/75 batches, loss: 0.5299Epoch 4/15: [========                      ] 22/75 batches, loss: 0.5325Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.5310Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.5298Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.5268Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.5268Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.5272Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.5273Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.5302Epoch 4/15: [============                  ] 30/75 batches, loss: 0.5312Epoch 4/15: [============                  ] 31/75 batches, loss: 0.5297Epoch 4/15: [============                  ] 32/75 batches, loss: 0.5290Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.5256Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.5252Epoch 4/15: [==============                ] 35/75 batches, loss: 0.5240Epoch 4/15: [==============                ] 36/75 batches, loss: 0.5256Epoch 4/15: [==============                ] 37/75 batches, loss: 0.5270Epoch 4/15: [===============               ] 38/75 batches, loss: 0.5297Epoch 4/15: [===============               ] 39/75 batches, loss: 0.5312Epoch 4/15: [================              ] 40/75 batches, loss: 0.5305Epoch 4/15: [================              ] 41/75 batches, loss: 0.5310Epoch 4/15: [================              ] 42/75 batches, loss: 0.5295Epoch 4/15: [=================             ] 43/75 batches, loss: 0.5289Epoch 4/15: [=================             ] 44/75 batches, loss: 0.5259Epoch 4/15: [==================            ] 45/75 batches, loss: 0.5235Epoch 4/15: [==================            ] 46/75 batches, loss: 0.5238Epoch 4/15: [==================            ] 47/75 batches, loss: 0.5241Epoch 4/15: [===================           ] 48/75 batches, loss: 0.5248Epoch 4/15: [===================           ] 49/75 batches, loss: 0.5254Epoch 4/15: [====================          ] 50/75 batches, loss: 0.5254Epoch 4/15: [====================          ] 51/75 batches, loss: 0.5261Epoch 4/15: [====================          ] 52/75 batches, loss: 0.5267Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.5274Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.5262Epoch 4/15: [======================        ] 55/75 batches, loss: 0.5258Epoch 4/15: [======================        ] 56/75 batches, loss: 0.5268Epoch 4/15: [======================        ] 57/75 batches, loss: 0.5274Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.5279Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.5278Epoch 4/15: [========================      ] 60/75 batches, loss: 0.5269Epoch 4/15: [========================      ] 61/75 batches, loss: 0.5277Epoch 4/15: [========================      ] 62/75 batches, loss: 0.5286Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.5284Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.5289Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.5281Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.5267Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.5257Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.5263Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.5285Epoch 4/15: [============================  ] 70/75 batches, loss: 0.5279Epoch 4/15: [============================  ] 71/75 batches, loss: 0.5286Epoch 4/15: [============================  ] 72/75 batches, loss: 0.5284Epoch 4/15: [============================= ] 73/75 batches, loss: 0.5289Epoch 4/15: [============================= ] 74/75 batches, loss: 0.5271Epoch 4/15: [==============================] 75/75 batches, loss: 0.5279
[2025-05-03 16:23:45,742][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5279
[2025-05-03 16:23:46,124][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5270, Metrics: {'accuracy': 0.9347826086956522, 'f1': 0.9387755102040817, 'precision': 0.92, 'recall': 0.9583333333333334}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.4838Epoch 5/15: [                              ] 2/75 batches, loss: 0.5063Epoch 5/15: [=                             ] 3/75 batches, loss: 0.4998Epoch 5/15: [=                             ] 4/75 batches, loss: 0.4929Epoch 5/15: [==                            ] 5/75 batches, loss: 0.5057Epoch 5/15: [==                            ] 6/75 batches, loss: 0.5018Epoch 5/15: [==                            ] 7/75 batches, loss: 0.5075Epoch 5/15: [===                           ] 8/75 batches, loss: 0.5178Epoch 5/15: [===                           ] 9/75 batches, loss: 0.5250Epoch 5/15: [====                          ] 10/75 batches, loss: 0.5189Epoch 5/15: [====                          ] 11/75 batches, loss: 0.5181Epoch 5/15: [====                          ] 12/75 batches, loss: 0.5171Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.5233Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.5211Epoch 5/15: [======                        ] 15/75 batches, loss: 0.5231Epoch 5/15: [======                        ] 16/75 batches, loss: 0.5244Epoch 5/15: [======                        ] 17/75 batches, loss: 0.5216Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.5233Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.5268Epoch 5/15: [========                      ] 20/75 batches, loss: 0.5249Epoch 5/15: [========                      ] 21/75 batches, loss: 0.5226Epoch 5/15: [========                      ] 22/75 batches, loss: 0.5240Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.5246Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.5231Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.5217Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.5221Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.5210Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.5231Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.5233Epoch 5/15: [============                  ] 30/75 batches, loss: 0.5276Epoch 5/15: [============                  ] 31/75 batches, loss: 0.5258Epoch 5/15: [============                  ] 32/75 batches, loss: 0.5243Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.5248Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.5259Epoch 5/15: [==============                ] 35/75 batches, loss: 0.5258Epoch 5/15: [==============                ] 36/75 batches, loss: 0.5229Epoch 5/15: [==============                ] 37/75 batches, loss: 0.5217Epoch 5/15: [===============               ] 38/75 batches, loss: 0.5222Epoch 5/15: [===============               ] 39/75 batches, loss: 0.5219Epoch 5/15: [================              ] 40/75 batches, loss: 0.5223Epoch 5/15: [================              ] 41/75 batches, loss: 0.5214Epoch 5/15: [================              ] 42/75 batches, loss: 0.5221Epoch 5/15: [=================             ] 43/75 batches, loss: 0.5218Epoch 5/15: [=================             ] 44/75 batches, loss: 0.5216Epoch 5/15: [==================            ] 45/75 batches, loss: 0.5214Epoch 5/15: [==================            ] 46/75 batches, loss: 0.5210Epoch 5/15: [==================            ] 47/75 batches, loss: 0.5211Epoch 5/15: [===================           ] 48/75 batches, loss: 0.5209Epoch 5/15: [===================           ] 49/75 batches, loss: 0.5206Epoch 5/15: [====================          ] 50/75 batches, loss: 0.5198Epoch 5/15: [====================          ] 51/75 batches, loss: 0.5186Epoch 5/15: [====================          ] 52/75 batches, loss: 0.5187Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.5171Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.5165Epoch 5/15: [======================        ] 55/75 batches, loss: 0.5164Epoch 5/15: [======================        ] 56/75 batches, loss: 0.5166Epoch 5/15: [======================        ] 57/75 batches, loss: 0.5167Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.5166Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.5152Epoch 5/15: [========================      ] 60/75 batches, loss: 0.5151Epoch 5/15: [========================      ] 61/75 batches, loss: 0.5138Epoch 5/15: [========================      ] 62/75 batches, loss: 0.5144Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.5144Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.5134Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.5151Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.5143Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.5154Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.5150Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.5149Epoch 5/15: [============================  ] 70/75 batches, loss: 0.5160Epoch 5/15: [============================  ] 71/75 batches, loss: 0.5175Epoch 5/15: [============================  ] 72/75 batches, loss: 0.5187Epoch 5/15: [============================= ] 73/75 batches, loss: 0.5187Epoch 5/15: [============================= ] 74/75 batches, loss: 0.5183Epoch 5/15: [==============================] 75/75 batches, loss: 0.5185
[2025-05-03 16:23:48,856][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5185
[2025-05-03 16:23:49,252][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5262, Metrics: {'accuracy': 0.9347826086956522, 'f1': 0.9361702127659575, 'precision': 0.9565217391304348, 'recall': 0.9166666666666666}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.6023Epoch 6/15: [                              ] 2/75 batches, loss: 0.5948Epoch 6/15: [=                             ] 3/75 batches, loss: 0.5744Epoch 6/15: [=                             ] 4/75 batches, loss: 0.5812Epoch 6/15: [==                            ] 5/75 batches, loss: 0.5730Epoch 6/15: [==                            ] 6/75 batches, loss: 0.5505Epoch 6/15: [==                            ] 7/75 batches, loss: 0.5397Epoch 6/15: [===                           ] 8/75 batches, loss: 0.5324Epoch 6/15: [===                           ] 9/75 batches, loss: 0.5320Epoch 6/15: [====                          ] 10/75 batches, loss: 0.5336Epoch 6/15: [====                          ] 11/75 batches, loss: 0.5320Epoch 6/15: [====                          ] 12/75 batches, loss: 0.5286Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.5286Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.5291Epoch 6/15: [======                        ] 15/75 batches, loss: 0.5274Epoch 6/15: [======                        ] 16/75 batches, loss: 0.5276Epoch 6/15: [======                        ] 17/75 batches, loss: 0.5250Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.5244Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.5207Epoch 6/15: [========                      ] 20/75 batches, loss: 0.5172Epoch 6/15: [========                      ] 21/75 batches, loss: 0.5195Epoch 6/15: [========                      ] 22/75 batches, loss: 0.5206Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.5210Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.5205Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.5189Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.5178Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.5203Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.5214Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.5197Epoch 6/15: [============                  ] 30/75 batches, loss: 0.5187Epoch 6/15: [============                  ] 31/75 batches, loss: 0.5207Epoch 6/15: [============                  ] 32/75 batches, loss: 0.5217Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.5210Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.5213Epoch 6/15: [==============                ] 35/75 batches, loss: 0.5215Epoch 6/15: [==============                ] 36/75 batches, loss: 0.5224Epoch 6/15: [==============                ] 37/75 batches, loss: 0.5227Epoch 6/15: [===============               ] 38/75 batches, loss: 0.5217Epoch 6/15: [===============               ] 39/75 batches, loss: 0.5232Epoch 6/15: [================              ] 40/75 batches, loss: 0.5242Epoch 6/15: [================              ] 41/75 batches, loss: 0.5225Epoch 6/15: [================              ] 42/75 batches, loss: 0.5242Epoch 6/15: [=================             ] 43/75 batches, loss: 0.5242Epoch 6/15: [=================             ] 44/75 batches, loss: 0.5222Epoch 6/15: [==================            ] 45/75 batches, loss: 0.5212Epoch 6/15: [==================            ] 46/75 batches, loss: 0.5220Epoch 6/15: [==================            ] 47/75 batches, loss: 0.5217Epoch 6/15: [===================           ] 48/75 batches, loss: 0.5213Epoch 6/15: [===================           ] 49/75 batches, loss: 0.5214Epoch 6/15: [====================          ] 50/75 batches, loss: 0.5213Epoch 6/15: [====================          ] 51/75 batches, loss: 0.5197Epoch 6/15: [====================          ] 52/75 batches, loss: 0.5185Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.5173Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.5169Epoch 6/15: [======================        ] 55/75 batches, loss: 0.5168Epoch 6/15: [======================        ] 56/75 batches, loss: 0.5162Epoch 6/15: [======================        ] 57/75 batches, loss: 0.5164Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.5170Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.5161Epoch 6/15: [========================      ] 60/75 batches, loss: 0.5155Epoch 6/15: [========================      ] 61/75 batches, loss: 0.5157Epoch 6/15: [========================      ] 62/75 batches, loss: 0.5155Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.5163Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.5157Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.5163Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.5165Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.5160Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.5178Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.5168Epoch 6/15: [============================  ] 70/75 batches, loss: 0.5156Epoch 6/15: [============================  ] 71/75 batches, loss: 0.5157Epoch 6/15: [============================  ] 72/75 batches, loss: 0.5175Epoch 6/15: [============================= ] 73/75 batches, loss: 0.5170Epoch 6/15: [============================= ] 74/75 batches, loss: 0.5171Epoch 6/15: [==============================] 75/75 batches, loss: 0.5151
[2025-05-03 16:23:51,997][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5151
[2025-05-03 16:23:52,311][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5227, Metrics: {'accuracy': 0.9347826086956522, 'f1': 0.9387755102040817, 'precision': 0.92, 'recall': 0.9583333333333334}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.5071Epoch 7/15: [                              ] 2/75 batches, loss: 0.4702Epoch 7/15: [=                             ] 3/75 batches, loss: 0.4959Epoch 7/15: [=                             ] 4/75 batches, loss: 0.4809Epoch 7/15: [==                            ] 5/75 batches, loss: 0.4665Epoch 7/15: [==                            ] 6/75 batches, loss: 0.4648Epoch 7/15: [==                            ] 7/75 batches, loss: 0.4640Epoch 7/15: [===                           ] 8/75 batches, loss: 0.4702Epoch 7/15: [===                           ] 9/75 batches, loss: 0.4796Epoch 7/15: [====                          ] 10/75 batches, loss: 0.4773Epoch 7/15: [====                          ] 11/75 batches, loss: 0.4755Epoch 7/15: [====                          ] 12/75 batches, loss: 0.4804Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.4823Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.4879Epoch 7/15: [======                        ] 15/75 batches, loss: 0.4875Epoch 7/15: [======                        ] 16/75 batches, loss: 0.4856Epoch 7/15: [======                        ] 17/75 batches, loss: 0.4873Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.4898Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.4931Epoch 7/15: [========                      ] 20/75 batches, loss: 0.4972Epoch 7/15: [========                      ] 21/75 batches, loss: 0.4974Epoch 7/15: [========                      ] 22/75 batches, loss: 0.4988Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.5021Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.5017Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.5027Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.5058Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.5092Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.5100Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.5090Epoch 7/15: [============                  ] 30/75 batches, loss: 0.5074Epoch 7/15: [============                  ] 31/75 batches, loss: 0.5100Epoch 7/15: [============                  ] 32/75 batches, loss: 0.5099Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.5097Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.5117Epoch 7/15: [==============                ] 35/75 batches, loss: 0.5123Epoch 7/15: [==============                ] 36/75 batches, loss: 0.5114Epoch 7/15: [==============                ] 37/75 batches, loss: 0.5102Epoch 7/15: [===============               ] 38/75 batches, loss: 0.5121Epoch 7/15: [===============               ] 39/75 batches, loss: 0.5125Epoch 7/15: [================              ] 40/75 batches, loss: 0.5124Epoch 7/15: [================              ] 41/75 batches, loss: 0.5113Epoch 7/15: [================              ] 42/75 batches, loss: 0.5107Epoch 7/15: [=================             ] 43/75 batches, loss: 0.5127Epoch 7/15: [=================             ] 44/75 batches, loss: 0.5142Epoch 7/15: [==================            ] 45/75 batches, loss: 0.5150Epoch 7/15: [==================            ] 46/75 batches, loss: 0.5137Epoch 7/15: [==================            ] 47/75 batches, loss: 0.5141Epoch 7/15: [===================           ] 48/75 batches, loss: 0.5142Epoch 7/15: [===================           ] 49/75 batches, loss: 0.5163Epoch 7/15: [====================          ] 50/75 batches, loss: 0.5168Epoch 7/15: [====================          ] 51/75 batches, loss: 0.5171Epoch 7/15: [====================          ] 52/75 batches, loss: 0.5165Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.5159Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.5161Epoch 7/15: [======================        ] 55/75 batches, loss: 0.5177Epoch 7/15: [======================        ] 56/75 batches, loss: 0.5179Epoch 7/15: [======================        ] 57/75 batches, loss: 0.5178Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.5188Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.5182Epoch 7/15: [========================      ] 60/75 batches, loss: 0.5199Epoch 7/15: [========================      ] 61/75 batches, loss: 0.5189Epoch 7/15: [========================      ] 62/75 batches, loss: 0.5180Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.5199Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.5210Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.5219Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.5205Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.5189Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.5183Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.5178Epoch 7/15: [============================  ] 70/75 batches, loss: 0.5170Epoch 7/15: [============================  ] 71/75 batches, loss: 0.5168Epoch 7/15: [============================  ] 72/75 batches, loss: 0.5163Epoch 7/15: [============================= ] 73/75 batches, loss: 0.5158Epoch 7/15: [============================= ] 74/75 batches, loss: 0.5161Epoch 7/15: [==============================] 75/75 batches, loss: 0.5148
[2025-05-03 16:23:55,076][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5148
[2025-05-03 16:23:55,422][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5185, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.9583333333333334, 'precision': 0.9583333333333334, 'recall': 0.9583333333333334}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.5585Epoch 8/15: [                              ] 2/75 batches, loss: 0.5311Epoch 8/15: [=                             ] 3/75 batches, loss: 0.5460Epoch 8/15: [=                             ] 4/75 batches, loss: 0.5360Epoch 8/15: [==                            ] 5/75 batches, loss: 0.5411Epoch 8/15: [==                            ] 6/75 batches, loss: 0.5394Epoch 8/15: [==                            ] 7/75 batches, loss: 0.5309Epoch 8/15: [===                           ] 8/75 batches, loss: 0.5279Epoch 8/15: [===                           ] 9/75 batches, loss: 0.5152Epoch 8/15: [====                          ] 10/75 batches, loss: 0.5164Epoch 8/15: [====                          ] 11/75 batches, loss: 0.5178Epoch 8/15: [====                          ] 12/75 batches, loss: 0.5127Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.5089Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.5053Epoch 8/15: [======                        ] 15/75 batches, loss: 0.5054Epoch 8/15: [======                        ] 16/75 batches, loss: 0.5085Epoch 8/15: [======                        ] 17/75 batches, loss: 0.5085Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.5124Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.5083Epoch 8/15: [========                      ] 20/75 batches, loss: 0.5070Epoch 8/15: [========                      ] 21/75 batches, loss: 0.5081Epoch 8/15: [========                      ] 22/75 batches, loss: 0.5058Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.5049Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.5048Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.5054Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.5047Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.5067Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.5035Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.5027Epoch 8/15: [============                  ] 30/75 batches, loss: 0.5038Epoch 8/15: [============                  ] 31/75 batches, loss: 0.5062Epoch 8/15: [============                  ] 32/75 batches, loss: 0.5049Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.5070Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.5088Epoch 8/15: [==============                ] 35/75 batches, loss: 0.5100Epoch 8/15: [==============                ] 36/75 batches, loss: 0.5099Epoch 8/15: [==============                ] 37/75 batches, loss: 0.5072Epoch 8/15: [===============               ] 38/75 batches, loss: 0.5078Epoch 8/15: [===============               ] 39/75 batches, loss: 0.5059Epoch 8/15: [================              ] 40/75 batches, loss: 0.5059Epoch 8/15: [================              ] 41/75 batches, loss: 0.5058Epoch 8/15: [================              ] 42/75 batches, loss: 0.5075Epoch 8/15: [=================             ] 43/75 batches, loss: 0.5088Epoch 8/15: [=================             ] 44/75 batches, loss: 0.5087Epoch 8/15: [==================            ] 45/75 batches, loss: 0.5078Epoch 8/15: [==================            ] 46/75 batches, loss: 0.5068Epoch 8/15: [==================            ] 47/75 batches, loss: 0.5080Epoch 8/15: [===================           ] 48/75 batches, loss: 0.5098Epoch 8/15: [===================           ] 49/75 batches, loss: 0.5107Epoch 8/15: [====================          ] 50/75 batches, loss: 0.5101Epoch 8/15: [====================          ] 51/75 batches, loss: 0.5086Epoch 8/15: [====================          ] 52/75 batches, loss: 0.5099Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.5106Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.5114Epoch 8/15: [======================        ] 55/75 batches, loss: 0.5124Epoch 8/15: [======================        ] 56/75 batches, loss: 0.5120Epoch 8/15: [======================        ] 57/75 batches, loss: 0.5137Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.5128Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.5133Epoch 8/15: [========================      ] 60/75 batches, loss: 0.5127Epoch 8/15: [========================      ] 61/75 batches, loss: 0.5122Epoch 8/15: [========================      ] 62/75 batches, loss: 0.5128Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.5128Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.5123Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.5131Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.5115Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.5114Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.5116Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.5122Epoch 8/15: [============================  ] 70/75 batches, loss: 0.5119Epoch 8/15: [============================  ] 71/75 batches, loss: 0.5125Epoch 8/15: [============================  ] 72/75 batches, loss: 0.5127Epoch 8/15: [============================= ] 73/75 batches, loss: 0.5123Epoch 8/15: [============================= ] 74/75 batches, loss: 0.5124Epoch 8/15: [==============================] 75/75 batches, loss: 0.5127
[2025-05-03 16:23:58,133][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5127
[2025-05-03 16:23:58,384][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5171, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.9583333333333334, 'precision': 0.9583333333333334, 'recall': 0.9583333333333334}
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.5033Epoch 9/15: [                              ] 2/75 batches, loss: 0.5323Epoch 9/15: [=                             ] 3/75 batches, loss: 0.5424Epoch 9/15: [=                             ] 4/75 batches, loss: 0.5447Epoch 9/15: [==                            ] 5/75 batches, loss: 0.5249Epoch 9/15: [==                            ] 6/75 batches, loss: 0.5292Epoch 9/15: [==                            ] 7/75 batches, loss: 0.5304Epoch 9/15: [===                           ] 8/75 batches, loss: 0.5197Epoch 9/15: [===                           ] 9/75 batches, loss: 0.5308Epoch 9/15: [====                          ] 10/75 batches, loss: 0.5320Epoch 9/15: [====                          ] 11/75 batches, loss: 0.5186Epoch 9/15: [====                          ] 12/75 batches, loss: 0.5254Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.5219Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.5189Epoch 9/15: [======                        ] 15/75 batches, loss: 0.5179Epoch 9/15: [======                        ] 16/75 batches, loss: 0.5127Epoch 9/15: [======                        ] 17/75 batches, loss: 0.5123Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.5141Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.5136Epoch 9/15: [========                      ] 20/75 batches, loss: 0.5159Epoch 9/15: [========                      ] 21/75 batches, loss: 0.5166Epoch 9/15: [========                      ] 22/75 batches, loss: 0.5193Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.5214Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.5187Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.5200Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.5194Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.5214Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.5191Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.5232Epoch 9/15: [============                  ] 30/75 batches, loss: 0.5242Epoch 9/15: [============                  ] 31/75 batches, loss: 0.5235Epoch 9/15: [============                  ] 32/75 batches, loss: 0.5244Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.5252Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.5246Epoch 9/15: [==============                ] 35/75 batches, loss: 0.5240Epoch 9/15: [==============                ] 36/75 batches, loss: 0.5232Epoch 9/15: [==============                ] 37/75 batches, loss: 0.5208Epoch 9/15: [===============               ] 38/75 batches, loss: 0.5210Epoch 9/15: [===============               ] 39/75 batches, loss: 0.5213Epoch 9/15: [================              ] 40/75 batches, loss: 0.5197Epoch 9/15: [================              ] 41/75 batches, loss: 0.5199Epoch 9/15: [================              ] 42/75 batches, loss: 0.5174Epoch 9/15: [=================             ] 43/75 batches, loss: 0.5177Epoch 9/15: [=================             ] 44/75 batches, loss: 0.5164Epoch 9/15: [==================            ] 45/75 batches, loss: 0.5157Epoch 9/15: [==================            ] 46/75 batches, loss: 0.5155Epoch 9/15: [==================            ] 47/75 batches, loss: 0.5163Epoch 9/15: [===================           ] 48/75 batches, loss: 0.5161Epoch 9/15: [===================           ] 49/75 batches, loss: 0.5172Epoch 9/15: [====================          ] 50/75 batches, loss: 0.5165Epoch 9/15: [====================          ] 51/75 batches, loss: 0.5167Epoch 9/15: [====================          ] 52/75 batches, loss: 0.5175Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.5172Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.5176Epoch 9/15: [======================        ] 55/75 batches, loss: 0.5178Epoch 9/15: [======================        ] 56/75 batches, loss: 0.5181Epoch 9/15: [======================        ] 57/75 batches, loss: 0.5171Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.5161Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.5163Epoch 9/15: [========================      ] 60/75 batches, loss: 0.5161Epoch 9/15: [========================      ] 61/75 batches, loss: 0.5172Epoch 9/15: [========================      ] 62/75 batches, loss: 0.5170Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.5164Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.5158Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.5160Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.5144Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.5139Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.5134Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.5141Epoch 9/15: [============================  ] 70/75 batches, loss: 0.5131Epoch 9/15: [============================  ] 71/75 batches, loss: 0.5123Epoch 9/15: [============================  ] 72/75 batches, loss: 0.5120Epoch 9/15: [============================= ] 73/75 batches, loss: 0.5115Epoch 9/15: [============================= ] 74/75 batches, loss: 0.5111Epoch 9/15: [==============================] 75/75 batches, loss: 0.5107
[2025-05-03 16:24:01,070][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5107
[2025-05-03 16:24:01,404][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.5361, Metrics: {'accuracy': 0.9347826086956522, 'f1': 0.9411764705882353, 'precision': 0.8888888888888888, 'recall': 1.0}
[2025-05-03 16:24:01,405][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.4624Epoch 10/15: [                              ] 2/75 batches, loss: 0.4474Epoch 10/15: [=                             ] 3/75 batches, loss: 0.4726Epoch 10/15: [=                             ] 4/75 batches, loss: 0.4803Epoch 10/15: [==                            ] 5/75 batches, loss: 0.4803Epoch 10/15: [==                            ] 6/75 batches, loss: 0.4975Epoch 10/15: [==                            ] 7/75 batches, loss: 0.5052Epoch 10/15: [===                           ] 8/75 batches, loss: 0.5109Epoch 10/15: [===                           ] 9/75 batches, loss: 0.5050Epoch 10/15: [====                          ] 10/75 batches, loss: 0.5049Epoch 10/15: [====                          ] 11/75 batches, loss: 0.5093Epoch 10/15: [====                          ] 12/75 batches, loss: 0.5154Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.5157Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.5142Epoch 10/15: [======                        ] 15/75 batches, loss: 0.5120Epoch 10/15: [======                        ] 16/75 batches, loss: 0.5115Epoch 10/15: [======                        ] 17/75 batches, loss: 0.5122Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.5092Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.5091Epoch 10/15: [========                      ] 20/75 batches, loss: 0.5112Epoch 10/15: [========                      ] 21/75 batches, loss: 0.5144Epoch 10/15: [========                      ] 22/75 batches, loss: 0.5142Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.5152Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.5163Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.5202Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.5205Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.5199Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.5160Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.5173Epoch 10/15: [============                  ] 30/75 batches, loss: 0.5212Epoch 10/15: [============                  ] 31/75 batches, loss: 0.5186Epoch 10/15: [============                  ] 32/75 batches, loss: 0.5181Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.5169Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.5187Epoch 10/15: [==============                ] 35/75 batches, loss: 0.5195Epoch 10/15: [==============                ] 36/75 batches, loss: 0.5199Epoch 10/15: [==============                ] 37/75 batches, loss: 0.5185Epoch 10/15: [===============               ] 38/75 batches, loss: 0.5182Epoch 10/15: [===============               ] 39/75 batches, loss: 0.5179Epoch 10/15: [================              ] 40/75 batches, loss: 0.5181Epoch 10/15: [================              ] 41/75 batches, loss: 0.5184Epoch 10/15: [================              ] 42/75 batches, loss: 0.5164Epoch 10/15: [=================             ] 43/75 batches, loss: 0.5152Epoch 10/15: [=================             ] 44/75 batches, loss: 0.5155Epoch 10/15: [==================            ] 45/75 batches, loss: 0.5142Epoch 10/15: [==================            ] 46/75 batches, loss: 0.5150Epoch 10/15: [==================            ] 47/75 batches, loss: 0.5148Epoch 10/15: [===================           ] 48/75 batches, loss: 0.5146Epoch 10/15: [===================           ] 49/75 batches, loss: 0.5145Epoch 10/15: [====================          ] 50/75 batches, loss: 0.5148Epoch 10/15: [====================          ] 51/75 batches, loss: 0.5147Epoch 10/15: [====================          ] 52/75 batches, loss: 0.5147Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.5154Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.5138Epoch 10/15: [======================        ] 55/75 batches, loss: 0.5136Epoch 10/15: [======================        ] 56/75 batches, loss: 0.5134Epoch 10/15: [======================        ] 57/75 batches, loss: 0.5141Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.5124Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.5118Epoch 10/15: [========================      ] 60/75 batches, loss: 0.5109Epoch 10/15: [========================      ] 61/75 batches, loss: 0.5110Epoch 10/15: [========================      ] 62/75 batches, loss: 0.5128Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.5126Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.5124Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.5123Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.5115Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.5119Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.5114Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.5103Epoch 10/15: [============================  ] 70/75 batches, loss: 0.5112Epoch 10/15: [============================  ] 71/75 batches, loss: 0.5110Epoch 10/15: [============================  ] 72/75 batches, loss: 0.5122Epoch 10/15: [============================= ] 73/75 batches, loss: 0.5111Epoch 10/15: [============================= ] 74/75 batches, loss: 0.5114Epoch 10/15: [==============================] 75/75 batches, loss: 0.5124
[2025-05-03 16:24:03,784][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5124
[2025-05-03 16:24:04,112][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.5199, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9795918367346939, 'precision': 0.96, 'recall': 1.0}
[2025-05-03 16:24:04,113][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.4594Epoch 11/15: [                              ] 2/75 batches, loss: 0.4893Epoch 11/15: [=                             ] 3/75 batches, loss: 0.5386Epoch 11/15: [=                             ] 4/75 batches, loss: 0.5358Epoch 11/15: [==                            ] 5/75 batches, loss: 0.5342Epoch 11/15: [==                            ] 6/75 batches, loss: 0.5214Epoch 11/15: [==                            ] 7/75 batches, loss: 0.5189Epoch 11/15: [===                           ] 8/75 batches, loss: 0.5115Epoch 11/15: [===                           ] 9/75 batches, loss: 0.5135Epoch 11/15: [====                          ] 10/75 batches, loss: 0.5104Epoch 11/15: [====                          ] 11/75 batches, loss: 0.5119Epoch 11/15: [====                          ] 12/75 batches, loss: 0.5132Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.5157Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.5213Epoch 11/15: [======                        ] 15/75 batches, loss: 0.5232Epoch 11/15: [======                        ] 16/75 batches, loss: 0.5176Epoch 11/15: [======                        ] 17/75 batches, loss: 0.5162Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.5168Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.5123Epoch 11/15: [========                      ] 20/75 batches, loss: 0.5120Epoch 11/15: [========                      ] 21/75 batches, loss: 0.5116Epoch 11/15: [========                      ] 22/75 batches, loss: 0.5126Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.5112Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.5052Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.5045Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.5087Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.5123Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.5155Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.5163Epoch 11/15: [============                  ] 30/75 batches, loss: 0.5166Epoch 11/15: [============                  ] 31/75 batches, loss: 0.5187Epoch 11/15: [============                  ] 32/75 batches, loss: 0.5183Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.5164Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.5174Epoch 11/15: [==============                ] 35/75 batches, loss: 0.5165Epoch 11/15: [==============                ] 36/75 batches, loss: 0.5159Epoch 11/15: [==============                ] 37/75 batches, loss: 0.5137Epoch 11/15: [===============               ] 38/75 batches, loss: 0.5136Epoch 11/15: [===============               ] 39/75 batches, loss: 0.5127Epoch 11/15: [================              ] 40/75 batches, loss: 0.5137Epoch 11/15: [================              ] 41/75 batches, loss: 0.5123Epoch 11/15: [================              ] 42/75 batches, loss: 0.5138Epoch 11/15: [=================             ] 43/75 batches, loss: 0.5136Epoch 11/15: [=================             ] 44/75 batches, loss: 0.5113Epoch 11/15: [==================            ] 45/75 batches, loss: 0.5111Epoch 11/15: [==================            ] 46/75 batches, loss: 0.5105Epoch 11/15: [==================            ] 47/75 batches, loss: 0.5103Epoch 11/15: [===================           ] 48/75 batches, loss: 0.5092Epoch 11/15: [===================           ] 49/75 batches, loss: 0.5091Epoch 11/15: [====================          ] 50/75 batches, loss: 0.5100Epoch 11/15: [====================          ] 51/75 batches, loss: 0.5089Epoch 11/15: [====================          ] 52/75 batches, loss: 0.5100Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.5113Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.5107Epoch 11/15: [======================        ] 55/75 batches, loss: 0.5097Epoch 11/15: [======================        ] 56/75 batches, loss: 0.5089Epoch 11/15: [======================        ] 57/75 batches, loss: 0.5101Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.5088Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.5087Epoch 11/15: [========================      ] 60/75 batches, loss: 0.5087Epoch 11/15: [========================      ] 61/75 batches, loss: 0.5105Epoch 11/15: [========================      ] 62/75 batches, loss: 0.5113Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.5116Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.5122Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.5117Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.5116Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.5108Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.5112Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.5108Epoch 11/15: [============================  ] 70/75 batches, loss: 0.5104Epoch 11/15: [============================  ] 71/75 batches, loss: 0.5103Epoch 11/15: [============================  ] 72/75 batches, loss: 0.5103Epoch 11/15: [============================= ] 73/75 batches, loss: 0.5108Epoch 11/15: [============================= ] 74/75 batches, loss: 0.5120Epoch 11/15: [==============================] 75/75 batches, loss: 0.5123
[2025-05-03 16:24:06,512][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5123
[2025-05-03 16:24:06,773][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.5336, Metrics: {'accuracy': 0.9130434782608695, 'f1': 0.9230769230769231, 'precision': 0.8571428571428571, 'recall': 1.0}
[2025-05-03 16:24:06,774][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-03 16:24:06,774][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 11
[2025-05-03 16:24:06,774][src.training.lm_trainer][INFO] - Training completed in 33.38 seconds
[2025-05-03 16:24:06,774][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-03 16:24:10,232][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9924433249370277, 'f1': 0.992430613961312, 'precision': 0.9949409780775716, 'recall': 0.9899328859060402}
[2025-05-03 16:24:10,233][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9565217391304348, 'f1': 0.9583333333333334, 'precision': 0.9583333333333334, 'recall': 0.9583333333333334}
[2025-05-03 16:24:10,233][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.6847826086956522, 'f1': 0.7786259541984732, 'precision': 0.6710526315789473, 'recall': 0.9272727272727272}
[2025-05-03 16:24:11,871][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/ja/ja/model.pt
[2025-05-03 16:24:11,873][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▅▇▇▇▇██
wandb:           best_val_f1 ▁▆▇▇▇▇██
wandb:         best_val_loss █▄▂▂▂▂▁▁
wandb:    best_val_precision ▇▁█▅█▅██
wandb:       best_val_recall ▁█▇█▇███
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▂▃▃▃▃▃▂▃
wandb:            train_loss █▃▂▂▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▅▆▆▆▆▇▇▆█▆
wandb:                val_f1 ▁▆▇▇▇▇▇▇▇█▆
wandb:              val_loss █▄▂▂▂▂▁▁▃▁▂
wandb:         val_precision ▇▁█▅█▅██▃█▁
wandb:            val_recall ▁▇▆▇▆▇▇▇███
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.95652
wandb:           best_val_f1 0.95833
wandb:         best_val_loss 0.51711
wandb:    best_val_precision 0.95833
wandb:       best_val_recall 0.95833
wandb:      early_stop_epoch 11
wandb:                 epoch 11
wandb:   final_test_accuracy 0.68478
wandb:         final_test_f1 0.77863
wandb:  final_test_precision 0.67105
wandb:     final_test_recall 0.92727
wandb:  final_train_accuracy 0.99244
wandb:        final_train_f1 0.99243
wandb: final_train_precision 0.99494
wandb:    final_train_recall 0.98993
wandb:    final_val_accuracy 0.95652
wandb:          final_val_f1 0.95833
wandb:   final_val_precision 0.95833
wandb:      final_val_recall 0.95833
wandb:         learning_rate 0.0001
wandb:            train_loss 0.51227
wandb:            train_time 33.38328
wandb:          val_accuracy 0.91304
wandb:                val_f1 0.92308
wandb:              val_loss 0.53362
wandb:         val_precision 0.85714
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_162307-omfqs71t
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_162307-omfqs71t/logs
Experiment probe_layer2_question_type_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_complexity_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_ja"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/layer2/ja"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-03 16:24:59,145][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/ja
experiment_name: probe_layer2_complexity_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-03 16:24:59,145][__main__][INFO] - Normalized task: complexity
[2025-05-03 16:24:59,145][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-03 16:24:59,145][__main__][INFO] - Determined Task Type: regression
[2025-05-03 16:24:59,149][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-05-03 16:24:59,149][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-03 16:25:05,965][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-03 16:25:08,594][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-03 16:25:08,594][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:25:09,029][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:25:09,208][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:25:09,497][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-03 16:25:09,506][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:25:09,506][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-03 16:25:09,518][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:25:09,774][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:25:09,905][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:25:09,930][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-03 16:25:09,931][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:25:09,932][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-03 16:25:09,933][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:25:10,154][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:25:10,337][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:25:10,423][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-03 16:25:10,424][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:25:10,424][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-03 16:25:10,425][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-03 16:25:10,425][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:25:10,426][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:25:10,426][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:25:10,426][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:25:10,426][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:25:10,426][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-05-03 16:25:10,426][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-03 16:25:10,426][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-05-03 16:25:10,427][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:25:10,427][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:25:10,427][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:25:10,427][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:25:10,427][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:25:10,427][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-05-03 16:25:10,427][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-03 16:25:10,427][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-05-03 16:25:10,427][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:25:10,427][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:25:10,427][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:25:10,428][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:25:10,428][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:25:10,428][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-05-03 16:25:10,428][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-03 16:25:10,428][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-05-03 16:25:10,428][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-03 16:25:10,428][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-03 16:25:10,428][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-03 16:25:10,429][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-03 16:25:10,429][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-03 16:25:21,506][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-03 16:25:21,507][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-03 16:25:21,507][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-03 16:25:21,507][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-03 16:25:21,510][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-03 16:25:21,510][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-03 16:25:21,510][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-03 16:25:21,510][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-03 16:25:21,510][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-03 16:25:21,511][__main__][INFO] - Total parameters: 394,255,105
[2025-05-03 16:25:21,511][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4297Epoch 1/15: [                              ] 2/75 batches, loss: 0.4469Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4509Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4666Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4441Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4199Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4280Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4477Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4356Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4251Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4218Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4388Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4187Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4370Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4292Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4366Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4351Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4485Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4363Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4357Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4276Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4266Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4147Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4025Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3954Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3884Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3847Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3768Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3734Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3662Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3618Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3566Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3532Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3534Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3516Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3535Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3491Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3443Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3451Epoch 1/15: [================              ] 40/75 batches, loss: 0.3393Epoch 1/15: [================              ] 41/75 batches, loss: 0.3358Epoch 1/15: [================              ] 42/75 batches, loss: 0.3357Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3352Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3355Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3341Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3296Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3281Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3254Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3224Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3207Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3198Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3222Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3180Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3206Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3206Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3158Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3131Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3123Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3108Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3086Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3066Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3053Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3058Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3052Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3051Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3024Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3011Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2979Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2962Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2941Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2929Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2951Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2935Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2933Epoch 1/15: [==============================] 75/75 batches, loss: 0.2898
[2025-05-03 16:25:29,923][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2898
[2025-05-03 16:25:30,191][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0626, Metrics: {'mse': 0.06254742294549942, 'rmse': 0.2500948279063352, 'r2': -0.019393205642700195}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1960Epoch 2/15: [                              ] 2/75 batches, loss: 0.1780Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1800Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1881Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1979Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1855Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1852Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1801Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1897Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1965Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1882Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1877Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1869Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1843Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1891Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1878Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1865Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1838Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1786Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1739Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1714Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1696Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1694Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1667Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1629Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1668Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1652Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1637Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1646Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1644Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1629Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1670Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1647Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1630Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1646Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1623Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1618Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1613Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1606Epoch 2/15: [================              ] 40/75 batches, loss: 0.1604Epoch 2/15: [================              ] 41/75 batches, loss: 0.1591Epoch 2/15: [================              ] 42/75 batches, loss: 0.1608Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1605Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1623Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1611Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1590Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1579Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1561Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1561Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1550Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1539Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1528Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1518Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1525Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1522Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1528Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1513Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1500Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1496Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1485Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1484Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1482Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1479Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1466Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1456Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1447Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1452Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1454Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1446Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1445Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1442Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1435Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1422Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1411Epoch 2/15: [==============================] 75/75 batches, loss: 0.1411
[2025-05-03 16:25:32,945][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1411
[2025-05-03 16:25:33,299][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0599, Metrics: {'mse': 0.059586431831121445, 'rmse': 0.2441033220403226, 'r2': 0.028864800930023193}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1336Epoch 3/15: [                              ] 2/75 batches, loss: 0.1397Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1221Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1060Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1137Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1155Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1112Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1212Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1204Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1182Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1190Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1119Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1111Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1102Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1063Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1050Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1021Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1037Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1033Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1077Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1120Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1103Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1135Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1119Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1102Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1097Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1122Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1119Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1101Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1082Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1092Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1146Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1137Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1137Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1129Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1125Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1157Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1147Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1139Epoch 3/15: [================              ] 40/75 batches, loss: 0.1131Epoch 3/15: [================              ] 41/75 batches, loss: 0.1142Epoch 3/15: [================              ] 42/75 batches, loss: 0.1161Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1158Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1152Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1149Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1149Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1151Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1139Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1140Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1132Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1137Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1142Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1135Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1121Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1113Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1110Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1107Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1112Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1105Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1096Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1098Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1090Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1085Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1078Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1083Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1079Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1074Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1074Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1075Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1075Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1069Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1068Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1079Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1075Epoch 3/15: [==============================] 75/75 batches, loss: 0.1077
[2025-05-03 16:25:36,146][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1077
[2025-05-03 16:25:36,577][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0643, Metrics: {'mse': 0.06392712891101837, 'rmse': 0.252838147657782, 'r2': -0.04187953472137451}
[2025-05-03 16:25:36,577][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1379Epoch 4/15: [                              ] 2/75 batches, loss: 0.1158Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1496Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1373Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1186Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1115Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1155Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1077Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1054Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1039Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1093Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1078Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1049Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1050Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1048Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1037Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1023Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1001Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1024Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1030Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1013Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0998Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0998Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1005Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0992Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0970Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0997Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0994Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0983Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0977Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0979Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0976Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0965Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0969Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0963Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0958Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0951Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0952Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0941Epoch 4/15: [================              ] 40/75 batches, loss: 0.0942Epoch 4/15: [================              ] 41/75 batches, loss: 0.0935Epoch 4/15: [================              ] 42/75 batches, loss: 0.0936Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0943Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0935Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0933Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0939Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0942Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0941Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0936Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0927Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0926Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0927Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0925Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0920Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0917Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0916Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0918Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0924Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0924Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0922Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0928Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0922Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0925Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0925Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0924Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0917Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0920Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0917Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0921Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0920Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0916Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0916Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0919Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0925Epoch 4/15: [==============================] 75/75 batches, loss: 0.0920
[2025-05-03 16:25:38,988][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0920
[2025-05-03 16:25:39,366][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0532, Metrics: {'mse': 0.0528947152197361, 'rmse': 0.22998851106030513, 'r2': 0.1379258632659912}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0550Epoch 5/15: [                              ] 2/75 batches, loss: 0.0656Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0627Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0705Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0691Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0733Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0769Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0775Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0787Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0765Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0807Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0835Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0855Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0850Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0815Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0820Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0820Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0798Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0793Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0785Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0770Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0813Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0807Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0813Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0816Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0809Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0820Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0805Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0812Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0820Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0832Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0830Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0816Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0811Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0818Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0810Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0816Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0814Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0815Epoch 5/15: [================              ] 40/75 batches, loss: 0.0811Epoch 5/15: [================              ] 41/75 batches, loss: 0.0807Epoch 5/15: [================              ] 42/75 batches, loss: 0.0823Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0818Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0814Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0809Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0808Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0803Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0795Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0800Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0802Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0798Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0795Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0792Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0789Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0784Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0782Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0776Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0768Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0768Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0765Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0775Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0781Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0779Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0775Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0782Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0785Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0779Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0773Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0775Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0777Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0774Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0771Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0763Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0760Epoch 5/15: [==============================] 75/75 batches, loss: 0.0768
[2025-05-03 16:25:42,170][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0768
[2025-05-03 16:25:42,511][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0539, Metrics: {'mse': 0.05354180932044983, 'rmse': 0.23139103120140553, 'r2': 0.12737959623336792}
[2025-05-03 16:25:42,512][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0658Epoch 6/15: [                              ] 2/75 batches, loss: 0.0677Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0698Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0633Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0739Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0735Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0734Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0764Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0722Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0681Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0647Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0688Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0673Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0686Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0673Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0676Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0674Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0669Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0657Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0659Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0661Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0654Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0648Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0651Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0654Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0662Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0654Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0665Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0663Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0667Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0662Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0665Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0661Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0659Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0665Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0673Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0678Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0680Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0683Epoch 6/15: [================              ] 40/75 batches, loss: 0.0687Epoch 6/15: [================              ] 41/75 batches, loss: 0.0692Epoch 6/15: [================              ] 42/75 batches, loss: 0.0687Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0679Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0676Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0677Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0678Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0673Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0674Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0672Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0675Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0674Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0673Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0680Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0673Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0672Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0668Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0669Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0666Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0671Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0668Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0665Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0665Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0663Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0665Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0661Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0658Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0660Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0661Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0661Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0659Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0667Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0664Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0659Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0655Epoch 6/15: [==============================] 75/75 batches, loss: 0.0664
[2025-05-03 16:25:44,821][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0664
[2025-05-03 16:25:45,161][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0428, Metrics: {'mse': 0.042521845549345016, 'rmse': 0.20620825771376136, 'r2': 0.306982159614563}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0504Epoch 7/15: [                              ] 2/75 batches, loss: 0.0509Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0467Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0480Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0499Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0544Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0564Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0637Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0618Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0626Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0621Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0606Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0657Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0642Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0644Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0632Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0627Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0644Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0661Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0668Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0651Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0637Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0630Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0626Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0625Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0620Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0638Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0636Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0645Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0659Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0652Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0642Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0638Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0660Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0659Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0653Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0658Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0650Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0664Epoch 7/15: [================              ] 40/75 batches, loss: 0.0661Epoch 7/15: [================              ] 41/75 batches, loss: 0.0652Epoch 7/15: [================              ] 42/75 batches, loss: 0.0655Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0652Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0653Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0654Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0650Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0654Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0651Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0646Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0642Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0639Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0632Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0626Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0622Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0626Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0625Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0623Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0625Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0620Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0623Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0617Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0615Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0613Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0616Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0619Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0623Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0622Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0621Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0621Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0626Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0622Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0619Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0618Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0617Epoch 7/15: [==============================] 75/75 batches, loss: 0.0616
[2025-05-03 16:25:48,067][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0616
[2025-05-03 16:25:48,417][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0566, Metrics: {'mse': 0.05609867349267006, 'rmse': 0.23685158537081835, 'r2': 0.08570802211761475}
[2025-05-03 16:25:48,418][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0592Epoch 8/15: [                              ] 2/75 batches, loss: 0.0461Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0525Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0559Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0544Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0548Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0524Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0538Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0520Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0517Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0540Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0540Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0553Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0550Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0560Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0543Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0556Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0572Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0597Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0602Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0600Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0598Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0596Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0606Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0605Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0596Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0601Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0588Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0589Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0595Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0591Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0590Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0582Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0575Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0580Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0581Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0578Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0576Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0577Epoch 8/15: [================              ] 40/75 batches, loss: 0.0573Epoch 8/15: [================              ] 41/75 batches, loss: 0.0577Epoch 8/15: [================              ] 42/75 batches, loss: 0.0580Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0575Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0567Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0567Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0568Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0561Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0561Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0554Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0558Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0556Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0554Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0550Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0546Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0545Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0550Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0550Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0554Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0550Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0550Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0549Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0546Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0546Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0545Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0544Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0548Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0547Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0550Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0550Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0547Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0548Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0548Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0548Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0547Epoch 8/15: [==============================] 75/75 batches, loss: 0.0547
[2025-05-03 16:25:50,840][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0547
[2025-05-03 16:25:51,244][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0565, Metrics: {'mse': 0.05593235045671463, 'rmse': 0.23650021238196517, 'r2': 0.08841872215270996}
[2025-05-03 16:25:51,245][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0119Epoch 9/15: [                              ] 2/75 batches, loss: 0.0296Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0382Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0439Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0467Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0478Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0501Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0508Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0514Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0545Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0560Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0557Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0528Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0542Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0564Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0583Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0581Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0580Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0586Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0579Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0572Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0568Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0563Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0573Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0580Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0586Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0579Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0578Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0577Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0581Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0569Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0565Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0572Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0564Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0563Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0557Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0560Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0563Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0559Epoch 9/15: [================              ] 40/75 batches, loss: 0.0558Epoch 9/15: [================              ] 41/75 batches, loss: 0.0550Epoch 9/15: [================              ] 42/75 batches, loss: 0.0546Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0550Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0554Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0551Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0554Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0550Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0545Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0543Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0544Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0545Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0541Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0540Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0548Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0545Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0545Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0542Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0541Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0544Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0544Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0542Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0540Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0540Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0544Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0544Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0545Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0542Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0541Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0540Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0537Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0536Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0536Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0532Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0530Epoch 9/15: [==============================] 75/75 batches, loss: 0.0531
[2025-05-03 16:25:53,609][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0531
[2025-05-03 16:25:54,068][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0507, Metrics: {'mse': 0.0502069927752018, 'rmse': 0.22406916962224366, 'r2': 0.1817302107810974}
[2025-05-03 16:25:54,068][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0763Epoch 10/15: [                              ] 2/75 batches, loss: 0.0567Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0583Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0577Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0538Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0521Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0503Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0549Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0537Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0508Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0502Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0498Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0495Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0485Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0483Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0484Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0491Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0482Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0473Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0468Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0484Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0490Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0476Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0474Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0478Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0477Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0471Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0466Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0468Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0474Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0470Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0464Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0468Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0470Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0468Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0470Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0468Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0466Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0462Epoch 10/15: [================              ] 40/75 batches, loss: 0.0457Epoch 10/15: [================              ] 41/75 batches, loss: 0.0459Epoch 10/15: [================              ] 42/75 batches, loss: 0.0459Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0464Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0468Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0470Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0474Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0473Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0479Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0476Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0476Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0478Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0477Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0475Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0478Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0478Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0474Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0474Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0480Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0478Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0474Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0473Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0471Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0478Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0473Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0477Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0473Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0471Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0469Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0471Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0471Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0470Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0468Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0468Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0466Epoch 10/15: [==============================] 75/75 batches, loss: 0.0466
[2025-05-03 16:25:56,557][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0466
[2025-05-03 16:25:56,911][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0403, Metrics: {'mse': 0.03991001099348068, 'rmse': 0.1997749008095879, 'r2': 0.34954965114593506}
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0256Epoch 11/15: [                              ] 2/75 batches, loss: 0.0325Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0345Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0368Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0410Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0398Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0434Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0434Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0436Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0418Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0403Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0439Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0450Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0438Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0430Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0430Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0429Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0436Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0447Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0458Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0465Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0468Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0464Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0465Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0464Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0466Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0458Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0457Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0455Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0449Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0449Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0450Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0455Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0453Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0453Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0454Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0452Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0447Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0444Epoch 11/15: [================              ] 40/75 batches, loss: 0.0445Epoch 11/15: [================              ] 41/75 batches, loss: 0.0443Epoch 11/15: [================              ] 42/75 batches, loss: 0.0448Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0451Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0452Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0452Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0449Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0445Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0441Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0443Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0442Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0437Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0431Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0431Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0429Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0432Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0435Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0432Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0434Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0432Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0438Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0441Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0438Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0441Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0440Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0438Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0445Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0443Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0442Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0441Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0441Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0442Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0446Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0447Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0450Epoch 11/15: [==============================] 75/75 batches, loss: 0.0452
[2025-05-03 16:25:59,722][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0452
[2025-05-03 16:26:00,089][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0346, Metrics: {'mse': 0.034355245530605316, 'rmse': 0.1853516806792032, 'r2': 0.44008082151412964}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0226Epoch 12/15: [                              ] 2/75 batches, loss: 0.0339Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0408Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0430Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0428Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0402Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0417Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0399Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0399Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0428Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0437Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0435Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0451Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0438Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0427Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0428Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0428Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0428Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0424Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0418Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0418Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0418Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0423Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0424Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0419Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0421Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0420Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0424Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0422Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0426Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0427Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0424Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0424Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0424Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0423Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0417Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0413Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0413Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0409Epoch 12/15: [================              ] 40/75 batches, loss: 0.0409Epoch 12/15: [================              ] 41/75 batches, loss: 0.0413Epoch 12/15: [================              ] 42/75 batches, loss: 0.0411Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0407Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0405Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0404Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0401Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0407Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0410Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0406Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0404Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0407Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0409Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0413Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0412Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0416Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0413Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0409Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0412Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0411Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0413Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0416Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0415Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0416Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0416Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0414Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0410Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0408Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0405Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0401Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0402Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0401Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0400Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0401Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0402Epoch 12/15: [==============================] 75/75 batches, loss: 0.0402
[2025-05-03 16:26:02,847][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0402
[2025-05-03 16:26:03,254][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0471, Metrics: {'mse': 0.0465625636279583, 'rmse': 0.2157836037050969, 'r2': 0.24112683534622192}
[2025-05-03 16:26:03,255][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0901Epoch 13/15: [                              ] 2/75 batches, loss: 0.0676Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0572Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0503Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0492Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0445Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0449Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0498Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0508Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0506Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0501Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0492Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0481Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0466Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0469Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0452Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0462Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0451Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0453Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0454Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0464Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0461Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0459Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0455Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0458Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0460Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0455Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0449Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0449Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0447Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0451Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0448Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0442Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0435Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0438Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0443Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0443Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0444Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0452Epoch 13/15: [================              ] 40/75 batches, loss: 0.0451Epoch 13/15: [================              ] 41/75 batches, loss: 0.0450Epoch 13/15: [================              ] 42/75 batches, loss: 0.0450Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0451Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0452Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0451Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0448Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0445Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0441Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0442Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0441Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0437Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0434Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0431Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0430Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0432Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0433Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0432Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0433Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0435Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0431Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0433Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0433Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0435Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0435Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0432Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0431Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0427Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0429Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0428Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0427Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0427Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0426Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0429Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0429Epoch 13/15: [==============================] 75/75 batches, loss: 0.0433
[2025-05-03 16:26:05,715][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0433
[2025-05-03 16:26:06,139][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0351, Metrics: {'mse': 0.034623030573129654, 'rmse': 0.1860726486432911, 'r2': 0.4357163906097412}
[2025-05-03 16:26:06,139][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0370Epoch 14/15: [                              ] 2/75 batches, loss: 0.0454Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0460Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0447Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0410Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0374Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0363Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0374Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0403Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0431Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0434Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0412Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0401Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0403Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0403Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0397Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0395Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0392Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0390Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0382Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0380Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0382Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0383Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0384Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0378Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0377Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0372Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0378Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0371Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0366Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0361Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0359Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0364Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0363Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0379Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0377Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0378Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0386Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0386Epoch 14/15: [================              ] 40/75 batches, loss: 0.0387Epoch 14/15: [================              ] 41/75 batches, loss: 0.0386Epoch 14/15: [================              ] 42/75 batches, loss: 0.0382Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0382Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0379Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0390Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0386Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0385Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0385Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0383Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0391Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0398Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0397Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0399Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0395Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0394Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0391Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0390Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0390Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0389Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0388Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0387Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0385Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0387Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0387Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0384Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0388Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0387Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0386Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0387Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0386Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0384Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0384Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0380Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0379Epoch 14/15: [==============================] 75/75 batches, loss: 0.0384
[2025-05-03 16:26:08,545][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0384
[2025-05-03 16:26:09,001][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0360, Metrics: {'mse': 0.03553222119808197, 'rmse': 0.18849992360232395, 'r2': 0.42089855670928955}
[2025-05-03 16:26:09,001][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0382Epoch 15/15: [                              ] 2/75 batches, loss: 0.0346Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0311Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0302Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0303Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0281Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0278Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0281Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0273Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0277Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0273Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0267Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0304Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0293Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0282Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0292Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0293Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0298Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0300Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0298Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0298Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0295Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0300Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0292Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0289Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0295Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0291Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0293Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0288Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0293Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0294Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0297Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0307Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0311Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0307Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0308Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0309Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0308Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0309Epoch 15/15: [================              ] 40/75 batches, loss: 0.0308Epoch 15/15: [================              ] 41/75 batches, loss: 0.0304Epoch 15/15: [================              ] 42/75 batches, loss: 0.0308Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0312Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0312Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0312Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0315Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0317Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0316Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0318Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0320Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0319Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0320Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0319Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0323Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0323Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0321Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0320Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0321Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0321Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0320Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0321Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0322Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0322Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0322Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0324Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0325Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0324Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0325Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0325Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0325Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0327Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0328Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0327Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0324Epoch 15/15: [==============================] 75/75 batches, loss: 0.0323
[2025-05-03 16:26:11,491][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0323
[2025-05-03 16:26:11,764][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0363, Metrics: {'mse': 0.03570163995027542, 'rmse': 0.18894877599570584, 'r2': 0.4181373119354248}
[2025-05-03 16:26:11,765][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-03 16:26:11,765][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 15
[2025-05-03 16:26:11,765][src.training.lm_trainer][INFO] - Training completed in 44.99 seconds
[2025-05-03 16:26:11,765][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-03 16:26:15,197][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.0193499606102705, 'rmse': 0.13910413584890458, 'r2': 0.5170398950576782}
[2025-05-03 16:26:15,197][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.034355245530605316, 'rmse': 0.1853516806792032, 'r2': 0.44008082151412964}
[2025-05-03 16:26:15,197][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.027955954894423485, 'rmse': 0.16720034358344926, 'r2': 0.4629901647567749}
[2025-05-03 16:26:16,879][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/ja/ja/model.pt
[2025-05-03 16:26:16,880][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▆▃▂▁
wandb:     best_val_mse █▇▆▃▂▁
wandb:      best_val_r2 ▁▂▃▆▇█
wandb:    best_val_rmse █▇▆▃▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▁▃▃▅▃▃▄▅▆▄▆▆
wandb:       train_loss █▄▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇█▅▆▃▆▆▅▂▁▄▁▁▁
wandb:          val_mse █▇█▅▆▃▆▆▅▂▁▄▁▁▁
wandb:           val_r2 ▁▂▁▄▃▆▃▃▄▇█▅███
wandb:         val_rmse █▇█▆▆▃▆▆▅▂▁▄▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03464
wandb:     best_val_mse 0.03436
wandb:      best_val_r2 0.44008
wandb:    best_val_rmse 0.18535
wandb: early_stop_epoch 15
wandb:            epoch 15
wandb:   final_test_mse 0.02796
wandb:    final_test_r2 0.46299
wandb:  final_test_rmse 0.1672
wandb:  final_train_mse 0.01935
wandb:   final_train_r2 0.51704
wandb: final_train_rmse 0.1391
wandb:    final_val_mse 0.03436
wandb:     final_val_r2 0.44008
wandb:   final_val_rmse 0.18535
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03227
wandb:       train_time 44.99158
wandb:         val_loss 0.03627
wandb:          val_mse 0.0357
wandb:           val_r2 0.41814
wandb:         val_rmse 0.18895
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_162459-asvsr5qf
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_162459-asvsr5qf/logs
Experiment probe_layer2_complexity_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_question_type_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_question_type_ko"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/layer2/ko"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-03 16:27:06,323][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/ko
experiment_name: probe_layer2_question_type_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-03 16:27:06,323][__main__][INFO] - Normalized task: question_type
[2025-05-03 16:27:06,324][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-03 16:27:06,324][__main__][INFO] - Determined Task Type: classification
[2025-05-03 16:27:06,328][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ko']
[2025-05-03 16:27:06,328][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-03 16:27:13,623][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-03 16:27:16,094][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-03 16:27:16,094][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:27:16,712][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:27:16,864][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:27:17,266][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-03 16:27:17,272][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:27:17,272][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-03 16:27:17,273][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:27:17,439][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:27:17,617][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:27:17,640][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-03 16:27:17,641][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:27:17,642][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-03 16:27:17,643][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:27:17,839][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:27:18,094][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:27:18,131][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-03 16:27:18,133][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:27:18,133][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-03 16:27:18,134][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-03 16:27:18,134][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:27:18,135][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:27:18,135][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:27:18,135][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:27:18,135][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-05-03 16:27:18,135][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-05-03 16:27:18,135][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-03 16:27:18,135][src.data.datasets][INFO] - Sample label: 0
[2025-05-03 16:27:18,135][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:27:18,135][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:27:18,136][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:27:18,136][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:27:18,136][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-03 16:27:18,136][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-03 16:27:18,136][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-03 16:27:18,136][src.data.datasets][INFO] - Sample label: 0
[2025-05-03 16:27:18,136][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:27:18,136][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:27:18,136][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:27:18,136][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:27:18,137][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-03 16:27:18,137][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-03 16:27:18,137][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-03 16:27:18,137][src.data.datasets][INFO] - Sample label: 1
[2025-05-03 16:27:18,137][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-03 16:27:18,137][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-03 16:27:18,137][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-03 16:27:18,137][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-03 16:27:18,138][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-03 16:27:30,091][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-03 16:27:30,091][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-03 16:27:30,091][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-03 16:27:30,092][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-03 16:27:30,097][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-03 16:27:30,098][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-03 16:27:30,098][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-03 16:27:30,098][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-03 16:27:30,098][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-03 16:27:30,099][__main__][INFO] - Total parameters: 394,568,839
[2025-05-03 16:27:30,099][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-03 16:27:30,100][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.6768Epoch 1/15: [=                             ] 2/47 batches, loss: 0.6743Epoch 1/15: [=                             ] 3/47 batches, loss: 0.6932Epoch 1/15: [==                            ] 4/47 batches, loss: 0.6949Epoch 1/15: [===                           ] 5/47 batches, loss: 0.7029Epoch 1/15: [===                           ] 6/47 batches, loss: 0.7056Epoch 1/15: [====                          ] 7/47 batches, loss: 0.7027Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.7016Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.7000Epoch 1/15: [======                        ] 10/47 batches, loss: 0.6987Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.6984Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.6969Epoch 1/15: [========                      ] 13/47 batches, loss: 0.6962Epoch 1/15: [========                      ] 14/47 batches, loss: 0.6958Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.6953Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.6943Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.6942Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.6941Epoch 1/15: [============                  ] 19/47 batches, loss: 0.6939Epoch 1/15: [============                  ] 20/47 batches, loss: 0.6935Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.6931Epoch 1/15: [==============                ] 22/47 batches, loss: 0.6927Epoch 1/15: [==============                ] 23/47 batches, loss: 0.6924Epoch 1/15: [===============               ] 24/47 batches, loss: 0.6918Epoch 1/15: [===============               ] 25/47 batches, loss: 0.6917Epoch 1/15: [================              ] 26/47 batches, loss: 0.6908Epoch 1/15: [=================             ] 27/47 batches, loss: 0.6896Epoch 1/15: [=================             ] 28/47 batches, loss: 0.6889Epoch 1/15: [==================            ] 29/47 batches, loss: 0.6887Epoch 1/15: [===================           ] 30/47 batches, loss: 0.6873Epoch 1/15: [===================           ] 31/47 batches, loss: 0.6867Epoch 1/15: [====================          ] 32/47 batches, loss: 0.6848Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.6820Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.6830Epoch 1/15: [======================        ] 35/47 batches, loss: 0.6812Epoch 1/15: [======================        ] 36/47 batches, loss: 0.6827Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.6816Epoch 1/15: [========================      ] 38/47 batches, loss: 0.6807Epoch 1/15: [========================      ] 39/47 batches, loss: 0.6798Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.6781Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.6780Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.6785Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.6786Epoch 1/15: [============================  ] 44/47 batches, loss: 0.6782Epoch 1/15: [============================  ] 45/47 batches, loss: 0.6759Epoch 1/15: [============================= ] 46/47 batches, loss: 0.6750Epoch 1/15: [==============================] 47/47 batches, loss: 0.6759
[2025-05-03 16:27:38,654][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6759
[2025-05-03 16:27:39,099][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6680, Metrics: {'accuracy': 0.5555555555555556, 'f1': 0.36, 'precision': 0.6428571428571429, 'recall': 0.25}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.6169Epoch 2/15: [=                             ] 2/47 batches, loss: 0.6378Epoch 2/15: [=                             ] 3/47 batches, loss: 0.6488Epoch 2/15: [==                            ] 4/47 batches, loss: 0.6388Epoch 2/15: [===                           ] 5/47 batches, loss: 0.6350Epoch 2/15: [===                           ] 6/47 batches, loss: 0.6408Epoch 2/15: [====                          ] 7/47 batches, loss: 0.6427Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.6318Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.6386Epoch 2/15: [======                        ] 10/47 batches, loss: 0.6409Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.6418Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.6454Epoch 2/15: [========                      ] 13/47 batches, loss: 0.6457Epoch 2/15: [========                      ] 14/47 batches, loss: 0.6427Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.6381Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.6348Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.6335Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.6281Epoch 2/15: [============                  ] 19/47 batches, loss: 0.6276Epoch 2/15: [============                  ] 20/47 batches, loss: 0.6262Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.6241Epoch 2/15: [==============                ] 22/47 batches, loss: 0.6213Epoch 2/15: [==============                ] 23/47 batches, loss: 0.6193Epoch 2/15: [===============               ] 24/47 batches, loss: 0.6183Epoch 2/15: [===============               ] 25/47 batches, loss: 0.6176Epoch 2/15: [================              ] 26/47 batches, loss: 0.6214Epoch 2/15: [=================             ] 27/47 batches, loss: 0.6230Epoch 2/15: [=================             ] 28/47 batches, loss: 0.6213Epoch 2/15: [==================            ] 29/47 batches, loss: 0.6208Epoch 2/15: [===================           ] 30/47 batches, loss: 0.6222Epoch 2/15: [===================           ] 31/47 batches, loss: 0.6282Epoch 2/15: [====================          ] 32/47 batches, loss: 0.6262Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.6253Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.6244Epoch 2/15: [======================        ] 35/47 batches, loss: 0.6239Epoch 2/15: [======================        ] 36/47 batches, loss: 0.6254Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.6252Epoch 2/15: [========================      ] 38/47 batches, loss: 0.6245Epoch 2/15: [========================      ] 39/47 batches, loss: 0.6236Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.6234Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.6227Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.6220Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.6223Epoch 2/15: [============================  ] 44/47 batches, loss: 0.6220Epoch 2/15: [============================  ] 45/47 batches, loss: 0.6223Epoch 2/15: [============================= ] 46/47 batches, loss: 0.6247Epoch 2/15: [==============================] 47/47 batches, loss: 0.6234
[2025-05-03 16:27:41,291][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6234
[2025-05-03 16:27:41,748][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6583, Metrics: {'accuracy': 0.6527777777777778, 'f1': 0.6376811594202898, 'precision': 0.6666666666666666, 'recall': 0.6111111111111112}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.6122Epoch 3/15: [=                             ] 2/47 batches, loss: 0.6368Epoch 3/15: [=                             ] 3/47 batches, loss: 0.6247Epoch 3/15: [==                            ] 4/47 batches, loss: 0.6039Epoch 3/15: [===                           ] 5/47 batches, loss: 0.6190Epoch 3/15: [===                           ] 6/47 batches, loss: 0.6294Epoch 3/15: [====                          ] 7/47 batches, loss: 0.6307Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.6353Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.6378Epoch 3/15: [======                        ] 10/47 batches, loss: 0.6306Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.6217Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.6229Epoch 3/15: [========                      ] 13/47 batches, loss: 0.6238Epoch 3/15: [========                      ] 14/47 batches, loss: 0.6205Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.6194Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.6173Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.6156Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.6184Epoch 3/15: [============                  ] 19/47 batches, loss: 0.6175Epoch 3/15: [============                  ] 20/47 batches, loss: 0.6139Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.6158Epoch 3/15: [==============                ] 22/47 batches, loss: 0.6126Epoch 3/15: [==============                ] 23/47 batches, loss: 0.6112Epoch 3/15: [===============               ] 24/47 batches, loss: 0.6104Epoch 3/15: [===============               ] 25/47 batches, loss: 0.6076Epoch 3/15: [================              ] 26/47 batches, loss: 0.6074Epoch 3/15: [=================             ] 27/47 batches, loss: 0.6073Epoch 3/15: [=================             ] 28/47 batches, loss: 0.6025Epoch 3/15: [==================            ] 29/47 batches, loss: 0.6011Epoch 3/15: [===================           ] 30/47 batches, loss: 0.6008Epoch 3/15: [===================           ] 31/47 batches, loss: 0.6015Epoch 3/15: [====================          ] 32/47 batches, loss: 0.6012Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.5995Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.5996Epoch 3/15: [======================        ] 35/47 batches, loss: 0.5984Epoch 3/15: [======================        ] 36/47 batches, loss: 0.5982Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.5984Epoch 3/15: [========================      ] 38/47 batches, loss: 0.5968Epoch 3/15: [========================      ] 39/47 batches, loss: 0.5971Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.5993Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.5983Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.6012Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.5999Epoch 3/15: [============================  ] 44/47 batches, loss: 0.6001Epoch 3/15: [============================  ] 45/47 batches, loss: 0.6008Epoch 3/15: [============================= ] 46/47 batches, loss: 0.6009Epoch 3/15: [==============================] 47/47 batches, loss: 0.6029
[2025-05-03 16:27:43,813][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6029
[2025-05-03 16:27:44,132][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6475, Metrics: {'accuracy': 0.6666666666666666, 'f1': 0.6470588235294118, 'precision': 0.6875, 'recall': 0.6111111111111112}
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.5768Epoch 4/15: [=                             ] 2/47 batches, loss: 0.5941Epoch 4/15: [=                             ] 3/47 batches, loss: 0.6152Epoch 4/15: [==                            ] 4/47 batches, loss: 0.6177Epoch 4/15: [===                           ] 5/47 batches, loss: 0.6076Epoch 4/15: [===                           ] 6/47 batches, loss: 0.6037Epoch 4/15: [====                          ] 7/47 batches, loss: 0.6050Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.6103Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.6149Epoch 4/15: [======                        ] 10/47 batches, loss: 0.6043Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.5990Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.5991Epoch 4/15: [========                      ] 13/47 batches, loss: 0.5975Epoch 4/15: [========                      ] 14/47 batches, loss: 0.5989Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.5977Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.5916Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.5832Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.5871Epoch 4/15: [============                  ] 19/47 batches, loss: 0.5836Epoch 4/15: [============                  ] 20/47 batches, loss: 0.5803Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.5827Epoch 4/15: [==============                ] 22/47 batches, loss: 0.5821Epoch 4/15: [==============                ] 23/47 batches, loss: 0.5825Epoch 4/15: [===============               ] 24/47 batches, loss: 0.5826Epoch 4/15: [===============               ] 25/47 batches, loss: 0.5815Epoch 4/15: [================              ] 26/47 batches, loss: 0.5847Epoch 4/15: [=================             ] 27/47 batches, loss: 0.5832Epoch 4/15: [=================             ] 28/47 batches, loss: 0.5853Epoch 4/15: [==================            ] 29/47 batches, loss: 0.5831Epoch 4/15: [===================           ] 30/47 batches, loss: 0.5828Epoch 4/15: [===================           ] 31/47 batches, loss: 0.5824Epoch 4/15: [====================          ] 32/47 batches, loss: 0.5841Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.5852Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.5827Epoch 4/15: [======================        ] 35/47 batches, loss: 0.5851Epoch 4/15: [======================        ] 36/47 batches, loss: 0.5851Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.5856Epoch 4/15: [========================      ] 38/47 batches, loss: 0.5870Epoch 4/15: [========================      ] 39/47 batches, loss: 0.5870Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.5866Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.5875Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.5877Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.5883Epoch 4/15: [============================  ] 44/47 batches, loss: 0.5881Epoch 4/15: [============================  ] 45/47 batches, loss: 0.5892Epoch 4/15: [============================= ] 46/47 batches, loss: 0.5895Epoch 4/15: [==============================] 47/47 batches, loss: 0.5898
[2025-05-03 16:27:46,155][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5898
[2025-05-03 16:27:46,635][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6370, Metrics: {'accuracy': 0.6944444444444444, 'f1': 0.6666666666666666, 'precision': 0.7333333333333333, 'recall': 0.6111111111111112}
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.5777Epoch 5/15: [=                             ] 2/47 batches, loss: 0.5833Epoch 5/15: [=                             ] 3/47 batches, loss: 0.5966Epoch 5/15: [==                            ] 4/47 batches, loss: 0.5989Epoch 5/15: [===                           ] 5/47 batches, loss: 0.5951Epoch 5/15: [===                           ] 6/47 batches, loss: 0.5839Epoch 5/15: [====                          ] 7/47 batches, loss: 0.5804Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.5817Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.5890Epoch 5/15: [======                        ] 10/47 batches, loss: 0.5981Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.6018Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.6007Epoch 5/15: [========                      ] 13/47 batches, loss: 0.5990Epoch 5/15: [========                      ] 14/47 batches, loss: 0.5989Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.6007Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.5983Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.5946Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.5928Epoch 5/15: [============                  ] 19/47 batches, loss: 0.5890Epoch 5/15: [============                  ] 20/47 batches, loss: 0.5860Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.5881Epoch 5/15: [==============                ] 22/47 batches, loss: 0.5875Epoch 5/15: [==============                ] 23/47 batches, loss: 0.5829Epoch 5/15: [===============               ] 24/47 batches, loss: 0.5820Epoch 5/15: [===============               ] 25/47 batches, loss: 0.5803Epoch 5/15: [================              ] 26/47 batches, loss: 0.5812Epoch 5/15: [=================             ] 27/47 batches, loss: 0.5814Epoch 5/15: [=================             ] 28/47 batches, loss: 0.5811Epoch 5/15: [==================            ] 29/47 batches, loss: 0.5821Epoch 5/15: [===================           ] 30/47 batches, loss: 0.5809Epoch 5/15: [===================           ] 31/47 batches, loss: 0.5808Epoch 5/15: [====================          ] 32/47 batches, loss: 0.5822Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.5823Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.5824Epoch 5/15: [======================        ] 35/47 batches, loss: 0.5844Epoch 5/15: [======================        ] 36/47 batches, loss: 0.5826Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.5815Epoch 5/15: [========================      ] 38/47 batches, loss: 0.5820Epoch 5/15: [========================      ] 39/47 batches, loss: 0.5820Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.5808Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.5811Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.5813Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.5793Epoch 5/15: [============================  ] 44/47 batches, loss: 0.5786Epoch 5/15: [============================  ] 45/47 batches, loss: 0.5774Epoch 5/15: [============================= ] 46/47 batches, loss: 0.5775Epoch 5/15: [==============================] 47/47 batches, loss: 0.5719
[2025-05-03 16:27:48,622][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5719
[2025-05-03 16:27:49,071][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6328, Metrics: {'accuracy': 0.7222222222222222, 'f1': 0.7142857142857143, 'precision': 0.7352941176470589, 'recall': 0.6944444444444444}
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.5475Epoch 6/15: [=                             ] 2/47 batches, loss: 0.5270Epoch 6/15: [=                             ] 3/47 batches, loss: 0.5538Epoch 6/15: [==                            ] 4/47 batches, loss: 0.5822Epoch 6/15: [===                           ] 5/47 batches, loss: 0.5956Epoch 6/15: [===                           ] 6/47 batches, loss: 0.5739Epoch 6/15: [====                          ] 7/47 batches, loss: 0.5695Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.5797Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.5819Epoch 6/15: [======                        ] 10/47 batches, loss: 0.5754Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.5662Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.5688Epoch 6/15: [========                      ] 13/47 batches, loss: 0.5692Epoch 6/15: [========                      ] 14/47 batches, loss: 0.5720Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.5678Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.5660Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.5668Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.5644Epoch 6/15: [============                  ] 19/47 batches, loss: 0.5626Epoch 6/15: [============                  ] 20/47 batches, loss: 0.5630Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.5646Epoch 6/15: [==============                ] 22/47 batches, loss: 0.5646Epoch 6/15: [==============                ] 23/47 batches, loss: 0.5649Epoch 6/15: [===============               ] 24/47 batches, loss: 0.5677Epoch 6/15: [===============               ] 25/47 batches, loss: 0.5661Epoch 6/15: [================              ] 26/47 batches, loss: 0.5659Epoch 6/15: [=================             ] 27/47 batches, loss: 0.5619Epoch 6/15: [=================             ] 28/47 batches, loss: 0.5618Epoch 6/15: [==================            ] 29/47 batches, loss: 0.5597Epoch 6/15: [===================           ] 30/47 batches, loss: 0.5597Epoch 6/15: [===================           ] 31/47 batches, loss: 0.5598Epoch 6/15: [====================          ] 32/47 batches, loss: 0.5582Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.5576Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.5579Epoch 6/15: [======================        ] 35/47 batches, loss: 0.5620Epoch 6/15: [======================        ] 36/47 batches, loss: 0.5603Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.5588Epoch 6/15: [========================      ] 38/47 batches, loss: 0.5580Epoch 6/15: [========================      ] 39/47 batches, loss: 0.5582Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.5597Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.5601Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.5616Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.5622Epoch 6/15: [============================  ] 44/47 batches, loss: 0.5633Epoch 6/15: [============================  ] 45/47 batches, loss: 0.5646Epoch 6/15: [============================= ] 46/47 batches, loss: 0.5649Epoch 6/15: [==============================] 47/47 batches, loss: 0.5653
[2025-05-03 16:27:51,063][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5653
[2025-05-03 16:27:51,593][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6185, Metrics: {'accuracy': 0.75, 'f1': 0.7272727272727273, 'precision': 0.8, 'recall': 0.6666666666666666}
Epoch 7/15: [Epoch 7/15: [                              ] 1/47 batches, loss: 0.5378Epoch 7/15: [=                             ] 2/47 batches, loss: 0.5797Epoch 7/15: [=                             ] 3/47 batches, loss: 0.5720Epoch 7/15: [==                            ] 4/47 batches, loss: 0.5564Epoch 7/15: [===                           ] 5/47 batches, loss: 0.5395Epoch 7/15: [===                           ] 6/47 batches, loss: 0.5550Epoch 7/15: [====                          ] 7/47 batches, loss: 0.5535Epoch 7/15: [=====                         ] 8/47 batches, loss: 0.5513Epoch 7/15: [=====                         ] 9/47 batches, loss: 0.5599Epoch 7/15: [======                        ] 10/47 batches, loss: 0.5581Epoch 7/15: [=======                       ] 11/47 batches, loss: 0.5624Epoch 7/15: [=======                       ] 12/47 batches, loss: 0.5567Epoch 7/15: [========                      ] 13/47 batches, loss: 0.5590Epoch 7/15: [========                      ] 14/47 batches, loss: 0.5584Epoch 7/15: [=========                     ] 15/47 batches, loss: 0.5520Epoch 7/15: [==========                    ] 16/47 batches, loss: 0.5545Epoch 7/15: [==========                    ] 17/47 batches, loss: 0.5566Epoch 7/15: [===========                   ] 18/47 batches, loss: 0.5581Epoch 7/15: [============                  ] 19/47 batches, loss: 0.5649Epoch 7/15: [============                  ] 20/47 batches, loss: 0.5634Epoch 7/15: [=============                 ] 21/47 batches, loss: 0.5651Epoch 7/15: [==============                ] 22/47 batches, loss: 0.5681Epoch 7/15: [==============                ] 23/47 batches, loss: 0.5667Epoch 7/15: [===============               ] 24/47 batches, loss: 0.5648Epoch 7/15: [===============               ] 25/47 batches, loss: 0.5663Epoch 7/15: [================              ] 26/47 batches, loss: 0.5660Epoch 7/15: [=================             ] 27/47 batches, loss: 0.5672Epoch 7/15: [=================             ] 28/47 batches, loss: 0.5679Epoch 7/15: [==================            ] 29/47 batches, loss: 0.5696Epoch 7/15: [===================           ] 30/47 batches, loss: 0.5669Epoch 7/15: [===================           ] 31/47 batches, loss: 0.5672Epoch 7/15: [====================          ] 32/47 batches, loss: 0.5657Epoch 7/15: [=====================         ] 33/47 batches, loss: 0.5654Epoch 7/15: [=====================         ] 34/47 batches, loss: 0.5635Epoch 7/15: [======================        ] 35/47 batches, loss: 0.5613Epoch 7/15: [======================        ] 36/47 batches, loss: 0.5598Epoch 7/15: [=======================       ] 37/47 batches, loss: 0.5598Epoch 7/15: [========================      ] 38/47 batches, loss: 0.5620Epoch 7/15: [========================      ] 39/47 batches, loss: 0.5616Epoch 7/15: [=========================     ] 40/47 batches, loss: 0.5607Epoch 7/15: [==========================    ] 41/47 batches, loss: 0.5585Epoch 7/15: [==========================    ] 42/47 batches, loss: 0.5586Epoch 7/15: [===========================   ] 43/47 batches, loss: 0.5585Epoch 7/15: [============================  ] 44/47 batches, loss: 0.5587Epoch 7/15: [============================  ] 45/47 batches, loss: 0.5577Epoch 7/15: [============================= ] 46/47 batches, loss: 0.5594Epoch 7/15: [==============================] 47/47 batches, loss: 0.5595
[2025-05-03 16:27:53,600][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5595
[2025-05-03 16:27:54,083][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.6155, Metrics: {'accuracy': 0.7361111111111112, 'f1': 0.6984126984126984, 'precision': 0.8148148148148148, 'recall': 0.6111111111111112}
Epoch 8/15: [Epoch 8/15: [                              ] 1/47 batches, loss: 0.4874Epoch 8/15: [=                             ] 2/47 batches, loss: 0.5206Epoch 8/15: [=                             ] 3/47 batches, loss: 0.5089Epoch 8/15: [==                            ] 4/47 batches, loss: 0.5160Epoch 8/15: [===                           ] 5/47 batches, loss: 0.5331Epoch 8/15: [===                           ] 6/47 batches, loss: 0.5528Epoch 8/15: [====                          ] 7/47 batches, loss: 0.5576Epoch 8/15: [=====                         ] 8/47 batches, loss: 0.5623Epoch 8/15: [=====                         ] 9/47 batches, loss: 0.5564Epoch 8/15: [======                        ] 10/47 batches, loss: 0.5569Epoch 8/15: [=======                       ] 11/47 batches, loss: 0.5601Epoch 8/15: [=======                       ] 12/47 batches, loss: 0.5577Epoch 8/15: [========                      ] 13/47 batches, loss: 0.5600Epoch 8/15: [========                      ] 14/47 batches, loss: 0.5595Epoch 8/15: [=========                     ] 15/47 batches, loss: 0.5607Epoch 8/15: [==========                    ] 16/47 batches, loss: 0.5627Epoch 8/15: [==========                    ] 17/47 batches, loss: 0.5636Epoch 8/15: [===========                   ] 18/47 batches, loss: 0.5594Epoch 8/15: [============                  ] 19/47 batches, loss: 0.5602Epoch 8/15: [============                  ] 20/47 batches, loss: 0.5611Epoch 8/15: [=============                 ] 21/47 batches, loss: 0.5600Epoch 8/15: [==============                ] 22/47 batches, loss: 0.5568Epoch 8/15: [==============                ] 23/47 batches, loss: 0.5549Epoch 8/15: [===============               ] 24/47 batches, loss: 0.5535Epoch 8/15: [===============               ] 25/47 batches, loss: 0.5546Epoch 8/15: [================              ] 26/47 batches, loss: 0.5541Epoch 8/15: [=================             ] 27/47 batches, loss: 0.5529Epoch 8/15: [=================             ] 28/47 batches, loss: 0.5561Epoch 8/15: [==================            ] 29/47 batches, loss: 0.5580Epoch 8/15: [===================           ] 30/47 batches, loss: 0.5542Epoch 8/15: [===================           ] 31/47 batches, loss: 0.5538Epoch 8/15: [====================          ] 32/47 batches, loss: 0.5540Epoch 8/15: [=====================         ] 33/47 batches, loss: 0.5543Epoch 8/15: [=====================         ] 34/47 batches, loss: 0.5557Epoch 8/15: [======================        ] 35/47 batches, loss: 0.5545Epoch 8/15: [======================        ] 36/47 batches, loss: 0.5546Epoch 8/15: [=======================       ] 37/47 batches, loss: 0.5551Epoch 8/15: [========================      ] 38/47 batches, loss: 0.5551Epoch 8/15: [========================      ] 39/47 batches, loss: 0.5552Epoch 8/15: [=========================     ] 40/47 batches, loss: 0.5556Epoch 8/15: [==========================    ] 41/47 batches, loss: 0.5534Epoch 8/15: [==========================    ] 42/47 batches, loss: 0.5525Epoch 8/15: [===========================   ] 43/47 batches, loss: 0.5509Epoch 8/15: [============================  ] 44/47 batches, loss: 0.5519Epoch 8/15: [============================  ] 45/47 batches, loss: 0.5512Epoch 8/15: [============================= ] 46/47 batches, loss: 0.5502Epoch 8/15: [==============================] 47/47 batches, loss: 0.5507
[2025-05-03 16:27:55,951][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5507
[2025-05-03 16:27:56,500][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.6038, Metrics: {'accuracy': 0.7777777777777778, 'f1': 0.7575757575757576, 'precision': 0.8333333333333334, 'recall': 0.6944444444444444}
Epoch 9/15: [Epoch 9/15: [                              ] 1/47 batches, loss: 0.5816Epoch 9/15: [=                             ] 2/47 batches, loss: 0.5671Epoch 9/15: [=                             ] 3/47 batches, loss: 0.5556Epoch 9/15: [==                            ] 4/47 batches, loss: 0.5434Epoch 9/15: [===                           ] 5/47 batches, loss: 0.5456Epoch 9/15: [===                           ] 6/47 batches, loss: 0.5313Epoch 9/15: [====                          ] 7/47 batches, loss: 0.5382Epoch 9/15: [=====                         ] 8/47 batches, loss: 0.5438Epoch 9/15: [=====                         ] 9/47 batches, loss: 0.5453Epoch 9/15: [======                        ] 10/47 batches, loss: 0.5483Epoch 9/15: [=======                       ] 11/47 batches, loss: 0.5512Epoch 9/15: [=======                       ] 12/47 batches, loss: 0.5556Epoch 9/15: [========                      ] 13/47 batches, loss: 0.5579Epoch 9/15: [========                      ] 14/47 batches, loss: 0.5538Epoch 9/15: [=========                     ] 15/47 batches, loss: 0.5503Epoch 9/15: [==========                    ] 16/47 batches, loss: 0.5579Epoch 9/15: [==========                    ] 17/47 batches, loss: 0.5542Epoch 9/15: [===========                   ] 18/47 batches, loss: 0.5555Epoch 9/15: [============                  ] 19/47 batches, loss: 0.5574Epoch 9/15: [============                  ] 20/47 batches, loss: 0.5563Epoch 9/15: [=============                 ] 21/47 batches, loss: 0.5519Epoch 9/15: [==============                ] 22/47 batches, loss: 0.5529Epoch 9/15: [==============                ] 23/47 batches, loss: 0.5537Epoch 9/15: [===============               ] 24/47 batches, loss: 0.5538Epoch 9/15: [===============               ] 25/47 batches, loss: 0.5520Epoch 9/15: [================              ] 26/47 batches, loss: 0.5531Epoch 9/15: [=================             ] 27/47 batches, loss: 0.5568Epoch 9/15: [=================             ] 28/47 batches, loss: 0.5560Epoch 9/15: [==================            ] 29/47 batches, loss: 0.5543Epoch 9/15: [===================           ] 30/47 batches, loss: 0.5497Epoch 9/15: [===================           ] 31/47 batches, loss: 0.5468Epoch 9/15: [====================          ] 32/47 batches, loss: 0.5473Epoch 9/15: [=====================         ] 33/47 batches, loss: 0.5480Epoch 9/15: [=====================         ] 34/47 batches, loss: 0.5467Epoch 9/15: [======================        ] 35/47 batches, loss: 0.5461Epoch 9/15: [======================        ] 36/47 batches, loss: 0.5476Epoch 9/15: [=======================       ] 37/47 batches, loss: 0.5484Epoch 9/15: [========================      ] 38/47 batches, loss: 0.5487Epoch 9/15: [========================      ] 39/47 batches, loss: 0.5468Epoch 9/15: [=========================     ] 40/47 batches, loss: 0.5452Epoch 9/15: [==========================    ] 41/47 batches, loss: 0.5453Epoch 9/15: [==========================    ] 42/47 batches, loss: 0.5470Epoch 9/15: [===========================   ] 43/47 batches, loss: 0.5466Epoch 9/15: [============================  ] 44/47 batches, loss: 0.5460Epoch 9/15: [============================  ] 45/47 batches, loss: 0.5465Epoch 9/15: [============================= ] 46/47 batches, loss: 0.5472Epoch 9/15: [==============================] 47/47 batches, loss: 0.5577
[2025-05-03 16:27:58,662][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5577
[2025-05-03 16:27:59,223][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.5999, Metrics: {'accuracy': 0.7777777777777778, 'f1': 0.7575757575757576, 'precision': 0.8333333333333334, 'recall': 0.6944444444444444}
Epoch 10/15: [Epoch 10/15: [                              ] 1/47 batches, loss: 0.5515Epoch 10/15: [=                             ] 2/47 batches, loss: 0.5529Epoch 10/15: [=                             ] 3/47 batches, loss: 0.5529Epoch 10/15: [==                            ] 4/47 batches, loss: 0.5414Epoch 10/15: [===                           ] 5/47 batches, loss: 0.5530Epoch 10/15: [===                           ] 6/47 batches, loss: 0.5605Epoch 10/15: [====                          ] 7/47 batches, loss: 0.5533Epoch 10/15: [=====                         ] 8/47 batches, loss: 0.5510Epoch 10/15: [=====                         ] 9/47 batches, loss: 0.5562Epoch 10/15: [======                        ] 10/47 batches, loss: 0.5495Epoch 10/15: [=======                       ] 11/47 batches, loss: 0.5501Epoch 10/15: [=======                       ] 12/47 batches, loss: 0.5475Epoch 10/15: [========                      ] 13/47 batches, loss: 0.5474Epoch 10/15: [========                      ] 14/47 batches, loss: 0.5440Epoch 10/15: [=========                     ] 15/47 batches, loss: 0.5410Epoch 10/15: [==========                    ] 16/47 batches, loss: 0.5457Epoch 10/15: [==========                    ] 17/47 batches, loss: 0.5493Epoch 10/15: [===========                   ] 18/47 batches, loss: 0.5539Epoch 10/15: [============                  ] 19/47 batches, loss: 0.5471Epoch 10/15: [============                  ] 20/47 batches, loss: 0.5488Epoch 10/15: [=============                 ] 21/47 batches, loss: 0.5506Epoch 10/15: [==============                ] 22/47 batches, loss: 0.5501Epoch 10/15: [==============                ] 23/47 batches, loss: 0.5485Epoch 10/15: [===============               ] 24/47 batches, loss: 0.5471Epoch 10/15: [===============               ] 25/47 batches, loss: 0.5443Epoch 10/15: [================              ] 26/47 batches, loss: 0.5468Epoch 10/15: [=================             ] 27/47 batches, loss: 0.5471Epoch 10/15: [=================             ] 28/47 batches, loss: 0.5452Epoch 10/15: [==================            ] 29/47 batches, loss: 0.5447Epoch 10/15: [===================           ] 30/47 batches, loss: 0.5447Epoch 10/15: [===================           ] 31/47 batches, loss: 0.5450Epoch 10/15: [====================          ] 32/47 batches, loss: 0.5459Epoch 10/15: [=====================         ] 33/47 batches, loss: 0.5456Epoch 10/15: [=====================         ] 34/47 batches, loss: 0.5481Epoch 10/15: [======================        ] 35/47 batches, loss: 0.5479Epoch 10/15: [======================        ] 36/47 batches, loss: 0.5487Epoch 10/15: [=======================       ] 37/47 batches, loss: 0.5465Epoch 10/15: [========================      ] 38/47 batches, loss: 0.5465Epoch 10/15: [========================      ] 39/47 batches, loss: 0.5451Epoch 10/15: [=========================     ] 40/47 batches, loss: 0.5438Epoch 10/15: [==========================    ] 41/47 batches, loss: 0.5447Epoch 10/15: [==========================    ] 42/47 batches, loss: 0.5428Epoch 10/15: [===========================   ] 43/47 batches, loss: 0.5437Epoch 10/15: [============================  ] 44/47 batches, loss: 0.5458Epoch 10/15: [============================  ] 45/47 batches, loss: 0.5461Epoch 10/15: [============================= ] 46/47 batches, loss: 0.5458Epoch 10/15: [==============================] 47/47 batches, loss: 0.5465
[2025-05-03 16:28:01,283][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5465
[2025-05-03 16:28:01,823][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.5985, Metrics: {'accuracy': 0.7638888888888888, 'f1': 0.7384615384615385, 'precision': 0.8275862068965517, 'recall': 0.6666666666666666}
Epoch 11/15: [Epoch 11/15: [                              ] 1/47 batches, loss: 0.4967Epoch 11/15: [=                             ] 2/47 batches, loss: 0.5357Epoch 11/15: [=                             ] 3/47 batches, loss: 0.5226Epoch 11/15: [==                            ] 4/47 batches, loss: 0.5511Epoch 11/15: [===                           ] 5/47 batches, loss: 0.5304Epoch 11/15: [===                           ] 6/47 batches, loss: 0.5360Epoch 11/15: [====                          ] 7/47 batches, loss: 0.5298Epoch 11/15: [=====                         ] 8/47 batches, loss: 0.5238Epoch 11/15: [=====                         ] 9/47 batches, loss: 0.5278Epoch 11/15: [======                        ] 10/47 batches, loss: 0.5269Epoch 11/15: [=======                       ] 11/47 batches, loss: 0.5321Epoch 11/15: [=======                       ] 12/47 batches, loss: 0.5288Epoch 11/15: [========                      ] 13/47 batches, loss: 0.5335Epoch 11/15: [========                      ] 14/47 batches, loss: 0.5334Epoch 11/15: [=========                     ] 15/47 batches, loss: 0.5331Epoch 11/15: [==========                    ] 16/47 batches, loss: 0.5289Epoch 11/15: [==========                    ] 17/47 batches, loss: 0.5294Epoch 11/15: [===========                   ] 18/47 batches, loss: 0.5322Epoch 11/15: [============                  ] 19/47 batches, loss: 0.5308Epoch 11/15: [============                  ] 20/47 batches, loss: 0.5336Epoch 11/15: [=============                 ] 21/47 batches, loss: 0.5318Epoch 11/15: [==============                ] 22/47 batches, loss: 0.5330Epoch 11/15: [==============                ] 23/47 batches, loss: 0.5324Epoch 11/15: [===============               ] 24/47 batches, loss: 0.5303Epoch 11/15: [===============               ] 25/47 batches, loss: 0.5287Epoch 11/15: [================              ] 26/47 batches, loss: 0.5294Epoch 11/15: [=================             ] 27/47 batches, loss: 0.5278Epoch 11/15: [=================             ] 28/47 batches, loss: 0.5261Epoch 11/15: [==================            ] 29/47 batches, loss: 0.5275Epoch 11/15: [===================           ] 30/47 batches, loss: 0.5300Epoch 11/15: [===================           ] 31/47 batches, loss: 0.5325Epoch 11/15: [====================          ] 32/47 batches, loss: 0.5319Epoch 11/15: [=====================         ] 33/47 batches, loss: 0.5332Epoch 11/15: [=====================         ] 34/47 batches, loss: 0.5345Epoch 11/15: [======================        ] 35/47 batches, loss: 0.5369Epoch 11/15: [======================        ] 36/47 batches, loss: 0.5366Epoch 11/15: [=======================       ] 37/47 batches, loss: 0.5387Epoch 11/15: [========================      ] 38/47 batches, loss: 0.5387Epoch 11/15: [========================      ] 39/47 batches, loss: 0.5392Epoch 11/15: [=========================     ] 40/47 batches, loss: 0.5414Epoch 11/15: [==========================    ] 41/47 batches, loss: 0.5406Epoch 11/15: [==========================    ] 42/47 batches, loss: 0.5411Epoch 11/15: [===========================   ] 43/47 batches, loss: 0.5420Epoch 11/15: [============================  ] 44/47 batches, loss: 0.5423Epoch 11/15: [============================  ] 45/47 batches, loss: 0.5420Epoch 11/15: [============================= ] 46/47 batches, loss: 0.5398Epoch 11/15: [==============================] 47/47 batches, loss: 0.5404
[2025-05-03 16:28:03,756][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5404
[2025-05-03 16:28:04,220][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.5944, Metrics: {'accuracy': 0.7777777777777778, 'f1': 0.7575757575757576, 'precision': 0.8333333333333334, 'recall': 0.6944444444444444}
Epoch 12/15: [Epoch 12/15: [                              ] 1/47 batches, loss: 0.5789Epoch 12/15: [=                             ] 2/47 batches, loss: 0.5668Epoch 12/15: [=                             ] 3/47 batches, loss: 0.5652Epoch 12/15: [==                            ] 4/47 batches, loss: 0.5650Epoch 12/15: [===                           ] 5/47 batches, loss: 0.5583Epoch 12/15: [===                           ] 6/47 batches, loss: 0.5421Epoch 12/15: [====                          ] 7/47 batches, loss: 0.5309Epoch 12/15: [=====                         ] 8/47 batches, loss: 0.5351Epoch 12/15: [=====                         ] 9/47 batches, loss: 0.5345Epoch 12/15: [======                        ] 10/47 batches, loss: 0.5348Epoch 12/15: [=======                       ] 11/47 batches, loss: 0.5301Epoch 12/15: [=======                       ] 12/47 batches, loss: 0.5353Epoch 12/15: [========                      ] 13/47 batches, loss: 0.5342Epoch 12/15: [========                      ] 14/47 batches, loss: 0.5323Epoch 12/15: [=========                     ] 15/47 batches, loss: 0.5341Epoch 12/15: [==========                    ] 16/47 batches, loss: 0.5329Epoch 12/15: [==========                    ] 17/47 batches, loss: 0.5372Epoch 12/15: [===========                   ] 18/47 batches, loss: 0.5368Epoch 12/15: [============                  ] 19/47 batches, loss: 0.5384Epoch 12/15: [============                  ] 20/47 batches, loss: 0.5340Epoch 12/15: [=============                 ] 21/47 batches, loss: 0.5348Epoch 12/15: [==============                ] 22/47 batches, loss: 0.5324Epoch 12/15: [==============                ] 23/47 batches, loss: 0.5324Epoch 12/15: [===============               ] 24/47 batches, loss: 0.5332Epoch 12/15: [===============               ] 25/47 batches, loss: 0.5333Epoch 12/15: [================              ] 26/47 batches, loss: 0.5320Epoch 12/15: [=================             ] 27/47 batches, loss: 0.5332Epoch 12/15: [=================             ] 28/47 batches, loss: 0.5321Epoch 12/15: [==================            ] 29/47 batches, loss: 0.5303Epoch 12/15: [===================           ] 30/47 batches, loss: 0.5347Epoch 12/15: [===================           ] 31/47 batches, loss: 0.5348Epoch 12/15: [====================          ] 32/47 batches, loss: 0.5334Epoch 12/15: [=====================         ] 33/47 batches, loss: 0.5335Epoch 12/15: [=====================         ] 34/47 batches, loss: 0.5353Epoch 12/15: [======================        ] 35/47 batches, loss: 0.5373Epoch 12/15: [======================        ] 36/47 batches, loss: 0.5398Epoch 12/15: [=======================       ] 37/47 batches, loss: 0.5395Epoch 12/15: [========================      ] 38/47 batches, loss: 0.5380Epoch 12/15: [========================      ] 39/47 batches, loss: 0.5390Epoch 12/15: [=========================     ] 40/47 batches, loss: 0.5407Epoch 12/15: [==========================    ] 41/47 batches, loss: 0.5415Epoch 12/15: [==========================    ] 42/47 batches, loss: 0.5396Epoch 12/15: [===========================   ] 43/47 batches, loss: 0.5393Epoch 12/15: [============================  ] 44/47 batches, loss: 0.5395Epoch 12/15: [============================  ] 45/47 batches, loss: 0.5378Epoch 12/15: [============================= ] 46/47 batches, loss: 0.5375Epoch 12/15: [==============================] 47/47 batches, loss: 0.5409
[2025-05-03 16:28:06,202][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.5409
[2025-05-03 16:28:06,627][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.5975, Metrics: {'accuracy': 0.7638888888888888, 'f1': 0.7384615384615385, 'precision': 0.8275862068965517, 'recall': 0.6666666666666666}
[2025-05-03 16:28:06,628][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 13/15: [Epoch 13/15: [                              ] 1/47 batches, loss: 0.5166Epoch 13/15: [=                             ] 2/47 batches, loss: 0.5164Epoch 13/15: [=                             ] 3/47 batches, loss: 0.5126Epoch 13/15: [==                            ] 4/47 batches, loss: 0.5283Epoch 13/15: [===                           ] 5/47 batches, loss: 0.5326Epoch 13/15: [===                           ] 6/47 batches, loss: 0.5337Epoch 13/15: [====                          ] 7/47 batches, loss: 0.5238Epoch 13/15: [=====                         ] 8/47 batches, loss: 0.5193Epoch 13/15: [=====                         ] 9/47 batches, loss: 0.5248Epoch 13/15: [======                        ] 10/47 batches, loss: 0.5231Epoch 13/15: [=======                       ] 11/47 batches, loss: 0.5230Epoch 13/15: [=======                       ] 12/47 batches, loss: 0.5236Epoch 13/15: [========                      ] 13/47 batches, loss: 0.5211Epoch 13/15: [========                      ] 14/47 batches, loss: 0.5223Epoch 13/15: [=========                     ] 15/47 batches, loss: 0.5229Epoch 13/15: [==========                    ] 16/47 batches, loss: 0.5276Epoch 13/15: [==========                    ] 17/47 batches, loss: 0.5276Epoch 13/15: [===========                   ] 18/47 batches, loss: 0.5290Epoch 13/15: [============                  ] 19/47 batches, loss: 0.5265Epoch 13/15: [============                  ] 20/47 batches, loss: 0.5230Epoch 13/15: [=============                 ] 21/47 batches, loss: 0.5213Epoch 13/15: [==============                ] 22/47 batches, loss: 0.5195Epoch 13/15: [==============                ] 23/47 batches, loss: 0.5189Epoch 13/15: [===============               ] 24/47 batches, loss: 0.5172Epoch 13/15: [===============               ] 25/47 batches, loss: 0.5196Epoch 13/15: [================              ] 26/47 batches, loss: 0.5220Epoch 13/15: [=================             ] 27/47 batches, loss: 0.5232Epoch 13/15: [=================             ] 28/47 batches, loss: 0.5244Epoch 13/15: [==================            ] 29/47 batches, loss: 0.5251Epoch 13/15: [===================           ] 30/47 batches, loss: 0.5252Epoch 13/15: [===================           ] 31/47 batches, loss: 0.5274Epoch 13/15: [====================          ] 32/47 batches, loss: 0.5294Epoch 13/15: [=====================         ] 33/47 batches, loss: 0.5294Epoch 13/15: [=====================         ] 34/47 batches, loss: 0.5303Epoch 13/15: [======================        ] 35/47 batches, loss: 0.5316Epoch 13/15: [======================        ] 36/47 batches, loss: 0.5309Epoch 13/15: [=======================       ] 37/47 batches, loss: 0.5336Epoch 13/15: [========================      ] 38/47 batches, loss: 0.5354Epoch 13/15: [========================      ] 39/47 batches, loss: 0.5346Epoch 13/15: [=========================     ] 40/47 batches, loss: 0.5369Epoch 13/15: [==========================    ] 41/47 batches, loss: 0.5366Epoch 13/15: [==========================    ] 42/47 batches, loss: 0.5389Epoch 13/15: [===========================   ] 43/47 batches, loss: 0.5390Epoch 13/15: [============================  ] 44/47 batches, loss: 0.5390Epoch 13/15: [============================  ] 45/47 batches, loss: 0.5397Epoch 13/15: [============================= ] 46/47 batches, loss: 0.5375Epoch 13/15: [==============================] 47/47 batches, loss: 0.5354
[2025-05-03 16:28:08,263][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.5354
[2025-05-03 16:28:08,716][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.6016, Metrics: {'accuracy': 0.7777777777777778, 'f1': 0.7575757575757576, 'precision': 0.8333333333333334, 'recall': 0.6944444444444444}
[2025-05-03 16:28:08,717][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 14/15: [Epoch 14/15: [                              ] 1/47 batches, loss: 0.5358Epoch 14/15: [=                             ] 2/47 batches, loss: 0.5598Epoch 14/15: [=                             ] 3/47 batches, loss: 0.5266Epoch 14/15: [==                            ] 4/47 batches, loss: 0.5338Epoch 14/15: [===                           ] 5/47 batches, loss: 0.5120Epoch 14/15: [===                           ] 6/47 batches, loss: 0.5152Epoch 14/15: [====                          ] 7/47 batches, loss: 0.5134Epoch 14/15: [=====                         ] 8/47 batches, loss: 0.5246Epoch 14/15: [=====                         ] 9/47 batches, loss: 0.5309Epoch 14/15: [======                        ] 10/47 batches, loss: 0.5308Epoch 14/15: [=======                       ] 11/47 batches, loss: 0.5319Epoch 14/15: [=======                       ] 12/47 batches, loss: 0.5310Epoch 14/15: [========                      ] 13/47 batches, loss: 0.5263Epoch 14/15: [========                      ] 14/47 batches, loss: 0.5351Epoch 14/15: [=========                     ] 15/47 batches, loss: 0.5347Epoch 14/15: [==========                    ] 16/47 batches, loss: 0.5274Epoch 14/15: [==========                    ] 17/47 batches, loss: 0.5292Epoch 14/15: [===========                   ] 18/47 batches, loss: 0.5334Epoch 14/15: [============                  ] 19/47 batches, loss: 0.5314Epoch 14/15: [============                  ] 20/47 batches, loss: 0.5286Epoch 14/15: [=============                 ] 21/47 batches, loss: 0.5299Epoch 14/15: [==============                ] 22/47 batches, loss: 0.5300Epoch 14/15: [==============                ] 23/47 batches, loss: 0.5310Epoch 14/15: [===============               ] 24/47 batches, loss: 0.5329Epoch 14/15: [===============               ] 25/47 batches, loss: 0.5309Epoch 14/15: [================              ] 26/47 batches, loss: 0.5321Epoch 14/15: [=================             ] 27/47 batches, loss: 0.5342Epoch 14/15: [=================             ] 28/47 batches, loss: 0.5331Epoch 14/15: [==================            ] 29/47 batches, loss: 0.5359Epoch 14/15: [===================           ] 30/47 batches, loss: 0.5377Epoch 14/15: [===================           ] 31/47 batches, loss: 0.5390Epoch 14/15: [====================          ] 32/47 batches, loss: 0.5389Epoch 14/15: [=====================         ] 33/47 batches, loss: 0.5378Epoch 14/15: [=====================         ] 34/47 batches, loss: 0.5378Epoch 14/15: [======================        ] 35/47 batches, loss: 0.5379Epoch 14/15: [======================        ] 36/47 batches, loss: 0.5357Epoch 14/15: [=======================       ] 37/47 batches, loss: 0.5363Epoch 14/15: [========================      ] 38/47 batches, loss: 0.5348Epoch 14/15: [========================      ] 39/47 batches, loss: 0.5340Epoch 14/15: [=========================     ] 40/47 batches, loss: 0.5345Epoch 14/15: [==========================    ] 41/47 batches, loss: 0.5369Epoch 14/15: [==========================    ] 42/47 batches, loss: 0.5379Epoch 14/15: [===========================   ] 43/47 batches, loss: 0.5361Epoch 14/15: [============================  ] 44/47 batches, loss: 0.5365Epoch 14/15: [============================  ] 45/47 batches, loss: 0.5369Epoch 14/15: [============================= ] 46/47 batches, loss: 0.5368Epoch 14/15: [==============================] 47/47 batches, loss: 0.5401
[2025-05-03 16:28:10,385][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.5401
[2025-05-03 16:28:10,777][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.5916, Metrics: {'accuracy': 0.7916666666666666, 'f1': 0.782608695652174, 'precision': 0.8181818181818182, 'recall': 0.75}
Epoch 15/15: [Epoch 15/15: [                              ] 1/47 batches, loss: 0.4779Epoch 15/15: [=                             ] 2/47 batches, loss: 0.5165Epoch 15/15: [=                             ] 3/47 batches, loss: 0.5300Epoch 15/15: [==                            ] 4/47 batches, loss: 0.5289Epoch 15/15: [===                           ] 5/47 batches, loss: 0.5240Epoch 15/15: [===                           ] 6/47 batches, loss: 0.5343Epoch 15/15: [====                          ] 7/47 batches, loss: 0.5284Epoch 15/15: [=====                         ] 8/47 batches, loss: 0.5311Epoch 15/15: [=====                         ] 9/47 batches, loss: 0.5233Epoch 15/15: [======                        ] 10/47 batches, loss: 0.5239Epoch 15/15: [=======                       ] 11/47 batches, loss: 0.5333Epoch 15/15: [=======                       ] 12/47 batches, loss: 0.5269Epoch 15/15: [========                      ] 13/47 batches, loss: 0.5279Epoch 15/15: [========                      ] 14/47 batches, loss: 0.5299Epoch 15/15: [=========                     ] 15/47 batches, loss: 0.5319Epoch 15/15: [==========                    ] 16/47 batches, loss: 0.5327Epoch 15/15: [==========                    ] 17/47 batches, loss: 0.5325Epoch 15/15: [===========                   ] 18/47 batches, loss: 0.5310Epoch 15/15: [============                  ] 19/47 batches, loss: 0.5311Epoch 15/15: [============                  ] 20/47 batches, loss: 0.5310Epoch 15/15: [=============                 ] 21/47 batches, loss: 0.5330Epoch 15/15: [==============                ] 22/47 batches, loss: 0.5299Epoch 15/15: [==============                ] 23/47 batches, loss: 0.5309Epoch 15/15: [===============               ] 24/47 batches, loss: 0.5298Epoch 15/15: [===============               ] 25/47 batches, loss: 0.5309Epoch 15/15: [================              ] 26/47 batches, loss: 0.5290Epoch 15/15: [=================             ] 27/47 batches, loss: 0.5269Epoch 15/15: [=================             ] 28/47 batches, loss: 0.5283Epoch 15/15: [==================            ] 29/47 batches, loss: 0.5340Epoch 15/15: [===================           ] 30/47 batches, loss: 0.5356Epoch 15/15: [===================           ] 31/47 batches, loss: 0.5347Epoch 15/15: [====================          ] 32/47 batches, loss: 0.5323Epoch 15/15: [=====================         ] 33/47 batches, loss: 0.5308Epoch 15/15: [=====================         ] 34/47 batches, loss: 0.5315Epoch 15/15: [======================        ] 35/47 batches, loss: 0.5317Epoch 15/15: [======================        ] 36/47 batches, loss: 0.5328Epoch 15/15: [=======================       ] 37/47 batches, loss: 0.5334Epoch 15/15: [========================      ] 38/47 batches, loss: 0.5339Epoch 15/15: [========================      ] 39/47 batches, loss: 0.5326Epoch 15/15: [=========================     ] 40/47 batches, loss: 0.5332Epoch 15/15: [==========================    ] 41/47 batches, loss: 0.5331Epoch 15/15: [==========================    ] 42/47 batches, loss: 0.5333Epoch 15/15: [===========================   ] 43/47 batches, loss: 0.5316Epoch 15/15: [============================  ] 44/47 batches, loss: 0.5316Epoch 15/15: [============================  ] 45/47 batches, loss: 0.5330Epoch 15/15: [============================= ] 46/47 batches, loss: 0.5346Epoch 15/15: [==============================] 47/47 batches, loss: 0.5326
[2025-05-03 16:28:12,825][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.5326
[2025-05-03 16:28:13,225][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.5949, Metrics: {'accuracy': 0.7777777777777778, 'f1': 0.7714285714285715, 'precision': 0.7941176470588235, 'recall': 0.75}
[2025-05-03 16:28:13,226][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
[2025-05-03 16:28:13,226][src.training.lm_trainer][INFO] - Training completed in 37.04 seconds
[2025-05-03 16:28:13,226][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-03 16:28:15,841][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.979702300405954, 'f1': 0.9778434268833087, 'precision': 0.9851190476190477, 'recall': 0.9706744868035191}
[2025-05-03 16:28:15,842][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.7916666666666666, 'f1': 0.782608695652174, 'precision': 0.8181818181818182, 'recall': 0.75}
[2025-05-03 16:28:15,842][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.6818181818181818, 'f1': 0.6534653465346535, 'precision': 0.717391304347826, 'recall': 0.6}
[2025-05-03 16:28:17,485][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/ko/ko/model.pt
[2025-05-03 16:28:17,487][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▄▄▅▆▇▆██▇██
wandb:           best_val_f1 ▁▆▆▆▇▇▇██▇██
wandb:         best_val_loss █▇▆▅▅▃▃▂▂▂▁▁
wandb:    best_val_precision ▁▂▃▄▄▇▇████▇
wandb:       best_val_recall ▁▆▆▆▇▇▆▇▇▇▇█
wandb:                 epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▂▂▂▂▂▂▂▂▂▂▃
wandb:            train_loss █▅▄▄▃▃▂▂▂▂▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▄▄▅▆▇▆██▇█▇███
wandb:                val_f1 ▁▆▆▆▇▇▇██▇█▇███
wandb:              val_loss █▇▆▅▅▃▃▂▂▂▁▂▂▁▁
wandb:         val_precision ▁▂▃▄▄▇▇██████▇▇
wandb:            val_recall ▁▆▆▆▇▇▆▇▇▇▇▇▇██
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.79167
wandb:           best_val_f1 0.78261
wandb:         best_val_loss 0.59157
wandb:    best_val_precision 0.81818
wandb:       best_val_recall 0.75
wandb:                 epoch 15
wandb:   final_test_accuracy 0.68182
wandb:         final_test_f1 0.65347
wandb:  final_test_precision 0.71739
wandb:     final_test_recall 0.6
wandb:  final_train_accuracy 0.9797
wandb:        final_train_f1 0.97784
wandb: final_train_precision 0.98512
wandb:    final_train_recall 0.97067
wandb:    final_val_accuracy 0.79167
wandb:          final_val_f1 0.78261
wandb:   final_val_precision 0.81818
wandb:      final_val_recall 0.75
wandb:         learning_rate 0.0001
wandb:            train_loss 0.53261
wandb:            train_time 37.03719
wandb:          val_accuracy 0.77778
wandb:                val_f1 0.77143
wandb:              val_loss 0.59492
wandb:         val_precision 0.79412
wandb:            val_recall 0.75
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_162706-3k6a9poc
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_162706-3k6a9poc/logs
Experiment probe_layer2_question_type_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_complexity_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_ko"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/layer2/ko"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-03 16:29:10,972][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/ko
experiment_name: probe_layer2_complexity_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-03 16:29:10,973][__main__][INFO] - Normalized task: complexity
[2025-05-03 16:29:10,973][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-03 16:29:10,973][__main__][INFO] - Determined Task Type: regression
[2025-05-03 16:29:10,977][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ko']
[2025-05-03 16:29:10,977][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-03 16:29:18,117][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-03 16:29:20,517][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-03 16:29:20,517][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:29:20,916][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:29:21,044][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:29:21,462][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-03 16:29:21,473][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:29:21,475][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-03 16:29:21,478][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:29:21,768][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:29:21,886][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:29:21,966][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-03 16:29:21,967][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:29:21,968][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-03 16:29:21,970][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:29:22,222][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:29:22,467][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:29:22,481][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-03 16:29:22,483][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:29:22,483][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-03 16:29:22,484][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-03 16:29:22,484][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:29:22,485][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:29:22,485][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:29:22,485][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:29:22,485][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:29:22,485][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-05-03 16:29:22,485][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-03 16:29:22,485][src.data.datasets][INFO] - Sample label: 0.5104557871818542
[2025-05-03 16:29:22,485][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:29:22,486][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:29:22,486][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:29:22,486][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:29:22,486][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:29:22,486][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-05-03 16:29:22,486][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-03 16:29:22,486][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-05-03 16:29:22,486][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:29:22,486][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:29:22,487][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:29:22,487][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:29:22,487][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:29:22,487][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-05-03 16:29:22,487][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-03 16:29:22,487][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-05-03 16:29:22,487][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-03 16:29:22,487][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-03 16:29:22,487][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-03 16:29:22,488][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-03 16:29:22,488][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-03 16:29:33,085][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-03 16:29:33,086][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-03 16:29:33,086][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-03 16:29:33,086][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-03 16:29:33,089][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-03 16:29:33,090][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-03 16:29:33,090][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-03 16:29:33,090][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-03 16:29:33,090][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-03 16:29:33,091][__main__][INFO] - Total parameters: 394,255,105
[2025-05-03 16:29:33,091][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.3921Epoch 1/15: [=                             ] 2/47 batches, loss: 0.5526Epoch 1/15: [=                             ] 3/47 batches, loss: 0.4921Epoch 1/15: [==                            ] 4/47 batches, loss: 0.4927Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4735Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4248Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4057Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4141Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4176Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4098Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4005Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.3985Epoch 1/15: [========                      ] 13/47 batches, loss: 0.3824Epoch 1/15: [========                      ] 14/47 batches, loss: 0.3886Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.3775Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.3810Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.3721Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.3874Epoch 1/15: [============                  ] 19/47 batches, loss: 0.3793Epoch 1/15: [============                  ] 20/47 batches, loss: 0.3749Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.3816Epoch 1/15: [==============                ] 22/47 batches, loss: 0.3813Epoch 1/15: [==============                ] 23/47 batches, loss: 0.3695Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3613Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3584Epoch 1/15: [================              ] 26/47 batches, loss: 0.3534Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3530Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3463Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3515Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3481Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3414Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3360Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3339Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3359Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3309Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3286Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3233Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3213Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3181Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3188Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3184Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3162Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3137Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3109Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3120Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3073Epoch 1/15: [==============================] 47/47 batches, loss: 0.3097
[2025-05-03 16:29:40,794][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3097
[2025-05-03 16:29:41,194][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0916, Metrics: {'mse': 0.09674559533596039, 'rmse': 0.31103953982727084, 'r2': -1.0535712242126465}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2696Epoch 2/15: [=                             ] 2/47 batches, loss: 0.1937Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2166Epoch 2/15: [==                            ] 4/47 batches, loss: 0.1809Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2032Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2240Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2136Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2111Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2123Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2038Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2091Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2093Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2094Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2102Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2079Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2012Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.1968Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2029Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2016Epoch 2/15: [============                  ] 20/47 batches, loss: 0.1967Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.1943Epoch 2/15: [==============                ] 22/47 batches, loss: 0.1936Epoch 2/15: [==============                ] 23/47 batches, loss: 0.1940Epoch 2/15: [===============               ] 24/47 batches, loss: 0.1945Epoch 2/15: [===============               ] 25/47 batches, loss: 0.1933Epoch 2/15: [================              ] 26/47 batches, loss: 0.1898Epoch 2/15: [=================             ] 27/47 batches, loss: 0.1877Epoch 2/15: [=================             ] 28/47 batches, loss: 0.1866Epoch 2/15: [==================            ] 29/47 batches, loss: 0.1833Epoch 2/15: [===================           ] 30/47 batches, loss: 0.1842Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1834Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1830Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1854Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1855Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1822Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1796Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1800Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1770Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1753Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1741Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1721Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1704Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1685Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1667Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1662Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1651Epoch 2/15: [==============================] 47/47 batches, loss: 0.1651
[2025-05-03 16:29:43,188][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1651
[2025-05-03 16:29:43,571][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0548, Metrics: {'mse': 0.05791819095611572, 'rmse': 0.2406619848586721, 'r2': -0.22940099239349365}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1579Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1167Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1119Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1009Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1067Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1313Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1287Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1211Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1282Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1261Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1257Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1277Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1308Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1265Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1256Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1250Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1295Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1307Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1289Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1255Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1250Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1225Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1238Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1238Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1213Epoch 3/15: [================              ] 26/47 batches, loss: 0.1214Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1228Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1216Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1199Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1205Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1223Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1210Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1211Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1209Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1220Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1203Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1202Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1189Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1171Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1168Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1157Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1151Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1150Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1173Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1181Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1175Epoch 3/15: [==============================] 47/47 batches, loss: 0.1189
[2025-05-03 16:29:45,615][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1189
[2025-05-03 16:29:46,083][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0667, Metrics: {'mse': 0.07061457633972168, 'rmse': 0.26573403308519156, 'r2': -0.4989008903503418}
[2025-05-03 16:29:46,083][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1444Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1117Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1057Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1144Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1126Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1110Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1124Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1138Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1102Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1114Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1183Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1167Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1135Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1099Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1085Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1051Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1049Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1032Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1022Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1071Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1072Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1062Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1049Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1056Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1054Epoch 4/15: [================              ] 26/47 batches, loss: 0.1040Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1045Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1047Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1058Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1040Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1052Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1073Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1051Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1051Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1054Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1039Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1047Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1040Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1030Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1026Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1020Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1017Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1002Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1001Epoch 4/15: [============================  ] 45/47 batches, loss: 0.0993Epoch 4/15: [============================= ] 46/47 batches, loss: 0.0987Epoch 4/15: [==============================] 47/47 batches, loss: 0.0989
[2025-05-03 16:29:47,725][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0989
[2025-05-03 16:29:48,184][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0594, Metrics: {'mse': 0.06301184743642807, 'rmse': 0.25102160750905106, 'r2': -0.33752143383026123}
[2025-05-03 16:29:48,185][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.0907Epoch 5/15: [=                             ] 2/47 batches, loss: 0.0811Epoch 5/15: [=                             ] 3/47 batches, loss: 0.0813Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0687Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0623Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0688Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0712Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0729Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0737Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0717Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0732Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0743Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0718Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0705Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0697Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0706Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0684Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0696Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0685Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0691Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0695Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0699Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0706Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0717Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0711Epoch 5/15: [================              ] 26/47 batches, loss: 0.0703Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0701Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0692Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0700Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0705Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0716Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0715Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0703Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0694Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0703Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0698Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0695Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0696Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0701Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0703Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0701Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0698Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0694Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0695Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0710Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0708Epoch 5/15: [==============================] 47/47 batches, loss: 0.0708
[2025-05-03 16:29:49,832][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0708
[2025-05-03 16:29:50,167][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0672, Metrics: {'mse': 0.07110831141471863, 'rmse': 0.2666614171842613, 'r2': -0.5093811750411987}
[2025-05-03 16:29:50,168][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.0856Epoch 6/15: [=                             ] 2/47 batches, loss: 0.0773Epoch 6/15: [=                             ] 3/47 batches, loss: 0.0861Epoch 6/15: [==                            ] 4/47 batches, loss: 0.0835Epoch 6/15: [===                           ] 5/47 batches, loss: 0.0919Epoch 6/15: [===                           ] 6/47 batches, loss: 0.0910Epoch 6/15: [====                          ] 7/47 batches, loss: 0.0845Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.0867Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.0818Epoch 6/15: [======                        ] 10/47 batches, loss: 0.0837Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.0886Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.0887Epoch 6/15: [========                      ] 13/47 batches, loss: 0.0841Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0808Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0799Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.0812Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.0828Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.0826Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0798Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0801Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0774Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0769Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0765Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0752Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0752Epoch 6/15: [================              ] 26/47 batches, loss: 0.0744Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0739Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0736Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0735Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0727Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0727Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0723Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0722Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0724Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0722Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0712Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0706Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0702Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0701Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0696Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0698Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0695Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0692Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0695Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0694Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0691Epoch 6/15: [==============================] 47/47 batches, loss: 0.0692
[2025-05-03 16:29:51,682][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0692
[2025-05-03 16:29:52,088][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0649, Metrics: {'mse': 0.06866516917943954, 'rmse': 0.2620403960831985, 'r2': -0.4575216770172119}
[2025-05-03 16:29:52,088][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-03 16:29:52,089][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-03 16:29:52,089][src.training.lm_trainer][INFO] - Training completed in 13.73 seconds
[2025-05-03 16:29:52,089][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-03 16:29:54,779][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.019038164988160133, 'rmse': 0.13797885703309812, 'r2': 0.1447957158088684}
[2025-05-03 16:29:54,779][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05791819095611572, 'rmse': 0.2406619848586721, 'r2': -0.22940099239349365}
[2025-05-03 16:29:54,779][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04128776490688324, 'rmse': 0.20319390962054754, 'r2': -0.2819925546646118}
[2025-05-03 16:29:56,412][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/ko/ko/model.pt
[2025-05-03 16:29:56,413][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▅▄▅▄
wandb:       train_loss █▄▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▃▂▃▃
wandb:          val_mse █▁▃▂▃▃
wandb:           val_r2 ▁█▆▇▆▆
wandb:         val_rmse █▁▃▂▄▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05485
wandb:     best_val_mse 0.05792
wandb:      best_val_r2 -0.2294
wandb:    best_val_rmse 0.24066
wandb: early_stop_epoch 6
wandb:            epoch 6
wandb:   final_test_mse 0.04129
wandb:    final_test_r2 -0.28199
wandb:  final_test_rmse 0.20319
wandb:  final_train_mse 0.01904
wandb:   final_train_r2 0.1448
wandb: final_train_rmse 0.13798
wandb:    final_val_mse 0.05792
wandb:     final_val_r2 -0.2294
wandb:   final_val_rmse 0.24066
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06922
wandb:       train_time 13.73318
wandb:         val_loss 0.06487
wandb:          val_mse 0.06867
wandb:           val_r2 -0.45752
wandb:         val_rmse 0.26204
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_162911-nouwf7go
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_162911-nouwf7go/logs
Experiment probe_layer2_complexity_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_question_type_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_question_type_ru"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/layer2/ru"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-03 16:30:45,291][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/ru
experiment_name: probe_layer2_question_type_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-03 16:30:45,291][__main__][INFO] - Normalized task: question_type
[2025-05-03 16:30:45,291][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-03 16:30:45,291][__main__][INFO] - Determined Task Type: classification
[2025-05-03 16:30:45,295][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ru']
[2025-05-03 16:30:45,296][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-03 16:30:51,687][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-03 16:30:54,279][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-03 16:30:54,279][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:30:54,758][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:30:54,913][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:30:55,359][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-03 16:30:55,369][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:30:55,370][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-03 16:30:55,380][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:30:55,429][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:30:55,577][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:30:55,670][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-03 16:30:55,672][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:30:55,673][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-03 16:30:55,674][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:30:55,882][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:30:56,126][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:30:56,183][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-03 16:30:56,185][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:30:56,185][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-03 16:30:56,209][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-03 16:30:56,209][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:30:56,209][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:30:56,209][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:30:56,209][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:30:56,209][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-05-03 16:30:56,210][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-05-03 16:30:56,210][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-03 16:30:56,210][src.data.datasets][INFO] - Sample label: 0
[2025-05-03 16:30:56,210][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:30:56,210][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:30:56,210][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:30:56,210][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:30:56,210][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-03 16:30:56,210][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-03 16:30:56,210][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-03 16:30:56,210][src.data.datasets][INFO] - Sample label: 1
[2025-05-03 16:30:56,211][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:30:56,211][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:30:56,211][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:30:56,211][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:30:56,211][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-03 16:30:56,211][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-03 16:30:56,211][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-03 16:30:56,211][src.data.datasets][INFO] - Sample label: 1
[2025-05-03 16:30:56,211][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-03 16:30:56,211][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-03 16:30:56,212][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-03 16:30:56,212][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-03 16:30:56,212][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-03 16:31:07,372][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-03 16:31:07,373][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-03 16:31:07,373][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-03 16:31:07,373][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-03 16:31:07,379][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-03 16:31:07,380][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-03 16:31:07,380][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-03 16:31:07,380][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-03 16:31:07,380][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-03 16:31:07,381][__main__][INFO] - Total parameters: 394,568,839
[2025-05-03 16:31:07,381][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-03 16:31:07,382][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7251Epoch 1/15: [                              ] 2/75 batches, loss: 0.7648Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7362Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7320Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7228Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7175Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7140Epoch 1/15: [===                           ] 8/75 batches, loss: 0.7110Epoch 1/15: [===                           ] 9/75 batches, loss: 0.7083Epoch 1/15: [====                          ] 10/75 batches, loss: 0.7073Epoch 1/15: [====                          ] 11/75 batches, loss: 0.7069Epoch 1/15: [====                          ] 12/75 batches, loss: 0.7061Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.7058Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.7051Epoch 1/15: [======                        ] 15/75 batches, loss: 0.7043Epoch 1/15: [======                        ] 16/75 batches, loss: 0.7037Epoch 1/15: [======                        ] 17/75 batches, loss: 0.7030Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.7026Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.7022Epoch 1/15: [========                      ] 20/75 batches, loss: 0.7017Epoch 1/15: [========                      ] 21/75 batches, loss: 0.7012Epoch 1/15: [========                      ] 22/75 batches, loss: 0.7008Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.7004Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.7001Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6998Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6994Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6991Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6989Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6986Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6983Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6982Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6980Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6979Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6977Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6975Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6973Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6972Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6971Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6970Epoch 1/15: [================              ] 40/75 batches, loss: 0.6969Epoch 1/15: [================              ] 41/75 batches, loss: 0.6968Epoch 1/15: [================              ] 42/75 batches, loss: 0.6967Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6965Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6965Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6964Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6963Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6963Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6962Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6961Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6960Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6959Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6959Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6958Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6957Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6957Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6956Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6956Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6954Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6954Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6953Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6952Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6952Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6951Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6949Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6949Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6949Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6946Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6945Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6945Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6942Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6940Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6937Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6939Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6938Epoch 1/15: [==============================] 75/75 batches, loss: 0.6937
[2025-05-03 16:31:15,757][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6937
[2025-05-03 16:31:16,153][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6952, Metrics: {'accuracy': 0.5138888888888888, 'f1': 0.05405405405405406, 'precision': 1.0, 'recall': 0.027777777777777776}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.7095Epoch 2/15: [                              ] 2/75 batches, loss: 0.6791Epoch 2/15: [=                             ] 3/75 batches, loss: 0.7054Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6976Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6869Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6825Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6798Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6869Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6882Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6930Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6910Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6863Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6874Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6870Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6855Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6846Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6835Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6832Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6829Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6837Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6824Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6808Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6800Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6765Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6773Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6752Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6774Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6770Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6763Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6710Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6706Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6721Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6714Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6693Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6675Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6654Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6627Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6610Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6609Epoch 2/15: [================              ] 40/75 batches, loss: 0.6625Epoch 2/15: [================              ] 41/75 batches, loss: 0.6635Epoch 2/15: [================              ] 42/75 batches, loss: 0.6648Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6651Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6630Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6631Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6621Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6636Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6639Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6627Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6623Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6618Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6619Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6612Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6604Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6596Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6593Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6602Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6590Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6576Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6560Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6562Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6548Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6533Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6536Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6555Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6547Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6553Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6559Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6562Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6556Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6547Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6541Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6542Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6530Epoch 2/15: [==============================] 75/75 batches, loss: 0.6526
[2025-05-03 16:31:18,992][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6526
[2025-05-03 16:31:19,402][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6481, Metrics: {'accuracy': 0.6388888888888888, 'f1': 0.43478260869565216, 'precision': 1.0, 'recall': 0.2777777777777778}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.5953Epoch 3/15: [                              ] 2/75 batches, loss: 0.6163Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6284Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6137Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6078Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6100Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6088Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6179Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6124Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6109Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6111Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6123Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6136Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6205Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6144Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6183Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6158Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6135Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6146Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6136Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6129Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6110Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6102Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6128Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6134Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6122Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6114Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6126Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6155Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6150Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6123Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6133Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6133Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6114Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6136Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6149Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6157Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6156Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6165Epoch 3/15: [================              ] 40/75 batches, loss: 0.6139Epoch 3/15: [================              ] 41/75 batches, loss: 0.6115Epoch 3/15: [================              ] 42/75 batches, loss: 0.6103Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6114Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6106Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6082Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6070Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6066Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6062Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6077Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6091Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6075Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6079Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6084Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6074Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6075Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6088Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6086Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6081Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6079Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6079Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6069Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6058Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6063Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6067Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6056Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6041Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6042Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6027Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6024Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6008Epoch 3/15: [============================  ] 71/75 batches, loss: 0.5998Epoch 3/15: [============================  ] 72/75 batches, loss: 0.5998Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6013Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6012Epoch 3/15: [==============================] 75/75 batches, loss: 0.6004
[2025-05-03 16:31:22,121][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6004
[2025-05-03 16:31:22,570][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.5944, Metrics: {'accuracy': 0.8194444444444444, 'f1': 0.7868852459016393, 'precision': 0.96, 'recall': 0.6666666666666666}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.5890Epoch 4/15: [                              ] 2/75 batches, loss: 0.5762Epoch 4/15: [=                             ] 3/75 batches, loss: 0.5643Epoch 4/15: [=                             ] 4/75 batches, loss: 0.5786Epoch 4/15: [==                            ] 5/75 batches, loss: 0.5743Epoch 4/15: [==                            ] 6/75 batches, loss: 0.5775Epoch 4/15: [==                            ] 7/75 batches, loss: 0.5735Epoch 4/15: [===                           ] 8/75 batches, loss: 0.5715Epoch 4/15: [===                           ] 9/75 batches, loss: 0.5728Epoch 4/15: [====                          ] 10/75 batches, loss: 0.5839Epoch 4/15: [====                          ] 11/75 batches, loss: 0.5819Epoch 4/15: [====                          ] 12/75 batches, loss: 0.5699Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.5764Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.5801Epoch 4/15: [======                        ] 15/75 batches, loss: 0.5847Epoch 4/15: [======                        ] 16/75 batches, loss: 0.5784Epoch 4/15: [======                        ] 17/75 batches, loss: 0.5797Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.5796Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.5819Epoch 4/15: [========                      ] 20/75 batches, loss: 0.5846Epoch 4/15: [========                      ] 21/75 batches, loss: 0.5831Epoch 4/15: [========                      ] 22/75 batches, loss: 0.5822Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.5901Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.5873Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.5852Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.5848Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.5836Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.5831Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.5836Epoch 4/15: [============                  ] 30/75 batches, loss: 0.5841Epoch 4/15: [============                  ] 31/75 batches, loss: 0.5845Epoch 4/15: [============                  ] 32/75 batches, loss: 0.5836Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.5855Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.5849Epoch 4/15: [==============                ] 35/75 batches, loss: 0.5843Epoch 4/15: [==============                ] 36/75 batches, loss: 0.5842Epoch 4/15: [==============                ] 37/75 batches, loss: 0.5817Epoch 4/15: [===============               ] 38/75 batches, loss: 0.5795Epoch 4/15: [===============               ] 39/75 batches, loss: 0.5812Epoch 4/15: [================              ] 40/75 batches, loss: 0.5823Epoch 4/15: [================              ] 41/75 batches, loss: 0.5812Epoch 4/15: [================              ] 42/75 batches, loss: 0.5808Epoch 4/15: [=================             ] 43/75 batches, loss: 0.5788Epoch 4/15: [=================             ] 44/75 batches, loss: 0.5765Epoch 4/15: [==================            ] 45/75 batches, loss: 0.5741Epoch 4/15: [==================            ] 46/75 batches, loss: 0.5745Epoch 4/15: [==================            ] 47/75 batches, loss: 0.5742Epoch 4/15: [===================           ] 48/75 batches, loss: 0.5762Epoch 4/15: [===================           ] 49/75 batches, loss: 0.5776Epoch 4/15: [====================          ] 50/75 batches, loss: 0.5787Epoch 4/15: [====================          ] 51/75 batches, loss: 0.5778Epoch 4/15: [====================          ] 52/75 batches, loss: 0.5780Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.5768Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.5772Epoch 4/15: [======================        ] 55/75 batches, loss: 0.5784Epoch 4/15: [======================        ] 56/75 batches, loss: 0.5791Epoch 4/15: [======================        ] 57/75 batches, loss: 0.5790Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.5783Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.5788Epoch 4/15: [========================      ] 60/75 batches, loss: 0.5786Epoch 4/15: [========================      ] 61/75 batches, loss: 0.5784Epoch 4/15: [========================      ] 62/75 batches, loss: 0.5781Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.5775Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.5772Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.5758Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.5771Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.5768Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.5762Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.5751Epoch 4/15: [============================  ] 70/75 batches, loss: 0.5749Epoch 4/15: [============================  ] 71/75 batches, loss: 0.5749Epoch 4/15: [============================  ] 72/75 batches, loss: 0.5751Epoch 4/15: [============================= ] 73/75 batches, loss: 0.5735Epoch 4/15: [============================= ] 74/75 batches, loss: 0.5728Epoch 4/15: [==============================] 75/75 batches, loss: 0.5729
[2025-05-03 16:31:25,383][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5729
[2025-05-03 16:31:25,883][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5776, Metrics: {'accuracy': 0.8472222222222222, 'f1': 0.819672131147541, 'precision': 1.0, 'recall': 0.6944444444444444}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.6446Epoch 5/15: [                              ] 2/75 batches, loss: 0.6082Epoch 5/15: [=                             ] 3/75 batches, loss: 0.6037Epoch 5/15: [=                             ] 4/75 batches, loss: 0.5729Epoch 5/15: [==                            ] 5/75 batches, loss: 0.5700Epoch 5/15: [==                            ] 6/75 batches, loss: 0.5678Epoch 5/15: [==                            ] 7/75 batches, loss: 0.5551Epoch 5/15: [===                           ] 8/75 batches, loss: 0.5565Epoch 5/15: [===                           ] 9/75 batches, loss: 0.5469Epoch 5/15: [====                          ] 10/75 batches, loss: 0.5499Epoch 5/15: [====                          ] 11/75 batches, loss: 0.5405Epoch 5/15: [====                          ] 12/75 batches, loss: 0.5474Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.5455Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.5446Epoch 5/15: [======                        ] 15/75 batches, loss: 0.5482Epoch 5/15: [======                        ] 16/75 batches, loss: 0.5464Epoch 5/15: [======                        ] 17/75 batches, loss: 0.5438Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.5413Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.5398Epoch 5/15: [========                      ] 20/75 batches, loss: 0.5376Epoch 5/15: [========                      ] 21/75 batches, loss: 0.5385Epoch 5/15: [========                      ] 22/75 batches, loss: 0.5408Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.5421Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.5431Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.5452Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.5438Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.5447Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.5438Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.5447Epoch 5/15: [============                  ] 30/75 batches, loss: 0.5440Epoch 5/15: [============                  ] 31/75 batches, loss: 0.5433Epoch 5/15: [============                  ] 32/75 batches, loss: 0.5451Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.5467Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.5462Epoch 5/15: [==============                ] 35/75 batches, loss: 0.5468Epoch 5/15: [==============                ] 36/75 batches, loss: 0.5470Epoch 5/15: [==============                ] 37/75 batches, loss: 0.5449Epoch 5/15: [===============               ] 38/75 batches, loss: 0.5449Epoch 5/15: [===============               ] 39/75 batches, loss: 0.5453Epoch 5/15: [================              ] 40/75 batches, loss: 0.5440Epoch 5/15: [================              ] 41/75 batches, loss: 0.5420Epoch 5/15: [================              ] 42/75 batches, loss: 0.5418Epoch 5/15: [=================             ] 43/75 batches, loss: 0.5425Epoch 5/15: [=================             ] 44/75 batches, loss: 0.5440Epoch 5/15: [==================            ] 45/75 batches, loss: 0.5450Epoch 5/15: [==================            ] 46/75 batches, loss: 0.5454Epoch 5/15: [==================            ] 47/75 batches, loss: 0.5462Epoch 5/15: [===================           ] 48/75 batches, loss: 0.5451Epoch 5/15: [===================           ] 49/75 batches, loss: 0.5479Epoch 5/15: [====================          ] 50/75 batches, loss: 0.5491Epoch 5/15: [====================          ] 51/75 batches, loss: 0.5491Epoch 5/15: [====================          ] 52/75 batches, loss: 0.5501Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.5508Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.5527Epoch 5/15: [======================        ] 55/75 batches, loss: 0.5526Epoch 5/15: [======================        ] 56/75 batches, loss: 0.5526Epoch 5/15: [======================        ] 57/75 batches, loss: 0.5522Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.5518Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.5520Epoch 5/15: [========================      ] 60/75 batches, loss: 0.5533Epoch 5/15: [========================      ] 61/75 batches, loss: 0.5529Epoch 5/15: [========================      ] 62/75 batches, loss: 0.5557Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.5578Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.5580Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.5562Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.5573Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.5562Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.5566Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.5576Epoch 5/15: [============================  ] 70/75 batches, loss: 0.5593Epoch 5/15: [============================  ] 71/75 batches, loss: 0.5605Epoch 5/15: [============================  ] 72/75 batches, loss: 0.5615Epoch 5/15: [============================= ] 73/75 batches, loss: 0.5614Epoch 5/15: [============================= ] 74/75 batches, loss: 0.5617Epoch 5/15: [==============================] 75/75 batches, loss: 0.5631
[2025-05-03 16:31:28,764][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5631
[2025-05-03 16:31:29,230][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5749, Metrics: {'accuracy': 0.8194444444444444, 'f1': 0.7796610169491526, 'precision': 1.0, 'recall': 0.6388888888888888}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.6034Epoch 6/15: [                              ] 2/75 batches, loss: 0.5597Epoch 6/15: [=                             ] 3/75 batches, loss: 0.5744Epoch 6/15: [=                             ] 4/75 batches, loss: 0.5536Epoch 6/15: [==                            ] 5/75 batches, loss: 0.5554Epoch 6/15: [==                            ] 6/75 batches, loss: 0.5533Epoch 6/15: [==                            ] 7/75 batches, loss: 0.5620Epoch 6/15: [===                           ] 8/75 batches, loss: 0.5651Epoch 6/15: [===                           ] 9/75 batches, loss: 0.5652Epoch 6/15: [====                          ] 10/75 batches, loss: 0.5650Epoch 6/15: [====                          ] 11/75 batches, loss: 0.5693Epoch 6/15: [====                          ] 12/75 batches, loss: 0.5728Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.5685Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.5704Epoch 6/15: [======                        ] 15/75 batches, loss: 0.5695Epoch 6/15: [======                        ] 16/75 batches, loss: 0.5724Epoch 6/15: [======                        ] 17/75 batches, loss: 0.5681Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.5704Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.5723Epoch 6/15: [========                      ] 20/75 batches, loss: 0.5736Epoch 6/15: [========                      ] 21/75 batches, loss: 0.5692Epoch 6/15: [========                      ] 22/75 batches, loss: 0.5673Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.5677Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.5674Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.5706Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.5702Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.5703Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.5691Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.5674Epoch 6/15: [============                  ] 30/75 batches, loss: 0.5653Epoch 6/15: [============                  ] 31/75 batches, loss: 0.5668Epoch 6/15: [============                  ] 32/75 batches, loss: 0.5673Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.5675Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.5662Epoch 6/15: [==============                ] 35/75 batches, loss: 0.5626Epoch 6/15: [==============                ] 36/75 batches, loss: 0.5590Epoch 6/15: [==============                ] 37/75 batches, loss: 0.5614Epoch 6/15: [===============               ] 38/75 batches, loss: 0.5592Epoch 6/15: [===============               ] 39/75 batches, loss: 0.5582Epoch 6/15: [================              ] 40/75 batches, loss: 0.5575Epoch 6/15: [================              ] 41/75 batches, loss: 0.5570Epoch 6/15: [================              ] 42/75 batches, loss: 0.5560Epoch 6/15: [=================             ] 43/75 batches, loss: 0.5552Epoch 6/15: [=================             ] 44/75 batches, loss: 0.5574Epoch 6/15: [==================            ] 45/75 batches, loss: 0.5574Epoch 6/15: [==================            ] 46/75 batches, loss: 0.5572Epoch 6/15: [==================            ] 47/75 batches, loss: 0.5567Epoch 6/15: [===================           ] 48/75 batches, loss: 0.5571Epoch 6/15: [===================           ] 49/75 batches, loss: 0.5549Epoch 6/15: [====================          ] 50/75 batches, loss: 0.5554Epoch 6/15: [====================          ] 51/75 batches, loss: 0.5547Epoch 6/15: [====================          ] 52/75 batches, loss: 0.5556Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.5556Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.5551Epoch 6/15: [======================        ] 55/75 batches, loss: 0.5551Epoch 6/15: [======================        ] 56/75 batches, loss: 0.5548Epoch 6/15: [======================        ] 57/75 batches, loss: 0.5543Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.5555Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.5576Epoch 6/15: [========================      ] 60/75 batches, loss: 0.5565Epoch 6/15: [========================      ] 61/75 batches, loss: 0.5561Epoch 6/15: [========================      ] 62/75 batches, loss: 0.5548Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.5560Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.5544Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.5547Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.5547Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.5546Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.5551Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.5541Epoch 6/15: [============================  ] 70/75 batches, loss: 0.5540Epoch 6/15: [============================  ] 71/75 batches, loss: 0.5530Epoch 6/15: [============================  ] 72/75 batches, loss: 0.5524Epoch 6/15: [============================= ] 73/75 batches, loss: 0.5522Epoch 6/15: [============================= ] 74/75 batches, loss: 0.5534Epoch 6/15: [==============================] 75/75 batches, loss: 0.5541
[2025-05-03 16:31:31,983][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5541
[2025-05-03 16:31:32,344][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5700, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8, 'precision': 1.0, 'recall': 0.6666666666666666}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.5746Epoch 7/15: [                              ] 2/75 batches, loss: 0.5425Epoch 7/15: [=                             ] 3/75 batches, loss: 0.5389Epoch 7/15: [=                             ] 4/75 batches, loss: 0.5310Epoch 7/15: [==                            ] 5/75 batches, loss: 0.5237Epoch 7/15: [==                            ] 6/75 batches, loss: 0.5240Epoch 7/15: [==                            ] 7/75 batches, loss: 0.5288Epoch 7/15: [===                           ] 8/75 batches, loss: 0.5337Epoch 7/15: [===                           ] 9/75 batches, loss: 0.5347Epoch 7/15: [====                          ] 10/75 batches, loss: 0.5364Epoch 7/15: [====                          ] 11/75 batches, loss: 0.5451Epoch 7/15: [====                          ] 12/75 batches, loss: 0.5468Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.5411Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.5464Epoch 7/15: [======                        ] 15/75 batches, loss: 0.5440Epoch 7/15: [======                        ] 16/75 batches, loss: 0.5450Epoch 7/15: [======                        ] 17/75 batches, loss: 0.5456Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.5406Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.5388Epoch 7/15: [========                      ] 20/75 batches, loss: 0.5425Epoch 7/15: [========                      ] 21/75 batches, loss: 0.5420Epoch 7/15: [========                      ] 22/75 batches, loss: 0.5476Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.5460Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.5474Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.5455Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.5465Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.5465Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.5465Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.5471Epoch 7/15: [============                  ] 30/75 batches, loss: 0.5434Epoch 7/15: [============                  ] 31/75 batches, loss: 0.5444Epoch 7/15: [============                  ] 32/75 batches, loss: 0.5437Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.5415Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.5406Epoch 7/15: [==============                ] 35/75 batches, loss: 0.5407Epoch 7/15: [==============                ] 36/75 batches, loss: 0.5413Epoch 7/15: [==============                ] 37/75 batches, loss: 0.5452Epoch 7/15: [===============               ] 38/75 batches, loss: 0.5431Epoch 7/15: [===============               ] 39/75 batches, loss: 0.5432Epoch 7/15: [================              ] 40/75 batches, loss: 0.5441Epoch 7/15: [================              ] 41/75 batches, loss: 0.5425Epoch 7/15: [================              ] 42/75 batches, loss: 0.5433Epoch 7/15: [=================             ] 43/75 batches, loss: 0.5412Epoch 7/15: [=================             ] 44/75 batches, loss: 0.5405Epoch 7/15: [==================            ] 45/75 batches, loss: 0.5399Epoch 7/15: [==================            ] 46/75 batches, loss: 0.5404Epoch 7/15: [==================            ] 47/75 batches, loss: 0.5377Epoch 7/15: [===================           ] 48/75 batches, loss: 0.5376Epoch 7/15: [===================           ] 49/75 batches, loss: 0.5381Epoch 7/15: [====================          ] 50/75 batches, loss: 0.5384Epoch 7/15: [====================          ] 51/75 batches, loss: 0.5383Epoch 7/15: [====================          ] 52/75 batches, loss: 0.5395Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.5394Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.5403Epoch 7/15: [======================        ] 55/75 batches, loss: 0.5397Epoch 7/15: [======================        ] 56/75 batches, loss: 0.5406Epoch 7/15: [======================        ] 57/75 batches, loss: 0.5414Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.5412Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.5396Epoch 7/15: [========================      ] 60/75 batches, loss: 0.5403Epoch 7/15: [========================      ] 61/75 batches, loss: 0.5405Epoch 7/15: [========================      ] 62/75 batches, loss: 0.5417Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.5424Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.5435Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.5445Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.5450Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.5449Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.5448Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.5447Epoch 7/15: [============================  ] 70/75 batches, loss: 0.5448Epoch 7/15: [============================  ] 71/75 batches, loss: 0.5461Epoch 7/15: [============================  ] 72/75 batches, loss: 0.5467Epoch 7/15: [============================= ] 73/75 batches, loss: 0.5478Epoch 7/15: [============================= ] 74/75 batches, loss: 0.5478Epoch 7/15: [==============================] 75/75 batches, loss: 0.5479
[2025-05-03 16:31:35,281][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5479
[2025-05-03 16:31:35,727][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5665, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.8387096774193549, 'precision': 1.0, 'recall': 0.7222222222222222}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.6095Epoch 8/15: [                              ] 2/75 batches, loss: 0.5935Epoch 8/15: [=                             ] 3/75 batches, loss: 0.5874Epoch 8/15: [=                             ] 4/75 batches, loss: 0.5665Epoch 8/15: [==                            ] 5/75 batches, loss: 0.5787Epoch 8/15: [==                            ] 6/75 batches, loss: 0.5700Epoch 8/15: [==                            ] 7/75 batches, loss: 0.5836Epoch 8/15: [===                           ] 8/75 batches, loss: 0.5905Epoch 8/15: [===                           ] 9/75 batches, loss: 0.5851Epoch 8/15: [====                          ] 10/75 batches, loss: 0.5807Epoch 8/15: [====                          ] 11/75 batches, loss: 0.5881Epoch 8/15: [====                          ] 12/75 batches, loss: 0.5850Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.5826Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.5813Epoch 8/15: [======                        ] 15/75 batches, loss: 0.5778Epoch 8/15: [======                        ] 16/75 batches, loss: 0.5763Epoch 8/15: [======                        ] 17/75 batches, loss: 0.5791Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.5752Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.5711Epoch 8/15: [========                      ] 20/75 batches, loss: 0.5674Epoch 8/15: [========                      ] 21/75 batches, loss: 0.5663Epoch 8/15: [========                      ] 22/75 batches, loss: 0.5653Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.5630Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.5624Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.5569Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.5563Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.5559Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.5529Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.5512Epoch 8/15: [============                  ] 30/75 batches, loss: 0.5504Epoch 8/15: [============                  ] 31/75 batches, loss: 0.5513Epoch 8/15: [============                  ] 32/75 batches, loss: 0.5535Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.5500Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.5509Epoch 8/15: [==============                ] 35/75 batches, loss: 0.5489Epoch 8/15: [==============                ] 36/75 batches, loss: 0.5486Epoch 8/15: [==============                ] 37/75 batches, loss: 0.5482Epoch 8/15: [===============               ] 38/75 batches, loss: 0.5491Epoch 8/15: [===============               ] 39/75 batches, loss: 0.5495Epoch 8/15: [================              ] 40/75 batches, loss: 0.5508Epoch 8/15: [================              ] 41/75 batches, loss: 0.5483Epoch 8/15: [================              ] 42/75 batches, loss: 0.5487Epoch 8/15: [=================             ] 43/75 batches, loss: 0.5491Epoch 8/15: [=================             ] 44/75 batches, loss: 0.5475Epoch 8/15: [==================            ] 45/75 batches, loss: 0.5469Epoch 8/15: [==================            ] 46/75 batches, loss: 0.5445Epoch 8/15: [==================            ] 47/75 batches, loss: 0.5440Epoch 8/15: [===================           ] 48/75 batches, loss: 0.5450Epoch 8/15: [===================           ] 49/75 batches, loss: 0.5439Epoch 8/15: [====================          ] 50/75 batches, loss: 0.5438Epoch 8/15: [====================          ] 51/75 batches, loss: 0.5436Epoch 8/15: [====================          ] 52/75 batches, loss: 0.5422Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.5442Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.5436Epoch 8/15: [======================        ] 55/75 batches, loss: 0.5418Epoch 8/15: [======================        ] 56/75 batches, loss: 0.5422Epoch 8/15: [======================        ] 57/75 batches, loss: 0.5424Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.5429Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.5432Epoch 8/15: [========================      ] 60/75 batches, loss: 0.5426Epoch 8/15: [========================      ] 61/75 batches, loss: 0.5407Epoch 8/15: [========================      ] 62/75 batches, loss: 0.5402Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.5419Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.5414Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.5422Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.5422Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.5419Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.5418Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.5439Epoch 8/15: [============================  ] 70/75 batches, loss: 0.5434Epoch 8/15: [============================  ] 71/75 batches, loss: 0.5426Epoch 8/15: [============================  ] 72/75 batches, loss: 0.5433Epoch 8/15: [============================= ] 73/75 batches, loss: 0.5449Epoch 8/15: [============================= ] 74/75 batches, loss: 0.5450Epoch 8/15: [==============================] 75/75 batches, loss: 0.5446
[2025-05-03 16:31:38,555][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5446
[2025-05-03 16:31:38,988][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5664, Metrics: {'accuracy': 0.8472222222222222, 'f1': 0.819672131147541, 'precision': 1.0, 'recall': 0.6944444444444444}
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.5175Epoch 9/15: [                              ] 2/75 batches, loss: 0.5264Epoch 9/15: [=                             ] 3/75 batches, loss: 0.5474Epoch 9/15: [=                             ] 4/75 batches, loss: 0.5459Epoch 9/15: [==                            ] 5/75 batches, loss: 0.5354Epoch 9/15: [==                            ] 6/75 batches, loss: 0.5479Epoch 9/15: [==                            ] 7/75 batches, loss: 0.5499Epoch 9/15: [===                           ] 8/75 batches, loss: 0.5494Epoch 9/15: [===                           ] 9/75 batches, loss: 0.5419Epoch 9/15: [====                          ] 10/75 batches, loss: 0.5317Epoch 9/15: [====                          ] 11/75 batches, loss: 0.5258Epoch 9/15: [====                          ] 12/75 batches, loss: 0.5256Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.5321Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.5313Epoch 9/15: [======                        ] 15/75 batches, loss: 0.5316Epoch 9/15: [======                        ] 16/75 batches, loss: 0.5342Epoch 9/15: [======                        ] 17/75 batches, loss: 0.5326Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.5332Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.5320Epoch 9/15: [========                      ] 20/75 batches, loss: 0.5404Epoch 9/15: [========                      ] 21/75 batches, loss: 0.5412Epoch 9/15: [========                      ] 22/75 batches, loss: 0.5409Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.5425Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.5420Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.5429Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.5439Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.5463Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.5452Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.5469Epoch 9/15: [============                  ] 30/75 batches, loss: 0.5484Epoch 9/15: [============                  ] 31/75 batches, loss: 0.5483Epoch 9/15: [============                  ] 32/75 batches, loss: 0.5473Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.5474Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.5451Epoch 9/15: [==============                ] 35/75 batches, loss: 0.5435Epoch 9/15: [==============                ] 36/75 batches, loss: 0.5444Epoch 9/15: [==============                ] 37/75 batches, loss: 0.5447Epoch 9/15: [===============               ] 38/75 batches, loss: 0.5431Epoch 9/15: [===============               ] 39/75 batches, loss: 0.5428Epoch 9/15: [================              ] 40/75 batches, loss: 0.5421Epoch 9/15: [================              ] 41/75 batches, loss: 0.5423Epoch 9/15: [================              ] 42/75 batches, loss: 0.5416Epoch 9/15: [=================             ] 43/75 batches, loss: 0.5403Epoch 9/15: [=================             ] 44/75 batches, loss: 0.5417Epoch 9/15: [==================            ] 45/75 batches, loss: 0.5410Epoch 9/15: [==================            ] 46/75 batches, loss: 0.5413Epoch 9/15: [==================            ] 47/75 batches, loss: 0.5426Epoch 9/15: [===================           ] 48/75 batches, loss: 0.5412Epoch 9/15: [===================           ] 49/75 batches, loss: 0.5410Epoch 9/15: [====================          ] 50/75 batches, loss: 0.5399Epoch 9/15: [====================          ] 51/75 batches, loss: 0.5413Epoch 9/15: [====================          ] 52/75 batches, loss: 0.5401Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.5387Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.5395Epoch 9/15: [======================        ] 55/75 batches, loss: 0.5395Epoch 9/15: [======================        ] 56/75 batches, loss: 0.5397Epoch 9/15: [======================        ] 57/75 batches, loss: 0.5413Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.5402Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.5404Epoch 9/15: [========================      ] 60/75 batches, loss: 0.5400Epoch 9/15: [========================      ] 61/75 batches, loss: 0.5404Epoch 9/15: [========================      ] 62/75 batches, loss: 0.5398Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.5401Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.5406Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.5405Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.5409Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.5415Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.5414Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.5405Epoch 9/15: [============================  ] 70/75 batches, loss: 0.5409Epoch 9/15: [============================  ] 71/75 batches, loss: 0.5410Epoch 9/15: [============================  ] 72/75 batches, loss: 0.5411Epoch 9/15: [============================= ] 73/75 batches, loss: 0.5410Epoch 9/15: [============================= ] 74/75 batches, loss: 0.5407Epoch 9/15: [==============================] 75/75 batches, loss: 0.5414
[2025-05-03 16:31:41,757][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5414
[2025-05-03 16:31:42,203][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.5602, Metrics: {'accuracy': 0.875, 'f1': 0.8615384615384616, 'precision': 0.9655172413793104, 'recall': 0.7777777777777778}
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.5156Epoch 10/15: [                              ] 2/75 batches, loss: 0.4952Epoch 10/15: [=                             ] 3/75 batches, loss: 0.5202Epoch 10/15: [=                             ] 4/75 batches, loss: 0.5413Epoch 10/15: [==                            ] 5/75 batches, loss: 0.5556Epoch 10/15: [==                            ] 6/75 batches, loss: 0.5580Epoch 10/15: [==                            ] 7/75 batches, loss: 0.5615Epoch 10/15: [===                           ] 8/75 batches, loss: 0.5600Epoch 10/15: [===                           ] 9/75 batches, loss: 0.5618Epoch 10/15: [====                          ] 10/75 batches, loss: 0.5559Epoch 10/15: [====                          ] 11/75 batches, loss: 0.5500Epoch 10/15: [====                          ] 12/75 batches, loss: 0.5387Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.5343Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.5338Epoch 10/15: [======                        ] 15/75 batches, loss: 0.5303Epoch 10/15: [======                        ] 16/75 batches, loss: 0.5290Epoch 10/15: [======                        ] 17/75 batches, loss: 0.5294Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.5290Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.5300Epoch 10/15: [========                      ] 20/75 batches, loss: 0.5275Epoch 10/15: [========                      ] 21/75 batches, loss: 0.5265Epoch 10/15: [========                      ] 22/75 batches, loss: 0.5229Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.5278Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.5249Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.5241Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.5256Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.5279Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.5326Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.5354Epoch 10/15: [============                  ] 30/75 batches, loss: 0.5372Epoch 10/15: [============                  ] 31/75 batches, loss: 0.5373Epoch 10/15: [============                  ] 32/75 batches, loss: 0.5403Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.5436Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.5406Epoch 10/15: [==============                ] 35/75 batches, loss: 0.5410Epoch 10/15: [==============                ] 36/75 batches, loss: 0.5397Epoch 10/15: [==============                ] 37/75 batches, loss: 0.5398Epoch 10/15: [===============               ] 38/75 batches, loss: 0.5372Epoch 10/15: [===============               ] 39/75 batches, loss: 0.5384Epoch 10/15: [================              ] 40/75 batches, loss: 0.5394Epoch 10/15: [================              ] 41/75 batches, loss: 0.5379Epoch 10/15: [================              ] 42/75 batches, loss: 0.5381Epoch 10/15: [=================             ] 43/75 batches, loss: 0.5385Epoch 10/15: [=================             ] 44/75 batches, loss: 0.5408Epoch 10/15: [==================            ] 45/75 batches, loss: 0.5395Epoch 10/15: [==================            ] 46/75 batches, loss: 0.5398Epoch 10/15: [==================            ] 47/75 batches, loss: 0.5384Epoch 10/15: [===================           ] 48/75 batches, loss: 0.5391Epoch 10/15: [===================           ] 49/75 batches, loss: 0.5367Epoch 10/15: [====================          ] 50/75 batches, loss: 0.5380Epoch 10/15: [====================          ] 51/75 batches, loss: 0.5405Epoch 10/15: [====================          ] 52/75 batches, loss: 0.5428Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.5426Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.5429Epoch 10/15: [======================        ] 55/75 batches, loss: 0.5413Epoch 10/15: [======================        ] 56/75 batches, loss: 0.5411Epoch 10/15: [======================        ] 57/75 batches, loss: 0.5403Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.5400Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.5388Epoch 10/15: [========================      ] 60/75 batches, loss: 0.5394Epoch 10/15: [========================      ] 61/75 batches, loss: 0.5392Epoch 10/15: [========================      ] 62/75 batches, loss: 0.5393Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.5408Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.5405Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.5416Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.5403Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.5413Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.5451Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.5443Epoch 10/15: [============================  ] 70/75 batches, loss: 0.5439Epoch 10/15: [============================  ] 71/75 batches, loss: 0.5440Epoch 10/15: [============================  ] 72/75 batches, loss: 0.5442Epoch 10/15: [============================= ] 73/75 batches, loss: 0.5446Epoch 10/15: [============================= ] 74/75 batches, loss: 0.5447Epoch 10/15: [==============================] 75/75 batches, loss: 0.5454
[2025-05-03 16:31:45,024][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5454
[2025-05-03 16:31:45,454][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.5779, Metrics: {'accuracy': 0.8055555555555556, 'f1': 0.7586206896551724, 'precision': 1.0, 'recall': 0.6111111111111112}
[2025-05-03 16:31:45,455][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.5201Epoch 11/15: [                              ] 2/75 batches, loss: 0.5646Epoch 11/15: [=                             ] 3/75 batches, loss: 0.5580Epoch 11/15: [=                             ] 4/75 batches, loss: 0.5603Epoch 11/15: [==                            ] 5/75 batches, loss: 0.5557Epoch 11/15: [==                            ] 6/75 batches, loss: 0.5555Epoch 11/15: [==                            ] 7/75 batches, loss: 0.5546Epoch 11/15: [===                           ] 8/75 batches, loss: 0.5487Epoch 11/15: [===                           ] 9/75 batches, loss: 0.5551Epoch 11/15: [====                          ] 10/75 batches, loss: 0.5538Epoch 11/15: [====                          ] 11/75 batches, loss: 0.5436Epoch 11/15: [====                          ] 12/75 batches, loss: 0.5440Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.5413Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.5418Epoch 11/15: [======                        ] 15/75 batches, loss: 0.5406Epoch 11/15: [======                        ] 16/75 batches, loss: 0.5366Epoch 11/15: [======                        ] 17/75 batches, loss: 0.5368Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.5432Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.5437Epoch 11/15: [========                      ] 20/75 batches, loss: 0.5419Epoch 11/15: [========                      ] 21/75 batches, loss: 0.5444Epoch 11/15: [========                      ] 22/75 batches, loss: 0.5445Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.5433Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.5440Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.5450Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.5423Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.5401Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.5380Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.5404Epoch 11/15: [============                  ] 30/75 batches, loss: 0.5405Epoch 11/15: [============                  ] 31/75 batches, loss: 0.5388Epoch 11/15: [============                  ] 32/75 batches, loss: 0.5360Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.5365Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.5396Epoch 11/15: [==============                ] 35/75 batches, loss: 0.5391Epoch 11/15: [==============                ] 36/75 batches, loss: 0.5397Epoch 11/15: [==============                ] 37/75 batches, loss: 0.5414Epoch 11/15: [===============               ] 38/75 batches, loss: 0.5406Epoch 11/15: [===============               ] 39/75 batches, loss: 0.5397Epoch 11/15: [================              ] 40/75 batches, loss: 0.5404Epoch 11/15: [================              ] 41/75 batches, loss: 0.5416Epoch 11/15: [================              ] 42/75 batches, loss: 0.5396Epoch 11/15: [=================             ] 43/75 batches, loss: 0.5393Epoch 11/15: [=================             ] 44/75 batches, loss: 0.5370Epoch 11/15: [==================            ] 45/75 batches, loss: 0.5367Epoch 11/15: [==================            ] 46/75 batches, loss: 0.5365Epoch 11/15: [==================            ] 47/75 batches, loss: 0.5350Epoch 11/15: [===================           ] 48/75 batches, loss: 0.5374Epoch 11/15: [===================           ] 49/75 batches, loss: 0.5384Epoch 11/15: [====================          ] 50/75 batches, loss: 0.5387Epoch 11/15: [====================          ] 51/75 batches, loss: 0.5394Epoch 11/15: [====================          ] 52/75 batches, loss: 0.5393Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.5387Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.5400Epoch 11/15: [======================        ] 55/75 batches, loss: 0.5403Epoch 11/15: [======================        ] 56/75 batches, loss: 0.5407Epoch 11/15: [======================        ] 57/75 batches, loss: 0.5407Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.5404Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.5413Epoch 11/15: [========================      ] 60/75 batches, loss: 0.5411Epoch 11/15: [========================      ] 61/75 batches, loss: 0.5404Epoch 11/15: [========================      ] 62/75 batches, loss: 0.5401Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.5385Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.5389Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.5409Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.5404Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.5404Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.5408Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.5413Epoch 11/15: [============================  ] 70/75 batches, loss: 0.5418Epoch 11/15: [============================  ] 71/75 batches, loss: 0.5426Epoch 11/15: [============================  ] 72/75 batches, loss: 0.5431Epoch 11/15: [============================= ] 73/75 batches, loss: 0.5428Epoch 11/15: [============================= ] 74/75 batches, loss: 0.5416Epoch 11/15: [==============================] 75/75 batches, loss: 0.5407
[2025-05-03 16:31:48,099][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5407
[2025-05-03 16:31:48,620][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.5610, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.8387096774193549, 'precision': 1.0, 'recall': 0.7222222222222222}
[2025-05-03 16:31:48,621][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.4871Epoch 12/15: [                              ] 2/75 batches, loss: 0.5446Epoch 12/15: [=                             ] 3/75 batches, loss: 0.5207Epoch 12/15: [=                             ] 4/75 batches, loss: 0.5248Epoch 12/15: [==                            ] 5/75 batches, loss: 0.5313Epoch 12/15: [==                            ] 6/75 batches, loss: 0.5285Epoch 12/15: [==                            ] 7/75 batches, loss: 0.5326Epoch 12/15: [===                           ] 8/75 batches, loss: 0.5381Epoch 12/15: [===                           ] 9/75 batches, loss: 0.5412Epoch 12/15: [====                          ] 10/75 batches, loss: 0.5451Epoch 12/15: [====                          ] 11/75 batches, loss: 0.5481Epoch 12/15: [====                          ] 12/75 batches, loss: 0.5439Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.5399Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.5504Epoch 12/15: [======                        ] 15/75 batches, loss: 0.5402Epoch 12/15: [======                        ] 16/75 batches, loss: 0.5413Epoch 12/15: [======                        ] 17/75 batches, loss: 0.5446Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.5438Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.5429Epoch 12/15: [========                      ] 20/75 batches, loss: 0.5397Epoch 12/15: [========                      ] 21/75 batches, loss: 0.5394Epoch 12/15: [========                      ] 22/75 batches, loss: 0.5379Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.5398Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.5389Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.5359Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.5348Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.5367Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.5346Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.5367Epoch 12/15: [============                  ] 30/75 batches, loss: 0.5340Epoch 12/15: [============                  ] 31/75 batches, loss: 0.5333Epoch 12/15: [============                  ] 32/75 batches, loss: 0.5354Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.5386Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.5429Epoch 12/15: [==============                ] 35/75 batches, loss: 0.5418Epoch 12/15: [==============                ] 36/75 batches, loss: 0.5436Epoch 12/15: [==============                ] 37/75 batches, loss: 0.5428Epoch 12/15: [===============               ] 38/75 batches, loss: 0.5445Epoch 12/15: [===============               ] 39/75 batches, loss: 0.5448Epoch 12/15: [================              ] 40/75 batches, loss: 0.5441Epoch 12/15: [================              ] 41/75 batches, loss: 0.5428Epoch 12/15: [================              ] 42/75 batches, loss: 0.5417Epoch 12/15: [=================             ] 43/75 batches, loss: 0.5403Epoch 12/15: [=================             ] 44/75 batches, loss: 0.5406Epoch 12/15: [==================            ] 45/75 batches, loss: 0.5393Epoch 12/15: [==================            ] 46/75 batches, loss: 0.5376Epoch 12/15: [==================            ] 47/75 batches, loss: 0.5392Epoch 12/15: [===================           ] 48/75 batches, loss: 0.5390Epoch 12/15: [===================           ] 49/75 batches, loss: 0.5411Epoch 12/15: [====================          ] 50/75 batches, loss: 0.5402Epoch 12/15: [====================          ] 51/75 batches, loss: 0.5388Epoch 12/15: [====================          ] 52/75 batches, loss: 0.5395Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.5397Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.5398Epoch 12/15: [======================        ] 55/75 batches, loss: 0.5411Epoch 12/15: [======================        ] 56/75 batches, loss: 0.5417Epoch 12/15: [======================        ] 57/75 batches, loss: 0.5405Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.5394Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.5374Epoch 12/15: [========================      ] 60/75 batches, loss: 0.5378Epoch 12/15: [========================      ] 61/75 batches, loss: 0.5377Epoch 12/15: [========================      ] 62/75 batches, loss: 0.5373Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.5361Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.5362Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.5371Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.5375Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.5384Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.5372Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.5376Epoch 12/15: [============================  ] 70/75 batches, loss: 0.5374Epoch 12/15: [============================  ] 71/75 batches, loss: 0.5370Epoch 12/15: [============================  ] 72/75 batches, loss: 0.5358Epoch 12/15: [============================= ] 73/75 batches, loss: 0.5374Epoch 12/15: [============================= ] 74/75 batches, loss: 0.5376Epoch 12/15: [==============================] 75/75 batches, loss: 0.5382
[2025-05-03 16:31:51,035][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.5382
[2025-05-03 16:31:51,477][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.5593, Metrics: {'accuracy': 0.875, 'f1': 0.8571428571428571, 'precision': 1.0, 'recall': 0.75}
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.5510Epoch 13/15: [                              ] 2/75 batches, loss: 0.5980Epoch 13/15: [=                             ] 3/75 batches, loss: 0.5927Epoch 13/15: [=                             ] 4/75 batches, loss: 0.5915Epoch 13/15: [==                            ] 5/75 batches, loss: 0.5845Epoch 13/15: [==                            ] 6/75 batches, loss: 0.5795Epoch 13/15: [==                            ] 7/75 batches, loss: 0.5786Epoch 13/15: [===                           ] 8/75 batches, loss: 0.5578Epoch 13/15: [===                           ] 9/75 batches, loss: 0.5593Epoch 13/15: [====                          ] 10/75 batches, loss: 0.5590Epoch 13/15: [====                          ] 11/75 batches, loss: 0.5546Epoch 13/15: [====                          ] 12/75 batches, loss: 0.5553Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.5564Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.5578Epoch 13/15: [======                        ] 15/75 batches, loss: 0.5560Epoch 13/15: [======                        ] 16/75 batches, loss: 0.5549Epoch 13/15: [======                        ] 17/75 batches, loss: 0.5537Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.5513Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.5517Epoch 13/15: [========                      ] 20/75 batches, loss: 0.5533Epoch 13/15: [========                      ] 21/75 batches, loss: 0.5514Epoch 13/15: [========                      ] 22/75 batches, loss: 0.5508Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.5518Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.5484Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.5463Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.5441Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.5429Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.5424Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.5403Epoch 13/15: [============                  ] 30/75 batches, loss: 0.5406Epoch 13/15: [============                  ] 31/75 batches, loss: 0.5394Epoch 13/15: [============                  ] 32/75 batches, loss: 0.5373Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.5339Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.5364Epoch 13/15: [==============                ] 35/75 batches, loss: 0.5361Epoch 13/15: [==============                ] 36/75 batches, loss: 0.5369Epoch 13/15: [==============                ] 37/75 batches, loss: 0.5379Epoch 13/15: [===============               ] 38/75 batches, loss: 0.5395Epoch 13/15: [===============               ] 39/75 batches, loss: 0.5371Epoch 13/15: [================              ] 40/75 batches, loss: 0.5366Epoch 13/15: [================              ] 41/75 batches, loss: 0.5366Epoch 13/15: [================              ] 42/75 batches, loss: 0.5390Epoch 13/15: [=================             ] 43/75 batches, loss: 0.5405Epoch 13/15: [=================             ] 44/75 batches, loss: 0.5408Epoch 13/15: [==================            ] 45/75 batches, loss: 0.5412Epoch 13/15: [==================            ] 46/75 batches, loss: 0.5420Epoch 13/15: [==================            ] 47/75 batches, loss: 0.5433Epoch 13/15: [===================           ] 48/75 batches, loss: 0.5446Epoch 13/15: [===================           ] 49/75 batches, loss: 0.5440Epoch 13/15: [====================          ] 50/75 batches, loss: 0.5432Epoch 13/15: [====================          ] 51/75 batches, loss: 0.5427Epoch 13/15: [====================          ] 52/75 batches, loss: 0.5407Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.5393Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.5400Epoch 13/15: [======================        ] 55/75 batches, loss: 0.5394Epoch 13/15: [======================        ] 56/75 batches, loss: 0.5388Epoch 13/15: [======================        ] 57/75 batches, loss: 0.5390Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.5399Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.5390Epoch 13/15: [========================      ] 60/75 batches, loss: 0.5408Epoch 13/15: [========================      ] 61/75 batches, loss: 0.5411Epoch 13/15: [========================      ] 62/75 batches, loss: 0.5403Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.5401Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.5406Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.5414Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.5399Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.5400Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.5399Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.5394Epoch 13/15: [============================  ] 70/75 batches, loss: 0.5405Epoch 13/15: [============================  ] 71/75 batches, loss: 0.5396Epoch 13/15: [============================  ] 72/75 batches, loss: 0.5400Epoch 13/15: [============================= ] 73/75 batches, loss: 0.5398Epoch 13/15: [============================= ] 74/75 batches, loss: 0.5396Epoch 13/15: [==============================] 75/75 batches, loss: 0.5397
[2025-05-03 16:31:54,376][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.5397
[2025-05-03 16:31:54,793][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.5575, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8787878787878788, 'precision': 0.9666666666666667, 'recall': 0.8055555555555556}
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.5591Epoch 14/15: [                              ] 2/75 batches, loss: 0.5105Epoch 14/15: [=                             ] 3/75 batches, loss: 0.4768Epoch 14/15: [=                             ] 4/75 batches, loss: 0.5147Epoch 14/15: [==                            ] 5/75 batches, loss: 0.5045Epoch 14/15: [==                            ] 6/75 batches, loss: 0.4927Epoch 14/15: [==                            ] 7/75 batches, loss: 0.4998Epoch 14/15: [===                           ] 8/75 batches, loss: 0.5006Epoch 14/15: [===                           ] 9/75 batches, loss: 0.5012Epoch 14/15: [====                          ] 10/75 batches, loss: 0.5044Epoch 14/15: [====                          ] 11/75 batches, loss: 0.5068Epoch 14/15: [====                          ] 12/75 batches, loss: 0.5127Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.5154Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.5142Epoch 14/15: [======                        ] 15/75 batches, loss: 0.5129Epoch 14/15: [======                        ] 16/75 batches, loss: 0.5192Epoch 14/15: [======                        ] 17/75 batches, loss: 0.5187Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.5200Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.5218Epoch 14/15: [========                      ] 20/75 batches, loss: 0.5275Epoch 14/15: [========                      ] 21/75 batches, loss: 0.5267Epoch 14/15: [========                      ] 22/75 batches, loss: 0.5246Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.5241Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.5259Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.5272Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.5282Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.5294Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.5325Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.5318Epoch 14/15: [============                  ] 30/75 batches, loss: 0.5331Epoch 14/15: [============                  ] 31/75 batches, loss: 0.5338Epoch 14/15: [============                  ] 32/75 batches, loss: 0.5363Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.5347Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.5360Epoch 14/15: [==============                ] 35/75 batches, loss: 0.5357Epoch 14/15: [==============                ] 36/75 batches, loss: 0.5349Epoch 14/15: [==============                ] 37/75 batches, loss: 0.5366Epoch 14/15: [===============               ] 38/75 batches, loss: 0.5351Epoch 14/15: [===============               ] 39/75 batches, loss: 0.5343Epoch 14/15: [================              ] 40/75 batches, loss: 0.5325Epoch 14/15: [================              ] 41/75 batches, loss: 0.5304Epoch 14/15: [================              ] 42/75 batches, loss: 0.5287Epoch 14/15: [=================             ] 43/75 batches, loss: 0.5309Epoch 14/15: [=================             ] 44/75 batches, loss: 0.5303Epoch 14/15: [==================            ] 45/75 batches, loss: 0.5297Epoch 14/15: [==================            ] 46/75 batches, loss: 0.5303Epoch 14/15: [==================            ] 47/75 batches, loss: 0.5321Epoch 14/15: [===================           ] 48/75 batches, loss: 0.5321Epoch 14/15: [===================           ] 49/75 batches, loss: 0.5312Epoch 14/15: [====================          ] 50/75 batches, loss: 0.5323Epoch 14/15: [====================          ] 51/75 batches, loss: 0.5328Epoch 14/15: [====================          ] 52/75 batches, loss: 0.5318Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.5336Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.5333Epoch 14/15: [======================        ] 55/75 batches, loss: 0.5320Epoch 14/15: [======================        ] 56/75 batches, loss: 0.5330Epoch 14/15: [======================        ] 57/75 batches, loss: 0.5321Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.5326Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.5320Epoch 14/15: [========================      ] 60/75 batches, loss: 0.5316Epoch 14/15: [========================      ] 61/75 batches, loss: 0.5315Epoch 14/15: [========================      ] 62/75 batches, loss: 0.5320Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.5333Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.5337Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.5335Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.5330Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.5327Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.5314Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.5323Epoch 14/15: [============================  ] 70/75 batches, loss: 0.5310Epoch 14/15: [============================  ] 71/75 batches, loss: 0.5309Epoch 14/15: [============================  ] 72/75 batches, loss: 0.5307Epoch 14/15: [============================= ] 73/75 batches, loss: 0.5305Epoch 14/15: [============================= ] 74/75 batches, loss: 0.5304Epoch 14/15: [==============================] 75/75 batches, loss: 0.5301
[2025-05-03 16:31:57,683][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.5301
[2025-05-03 16:31:58,251][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.5553, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8787878787878788, 'precision': 0.9666666666666667, 'recall': 0.8055555555555556}
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.5712Epoch 15/15: [                              ] 2/75 batches, loss: 0.5276Epoch 15/15: [=                             ] 3/75 batches, loss: 0.5151Epoch 15/15: [=                             ] 4/75 batches, loss: 0.5230Epoch 15/15: [==                            ] 5/75 batches, loss: 0.5238Epoch 15/15: [==                            ] 6/75 batches, loss: 0.5284Epoch 15/15: [==                            ] 7/75 batches, loss: 0.5338Epoch 15/15: [===                           ] 8/75 batches, loss: 0.5334Epoch 15/15: [===                           ] 9/75 batches, loss: 0.5399Epoch 15/15: [====                          ] 10/75 batches, loss: 0.5420Epoch 15/15: [====                          ] 11/75 batches, loss: 0.5358Epoch 15/15: [====                          ] 12/75 batches, loss: 0.5344Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.5323Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.5390Epoch 15/15: [======                        ] 15/75 batches, loss: 0.5335Epoch 15/15: [======                        ] 16/75 batches, loss: 0.5306Epoch 15/15: [======                        ] 17/75 batches, loss: 0.5263Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.5264Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.5284Epoch 15/15: [========                      ] 20/75 batches, loss: 0.5237Epoch 15/15: [========                      ] 21/75 batches, loss: 0.5302Epoch 15/15: [========                      ] 22/75 batches, loss: 0.5312Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.5316Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.5305Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.5284Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.5257Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.5256Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.5251Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.5252Epoch 15/15: [============                  ] 30/75 batches, loss: 0.5264Epoch 15/15: [============                  ] 31/75 batches, loss: 0.5294Epoch 15/15: [============                  ] 32/75 batches, loss: 0.5294Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.5267Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.5261Epoch 15/15: [==============                ] 35/75 batches, loss: 0.5253Epoch 15/15: [==============                ] 36/75 batches, loss: 0.5255Epoch 15/15: [==============                ] 37/75 batches, loss: 0.5255Epoch 15/15: [===============               ] 38/75 batches, loss: 0.5269Epoch 15/15: [===============               ] 39/75 batches, loss: 0.5276Epoch 15/15: [================              ] 40/75 batches, loss: 0.5288Epoch 15/15: [================              ] 41/75 batches, loss: 0.5276Epoch 15/15: [================              ] 42/75 batches, loss: 0.5261Epoch 15/15: [=================             ] 43/75 batches, loss: 0.5275Epoch 15/15: [=================             ] 44/75 batches, loss: 0.5294Epoch 15/15: [==================            ] 45/75 batches, loss: 0.5309Epoch 15/15: [==================            ] 46/75 batches, loss: 0.5315Epoch 15/15: [==================            ] 47/75 batches, loss: 0.5318Epoch 15/15: [===================           ] 48/75 batches, loss: 0.5308Epoch 15/15: [===================           ] 49/75 batches, loss: 0.5310Epoch 15/15: [====================          ] 50/75 batches, loss: 0.5316Epoch 15/15: [====================          ] 51/75 batches, loss: 0.5310Epoch 15/15: [====================          ] 52/75 batches, loss: 0.5316Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.5305Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.5312Epoch 15/15: [======================        ] 55/75 batches, loss: 0.5331Epoch 15/15: [======================        ] 56/75 batches, loss: 0.5335Epoch 15/15: [======================        ] 57/75 batches, loss: 0.5331Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.5326Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.5323Epoch 15/15: [========================      ] 60/75 batches, loss: 0.5313Epoch 15/15: [========================      ] 61/75 batches, loss: 0.5308Epoch 15/15: [========================      ] 62/75 batches, loss: 0.5303Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.5304Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.5308Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.5326Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.5336Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.5339Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.5332Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.5340Epoch 15/15: [============================  ] 70/75 batches, loss: 0.5335Epoch 15/15: [============================  ] 71/75 batches, loss: 0.5323Epoch 15/15: [============================  ] 72/75 batches, loss: 0.5330Epoch 15/15: [============================= ] 73/75 batches, loss: 0.5327Epoch 15/15: [============================= ] 74/75 batches, loss: 0.5317Epoch 15/15: [==============================] 75/75 batches, loss: 0.5304
[2025-05-03 16:32:01,215][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.5304
[2025-05-03 16:32:01,787][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.5597, Metrics: {'accuracy': 0.875, 'f1': 0.8615384615384616, 'precision': 0.9655172413793104, 'recall': 0.7777777777777778}
[2025-05-03 16:32:01,788][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
[2025-05-03 16:32:01,788][src.training.lm_trainer][INFO] - Training completed in 49.09 seconds
[2025-05-03 16:32:01,788][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-03 16:32:05,328][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9690117252931323, 'f1': 0.9690893901420217, 'precision': 0.9666666666666667, 'recall': 0.9715242881072027}
[2025-05-03 16:32:05,329][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8787878787878788, 'precision': 0.9666666666666667, 'recall': 0.8055555555555556}
[2025-05-03 16:32:05,329][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9, 'f1': 0.8910891089108911, 'precision': 0.9782608695652174, 'recall': 0.8181818181818182}
[2025-05-03 16:32:07,003][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/ru/ru/model.pt
[2025-05-03 16:32:07,005][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▃▇▇▇▇▇▇████
wandb:           best_val_f1 ▁▄▇▇▇▇█▇████
wandb:         best_val_loss █▆▃▂▂▂▂▂▁▁▁▁
wandb:    best_val_precision ██▁█████▂█▂▂
wandb:       best_val_recall ▁▃▇▇▆▇▇▇█▇██
wandb:                 epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▃▃▃▃▃▃▃▃▃▃▃▄
wandb:            train_loss █▆▄▃▂▂▂▂▁▂▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▃▇▇▇▇▇▇█▆▇████
wandb:                val_f1 ▁▄▇▇▇▇█▇█▇█████
wandb:              val_loss █▆▃▂▂▂▂▂▁▂▁▁▁▁▁
wandb:         val_precision ██▁█████▂███▂▂▂
wandb:            val_recall ▁▃▇▇▆▇▇▇█▆▇▇███
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.88889
wandb:           best_val_f1 0.87879
wandb:         best_val_loss 0.55529
wandb:    best_val_precision 0.96667
wandb:       best_val_recall 0.80556
wandb:                 epoch 15
wandb:   final_test_accuracy 0.9
wandb:         final_test_f1 0.89109
wandb:  final_test_precision 0.97826
wandb:     final_test_recall 0.81818
wandb:  final_train_accuracy 0.96901
wandb:        final_train_f1 0.96909
wandb: final_train_precision 0.96667
wandb:    final_train_recall 0.97152
wandb:    final_val_accuracy 0.88889
wandb:          final_val_f1 0.87879
wandb:   final_val_precision 0.96667
wandb:      final_val_recall 0.80556
wandb:         learning_rate 0.0001
wandb:            train_loss 0.53036
wandb:            train_time 49.09038
wandb:          val_accuracy 0.875
wandb:                val_f1 0.86154
wandb:              val_loss 0.55968
wandb:         val_precision 0.96552
wandb:            val_recall 0.77778
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_163045-3hfmexh6
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_163045-3hfmexh6/logs
Experiment probe_layer2_question_type_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/ru/ru/results.json for layer 2
Running experiment: probe_layer2_complexity_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_ru"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/layer2/ru"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-03 16:32:54,603][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/ru
experiment_name: probe_layer2_complexity_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-03 16:32:54,603][__main__][INFO] - Normalized task: complexity
[2025-05-03 16:32:54,603][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-03 16:32:54,603][__main__][INFO] - Determined Task Type: regression
[2025-05-03 16:32:54,607][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ru']
[2025-05-03 16:32:54,608][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-03 16:33:01,619][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-03 16:33:04,255][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-03 16:33:04,255][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:33:04,705][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:33:04,911][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:33:05,262][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-03 16:33:05,273][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:33:05,274][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-03 16:33:05,275][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:33:05,387][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:33:05,535][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:33:05,627][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-03 16:33:05,629][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:33:05,629][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-03 16:33:05,630][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:33:05,863][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:33:06,044][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:33:06,081][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-03 16:33:06,082][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:33:06,082][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-03 16:33:06,083][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-03 16:33:06,084][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:33:06,084][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:33:06,084][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:33:06,084][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:33:06,085][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:33:06,085][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-05-03 16:33:06,085][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-03 16:33:06,085][src.data.datasets][INFO] - Sample label: 0.2535911500453949
[2025-05-03 16:33:06,085][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:33:06,085][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:33:06,085][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:33:06,085][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:33:06,086][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:33:06,086][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-05-03 16:33:06,086][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-03 16:33:06,086][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-05-03 16:33:06,086][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:33:06,086][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:33:06,086][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:33:06,086][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:33:06,086][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:33:06,087][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-05-03 16:33:06,087][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-03 16:33:06,087][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-05-03 16:33:06,087][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-03 16:33:06,087][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-03 16:33:06,087][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-03 16:33:06,087][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-03 16:33:06,088][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-03 16:33:17,613][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-03 16:33:17,614][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-03 16:33:17,614][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-03 16:33:17,614][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-03 16:33:17,617][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-03 16:33:17,618][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-03 16:33:17,618][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-03 16:33:17,618][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-03 16:33:17,618][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-03 16:33:17,619][__main__][INFO] - Total parameters: 394,255,105
[2025-05-03 16:33:17,619][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.3108Epoch 1/15: [                              ] 2/75 batches, loss: 0.4121Epoch 1/15: [=                             ] 3/75 batches, loss: 0.3396Epoch 1/15: [=                             ] 4/75 batches, loss: 0.3880Epoch 1/15: [==                            ] 5/75 batches, loss: 0.3905Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3659Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4198Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4138Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4521Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4386Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4244Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4231Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4036Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3916Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3873Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3805Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3700Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3771Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3839Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3779Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3797Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3823Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3733Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3717Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3643Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3612Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3610Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3565Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3577Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3607Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3593Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3535Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3504Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3507Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3472Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3493Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3446Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3444Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3419Epoch 1/15: [================              ] 40/75 batches, loss: 0.3392Epoch 1/15: [================              ] 41/75 batches, loss: 0.3364Epoch 1/15: [================              ] 42/75 batches, loss: 0.3336Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3324Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3289Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3280Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3236Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3216Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3191Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3159Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3144Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3108Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3080Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3054Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3049Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3032Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3040Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3068Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3050Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3029Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3003Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2978Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2970Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2944Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2924Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2906Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2887Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2862Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2829Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2820Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2808Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2785Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2778Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2755Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2742Epoch 1/15: [==============================] 75/75 batches, loss: 0.2723
[2025-05-03 16:33:26,072][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2723
[2025-05-03 16:33:26,454][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0934, Metrics: {'mse': 0.09961844980716705, 'rmse': 0.31562390563321885, 'r2': -1.1414501667022705}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1017Epoch 2/15: [                              ] 2/75 batches, loss: 0.1567Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1298Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1767Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1574Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1528Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1575Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1533Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1470Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1499Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1522Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1521Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1562Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1528Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1478Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1445Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1438Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1488Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1468Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1461Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1462Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1426Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1453Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1449Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1448Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1457Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1462Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1446Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1446Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1409Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1397Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1384Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1411Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1409Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1415Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1399Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1393Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1396Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1402Epoch 2/15: [================              ] 40/75 batches, loss: 0.1385Epoch 2/15: [================              ] 41/75 batches, loss: 0.1377Epoch 2/15: [================              ] 42/75 batches, loss: 0.1386Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1394Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1377Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1371Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1364Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1364Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1357Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1352Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1337Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1339Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1343Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1344Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1337Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1330Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1338Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1331Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1320Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1317Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1307Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1304Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1298Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1286Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1284Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1279Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1271Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1266Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1259Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1252Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1244Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1236Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1231Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1230Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1226Epoch 2/15: [==============================] 75/75 batches, loss: 0.1220
[2025-05-03 16:33:29,418][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1220
[2025-05-03 16:33:30,003][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1159, Metrics: {'mse': 0.12409377098083496, 'rmse': 0.35226945791657127, 'r2': -1.6675844192504883}
[2025-05-03 16:33:30,003][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.0969Epoch 3/15: [                              ] 2/75 batches, loss: 0.1452Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1349Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1236Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1104Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1105Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1133Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1044Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1053Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1053Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1020Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1010Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1016Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1019Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0999Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0967Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0962Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0964Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0950Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0919Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0903Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0900Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0903Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0930Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0934Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0922Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0908Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0896Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0916Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0913Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0921Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0927Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0915Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0908Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0914Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0901Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0898Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0902Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0899Epoch 3/15: [================              ] 40/75 batches, loss: 0.0898Epoch 3/15: [================              ] 41/75 batches, loss: 0.0914Epoch 3/15: [================              ] 42/75 batches, loss: 0.0918Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0913Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0905Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0899Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0906Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0902Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0904Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0902Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0893Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0902Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0892Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0885Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0892Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0884Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0878Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0872Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0874Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0866Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0863Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0855Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0859Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0864Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0864Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0855Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0852Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0853Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0853Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0851Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0852Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0853Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0850Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0846Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0845Epoch 3/15: [==============================] 75/75 batches, loss: 0.0848
[2025-05-03 16:33:32,293][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0848
[2025-05-03 16:33:32,832][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0650, Metrics: {'mse': 0.06826744228601456, 'rmse': 0.2612803901673728, 'r2': -0.4675126075744629}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0403Epoch 4/15: [                              ] 2/75 batches, loss: 0.0602Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0496Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0561Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0568Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0595Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0640Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0644Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0679Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0647Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0656Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0651Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0626Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0638Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0652Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0663Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0642Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0635Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0639Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0644Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0649Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0669Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0681Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0678Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0703Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0713Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0714Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0717Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0727Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0719Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0718Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0706Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0700Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0701Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0701Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0695Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0700Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0717Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0707Epoch 4/15: [================              ] 40/75 batches, loss: 0.0703Epoch 4/15: [================              ] 41/75 batches, loss: 0.0704Epoch 4/15: [================              ] 42/75 batches, loss: 0.0702Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0712Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0709Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0715Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0720Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0720Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0712Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0711Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0705Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0718Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0729Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0728Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0732Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0733Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0732Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0730Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0721Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0721Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0719Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0720Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0715Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0717Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0711Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0713Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0708Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0711Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0711Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0712Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0709Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0704Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0707Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0711Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0715Epoch 4/15: [==============================] 75/75 batches, loss: 0.0724
[2025-05-03 16:33:35,625][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0724
[2025-05-03 16:33:36,306][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0599, Metrics: {'mse': 0.06261800974607468, 'rmse': 0.25023590818680413, 'r2': -0.34606945514678955}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0374Epoch 5/15: [                              ] 2/75 batches, loss: 0.0475Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0815Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0901Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0824Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0767Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0791Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0736Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0734Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0724Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0721Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0730Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0726Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0716Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0709Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0704Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0703Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0702Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0707Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0694Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0690Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0670Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0677Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0677Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0678Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0673Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0668Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0669Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0660Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0654Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0662Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0661Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0659Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0654Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0653Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0649Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0648Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0641Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0646Epoch 5/15: [================              ] 40/75 batches, loss: 0.0641Epoch 5/15: [================              ] 41/75 batches, loss: 0.0646Epoch 5/15: [================              ] 42/75 batches, loss: 0.0645Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0642Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0639Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0638Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0636Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0646Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0641Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0642Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0642Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0637Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0630Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0626Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0621Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0622Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0616Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0619Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0613Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0617Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0617Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0612Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0611Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0606Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0602Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0605Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0608Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0608Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0605Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0602Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0598Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0596Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0601Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0597Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0595Epoch 5/15: [==============================] 75/75 batches, loss: 0.0594
[2025-05-03 16:33:38,965][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0594
[2025-05-03 16:33:39,477][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0595, Metrics: {'mse': 0.062031492590904236, 'rmse': 0.24906122257570373, 'r2': -0.3334614038467407}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0225Epoch 6/15: [                              ] 2/75 batches, loss: 0.0286Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0375Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0358Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0444Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0463Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0472Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0451Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0432Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0432Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0423Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0435Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0424Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0422Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0414Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0430Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0422Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0433Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0440Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0433Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0439Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0461Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0483Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0503Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0510Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0519Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0525Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0523Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0517Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0516Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0518Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0510Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0509Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0515Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0515Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0517Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0520Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0511Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0521Epoch 6/15: [================              ] 40/75 batches, loss: 0.0520Epoch 6/15: [================              ] 41/75 batches, loss: 0.0523Epoch 6/15: [================              ] 42/75 batches, loss: 0.0524Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0534Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0534Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0541Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0540Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0544Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0542Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0550Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0551Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0548Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0551Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0551Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0545Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0538Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0537Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0532Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0531Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0531Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0531Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0533Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0531Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0529Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0528Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0526Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0523Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0521Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0519Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0518Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0518Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0517Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0513Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0510Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0508Epoch 6/15: [==============================] 75/75 batches, loss: 0.0506
[2025-05-03 16:33:42,495][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0506
[2025-05-03 16:33:43,028][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0681, Metrics: {'mse': 0.07136891782283783, 'rmse': 0.26714961692437034, 'r2': -0.5341835021972656}
[2025-05-03 16:33:43,029][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0688Epoch 7/15: [                              ] 2/75 batches, loss: 0.0623Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0476Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0519Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0488Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0438Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0429Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0418Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0408Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0403Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0400Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0419Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0435Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0431Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0419Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0429Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0431Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0424Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0416Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0428Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0429Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0426Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0421Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0436Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0448Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0445Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0445Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0437Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0438Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0439Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0434Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0435Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0440Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0442Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0446Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0441Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0435Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0435Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0428Epoch 7/15: [================              ] 40/75 batches, loss: 0.0430Epoch 7/15: [================              ] 41/75 batches, loss: 0.0432Epoch 7/15: [================              ] 42/75 batches, loss: 0.0442Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0440Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0437Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0433Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0435Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0435Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0431Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0438Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0436Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0438Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0442Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0442Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0447Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0447Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0444Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0444Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0446Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0444Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0444Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0440Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0438Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0436Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0436Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0435Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0435Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0435Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0433Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0431Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0435Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0431Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0434Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0433Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0432Epoch 7/15: [==============================] 75/75 batches, loss: 0.0432
[2025-05-03 16:33:45,500][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0432
[2025-05-03 16:33:45,828][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0692, Metrics: {'mse': 0.07269065827131271, 'rmse': 0.26961205142076405, 'r2': -0.562596321105957}
[2025-05-03 16:33:45,829][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0409Epoch 8/15: [                              ] 2/75 batches, loss: 0.0515Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0504Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0526Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0493Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0491Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0483Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0448Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0423Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0475Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0460Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0446Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0442Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0424Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0420Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0418Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0417Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0419Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0424Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0417Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0415Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0409Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0407Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0409Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0408Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0396Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0400Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0403Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0407Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0415Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0411Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0406Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0415Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0414Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0407Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0406Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0409Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0408Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0413Epoch 8/15: [================              ] 40/75 batches, loss: 0.0418Epoch 8/15: [================              ] 41/75 batches, loss: 0.0419Epoch 8/15: [================              ] 42/75 batches, loss: 0.0415Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0413Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0416Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0415Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0417Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0415Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0413Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0410Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0413Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0411Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0419Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0415Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0417Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0418Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0422Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0420Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0418Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0416Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0413Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0411Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0411Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0415Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0413Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0410Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0408Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0407Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0408Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0405Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0403Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0401Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0398Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0397Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0398Epoch 8/15: [==============================] 75/75 batches, loss: 0.0398
[2025-05-03 16:33:48,314][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0398
[2025-05-03 16:33:48,676][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0650, Metrics: {'mse': 0.0681576207280159, 'rmse': 0.26107014522540856, 'r2': -0.4651517868041992}
[2025-05-03 16:33:48,677][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0491Epoch 9/15: [                              ] 2/75 batches, loss: 0.0489Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0401Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0400Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0358Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0375Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0374Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0368Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0359Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0340Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0359Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0360Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0363Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0359Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0343Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0348Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0342Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0346Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0350Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0353Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0352Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0350Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0355Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0352Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0358Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0358Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0357Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0362Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0361Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0360Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0361Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0359Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0361Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0372Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0373Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0367Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0373Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0370Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0373Epoch 9/15: [================              ] 40/75 batches, loss: 0.0376Epoch 9/15: [================              ] 41/75 batches, loss: 0.0373Epoch 9/15: [================              ] 42/75 batches, loss: 0.0373Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0374Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0371Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0368Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0367Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0364Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0363Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0361Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0361Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0359Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0356Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0356Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0354Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0355Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0358Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0359Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0358Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0363Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0366Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0368Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0367Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0366Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0364Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0366Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0364Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0364Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0361Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0362Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0363Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0365Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0365Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0363Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0361Epoch 9/15: [==============================] 75/75 batches, loss: 0.0360
[2025-05-03 16:33:51,179][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0360
[2025-05-03 16:33:51,661][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0570, Metrics: {'mse': 0.05893178656697273, 'rmse': 0.24275870029099417, 'r2': -0.26682841777801514}
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0666Epoch 10/15: [                              ] 2/75 batches, loss: 0.0616Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0506Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0455Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0508Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0529Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0516Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0502Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0483Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0479Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0456Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0446Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0440Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0434Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0425Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0414Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0406Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0394Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0388Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0386Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0381Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0375Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0374Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0393Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0391Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0386Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0392Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0392Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0395Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0390Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0395Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0389Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0387Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0388Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0388Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0394Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0391Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0399Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0398Epoch 10/15: [================              ] 40/75 batches, loss: 0.0394Epoch 10/15: [================              ] 41/75 batches, loss: 0.0395Epoch 10/15: [================              ] 42/75 batches, loss: 0.0392Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0398Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0393Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0389Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0385Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0382Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0385Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0381Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0378Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0373Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0370Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0369Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0369Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0368Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0366Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0365Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0366Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0366Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0365Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0362Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0361Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0359Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0357Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0358Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0357Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0357Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0360Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0358Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0356Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0355Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0355Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0355Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0357Epoch 10/15: [==============================] 75/75 batches, loss: 0.0358
[2025-05-03 16:33:54,555][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0358
[2025-05-03 16:33:55,128][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0758, Metrics: {'mse': 0.07967965304851532, 'rmse': 0.2822758456696487, 'r2': -0.712835431098938}
[2025-05-03 16:33:55,129][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0342Epoch 11/15: [                              ] 2/75 batches, loss: 0.0270Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0285Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0329Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0358Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0340Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0378Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0365Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0348Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0355Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0346Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0348Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0349Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0336Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0346Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0340Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0334Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0332Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0336Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0336Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0345Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0343Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0333Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0348Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0343Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0348Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0348Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0346Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0350Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0351Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0347Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0346Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0342Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0342Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0345Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0339Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0337Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0334Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0332Epoch 11/15: [================              ] 40/75 batches, loss: 0.0334Epoch 11/15: [================              ] 41/75 batches, loss: 0.0337Epoch 11/15: [================              ] 42/75 batches, loss: 0.0337Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0335Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0335Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0337Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0338Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0337Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0333Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0333Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0329Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0325Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0326Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0323Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0323Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0322Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0322Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0322Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0320Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0317Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0317Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0317Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0316Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0315Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0315Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0317Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0318Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0318Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0315Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0315Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0315Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0314Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0319Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0317Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0316Epoch 11/15: [==============================] 75/75 batches, loss: 0.0316
[2025-05-03 16:33:57,581][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0316
[2025-05-03 16:33:57,993][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0603, Metrics: {'mse': 0.06241382658481598, 'rmse': 0.24982759372178243, 'r2': -0.3416801691055298}
[2025-05-03 16:33:57,994][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0411Epoch 12/15: [                              ] 2/75 batches, loss: 0.0321Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0386Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0358Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0335Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0335Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0315Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0333Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0333Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0345Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0327Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0325Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0316Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0300Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0309Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0310Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0313Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0306Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0309Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0306Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0305Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0300Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0296Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0291Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0292Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0292Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0290Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0297Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0300Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0296Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0300Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0301Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0302Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0302Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0301Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0298Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0292Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0295Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0295Epoch 12/15: [================              ] 40/75 batches, loss: 0.0297Epoch 12/15: [================              ] 41/75 batches, loss: 0.0296Epoch 12/15: [================              ] 42/75 batches, loss: 0.0302Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0299Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0295Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0297Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0300Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0300Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0303Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0308Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0308Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0311Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0314Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0314Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0316Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0320Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0320Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0318Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0319Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0318Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0319Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0319Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0321Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0321Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0319Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0320Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0321Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0320Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0320Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0318Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0317Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0315Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0315Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0315Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0314Epoch 12/15: [==============================] 75/75 batches, loss: 0.0312
[2025-05-03 16:34:00,434][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0312
[2025-05-03 16:34:00,886][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0570, Metrics: {'mse': 0.05863787233829498, 'rmse': 0.2421525806971608, 'r2': -0.2605102062225342}
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0203Epoch 13/15: [                              ] 2/75 batches, loss: 0.0354Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0324Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0327Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0334Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0355Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0334Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0324Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0335Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0311Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0304Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0302Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0299Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0299Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0293Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0285Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0281Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0273Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0277Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0275Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0273Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0266Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0266Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0283Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0286Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0278Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0276Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0274Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0272Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0268Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0270Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0269Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0263Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0269Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0267Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0269Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0279Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0279Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0283Epoch 13/15: [================              ] 40/75 batches, loss: 0.0286Epoch 13/15: [================              ] 41/75 batches, loss: 0.0289Epoch 13/15: [================              ] 42/75 batches, loss: 0.0288Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0286Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0289Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0288Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0287Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0290Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0289Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0290Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0292Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0291Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0292Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0292Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0295Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0294Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0293Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0297Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0296Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0293Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0296Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0297Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0296Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0295Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0296Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0297Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0293Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0293Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0293Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0293Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0293Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0292Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0290Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0295Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0295Epoch 13/15: [==============================] 75/75 batches, loss: 0.0295
[2025-05-03 16:34:03,961][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0295
[2025-05-03 16:34:04,510][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0661, Metrics: {'mse': 0.06901881098747253, 'rmse': 0.26271431439392967, 'r2': -0.4836643934249878}
[2025-05-03 16:34:04,511][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0124Epoch 14/15: [                              ] 2/75 batches, loss: 0.0197Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0232Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0235Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0231Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0218Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0207Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0216Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0213Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0219Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0245Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0259Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0248Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0243Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0243Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0244Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0245Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0247Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0248Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0250Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0249Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0254Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0262Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0259Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0254Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0251Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0252Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0250Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0256Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0264Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0261Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0259Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0268Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0264Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0270Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0268Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0267Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0269Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0268Epoch 14/15: [================              ] 40/75 batches, loss: 0.0266Epoch 14/15: [================              ] 41/75 batches, loss: 0.0266Epoch 14/15: [================              ] 42/75 batches, loss: 0.0265Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0266Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0273Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0272Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0274Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0276Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0276Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0274Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0276Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0274Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0274Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0274Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0273Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0274Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0271Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0275Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0274Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0276Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0275Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0274Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0274Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0273Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0275Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0274Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0274Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0275Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0277Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0278Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0277Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0279Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0278Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0276Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0275Epoch 14/15: [==============================] 75/75 batches, loss: 0.0275
[2025-05-03 16:34:06,979][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0275
[2025-05-03 16:34:07,498][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0528, Metrics: {'mse': 0.05404801666736603, 'rmse': 0.23248229323405692, 'r2': -0.1618443727493286}
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0385Epoch 15/15: [                              ] 2/75 batches, loss: 0.0334Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0292Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0294Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0271Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0292Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0290Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0295Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0284Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0295Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0294Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0308Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0308Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0305Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0295Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0290Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0284Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0291Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0294Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0297Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0290Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0296Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0294Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0294Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0300Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0294Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0288Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0287Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0291Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0289Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0287Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0286Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0286Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0282Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0285Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0280Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0280Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0282Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0277Epoch 15/15: [================              ] 40/75 batches, loss: 0.0275Epoch 15/15: [================              ] 41/75 batches, loss: 0.0273Epoch 15/15: [================              ] 42/75 batches, loss: 0.0273Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0270Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0270Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0269Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0273Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0271Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0271Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0271Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0268Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0273Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0273Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0278Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0278Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0276Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0279Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0278Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0276Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0277Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0278Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0277Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0278Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0277Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0277Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0277Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0280Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0279Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0277Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0278Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0276Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0275Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0274Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0277Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0276Epoch 15/15: [==============================] 75/75 batches, loss: 0.0275
[2025-05-03 16:34:10,270][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0275
[2025-05-03 16:34:10,757][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0593, Metrics: {'mse': 0.06154636666178703, 'rmse': 0.24808540195220483, 'r2': -0.3230327367782593}
[2025-05-03 16:34:10,758][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-03 16:34:10,758][src.training.lm_trainer][INFO] - Training completed in 47.84 seconds
[2025-05-03 16:34:10,758][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-03 16:34:14,275][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.01594678685069084, 'rmse': 0.12628058778248874, 'r2': 0.20014894008636475}
[2025-05-03 16:34:14,276][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05404801666736603, 'rmse': 0.23248229323405692, 'r2': -0.1618443727493286}
[2025-05-03 16:34:14,276][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.056273140013217926, 'rmse': 0.2372196029277891, 'r2': -0.42337608337402344}
[2025-05-03 16:34:15,943][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/ru/ru/model.pt
[2025-05-03 16:34:15,944][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▂▂▂▂▁
wandb:     best_val_mse █▃▂▂▂▂▁
wandb:      best_val_r2 ▁▆▇▇▇▇█
wandb:    best_val_rmse █▃▂▂▂▂▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▁▆▆▆▆▆▆▆▅▆▆▆▇
wandb:       train_loss █▄▃▂▂▂▁▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆█▂▂▂▃▃▂▁▄▂▁▂▁▂
wandb:          val_mse ▆█▂▂▂▃▃▂▁▄▂▁▂▁▂
wandb:           val_r2 ▃▁▇▇▇▆▆▇█▅▇█▇█▇
wandb:         val_rmse ▆█▃▂▂▃▃▃▂▄▂▂▃▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05282
wandb:     best_val_mse 0.05405
wandb:      best_val_r2 -0.16184
wandb:    best_val_rmse 0.23248
wandb:            epoch 15
wandb:   final_test_mse 0.05627
wandb:    final_test_r2 -0.42338
wandb:  final_test_rmse 0.23722
wandb:  final_train_mse 0.01595
wandb:   final_train_r2 0.20015
wandb: final_train_rmse 0.12628
wandb:    final_val_mse 0.05405
wandb:     final_val_r2 -0.16184
wandb:   final_val_rmse 0.23248
wandb:    learning_rate 0.0001
wandb:       train_loss 0.02754
wandb:       train_time 47.84071
wandb:         val_loss 0.05934
wandb:          val_mse 0.06155
wandb:           val_r2 -0.32303
wandb:         val_rmse 0.24809
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_163254-p5co6xtn
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_163254-p5co6xtn/logs
Experiment probe_layer2_complexity_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/ru/ru/results.json for layer 2
=======================
PROBING LAYER 3
=======================
Running experiment: probe_layer3_question_type_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=3"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer3_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/layer3/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-03 16:35:04,562][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/ar
experiment_name: probe_layer3_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-03 16:35:04,562][__main__][INFO] - Normalized task: question_type
[2025-05-03 16:35:04,562][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-03 16:35:04,562][__main__][INFO] - Determined Task Type: classification
[2025-05-03 16:35:04,566][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-03 16:35:04,566][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-03 16:35:10,739][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-03 16:35:13,127][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-03 16:35:13,127][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:35:13,572][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:35:13,705][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:35:14,043][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-03 16:35:14,050][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:35:14,051][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-03 16:35:14,052][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:35:14,205][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:35:14,339][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:35:14,377][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-03 16:35:14,378][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:35:14,378][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-03 16:35:14,379][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:35:14,507][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:35:14,641][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:35:14,677][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-03 16:35:14,678][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:35:14,679][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-03 16:35:14,679][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-03 16:35:14,679][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:35:14,680][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:35:14,680][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:35:14,680][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:35:14,680][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-03 16:35:14,680][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-03 16:35:14,680][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-03 16:35:14,680][src.data.datasets][INFO] - Sample label: 1
[2025-05-03 16:35:14,680][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:35:14,680][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:35:14,681][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:35:14,681][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:35:14,681][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-03 16:35:14,681][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-03 16:35:14,681][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-03 16:35:14,681][src.data.datasets][INFO] - Sample label: 0
[2025-05-03 16:35:14,681][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:35:14,681][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:35:14,681][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:35:14,681][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:35:14,682][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-03 16:35:14,682][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-03 16:35:14,682][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-03 16:35:14,682][src.data.datasets][INFO] - Sample label: 0
[2025-05-03 16:35:14,682][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-03 16:35:14,682][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-03 16:35:14,682][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-03 16:35:14,682][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-03 16:35:14,683][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-03 16:35:25,958][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-03 16:35:25,959][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-03 16:35:25,959][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=3, freeze_model=True
[2025-05-03 16:35:25,959][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-03 16:35:25,965][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-03 16:35:25,966][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-03 16:35:25,966][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-03 16:35:25,966][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-03 16:35:25,966][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-03 16:35:25,967][__main__][INFO] - Total parameters: 394,568,839
[2025-05-03 16:35:25,967][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-03 16:35:25,968][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7208Epoch 1/15: [                              ] 2/63 batches, loss: 0.7227Epoch 1/15: [=                             ] 3/63 batches, loss: 0.6970Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7030Epoch 1/15: [==                            ] 5/63 batches, loss: 0.6971Epoch 1/15: [==                            ] 6/63 batches, loss: 0.6983Epoch 1/15: [===                           ] 7/63 batches, loss: 0.6967Epoch 1/15: [===                           ] 8/63 batches, loss: 0.6974Epoch 1/15: [====                          ] 9/63 batches, loss: 0.6955Epoch 1/15: [====                          ] 10/63 batches, loss: 0.6943Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.6936Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.6941Epoch 1/15: [======                        ] 13/63 batches, loss: 0.6928Epoch 1/15: [======                        ] 14/63 batches, loss: 0.6903Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.6884Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.6876Epoch 1/15: [========                      ] 17/63 batches, loss: 0.6878Epoch 1/15: [========                      ] 18/63 batches, loss: 0.6885Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.6875Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.6867Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.6869Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.6861Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.6844Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.6849Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.6826Epoch 1/15: [============                  ] 26/63 batches, loss: 0.6811Epoch 1/15: [============                  ] 27/63 batches, loss: 0.6807Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.6810Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.6818Epoch 1/15: [==============                ] 30/63 batches, loss: 0.6842Epoch 1/15: [==============                ] 31/63 batches, loss: 0.6831Epoch 1/15: [===============               ] 32/63 batches, loss: 0.6813Epoch 1/15: [===============               ] 33/63 batches, loss: 0.6808Epoch 1/15: [================              ] 34/63 batches, loss: 0.6788Epoch 1/15: [================              ] 35/63 batches, loss: 0.6793Epoch 1/15: [=================             ] 36/63 batches, loss: 0.6780Epoch 1/15: [=================             ] 37/63 batches, loss: 0.6788Epoch 1/15: [==================            ] 38/63 batches, loss: 0.6782Epoch 1/15: [==================            ] 39/63 batches, loss: 0.6781Epoch 1/15: [===================           ] 40/63 batches, loss: 0.6772Epoch 1/15: [===================           ] 41/63 batches, loss: 0.6764Epoch 1/15: [====================          ] 42/63 batches, loss: 0.6766Epoch 1/15: [====================          ] 43/63 batches, loss: 0.6767Epoch 1/15: [====================          ] 44/63 batches, loss: 0.6772Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.6774Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.6771Epoch 1/15: [======================        ] 47/63 batches, loss: 0.6773Epoch 1/15: [======================        ] 48/63 batches, loss: 0.6761Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.6750Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.6752Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6737Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6730Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6725Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6721Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6711Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6696Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6693Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6695Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6696Epoch 1/15: [============================  ] 60/63 batches, loss: 0.6700Epoch 1/15: [============================= ] 61/63 batches, loss: 0.6709Epoch 1/15: [============================= ] 62/63 batches, loss: 0.6704Epoch 1/15: [==============================] 63/63 batches, loss: 0.6701
[2025-05-03 16:35:34,408][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6701
[2025-05-03 16:35:34,776][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6831, Metrics: {'accuracy': 0.6590909090909091, 'f1': 0.5714285714285714, 'precision': 0.6666666666666666, 'recall': 0.5}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.5990Epoch 2/15: [                              ] 2/63 batches, loss: 0.6040Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6100Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6065Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6196Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6236Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6259Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6236Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6204Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6181Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6205Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6108Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6109Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6145Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6194Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6202Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6188Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6200Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6230Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6214Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6210Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6209Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6217Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6203Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6215Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6226Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6230Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6210Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6193Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6187Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6200Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6201Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6206Epoch 2/15: [================              ] 34/63 batches, loss: 0.6199Epoch 2/15: [================              ] 35/63 batches, loss: 0.6198Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6194Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6185Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6183Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6144Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6147Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6144Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6135Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6108Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6110Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6101Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6104Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6090Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6085Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6102Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6108Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6096Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6085Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6064Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6063Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6053Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6049Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6046Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6043Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6049Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6044Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6021Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6008Epoch 2/15: [==============================] 63/63 batches, loss: 0.5989
[2025-05-03 16:35:37,252][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.5989
[2025-05-03 16:35:37,587][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6338, Metrics: {'accuracy': 0.8181818181818182, 'f1': 0.8333333333333334, 'precision': 0.7142857142857143, 'recall': 1.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.5996Epoch 3/15: [                              ] 2/63 batches, loss: 0.5673Epoch 3/15: [=                             ] 3/63 batches, loss: 0.5665Epoch 3/15: [=                             ] 4/63 batches, loss: 0.5786Epoch 3/15: [==                            ] 5/63 batches, loss: 0.5707Epoch 3/15: [==                            ] 6/63 batches, loss: 0.5608Epoch 3/15: [===                           ] 7/63 batches, loss: 0.5561Epoch 3/15: [===                           ] 8/63 batches, loss: 0.5534Epoch 3/15: [====                          ] 9/63 batches, loss: 0.5532Epoch 3/15: [====                          ] 10/63 batches, loss: 0.5487Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.5524Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.5497Epoch 3/15: [======                        ] 13/63 batches, loss: 0.5462Epoch 3/15: [======                        ] 14/63 batches, loss: 0.5476Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.5515Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.5471Epoch 3/15: [========                      ] 17/63 batches, loss: 0.5471Epoch 3/15: [========                      ] 18/63 batches, loss: 0.5439Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.5451Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.5472Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.5491Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.5482Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.5480Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.5498Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.5504Epoch 3/15: [============                  ] 26/63 batches, loss: 0.5538Epoch 3/15: [============                  ] 27/63 batches, loss: 0.5560Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.5554Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.5531Epoch 3/15: [==============                ] 30/63 batches, loss: 0.5505Epoch 3/15: [==============                ] 31/63 batches, loss: 0.5503Epoch 3/15: [===============               ] 32/63 batches, loss: 0.5476Epoch 3/15: [===============               ] 33/63 batches, loss: 0.5501Epoch 3/15: [================              ] 34/63 batches, loss: 0.5530Epoch 3/15: [================              ] 35/63 batches, loss: 0.5534Epoch 3/15: [=================             ] 36/63 batches, loss: 0.5515Epoch 3/15: [=================             ] 37/63 batches, loss: 0.5530Epoch 3/15: [==================            ] 38/63 batches, loss: 0.5522Epoch 3/15: [==================            ] 39/63 batches, loss: 0.5534Epoch 3/15: [===================           ] 40/63 batches, loss: 0.5549Epoch 3/15: [===================           ] 41/63 batches, loss: 0.5545Epoch 3/15: [====================          ] 42/63 batches, loss: 0.5525Epoch 3/15: [====================          ] 43/63 batches, loss: 0.5510Epoch 3/15: [====================          ] 44/63 batches, loss: 0.5527Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.5522Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.5534Epoch 3/15: [======================        ] 47/63 batches, loss: 0.5531Epoch 3/15: [======================        ] 48/63 batches, loss: 0.5527Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.5517Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.5513Epoch 3/15: [========================      ] 51/63 batches, loss: 0.5518Epoch 3/15: [========================      ] 52/63 batches, loss: 0.5529Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.5511Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.5511Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.5521Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.5523Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.5527Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.5528Epoch 3/15: [============================  ] 59/63 batches, loss: 0.5554Epoch 3/15: [============================  ] 60/63 batches, loss: 0.5558Epoch 3/15: [============================= ] 61/63 batches, loss: 0.5553Epoch 3/15: [============================= ] 62/63 batches, loss: 0.5550Epoch 3/15: [==============================] 63/63 batches, loss: 0.5533
[2025-05-03 16:35:40,089][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.5533
[2025-05-03 16:35:40,392][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.5870, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8837209302325582, 'precision': 0.8260869565217391, 'recall': 0.95}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.5928Epoch 4/15: [                              ] 2/63 batches, loss: 0.5323Epoch 4/15: [=                             ] 3/63 batches, loss: 0.5119Epoch 4/15: [=                             ] 4/63 batches, loss: 0.5264Epoch 4/15: [==                            ] 5/63 batches, loss: 0.5369Epoch 4/15: [==                            ] 6/63 batches, loss: 0.5526Epoch 4/15: [===                           ] 7/63 batches, loss: 0.5581Epoch 4/15: [===                           ] 8/63 batches, loss: 0.5618Epoch 4/15: [====                          ] 9/63 batches, loss: 0.5555Epoch 4/15: [====                          ] 10/63 batches, loss: 0.5572Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.5579Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.5534Epoch 4/15: [======                        ] 13/63 batches, loss: 0.5566Epoch 4/15: [======                        ] 14/63 batches, loss: 0.5583Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.5618Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.5637Epoch 4/15: [========                      ] 17/63 batches, loss: 0.5648Epoch 4/15: [========                      ] 18/63 batches, loss: 0.5634Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.5614Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.5618Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.5628Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.5629Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.5626Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.5621Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.5607Epoch 4/15: [============                  ] 26/63 batches, loss: 0.5634Epoch 4/15: [============                  ] 27/63 batches, loss: 0.5635Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.5616Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.5603Epoch 4/15: [==============                ] 30/63 batches, loss: 0.5604Epoch 4/15: [==============                ] 31/63 batches, loss: 0.5588Epoch 4/15: [===============               ] 32/63 batches, loss: 0.5581Epoch 4/15: [===============               ] 33/63 batches, loss: 0.5575Epoch 4/15: [================              ] 34/63 batches, loss: 0.5565Epoch 4/15: [================              ] 35/63 batches, loss: 0.5550Epoch 4/15: [=================             ] 36/63 batches, loss: 0.5511Epoch 4/15: [=================             ] 37/63 batches, loss: 0.5491Epoch 4/15: [==================            ] 38/63 batches, loss: 0.5500Epoch 4/15: [==================            ] 39/63 batches, loss: 0.5481Epoch 4/15: [===================           ] 40/63 batches, loss: 0.5488Epoch 4/15: [===================           ] 41/63 batches, loss: 0.5473Epoch 4/15: [====================          ] 42/63 batches, loss: 0.5472Epoch 4/15: [====================          ] 43/63 batches, loss: 0.5454Epoch 4/15: [====================          ] 44/63 batches, loss: 0.5449Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.5453Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.5432Epoch 4/15: [======================        ] 47/63 batches, loss: 0.5441Epoch 4/15: [======================        ] 48/63 batches, loss: 0.5434Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.5414Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.5411Epoch 4/15: [========================      ] 51/63 batches, loss: 0.5410Epoch 4/15: [========================      ] 52/63 batches, loss: 0.5398Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.5393Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.5383Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.5379Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.5356Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.5343Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.5354Epoch 4/15: [============================  ] 59/63 batches, loss: 0.5349Epoch 4/15: [============================  ] 60/63 batches, loss: 0.5335Epoch 4/15: [============================= ] 61/63 batches, loss: 0.5343Epoch 4/15: [============================= ] 62/63 batches, loss: 0.5345Epoch 4/15: [==============================] 63/63 batches, loss: 0.5371
[2025-05-03 16:35:42,758][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5371
[2025-05-03 16:35:43,176][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5765, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.5040Epoch 5/15: [                              ] 2/63 batches, loss: 0.5215Epoch 5/15: [=                             ] 3/63 batches, loss: 0.5138Epoch 5/15: [=                             ] 4/63 batches, loss: 0.5143Epoch 5/15: [==                            ] 5/63 batches, loss: 0.5281Epoch 5/15: [==                            ] 6/63 batches, loss: 0.5353Epoch 5/15: [===                           ] 7/63 batches, loss: 0.5324Epoch 5/15: [===                           ] 8/63 batches, loss: 0.5269Epoch 5/15: [====                          ] 9/63 batches, loss: 0.5284Epoch 5/15: [====                          ] 10/63 batches, loss: 0.5234Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.5161Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.5153Epoch 5/15: [======                        ] 13/63 batches, loss: 0.5139Epoch 5/15: [======                        ] 14/63 batches, loss: 0.5220Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.5241Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.5242Epoch 5/15: [========                      ] 17/63 batches, loss: 0.5306Epoch 5/15: [========                      ] 18/63 batches, loss: 0.5292Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.5295Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.5312Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.5300Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.5299Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.5282Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.5244Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.5236Epoch 5/15: [============                  ] 26/63 batches, loss: 0.5224Epoch 5/15: [============                  ] 27/63 batches, loss: 0.5210Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.5238Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.5227Epoch 5/15: [==============                ] 30/63 batches, loss: 0.5201Epoch 5/15: [==============                ] 31/63 batches, loss: 0.5190Epoch 5/15: [===============               ] 32/63 batches, loss: 0.5223Epoch 5/15: [===============               ] 33/63 batches, loss: 0.5215Epoch 5/15: [================              ] 34/63 batches, loss: 0.5204Epoch 5/15: [================              ] 35/63 batches, loss: 0.5216Epoch 5/15: [=================             ] 36/63 batches, loss: 0.5205Epoch 5/15: [=================             ] 37/63 batches, loss: 0.5206Epoch 5/15: [==================            ] 38/63 batches, loss: 0.5195Epoch 5/15: [==================            ] 39/63 batches, loss: 0.5172Epoch 5/15: [===================           ] 40/63 batches, loss: 0.5170Epoch 5/15: [===================           ] 41/63 batches, loss: 0.5175Epoch 5/15: [====================          ] 42/63 batches, loss: 0.5183Epoch 5/15: [====================          ] 43/63 batches, loss: 0.5204Epoch 5/15: [====================          ] 44/63 batches, loss: 0.5198Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.5206Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.5212Epoch 5/15: [======================        ] 47/63 batches, loss: 0.5218Epoch 5/15: [======================        ] 48/63 batches, loss: 0.5224Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.5217Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.5214Epoch 5/15: [========================      ] 51/63 batches, loss: 0.5209Epoch 5/15: [========================      ] 52/63 batches, loss: 0.5222Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.5236Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.5218Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.5232Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.5250Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.5247Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.5248Epoch 5/15: [============================  ] 59/63 batches, loss: 0.5238Epoch 5/15: [============================  ] 60/63 batches, loss: 0.5235Epoch 5/15: [============================= ] 61/63 batches, loss: 0.5231Epoch 5/15: [============================= ] 62/63 batches, loss: 0.5244Epoch 5/15: [==============================] 63/63 batches, loss: 0.5271
[2025-05-03 16:35:45,592][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5271
[2025-05-03 16:35:45,926][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5673, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9047619047619048, 'precision': 0.8636363636363636, 'recall': 0.95}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.5391Epoch 6/15: [                              ] 2/63 batches, loss: 0.5293Epoch 6/15: [=                             ] 3/63 batches, loss: 0.5357Epoch 6/15: [=                             ] 4/63 batches, loss: 0.5397Epoch 6/15: [==                            ] 5/63 batches, loss: 0.5315Epoch 6/15: [==                            ] 6/63 batches, loss: 0.5275Epoch 6/15: [===                           ] 7/63 batches, loss: 0.5328Epoch 6/15: [===                           ] 8/63 batches, loss: 0.5378Epoch 6/15: [====                          ] 9/63 batches, loss: 0.5271Epoch 6/15: [====                          ] 10/63 batches, loss: 0.5319Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.5249Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.5432Epoch 6/15: [======                        ] 13/63 batches, loss: 0.5410Epoch 6/15: [======                        ] 14/63 batches, loss: 0.5453Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.5442Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.5438Epoch 6/15: [========                      ] 17/63 batches, loss: 0.5408Epoch 6/15: [========                      ] 18/63 batches, loss: 0.5412Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.5398Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.5418Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.5435Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.5421Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.5420Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.5425Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.5419Epoch 6/15: [============                  ] 26/63 batches, loss: 0.5399Epoch 6/15: [============                  ] 27/63 batches, loss: 0.5389Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.5352Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.5345Epoch 6/15: [==============                ] 30/63 batches, loss: 0.5361Epoch 6/15: [==============                ] 31/63 batches, loss: 0.5355Epoch 6/15: [===============               ] 32/63 batches, loss: 0.5378Epoch 6/15: [===============               ] 33/63 batches, loss: 0.5373Epoch 6/15: [================              ] 34/63 batches, loss: 0.5370Epoch 6/15: [================              ] 35/63 batches, loss: 0.5394Epoch 6/15: [=================             ] 36/63 batches, loss: 0.5378Epoch 6/15: [=================             ] 37/63 batches, loss: 0.5378Epoch 6/15: [==================            ] 38/63 batches, loss: 0.5351Epoch 6/15: [==================            ] 39/63 batches, loss: 0.5338Epoch 6/15: [===================           ] 40/63 batches, loss: 0.5314Epoch 6/15: [===================           ] 41/63 batches, loss: 0.5301Epoch 6/15: [====================          ] 42/63 batches, loss: 0.5291Epoch 6/15: [====================          ] 43/63 batches, loss: 0.5301Epoch 6/15: [====================          ] 44/63 batches, loss: 0.5302Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.5287Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.5282Epoch 6/15: [======================        ] 47/63 batches, loss: 0.5272Epoch 6/15: [======================        ] 48/63 batches, loss: 0.5264Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.5265Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.5264Epoch 6/15: [========================      ] 51/63 batches, loss: 0.5253Epoch 6/15: [========================      ] 52/63 batches, loss: 0.5265Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.5259Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.5271Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.5263Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.5281Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.5277Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.5283Epoch 6/15: [============================  ] 59/63 batches, loss: 0.5270Epoch 6/15: [============================  ] 60/63 batches, loss: 0.5268Epoch 6/15: [============================= ] 61/63 batches, loss: 0.5269Epoch 6/15: [============================= ] 62/63 batches, loss: 0.5252Epoch 6/15: [==============================] 63/63 batches, loss: 0.5218
[2025-05-03 16:35:48,362][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5218
[2025-05-03 16:35:48,784][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5647, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.4558Epoch 7/15: [                              ] 2/63 batches, loss: 0.4973Epoch 7/15: [=                             ] 3/63 batches, loss: 0.4692Epoch 7/15: [=                             ] 4/63 batches, loss: 0.4899Epoch 7/15: [==                            ] 5/63 batches, loss: 0.5035Epoch 7/15: [==                            ] 6/63 batches, loss: 0.5120Epoch 7/15: [===                           ] 7/63 batches, loss: 0.5123Epoch 7/15: [===                           ] 8/63 batches, loss: 0.5143Epoch 7/15: [====                          ] 9/63 batches, loss: 0.5214Epoch 7/15: [====                          ] 10/63 batches, loss: 0.5171Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.5098Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.5119Epoch 7/15: [======                        ] 13/63 batches, loss: 0.5096Epoch 7/15: [======                        ] 14/63 batches, loss: 0.5105Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.5173Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.5170Epoch 7/15: [========                      ] 17/63 batches, loss: 0.5177Epoch 7/15: [========                      ] 18/63 batches, loss: 0.5105Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.5138Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.5087Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.5081Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.5100Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.5131Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.5148Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.5165Epoch 7/15: [============                  ] 26/63 batches, loss: 0.5143Epoch 7/15: [============                  ] 27/63 batches, loss: 0.5150Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.5165Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.5150Epoch 7/15: [==============                ] 30/63 batches, loss: 0.5162Epoch 7/15: [==============                ] 31/63 batches, loss: 0.5144Epoch 7/15: [===============               ] 32/63 batches, loss: 0.5112Epoch 7/15: [===============               ] 33/63 batches, loss: 0.5119Epoch 7/15: [================              ] 34/63 batches, loss: 0.5142Epoch 7/15: [================              ] 35/63 batches, loss: 0.5140Epoch 7/15: [=================             ] 36/63 batches, loss: 0.5151Epoch 7/15: [=================             ] 37/63 batches, loss: 0.5128Epoch 7/15: [==================            ] 38/63 batches, loss: 0.5130Epoch 7/15: [==================            ] 39/63 batches, loss: 0.5128Epoch 7/15: [===================           ] 40/63 batches, loss: 0.5125Epoch 7/15: [===================           ] 41/63 batches, loss: 0.5112Epoch 7/15: [====================          ] 42/63 batches, loss: 0.5122Epoch 7/15: [====================          ] 43/63 batches, loss: 0.5144Epoch 7/15: [====================          ] 44/63 batches, loss: 0.5149Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.5151Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.5144Epoch 7/15: [======================        ] 47/63 batches, loss: 0.5138Epoch 7/15: [======================        ] 48/63 batches, loss: 0.5142Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.5150Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.5148Epoch 7/15: [========================      ] 51/63 batches, loss: 0.5156Epoch 7/15: [========================      ] 52/63 batches, loss: 0.5167Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.5180Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.5167Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.5156Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.5166Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.5161Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.5164Epoch 7/15: [============================  ] 59/63 batches, loss: 0.5163Epoch 7/15: [============================  ] 60/63 batches, loss: 0.5160Epoch 7/15: [============================= ] 61/63 batches, loss: 0.5170Epoch 7/15: [============================= ] 62/63 batches, loss: 0.5177Epoch 7/15: [==============================] 63/63 batches, loss: 0.5144
[2025-05-03 16:35:51,368][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5144
[2025-05-03 16:35:51,711][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5699, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-03 16:35:51,712][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.5125Epoch 8/15: [                              ] 2/63 batches, loss: 0.5013Epoch 8/15: [=                             ] 3/63 batches, loss: 0.5180Epoch 8/15: [=                             ] 4/63 batches, loss: 0.5198Epoch 8/15: [==                            ] 5/63 batches, loss: 0.5225Epoch 8/15: [==                            ] 6/63 batches, loss: 0.5353Epoch 8/15: [===                           ] 7/63 batches, loss: 0.5347Epoch 8/15: [===                           ] 8/63 batches, loss: 0.5303Epoch 8/15: [====                          ] 9/63 batches, loss: 0.5363Epoch 8/15: [====                          ] 10/63 batches, loss: 0.5303Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.5192Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.5121Epoch 8/15: [======                        ] 13/63 batches, loss: 0.5098Epoch 8/15: [======                        ] 14/63 batches, loss: 0.5190Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.5171Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.5148Epoch 8/15: [========                      ] 17/63 batches, loss: 0.5145Epoch 8/15: [========                      ] 18/63 batches, loss: 0.5144Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.5140Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.5147Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.5097Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.5071Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.5071Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.5096Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.5113Epoch 8/15: [============                  ] 26/63 batches, loss: 0.5150Epoch 8/15: [============                  ] 27/63 batches, loss: 0.5138Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.5147Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.5133Epoch 8/15: [==============                ] 30/63 batches, loss: 0.5130Epoch 8/15: [==============                ] 31/63 batches, loss: 0.5128Epoch 8/15: [===============               ] 32/63 batches, loss: 0.5133Epoch 8/15: [===============               ] 33/63 batches, loss: 0.5116Epoch 8/15: [================              ] 34/63 batches, loss: 0.5104Epoch 8/15: [================              ] 35/63 batches, loss: 0.5096Epoch 8/15: [=================             ] 36/63 batches, loss: 0.5107Epoch 8/15: [=================             ] 37/63 batches, loss: 0.5097Epoch 8/15: [==================            ] 38/63 batches, loss: 0.5084Epoch 8/15: [==================            ] 39/63 batches, loss: 0.5074Epoch 8/15: [===================           ] 40/63 batches, loss: 0.5100Epoch 8/15: [===================           ] 41/63 batches, loss: 0.5093Epoch 8/15: [====================          ] 42/63 batches, loss: 0.5090Epoch 8/15: [====================          ] 43/63 batches, loss: 0.5094Epoch 8/15: [====================          ] 44/63 batches, loss: 0.5089Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.5103Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.5105Epoch 8/15: [======================        ] 47/63 batches, loss: 0.5124Epoch 8/15: [======================        ] 48/63 batches, loss: 0.5133Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.5127Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.5137Epoch 8/15: [========================      ] 51/63 batches, loss: 0.5129Epoch 8/15: [========================      ] 52/63 batches, loss: 0.5151Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.5173Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.5189Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.5183Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.5189Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.5176Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.5175Epoch 8/15: [============================  ] 59/63 batches, loss: 0.5160Epoch 8/15: [============================  ] 60/63 batches, loss: 0.5149Epoch 8/15: [============================= ] 61/63 batches, loss: 0.5143Epoch 8/15: [============================= ] 62/63 batches, loss: 0.5150Epoch 8/15: [==============================] 63/63 batches, loss: 0.5159
[2025-05-03 16:35:53,785][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5159
[2025-05-03 16:35:54,198][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5850, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-03 16:35:54,199][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.5413Epoch 9/15: [                              ] 2/63 batches, loss: 0.5948Epoch 9/15: [=                             ] 3/63 batches, loss: 0.5618Epoch 9/15: [=                             ] 4/63 batches, loss: 0.5475Epoch 9/15: [==                            ] 5/63 batches, loss: 0.5362Epoch 9/15: [==                            ] 6/63 batches, loss: 0.5199Epoch 9/15: [===                           ] 7/63 batches, loss: 0.5147Epoch 9/15: [===                           ] 8/63 batches, loss: 0.5192Epoch 9/15: [====                          ] 9/63 batches, loss: 0.5122Epoch 9/15: [====                          ] 10/63 batches, loss: 0.5069Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.5047Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.5031Epoch 9/15: [======                        ] 13/63 batches, loss: 0.4996Epoch 9/15: [======                        ] 14/63 batches, loss: 0.5052Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.5066Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.5112Epoch 9/15: [========                      ] 17/63 batches, loss: 0.5084Epoch 9/15: [========                      ] 18/63 batches, loss: 0.5052Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.5056Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.5068Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.5125Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.5173Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.5181Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.5188Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.5187Epoch 9/15: [============                  ] 26/63 batches, loss: 0.5210Epoch 9/15: [============                  ] 27/63 batches, loss: 0.5212Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.5207Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.5233Epoch 9/15: [==============                ] 30/63 batches, loss: 0.5243Epoch 9/15: [==============                ] 31/63 batches, loss: 0.5216Epoch 9/15: [===============               ] 32/63 batches, loss: 0.5198Epoch 9/15: [===============               ] 33/63 batches, loss: 0.5203Epoch 9/15: [================              ] 34/63 batches, loss: 0.5199Epoch 9/15: [================              ] 35/63 batches, loss: 0.5178Epoch 9/15: [=================             ] 36/63 batches, loss: 0.5162Epoch 9/15: [=================             ] 37/63 batches, loss: 0.5136Epoch 9/15: [==================            ] 38/63 batches, loss: 0.5138Epoch 9/15: [==================            ] 39/63 batches, loss: 0.5099Epoch 9/15: [===================           ] 40/63 batches, loss: 0.5112Epoch 9/15: [===================           ] 41/63 batches, loss: 0.5105Epoch 9/15: [====================          ] 42/63 batches, loss: 0.5104Epoch 9/15: [====================          ] 43/63 batches, loss: 0.5092Epoch 9/15: [====================          ] 44/63 batches, loss: 0.5081Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.5069Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.5084Epoch 9/15: [======================        ] 47/63 batches, loss: 0.5084Epoch 9/15: [======================        ] 48/63 batches, loss: 0.5089Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.5107Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.5092Epoch 9/15: [========================      ] 51/63 batches, loss: 0.5100Epoch 9/15: [========================      ] 52/63 batches, loss: 0.5110Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.5109Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.5117Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.5107Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.5113Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.5104Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.5108Epoch 9/15: [============================  ] 59/63 batches, loss: 0.5119Epoch 9/15: [============================  ] 60/63 batches, loss: 0.5135Epoch 9/15: [============================= ] 61/63 batches, loss: 0.5125Epoch 9/15: [============================= ] 62/63 batches, loss: 0.5128Epoch 9/15: [==============================] 63/63 batches, loss: 0.5157
[2025-05-03 16:35:56,296][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5157
[2025-05-03 16:35:56,570][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.5651, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
[2025-05-03 16:35:56,571][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-03 16:35:56,571][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 9
[2025-05-03 16:35:56,571][src.training.lm_trainer][INFO] - Training completed in 25.13 seconds
[2025-05-03 16:35:56,572][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-03 16:35:59,523][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9949748743718593, 'f1': 0.9949545913218971, 'precision': 0.9979757085020243, 'recall': 0.9919517102615694}
[2025-05-03 16:35:59,523][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
[2025-05-03 16:35:59,523][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7662337662337663, 'f1': 0.7096774193548387, 'precision': 0.55, 'recall': 1.0}
[2025-05-03 16:36:01,220][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/ar/ar/model.pt
[2025-05-03 16:36:01,222][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▅▇▇▇█
wandb:           best_val_f1 ▁▆▇███
wandb:         best_val_loss █▅▂▂▁▁
wandb:    best_val_precision ▁▃▇▇██
wandb:       best_val_recall ▁█▇█▇█
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▃▃▃▃▃▃
wandb:            train_loss █▅▃▂▂▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▅▇▇▇█▇▇█
wandb:                val_f1 ▁▆▇██████
wandb:              val_loss █▅▂▂▁▁▁▂▁
wandb:         val_precision ▁▃▇▇██▇▇█
wandb:            val_recall ▁█▇█▇████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.93182
wandb:           best_val_f1 0.93023
wandb:         best_val_loss 0.56468
wandb:    best_val_precision 0.86957
wandb:       best_val_recall 1
wandb:      early_stop_epoch 9
wandb:                 epoch 9
wandb:   final_test_accuracy 0.76623
wandb:         final_test_f1 0.70968
wandb:  final_test_precision 0.55
wandb:     final_test_recall 1
wandb:  final_train_accuracy 0.99497
wandb:        final_train_f1 0.99495
wandb: final_train_precision 0.99798
wandb:    final_train_recall 0.99195
wandb:    final_val_accuracy 0.93182
wandb:          final_val_f1 0.93023
wandb:   final_val_precision 0.86957
wandb:      final_val_recall 1
wandb:         learning_rate 0.0001
wandb:            train_loss 0.51567
wandb:            train_time 25.13474
wandb:          val_accuracy 0.93182
wandb:                val_f1 0.93023
wandb:              val_loss 0.5651
wandb:         val_precision 0.86957
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_163504-85y41x3p
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_163504-85y41x3p/logs
Experiment probe_layer3_question_type_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/ar/ar/results.json for layer 3
Running experiment: probe_layer3_complexity_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=3"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer3_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/layer3/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-03 16:36:47,647][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/ar
experiment_name: probe_layer3_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-03 16:36:47,647][__main__][INFO] - Normalized task: complexity
[2025-05-03 16:36:47,647][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-03 16:36:47,647][__main__][INFO] - Determined Task Type: regression
[2025-05-03 16:36:47,651][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-03 16:36:47,651][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-03 16:36:54,169][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-03 16:36:56,618][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-03 16:36:56,618][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:36:57,181][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:36:57,413][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:36:57,935][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-03 16:36:57,942][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:36:57,942][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-03 16:36:57,943][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:36:58,103][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:36:58,256][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:36:58,327][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-03 16:36:58,329][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:36:58,329][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-03 16:36:58,330][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:36:58,477][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:36:58,579][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:36:58,605][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-03 16:36:58,606][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:36:58,606][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-03 16:36:58,608][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-03 16:36:58,609][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:36:58,610][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:36:58,610][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:36:58,610][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:36:58,610][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:36:58,610][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-05-03 16:36:58,610][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-03 16:36:58,610][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-05-03 16:36:58,611][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:36:58,611][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:36:58,611][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:36:58,611][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:36:58,611][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:36:58,611][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-05-03 16:36:58,611][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-03 16:36:58,611][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-05-03 16:36:58,611][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:36:58,611][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:36:58,612][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:36:58,612][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:36:58,612][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:36:58,612][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-05-03 16:36:58,612][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-03 16:36:58,612][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-05-03 16:36:58,612][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-03 16:36:58,612][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-03 16:36:58,612][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-03 16:36:58,613][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-03 16:36:58,613][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-03 16:37:09,413][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-03 16:37:09,414][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-03 16:37:09,414][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=3, freeze_model=True
[2025-05-03 16:37:09,414][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-03 16:37:09,417][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-03 16:37:09,418][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-03 16:37:09,418][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-03 16:37:09,418][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-03 16:37:09,418][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-03 16:37:09,419][__main__][INFO] - Total parameters: 394,255,105
[2025-05-03 16:37:09,419][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.2793Epoch 1/15: [                              ] 2/63 batches, loss: 0.4165Epoch 1/15: [=                             ] 3/63 batches, loss: 0.3693Epoch 1/15: [=                             ] 4/63 batches, loss: 0.4068Epoch 1/15: [==                            ] 5/63 batches, loss: 0.4181Epoch 1/15: [==                            ] 6/63 batches, loss: 0.3811Epoch 1/15: [===                           ] 7/63 batches, loss: 0.3818Epoch 1/15: [===                           ] 8/63 batches, loss: 0.4011Epoch 1/15: [====                          ] 9/63 batches, loss: 0.4166Epoch 1/15: [====                          ] 10/63 batches, loss: 0.4152Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.3904Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.4023Epoch 1/15: [======                        ] 13/63 batches, loss: 0.3855Epoch 1/15: [======                        ] 14/63 batches, loss: 0.3782Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.3901Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.3946Epoch 1/15: [========                      ] 17/63 batches, loss: 0.3957Epoch 1/15: [========                      ] 18/63 batches, loss: 0.4045Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.3909Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.3902Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.3906Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.3858Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.3807Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.3751Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.3731Epoch 1/15: [============                  ] 26/63 batches, loss: 0.3682Epoch 1/15: [============                  ] 27/63 batches, loss: 0.3696Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.3630Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.3597Epoch 1/15: [==============                ] 30/63 batches, loss: 0.3611Epoch 1/15: [==============                ] 31/63 batches, loss: 0.3624Epoch 1/15: [===============               ] 32/63 batches, loss: 0.3630Epoch 1/15: [===============               ] 33/63 batches, loss: 0.3584Epoch 1/15: [================              ] 34/63 batches, loss: 0.3543Epoch 1/15: [================              ] 35/63 batches, loss: 0.3488Epoch 1/15: [=================             ] 36/63 batches, loss: 0.3534Epoch 1/15: [=================             ] 37/63 batches, loss: 0.3514Epoch 1/15: [==================            ] 38/63 batches, loss: 0.3484Epoch 1/15: [==================            ] 39/63 batches, loss: 0.3424Epoch 1/15: [===================           ] 40/63 batches, loss: 0.3401Epoch 1/15: [===================           ] 41/63 batches, loss: 0.3358Epoch 1/15: [====================          ] 42/63 batches, loss: 0.3298Epoch 1/15: [====================          ] 43/63 batches, loss: 0.3256Epoch 1/15: [====================          ] 44/63 batches, loss: 0.3226Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.3229Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.3195Epoch 1/15: [======================        ] 47/63 batches, loss: 0.3165Epoch 1/15: [======================        ] 48/63 batches, loss: 0.3146Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.3115Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.3088Epoch 1/15: [========================      ] 51/63 batches, loss: 0.3059Epoch 1/15: [========================      ] 52/63 batches, loss: 0.3033Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.3016Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.3026Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.3005Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.2961Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.2945Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.2929Epoch 1/15: [============================  ] 59/63 batches, loss: 0.2913Epoch 1/15: [============================  ] 60/63 batches, loss: 0.2901Epoch 1/15: [============================= ] 61/63 batches, loss: 0.2891Epoch 1/15: [============================= ] 62/63 batches, loss: 0.2887Epoch 1/15: [==============================] 63/63 batches, loss: 0.2860
[2025-05-03 16:37:17,487][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2860
[2025-05-03 16:37:17,850][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0706, Metrics: {'mse': 0.06913288682699203, 'rmse': 0.2629313348138484, 'r2': -0.0655677318572998}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.1391Epoch 2/15: [                              ] 2/63 batches, loss: 0.1559Epoch 2/15: [=                             ] 3/63 batches, loss: 0.1495Epoch 2/15: [=                             ] 4/63 batches, loss: 0.1701Epoch 2/15: [==                            ] 5/63 batches, loss: 0.1693Epoch 2/15: [==                            ] 6/63 batches, loss: 0.1926Epoch 2/15: [===                           ] 7/63 batches, loss: 0.1833Epoch 2/15: [===                           ] 8/63 batches, loss: 0.1882Epoch 2/15: [====                          ] 9/63 batches, loss: 0.2003Epoch 2/15: [====                          ] 10/63 batches, loss: 0.2010Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.1909Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.1906Epoch 2/15: [======                        ] 13/63 batches, loss: 0.1923Epoch 2/15: [======                        ] 14/63 batches, loss: 0.1848Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.1816Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.1759Epoch 2/15: [========                      ] 17/63 batches, loss: 0.1744Epoch 2/15: [========                      ] 18/63 batches, loss: 0.1770Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.1770Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.1746Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.1782Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.1760Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.1770Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.1773Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.1777Epoch 2/15: [============                  ] 26/63 batches, loss: 0.1745Epoch 2/15: [============                  ] 27/63 batches, loss: 0.1721Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.1707Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.1720Epoch 2/15: [==============                ] 30/63 batches, loss: 0.1697Epoch 2/15: [==============                ] 31/63 batches, loss: 0.1654Epoch 2/15: [===============               ] 32/63 batches, loss: 0.1648Epoch 2/15: [===============               ] 33/63 batches, loss: 0.1645Epoch 2/15: [================              ] 34/63 batches, loss: 0.1659Epoch 2/15: [================              ] 35/63 batches, loss: 0.1652Epoch 2/15: [=================             ] 36/63 batches, loss: 0.1636Epoch 2/15: [=================             ] 37/63 batches, loss: 0.1627Epoch 2/15: [==================            ] 38/63 batches, loss: 0.1638Epoch 2/15: [==================            ] 39/63 batches, loss: 0.1631Epoch 2/15: [===================           ] 40/63 batches, loss: 0.1628Epoch 2/15: [===================           ] 41/63 batches, loss: 0.1631Epoch 2/15: [====================          ] 42/63 batches, loss: 0.1632Epoch 2/15: [====================          ] 43/63 batches, loss: 0.1622Epoch 2/15: [====================          ] 44/63 batches, loss: 0.1628Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.1615Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.1640Epoch 2/15: [======================        ] 47/63 batches, loss: 0.1647Epoch 2/15: [======================        ] 48/63 batches, loss: 0.1632Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.1613Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.1601Epoch 2/15: [========================      ] 51/63 batches, loss: 0.1609Epoch 2/15: [========================      ] 52/63 batches, loss: 0.1603Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.1606Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.1610Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.1588Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.1574Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.1570Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.1556Epoch 2/15: [============================  ] 59/63 batches, loss: 0.1552Epoch 2/15: [============================  ] 60/63 batches, loss: 0.1564Epoch 2/15: [============================= ] 61/63 batches, loss: 0.1565Epoch 2/15: [============================= ] 62/63 batches, loss: 0.1558Epoch 2/15: [==============================] 63/63 batches, loss: 0.1545
[2025-05-03 16:37:20,315][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1545
[2025-05-03 16:37:20,546][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0559, Metrics: {'mse': 0.05532652512192726, 'rmse': 0.23521591171076683, 'r2': 0.14723420143127441}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.1365Epoch 3/15: [                              ] 2/63 batches, loss: 0.1099Epoch 3/15: [=                             ] 3/63 batches, loss: 0.1257Epoch 3/15: [=                             ] 4/63 batches, loss: 0.1203Epoch 3/15: [==                            ] 5/63 batches, loss: 0.1269Epoch 3/15: [==                            ] 6/63 batches, loss: 0.1247Epoch 3/15: [===                           ] 7/63 batches, loss: 0.1197Epoch 3/15: [===                           ] 8/63 batches, loss: 0.1223Epoch 3/15: [====                          ] 9/63 batches, loss: 0.1165Epoch 3/15: [====                          ] 10/63 batches, loss: 0.1179Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.1127Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.1174Epoch 3/15: [======                        ] 13/63 batches, loss: 0.1156Epoch 3/15: [======                        ] 14/63 batches, loss: 0.1129Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.1138Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.1116Epoch 3/15: [========                      ] 17/63 batches, loss: 0.1098Epoch 3/15: [========                      ] 18/63 batches, loss: 0.1093Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.1093Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.1073Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.1105Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.1083Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.1077Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.1056Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.1035Epoch 3/15: [============                  ] 26/63 batches, loss: 0.1023Epoch 3/15: [============                  ] 27/63 batches, loss: 0.1022Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.1042Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.1026Epoch 3/15: [==============                ] 30/63 batches, loss: 0.1026Epoch 3/15: [==============                ] 31/63 batches, loss: 0.1041Epoch 3/15: [===============               ] 32/63 batches, loss: 0.1045Epoch 3/15: [===============               ] 33/63 batches, loss: 0.1047Epoch 3/15: [================              ] 34/63 batches, loss: 0.1053Epoch 3/15: [================              ] 35/63 batches, loss: 0.1117Epoch 3/15: [=================             ] 36/63 batches, loss: 0.1109Epoch 3/15: [=================             ] 37/63 batches, loss: 0.1096Epoch 3/15: [==================            ] 38/63 batches, loss: 0.1094Epoch 3/15: [==================            ] 39/63 batches, loss: 0.1087Epoch 3/15: [===================           ] 40/63 batches, loss: 0.1077Epoch 3/15: [===================           ] 41/63 batches, loss: 0.1075Epoch 3/15: [====================          ] 42/63 batches, loss: 0.1091Epoch 3/15: [====================          ] 43/63 batches, loss: 0.1092Epoch 3/15: [====================          ] 44/63 batches, loss: 0.1110Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.1111Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.1096Epoch 3/15: [======================        ] 47/63 batches, loss: 0.1103Epoch 3/15: [======================        ] 48/63 batches, loss: 0.1101Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.1092Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.1094Epoch 3/15: [========================      ] 51/63 batches, loss: 0.1084Epoch 3/15: [========================      ] 52/63 batches, loss: 0.1074Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.1078Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.1072Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.1068Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.1067Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.1064Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.1071Epoch 3/15: [============================  ] 59/63 batches, loss: 0.1071Epoch 3/15: [============================  ] 60/63 batches, loss: 0.1062Epoch 3/15: [============================= ] 61/63 batches, loss: 0.1061Epoch 3/15: [============================= ] 62/63 batches, loss: 0.1047Epoch 3/15: [==============================] 63/63 batches, loss: 0.1038
[2025-05-03 16:37:22,856][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1038
[2025-05-03 16:37:23,161][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0598, Metrics: {'mse': 0.05961998924612999, 'rmse': 0.24417204845381052, 'r2': 0.08105766773223877}
[2025-05-03 16:37:23,161][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.0558Epoch 4/15: [                              ] 2/63 batches, loss: 0.0971Epoch 4/15: [=                             ] 3/63 batches, loss: 0.0870Epoch 4/15: [=                             ] 4/63 batches, loss: 0.0826Epoch 4/15: [==                            ] 5/63 batches, loss: 0.1024Epoch 4/15: [==                            ] 6/63 batches, loss: 0.1000Epoch 4/15: [===                           ] 7/63 batches, loss: 0.1020Epoch 4/15: [===                           ] 8/63 batches, loss: 0.1077Epoch 4/15: [====                          ] 9/63 batches, loss: 0.1057Epoch 4/15: [====                          ] 10/63 batches, loss: 0.1061Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.1095Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.1094Epoch 4/15: [======                        ] 13/63 batches, loss: 0.1065Epoch 4/15: [======                        ] 14/63 batches, loss: 0.1052Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.1033Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.1102Epoch 4/15: [========                      ] 17/63 batches, loss: 0.1086Epoch 4/15: [========                      ] 18/63 batches, loss: 0.1066Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.1045Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.1029Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.1008Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.0995Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.0981Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.0985Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.1008Epoch 4/15: [============                  ] 26/63 batches, loss: 0.1001Epoch 4/15: [============                  ] 27/63 batches, loss: 0.1006Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.1000Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.0997Epoch 4/15: [==============                ] 30/63 batches, loss: 0.0977Epoch 4/15: [==============                ] 31/63 batches, loss: 0.0982Epoch 4/15: [===============               ] 32/63 batches, loss: 0.0966Epoch 4/15: [===============               ] 33/63 batches, loss: 0.0965Epoch 4/15: [================              ] 34/63 batches, loss: 0.0958Epoch 4/15: [================              ] 35/63 batches, loss: 0.0942Epoch 4/15: [=================             ] 36/63 batches, loss: 0.0935Epoch 4/15: [=================             ] 37/63 batches, loss: 0.0923Epoch 4/15: [==================            ] 38/63 batches, loss: 0.0934Epoch 4/15: [==================            ] 39/63 batches, loss: 0.0933Epoch 4/15: [===================           ] 40/63 batches, loss: 0.0931Epoch 4/15: [===================           ] 41/63 batches, loss: 0.0942Epoch 4/15: [====================          ] 42/63 batches, loss: 0.0959Epoch 4/15: [====================          ] 43/63 batches, loss: 0.0956Epoch 4/15: [====================          ] 44/63 batches, loss: 0.0949Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.0945Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.0939Epoch 4/15: [======================        ] 47/63 batches, loss: 0.0942Epoch 4/15: [======================        ] 48/63 batches, loss: 0.0941Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.0927Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.0923Epoch 4/15: [========================      ] 51/63 batches, loss: 0.0929Epoch 4/15: [========================      ] 52/63 batches, loss: 0.0923Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.0917Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.0909Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.0906Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.0904Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.0912Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.0908Epoch 4/15: [============================  ] 59/63 batches, loss: 0.0903Epoch 4/15: [============================  ] 60/63 batches, loss: 0.0899Epoch 4/15: [============================= ] 61/63 batches, loss: 0.0904Epoch 4/15: [============================= ] 62/63 batches, loss: 0.0906Epoch 4/15: [==============================] 63/63 batches, loss: 0.0899
[2025-05-03 16:37:25,146][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0899
[2025-05-03 16:37:25,640][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0445, Metrics: {'mse': 0.04434819519519806, 'rmse': 0.210590111817241, 'r2': 0.31644684076309204}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.0683Epoch 5/15: [                              ] 2/63 batches, loss: 0.0776Epoch 5/15: [=                             ] 3/63 batches, loss: 0.0835Epoch 5/15: [=                             ] 4/63 batches, loss: 0.0748Epoch 5/15: [==                            ] 5/63 batches, loss: 0.0834Epoch 5/15: [==                            ] 6/63 batches, loss: 0.0799Epoch 5/15: [===                           ] 7/63 batches, loss: 0.0750Epoch 5/15: [===                           ] 8/63 batches, loss: 0.0718Epoch 5/15: [====                          ] 9/63 batches, loss: 0.0691Epoch 5/15: [====                          ] 10/63 batches, loss: 0.0668Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.0672Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.0685Epoch 5/15: [======                        ] 13/63 batches, loss: 0.0694Epoch 5/15: [======                        ] 14/63 batches, loss: 0.0728Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.0721Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.0712Epoch 5/15: [========                      ] 17/63 batches, loss: 0.0751Epoch 5/15: [========                      ] 18/63 batches, loss: 0.0734Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.0746Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.0754Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.0730Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.0734Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.0743Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.0731Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.0739Epoch 5/15: [============                  ] 26/63 batches, loss: 0.0743Epoch 5/15: [============                  ] 27/63 batches, loss: 0.0735Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.0737Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.0739Epoch 5/15: [==============                ] 30/63 batches, loss: 0.0765Epoch 5/15: [==============                ] 31/63 batches, loss: 0.0766Epoch 5/15: [===============               ] 32/63 batches, loss: 0.0764Epoch 5/15: [===============               ] 33/63 batches, loss: 0.0757Epoch 5/15: [================              ] 34/63 batches, loss: 0.0763Epoch 5/15: [================              ] 35/63 batches, loss: 0.0754Epoch 5/15: [=================             ] 36/63 batches, loss: 0.0747Epoch 5/15: [=================             ] 37/63 batches, loss: 0.0743Epoch 5/15: [==================            ] 38/63 batches, loss: 0.0737Epoch 5/15: [==================            ] 39/63 batches, loss: 0.0739Epoch 5/15: [===================           ] 40/63 batches, loss: 0.0737Epoch 5/15: [===================           ] 41/63 batches, loss: 0.0743Epoch 5/15: [====================          ] 42/63 batches, loss: 0.0746Epoch 5/15: [====================          ] 43/63 batches, loss: 0.0750Epoch 5/15: [====================          ] 44/63 batches, loss: 0.0749Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.0751Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.0751Epoch 5/15: [======================        ] 47/63 batches, loss: 0.0748Epoch 5/15: [======================        ] 48/63 batches, loss: 0.0743Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.0737Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.0735Epoch 5/15: [========================      ] 51/63 batches, loss: 0.0731Epoch 5/15: [========================      ] 52/63 batches, loss: 0.0732Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.0728Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.0722Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.0719Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.0717Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.0716Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.0712Epoch 5/15: [============================  ] 59/63 batches, loss: 0.0722Epoch 5/15: [============================  ] 60/63 batches, loss: 0.0724Epoch 5/15: [============================= ] 61/63 batches, loss: 0.0717Epoch 5/15: [============================= ] 62/63 batches, loss: 0.0719Epoch 5/15: [==============================] 63/63 batches, loss: 0.0727
[2025-05-03 16:37:28,099][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0727
[2025-05-03 16:37:28,479][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0443, Metrics: {'mse': 0.04411409795284271, 'rmse': 0.21003356387216476, 'r2': 0.3200550675392151}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.0662Epoch 6/15: [                              ] 2/63 batches, loss: 0.0759Epoch 6/15: [=                             ] 3/63 batches, loss: 0.0931Epoch 6/15: [=                             ] 4/63 batches, loss: 0.0857Epoch 6/15: [==                            ] 5/63 batches, loss: 0.0823Epoch 6/15: [==                            ] 6/63 batches, loss: 0.0743Epoch 6/15: [===                           ] 7/63 batches, loss: 0.0745Epoch 6/15: [===                           ] 8/63 batches, loss: 0.0696Epoch 6/15: [====                          ] 9/63 batches, loss: 0.0697Epoch 6/15: [====                          ] 10/63 batches, loss: 0.0672Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.0739Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.0715Epoch 6/15: [======                        ] 13/63 batches, loss: 0.0679Epoch 6/15: [======                        ] 14/63 batches, loss: 0.0657Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.0688Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.0686Epoch 6/15: [========                      ] 17/63 batches, loss: 0.0693Epoch 6/15: [========                      ] 18/63 batches, loss: 0.0717Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.0708Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.0701Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.0677Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.0695Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.0674Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.0671Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.0695Epoch 6/15: [============                  ] 26/63 batches, loss: 0.0687Epoch 6/15: [============                  ] 27/63 batches, loss: 0.0699Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.0687Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.0684Epoch 6/15: [==============                ] 30/63 batches, loss: 0.0691Epoch 6/15: [==============                ] 31/63 batches, loss: 0.0682Epoch 6/15: [===============               ] 32/63 batches, loss: 0.0669Epoch 6/15: [===============               ] 33/63 batches, loss: 0.0661Epoch 6/15: [================              ] 34/63 batches, loss: 0.0663Epoch 6/15: [================              ] 35/63 batches, loss: 0.0667Epoch 6/15: [=================             ] 36/63 batches, loss: 0.0669Epoch 6/15: [=================             ] 37/63 batches, loss: 0.0660Epoch 6/15: [==================            ] 38/63 batches, loss: 0.0654Epoch 6/15: [==================            ] 39/63 batches, loss: 0.0653Epoch 6/15: [===================           ] 40/63 batches, loss: 0.0643Epoch 6/15: [===================           ] 41/63 batches, loss: 0.0653Epoch 6/15: [====================          ] 42/63 batches, loss: 0.0644Epoch 6/15: [====================          ] 43/63 batches, loss: 0.0638Epoch 6/15: [====================          ] 44/63 batches, loss: 0.0633Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.0630Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.0628Epoch 6/15: [======================        ] 47/63 batches, loss: 0.0629Epoch 6/15: [======================        ] 48/63 batches, loss: 0.0628Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.0630Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.0632Epoch 6/15: [========================      ] 51/63 batches, loss: 0.0632Epoch 6/15: [========================      ] 52/63 batches, loss: 0.0639Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.0638Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.0633Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.0629Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.0625Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.0627Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.0622Epoch 6/15: [============================  ] 59/63 batches, loss: 0.0624Epoch 6/15: [============================  ] 60/63 batches, loss: 0.0633Epoch 6/15: [============================= ] 61/63 batches, loss: 0.0632Epoch 6/15: [============================= ] 62/63 batches, loss: 0.0629Epoch 6/15: [==============================] 63/63 batches, loss: 0.0629
[2025-05-03 16:37:30,953][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0629
[2025-05-03 16:37:31,320][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0435, Metrics: {'mse': 0.04341353476047516, 'rmse': 0.2083591484923932, 'r2': 0.33085304498672485}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.0339Epoch 7/15: [                              ] 2/63 batches, loss: 0.0505Epoch 7/15: [=                             ] 3/63 batches, loss: 0.0512Epoch 7/15: [=                             ] 4/63 batches, loss: 0.0594Epoch 7/15: [==                            ] 5/63 batches, loss: 0.0649Epoch 7/15: [==                            ] 6/63 batches, loss: 0.0641Epoch 7/15: [===                           ] 7/63 batches, loss: 0.0614Epoch 7/15: [===                           ] 8/63 batches, loss: 0.0602Epoch 7/15: [====                          ] 9/63 batches, loss: 0.0619Epoch 7/15: [====                          ] 10/63 batches, loss: 0.0575Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.0579Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.0574Epoch 7/15: [======                        ] 13/63 batches, loss: 0.0553Epoch 7/15: [======                        ] 14/63 batches, loss: 0.0554Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.0547Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.0552Epoch 7/15: [========                      ] 17/63 batches, loss: 0.0554Epoch 7/15: [========                      ] 18/63 batches, loss: 0.0537Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.0549Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.0545Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.0551Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.0544Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.0563Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.0558Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.0550Epoch 7/15: [============                  ] 26/63 batches, loss: 0.0555Epoch 7/15: [============                  ] 27/63 batches, loss: 0.0554Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.0557Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.0552Epoch 7/15: [==============                ] 30/63 batches, loss: 0.0542Epoch 7/15: [==============                ] 31/63 batches, loss: 0.0537Epoch 7/15: [===============               ] 32/63 batches, loss: 0.0547Epoch 7/15: [===============               ] 33/63 batches, loss: 0.0543Epoch 7/15: [================              ] 34/63 batches, loss: 0.0546Epoch 7/15: [================              ] 35/63 batches, loss: 0.0538Epoch 7/15: [=================             ] 36/63 batches, loss: 0.0542Epoch 7/15: [=================             ] 37/63 batches, loss: 0.0551Epoch 7/15: [==================            ] 38/63 batches, loss: 0.0547Epoch 7/15: [==================            ] 39/63 batches, loss: 0.0550Epoch 7/15: [===================           ] 40/63 batches, loss: 0.0558Epoch 7/15: [===================           ] 41/63 batches, loss: 0.0550Epoch 7/15: [====================          ] 42/63 batches, loss: 0.0548Epoch 7/15: [====================          ] 43/63 batches, loss: 0.0551Epoch 7/15: [====================          ] 44/63 batches, loss: 0.0552Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.0554Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.0558Epoch 7/15: [======================        ] 47/63 batches, loss: 0.0560Epoch 7/15: [======================        ] 48/63 batches, loss: 0.0563Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.0560Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.0561Epoch 7/15: [========================      ] 51/63 batches, loss: 0.0562Epoch 7/15: [========================      ] 52/63 batches, loss: 0.0557Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.0553Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.0550Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.0550Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.0551Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.0550Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.0551Epoch 7/15: [============================  ] 59/63 batches, loss: 0.0550Epoch 7/15: [============================  ] 60/63 batches, loss: 0.0563Epoch 7/15: [============================= ] 61/63 batches, loss: 0.0564Epoch 7/15: [============================= ] 62/63 batches, loss: 0.0567Epoch 7/15: [==============================] 63/63 batches, loss: 0.0565
[2025-05-03 16:37:33,777][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0565
[2025-05-03 16:37:34,172][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0438, Metrics: {'mse': 0.04362529143691063, 'rmse': 0.20886668340573283, 'r2': 0.327589213848114}
[2025-05-03 16:37:34,173][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0476Epoch 8/15: [                              ] 2/63 batches, loss: 0.0586Epoch 8/15: [=                             ] 3/63 batches, loss: 0.0687Epoch 8/15: [=                             ] 4/63 batches, loss: 0.0603Epoch 8/15: [==                            ] 5/63 batches, loss: 0.0593Epoch 8/15: [==                            ] 6/63 batches, loss: 0.0537Epoch 8/15: [===                           ] 7/63 batches, loss: 0.0510Epoch 8/15: [===                           ] 8/63 batches, loss: 0.0487Epoch 8/15: [====                          ] 9/63 batches, loss: 0.0500Epoch 8/15: [====                          ] 10/63 batches, loss: 0.0477Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.0480Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.0508Epoch 8/15: [======                        ] 13/63 batches, loss: 0.0510Epoch 8/15: [======                        ] 14/63 batches, loss: 0.0505Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.0487Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.0520Epoch 8/15: [========                      ] 17/63 batches, loss: 0.0508Epoch 8/15: [========                      ] 18/63 batches, loss: 0.0511Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.0506Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.0506Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.0500Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.0504Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.0504Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.0492Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.0485Epoch 8/15: [============                  ] 26/63 batches, loss: 0.0482Epoch 8/15: [============                  ] 27/63 batches, loss: 0.0481Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.0473Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.0482Epoch 8/15: [==============                ] 30/63 batches, loss: 0.0478Epoch 8/15: [==============                ] 31/63 batches, loss: 0.0481Epoch 8/15: [===============               ] 32/63 batches, loss: 0.0478Epoch 8/15: [===============               ] 33/63 batches, loss: 0.0490Epoch 8/15: [================              ] 34/63 batches, loss: 0.0489Epoch 8/15: [================              ] 35/63 batches, loss: 0.0494Epoch 8/15: [=================             ] 36/63 batches, loss: 0.0491Epoch 8/15: [=================             ] 37/63 batches, loss: 0.0495Epoch 8/15: [==================            ] 38/63 batches, loss: 0.0502Epoch 8/15: [==================            ] 39/63 batches, loss: 0.0514Epoch 8/15: [===================           ] 40/63 batches, loss: 0.0515Epoch 8/15: [===================           ] 41/63 batches, loss: 0.0512Epoch 8/15: [====================          ] 42/63 batches, loss: 0.0509Epoch 8/15: [====================          ] 43/63 batches, loss: 0.0516Epoch 8/15: [====================          ] 44/63 batches, loss: 0.0518Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.0516Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.0526Epoch 8/15: [======================        ] 47/63 batches, loss: 0.0528Epoch 8/15: [======================        ] 48/63 batches, loss: 0.0530Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.0528Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.0523Epoch 8/15: [========================      ] 51/63 batches, loss: 0.0518Epoch 8/15: [========================      ] 52/63 batches, loss: 0.0517Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.0521Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.0518Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.0517Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.0519Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.0522Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.0521Epoch 8/15: [============================  ] 59/63 batches, loss: 0.0518Epoch 8/15: [============================  ] 60/63 batches, loss: 0.0516Epoch 8/15: [============================= ] 61/63 batches, loss: 0.0521Epoch 8/15: [============================= ] 62/63 batches, loss: 0.0519Epoch 8/15: [==============================] 63/63 batches, loss: 0.0513
[2025-05-03 16:37:36,317][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0513
[2025-05-03 16:37:36,737][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0425, Metrics: {'mse': 0.042216941714286804, 'rmse': 0.20546761719133944, 'r2': 0.349296510219574}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.0321Epoch 9/15: [                              ] 2/63 batches, loss: 0.0300Epoch 9/15: [=                             ] 3/63 batches, loss: 0.0280Epoch 9/15: [=                             ] 4/63 batches, loss: 0.0349Epoch 9/15: [==                            ] 5/63 batches, loss: 0.0380Epoch 9/15: [==                            ] 6/63 batches, loss: 0.0394Epoch 9/15: [===                           ] 7/63 batches, loss: 0.0389Epoch 9/15: [===                           ] 8/63 batches, loss: 0.0395Epoch 9/15: [====                          ] 9/63 batches, loss: 0.0433Epoch 9/15: [====                          ] 10/63 batches, loss: 0.0426Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.0463Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.0441Epoch 9/15: [======                        ] 13/63 batches, loss: 0.0465Epoch 9/15: [======                        ] 14/63 batches, loss: 0.0454Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.0471Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.0464Epoch 9/15: [========                      ] 17/63 batches, loss: 0.0473Epoch 9/15: [========                      ] 18/63 batches, loss: 0.0477Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.0469Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.0470Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.0485Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.0491Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.0496Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.0495Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.0497Epoch 9/15: [============                  ] 26/63 batches, loss: 0.0494Epoch 9/15: [============                  ] 27/63 batches, loss: 0.0486Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.0479Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.0489Epoch 9/15: [==============                ] 30/63 batches, loss: 0.0490Epoch 9/15: [==============                ] 31/63 batches, loss: 0.0484Epoch 9/15: [===============               ] 32/63 batches, loss: 0.0479Epoch 9/15: [===============               ] 33/63 batches, loss: 0.0483Epoch 9/15: [================              ] 34/63 batches, loss: 0.0489Epoch 9/15: [================              ] 35/63 batches, loss: 0.0486Epoch 9/15: [=================             ] 36/63 batches, loss: 0.0480Epoch 9/15: [=================             ] 37/63 batches, loss: 0.0474Epoch 9/15: [==================            ] 38/63 batches, loss: 0.0474Epoch 9/15: [==================            ] 39/63 batches, loss: 0.0474Epoch 9/15: [===================           ] 40/63 batches, loss: 0.0478Epoch 9/15: [===================           ] 41/63 batches, loss: 0.0480Epoch 9/15: [====================          ] 42/63 batches, loss: 0.0475Epoch 9/15: [====================          ] 43/63 batches, loss: 0.0470Epoch 9/15: [====================          ] 44/63 batches, loss: 0.0472Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.0474Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.0474Epoch 9/15: [======================        ] 47/63 batches, loss: 0.0469Epoch 9/15: [======================        ] 48/63 batches, loss: 0.0467Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.0469Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.0478Epoch 9/15: [========================      ] 51/63 batches, loss: 0.0473Epoch 9/15: [========================      ] 52/63 batches, loss: 0.0476Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.0485Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.0482Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.0488Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.0484Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.0485Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.0486Epoch 9/15: [============================  ] 59/63 batches, loss: 0.0481Epoch 9/15: [============================  ] 60/63 batches, loss: 0.0478Epoch 9/15: [============================= ] 61/63 batches, loss: 0.0477Epoch 9/15: [============================= ] 62/63 batches, loss: 0.0480Epoch 9/15: [==============================] 63/63 batches, loss: 0.0481
[2025-05-03 16:37:39,309][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0481
[2025-05-03 16:37:39,635][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0426, Metrics: {'mse': 0.04234718158841133, 'rmse': 0.20578430841152912, 'r2': 0.3472890853881836}
[2025-05-03 16:37:39,636][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.0406Epoch 10/15: [                              ] 2/63 batches, loss: 0.0434Epoch 10/15: [=                             ] 3/63 batches, loss: 0.0449Epoch 10/15: [=                             ] 4/63 batches, loss: 0.0478Epoch 10/15: [==                            ] 5/63 batches, loss: 0.0500Epoch 10/15: [==                            ] 6/63 batches, loss: 0.0463Epoch 10/15: [===                           ] 7/63 batches, loss: 0.0454Epoch 10/15: [===                           ] 8/63 batches, loss: 0.0439Epoch 10/15: [====                          ] 9/63 batches, loss: 0.0451Epoch 10/15: [====                          ] 10/63 batches, loss: 0.0446Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.0429Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.0415Epoch 10/15: [======                        ] 13/63 batches, loss: 0.0418Epoch 10/15: [======                        ] 14/63 batches, loss: 0.0436Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.0453Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.0459Epoch 10/15: [========                      ] 17/63 batches, loss: 0.0456Epoch 10/15: [========                      ] 18/63 batches, loss: 0.0453Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.0446Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.0449Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.0445Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.0454Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.0456Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.0460Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.0461Epoch 10/15: [============                  ] 26/63 batches, loss: 0.0460Epoch 10/15: [============                  ] 27/63 batches, loss: 0.0463Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.0454Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.0452Epoch 10/15: [==============                ] 30/63 batches, loss: 0.0465Epoch 10/15: [==============                ] 31/63 batches, loss: 0.0456Epoch 10/15: [===============               ] 32/63 batches, loss: 0.0454Epoch 10/15: [===============               ] 33/63 batches, loss: 0.0451Epoch 10/15: [================              ] 34/63 batches, loss: 0.0449Epoch 10/15: [================              ] 35/63 batches, loss: 0.0447Epoch 10/15: [=================             ] 36/63 batches, loss: 0.0450Epoch 10/15: [=================             ] 37/63 batches, loss: 0.0452Epoch 10/15: [==================            ] 38/63 batches, loss: 0.0449Epoch 10/15: [==================            ] 39/63 batches, loss: 0.0448Epoch 10/15: [===================           ] 40/63 batches, loss: 0.0452Epoch 10/15: [===================           ] 41/63 batches, loss: 0.0456Epoch 10/15: [====================          ] 42/63 batches, loss: 0.0458Epoch 10/15: [====================          ] 43/63 batches, loss: 0.0456Epoch 10/15: [====================          ] 44/63 batches, loss: 0.0458Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.0457Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.0456Epoch 10/15: [======================        ] 47/63 batches, loss: 0.0466Epoch 10/15: [======================        ] 48/63 batches, loss: 0.0464Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.0467Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.0464Epoch 10/15: [========================      ] 51/63 batches, loss: 0.0462Epoch 10/15: [========================      ] 52/63 batches, loss: 0.0459Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.0456Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.0462Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.0457Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.0462Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.0459Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.0457Epoch 10/15: [============================  ] 59/63 batches, loss: 0.0460Epoch 10/15: [============================  ] 60/63 batches, loss: 0.0461Epoch 10/15: [============================= ] 61/63 batches, loss: 0.0461Epoch 10/15: [============================= ] 62/63 batches, loss: 0.0468Epoch 10/15: [==============================] 63/63 batches, loss: 0.0468
[2025-05-03 16:37:41,664][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0468
[2025-05-03 16:37:41,962][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0391, Metrics: {'mse': 0.03890368714928627, 'rmse': 0.19724017630616303, 'r2': 0.4003647565841675}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.0547Epoch 11/15: [                              ] 2/63 batches, loss: 0.0405Epoch 11/15: [=                             ] 3/63 batches, loss: 0.0442Epoch 11/15: [=                             ] 4/63 batches, loss: 0.0456Epoch 11/15: [==                            ] 5/63 batches, loss: 0.0442Epoch 11/15: [==                            ] 6/63 batches, loss: 0.0409Epoch 11/15: [===                           ] 7/63 batches, loss: 0.0453Epoch 11/15: [===                           ] 8/63 batches, loss: 0.0429Epoch 11/15: [====                          ] 9/63 batches, loss: 0.0434Epoch 11/15: [====                          ] 10/63 batches, loss: 0.0454Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.0440Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.0446Epoch 11/15: [======                        ] 13/63 batches, loss: 0.0445Epoch 11/15: [======                        ] 14/63 batches, loss: 0.0439Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.0449Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.0440Epoch 11/15: [========                      ] 17/63 batches, loss: 0.0433Epoch 11/15: [========                      ] 18/63 batches, loss: 0.0439Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.0439Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.0443Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.0440Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.0431Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.0431Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.0433Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.0427Epoch 11/15: [============                  ] 26/63 batches, loss: 0.0419Epoch 11/15: [============                  ] 27/63 batches, loss: 0.0436Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.0450Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.0458Epoch 11/15: [==============                ] 30/63 batches, loss: 0.0463Epoch 11/15: [==============                ] 31/63 batches, loss: 0.0459Epoch 11/15: [===============               ] 32/63 batches, loss: 0.0458Epoch 11/15: [===============               ] 33/63 batches, loss: 0.0453Epoch 11/15: [================              ] 34/63 batches, loss: 0.0447Epoch 11/15: [================              ] 35/63 batches, loss: 0.0445Epoch 11/15: [=================             ] 36/63 batches, loss: 0.0443Epoch 11/15: [=================             ] 37/63 batches, loss: 0.0444Epoch 11/15: [==================            ] 38/63 batches, loss: 0.0442Epoch 11/15: [==================            ] 39/63 batches, loss: 0.0439Epoch 11/15: [===================           ] 40/63 batches, loss: 0.0436Epoch 11/15: [===================           ] 41/63 batches, loss: 0.0431Epoch 11/15: [====================          ] 42/63 batches, loss: 0.0427Epoch 11/15: [====================          ] 43/63 batches, loss: 0.0425Epoch 11/15: [====================          ] 44/63 batches, loss: 0.0426Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.0435Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.0436Epoch 11/15: [======================        ] 47/63 batches, loss: 0.0438Epoch 11/15: [======================        ] 48/63 batches, loss: 0.0438Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.0442Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.0441Epoch 11/15: [========================      ] 51/63 batches, loss: 0.0437Epoch 11/15: [========================      ] 52/63 batches, loss: 0.0436Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.0435Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.0441Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.0440Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.0438Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.0440Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.0435Epoch 11/15: [============================  ] 59/63 batches, loss: 0.0436Epoch 11/15: [============================  ] 60/63 batches, loss: 0.0434Epoch 11/15: [============================= ] 61/63 batches, loss: 0.0434Epoch 11/15: [============================= ] 62/63 batches, loss: 0.0435Epoch 11/15: [==============================] 63/63 batches, loss: 0.0429
[2025-05-03 16:37:44,494][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0429
[2025-05-03 16:37:44,866][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0383, Metrics: {'mse': 0.03798212856054306, 'rmse': 0.19489004223033835, 'r2': 0.41456907987594604}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.0616Epoch 12/15: [                              ] 2/63 batches, loss: 0.0448Epoch 12/15: [=                             ] 3/63 batches, loss: 0.0502Epoch 12/15: [=                             ] 4/63 batches, loss: 0.0480Epoch 12/15: [==                            ] 5/63 batches, loss: 0.0475Epoch 12/15: [==                            ] 6/63 batches, loss: 0.0460Epoch 12/15: [===                           ] 7/63 batches, loss: 0.0446Epoch 12/15: [===                           ] 8/63 batches, loss: 0.0437Epoch 12/15: [====                          ] 9/63 batches, loss: 0.0437Epoch 12/15: [====                          ] 10/63 batches, loss: 0.0424Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.0427Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.0420Epoch 12/15: [======                        ] 13/63 batches, loss: 0.0432Epoch 12/15: [======                        ] 14/63 batches, loss: 0.0443Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.0434Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.0444Epoch 12/15: [========                      ] 17/63 batches, loss: 0.0442Epoch 12/15: [========                      ] 18/63 batches, loss: 0.0459Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.0459Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.0453Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.0460Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.0463Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.0468Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.0462Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.0469Epoch 12/15: [============                  ] 26/63 batches, loss: 0.0471Epoch 12/15: [============                  ] 27/63 batches, loss: 0.0476Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.0477Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.0471Epoch 12/15: [==============                ] 30/63 batches, loss: 0.0463Epoch 12/15: [==============                ] 31/63 batches, loss: 0.0454Epoch 12/15: [===============               ] 32/63 batches, loss: 0.0446Epoch 12/15: [===============               ] 33/63 batches, loss: 0.0442Epoch 12/15: [================              ] 34/63 batches, loss: 0.0440Epoch 12/15: [================              ] 35/63 batches, loss: 0.0437Epoch 12/15: [=================             ] 36/63 batches, loss: 0.0435Epoch 12/15: [=================             ] 37/63 batches, loss: 0.0431Epoch 12/15: [==================            ] 38/63 batches, loss: 0.0429Epoch 12/15: [==================            ] 39/63 batches, loss: 0.0427Epoch 12/15: [===================           ] 40/63 batches, loss: 0.0428Epoch 12/15: [===================           ] 41/63 batches, loss: 0.0423Epoch 12/15: [====================          ] 42/63 batches, loss: 0.0421Epoch 12/15: [====================          ] 43/63 batches, loss: 0.0420Epoch 12/15: [====================          ] 44/63 batches, loss: 0.0416Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.0421Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.0418Epoch 12/15: [======================        ] 47/63 batches, loss: 0.0419Epoch 12/15: [======================        ] 48/63 batches, loss: 0.0415Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.0416Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.0414Epoch 12/15: [========================      ] 51/63 batches, loss: 0.0411Epoch 12/15: [========================      ] 52/63 batches, loss: 0.0407Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.0410Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.0408Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.0411Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.0412Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.0411Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.0407Epoch 12/15: [============================  ] 59/63 batches, loss: 0.0406Epoch 12/15: [============================  ] 60/63 batches, loss: 0.0405Epoch 12/15: [============================= ] 61/63 batches, loss: 0.0406Epoch 12/15: [============================= ] 62/63 batches, loss: 0.0405Epoch 12/15: [==============================] 63/63 batches, loss: 0.0399
[2025-05-03 16:37:47,323][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0399
[2025-05-03 16:37:47,559][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0365, Metrics: {'mse': 0.036134153604507446, 'rmse': 0.19008985665865352, 'r2': 0.4430525302886963}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.0682Epoch 13/15: [                              ] 2/63 batches, loss: 0.0506Epoch 13/15: [=                             ] 3/63 batches, loss: 0.0480Epoch 13/15: [=                             ] 4/63 batches, loss: 0.0425Epoch 13/15: [==                            ] 5/63 batches, loss: 0.0413Epoch 13/15: [==                            ] 6/63 batches, loss: 0.0439Epoch 13/15: [===                           ] 7/63 batches, loss: 0.0432Epoch 13/15: [===                           ] 8/63 batches, loss: 0.0466Epoch 13/15: [====                          ] 9/63 batches, loss: 0.0449Epoch 13/15: [====                          ] 10/63 batches, loss: 0.0429Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.0428Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.0431Epoch 13/15: [======                        ] 13/63 batches, loss: 0.0411Epoch 13/15: [======                        ] 14/63 batches, loss: 0.0391Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.0402Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.0400Epoch 13/15: [========                      ] 17/63 batches, loss: 0.0398Epoch 13/15: [========                      ] 18/63 batches, loss: 0.0396Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.0404Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.0405Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.0401Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.0398Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.0404Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.0398Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.0391Epoch 13/15: [============                  ] 26/63 batches, loss: 0.0394Epoch 13/15: [============                  ] 27/63 batches, loss: 0.0393Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.0388Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.0385Epoch 13/15: [==============                ] 30/63 batches, loss: 0.0383Epoch 13/15: [==============                ] 31/63 batches, loss: 0.0377Epoch 13/15: [===============               ] 32/63 batches, loss: 0.0377Epoch 13/15: [===============               ] 33/63 batches, loss: 0.0386Epoch 13/15: [================              ] 34/63 batches, loss: 0.0386Epoch 13/15: [================              ] 35/63 batches, loss: 0.0383Epoch 13/15: [=================             ] 36/63 batches, loss: 0.0383Epoch 13/15: [=================             ] 37/63 batches, loss: 0.0382Epoch 13/15: [==================            ] 38/63 batches, loss: 0.0389Epoch 13/15: [==================            ] 39/63 batches, loss: 0.0386Epoch 13/15: [===================           ] 40/63 batches, loss: 0.0388Epoch 13/15: [===================           ] 41/63 batches, loss: 0.0386Epoch 13/15: [====================          ] 42/63 batches, loss: 0.0396Epoch 13/15: [====================          ] 43/63 batches, loss: 0.0391Epoch 13/15: [====================          ] 44/63 batches, loss: 0.0396Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.0393Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.0394Epoch 13/15: [======================        ] 47/63 batches, loss: 0.0392Epoch 13/15: [======================        ] 48/63 batches, loss: 0.0394Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.0397Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.0395Epoch 13/15: [========================      ] 51/63 batches, loss: 0.0395Epoch 13/15: [========================      ] 52/63 batches, loss: 0.0394Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.0392Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.0397Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.0396Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.0393Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.0397Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.0393Epoch 13/15: [============================  ] 59/63 batches, loss: 0.0391Epoch 13/15: [============================  ] 60/63 batches, loss: 0.0390Epoch 13/15: [============================= ] 61/63 batches, loss: 0.0391Epoch 13/15: [============================= ] 62/63 batches, loss: 0.0393Epoch 13/15: [==============================] 63/63 batches, loss: 0.0393
[2025-05-03 16:37:50,012][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0393
[2025-05-03 16:37:50,245][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0335, Metrics: {'mse': 0.033148691058158875, 'rmse': 0.18206781994124846, 'r2': 0.4890683889389038}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.0417Epoch 14/15: [                              ] 2/63 batches, loss: 0.0352Epoch 14/15: [=                             ] 3/63 batches, loss: 0.0441Epoch 14/15: [=                             ] 4/63 batches, loss: 0.0392Epoch 14/15: [==                            ] 5/63 batches, loss: 0.0361Epoch 14/15: [==                            ] 6/63 batches, loss: 0.0366Epoch 14/15: [===                           ] 7/63 batches, loss: 0.0364Epoch 14/15: [===                           ] 8/63 batches, loss: 0.0369Epoch 14/15: [====                          ] 9/63 batches, loss: 0.0354Epoch 14/15: [====                          ] 10/63 batches, loss: 0.0348Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.0340Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.0357Epoch 14/15: [======                        ] 13/63 batches, loss: 0.0351Epoch 14/15: [======                        ] 14/63 batches, loss: 0.0377Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.0375Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.0389Epoch 14/15: [========                      ] 17/63 batches, loss: 0.0395Epoch 14/15: [========                      ] 18/63 batches, loss: 0.0403Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.0396Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.0397Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.0391Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.0383Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.0382Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.0382Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.0379Epoch 14/15: [============                  ] 26/63 batches, loss: 0.0378Epoch 14/15: [============                  ] 27/63 batches, loss: 0.0383Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.0382Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.0382Epoch 14/15: [==============                ] 30/63 batches, loss: 0.0380Epoch 14/15: [==============                ] 31/63 batches, loss: 0.0375Epoch 14/15: [===============               ] 32/63 batches, loss: 0.0387Epoch 14/15: [===============               ] 33/63 batches, loss: 0.0386Epoch 14/15: [================              ] 34/63 batches, loss: 0.0381Epoch 14/15: [================              ] 35/63 batches, loss: 0.0380Epoch 14/15: [=================             ] 36/63 batches, loss: 0.0376Epoch 14/15: [=================             ] 37/63 batches, loss: 0.0379Epoch 14/15: [==================            ] 38/63 batches, loss: 0.0378Epoch 14/15: [==================            ] 39/63 batches, loss: 0.0376Epoch 14/15: [===================           ] 40/63 batches, loss: 0.0379Epoch 14/15: [===================           ] 41/63 batches, loss: 0.0376Epoch 14/15: [====================          ] 42/63 batches, loss: 0.0373Epoch 14/15: [====================          ] 43/63 batches, loss: 0.0370Epoch 14/15: [====================          ] 44/63 batches, loss: 0.0370Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.0377Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.0375Epoch 14/15: [======================        ] 47/63 batches, loss: 0.0371Epoch 14/15: [======================        ] 48/63 batches, loss: 0.0367Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.0367Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.0364Epoch 14/15: [========================      ] 51/63 batches, loss: 0.0364Epoch 14/15: [========================      ] 52/63 batches, loss: 0.0363Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.0365Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.0367Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.0364Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.0361Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.0358Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.0358Epoch 14/15: [============================  ] 59/63 batches, loss: 0.0357Epoch 14/15: [============================  ] 60/63 batches, loss: 0.0353Epoch 14/15: [============================= ] 61/63 batches, loss: 0.0349Epoch 14/15: [============================= ] 62/63 batches, loss: 0.0349Epoch 14/15: [==============================] 63/63 batches, loss: 0.0344
[2025-05-03 16:37:52,628][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0344
[2025-05-03 16:37:52,991][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0313, Metrics: {'mse': 0.03099973127245903, 'rmse': 0.17606740548000085, 'r2': 0.522191047668457}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.0367Epoch 15/15: [                              ] 2/63 batches, loss: 0.0375Epoch 15/15: [=                             ] 3/63 batches, loss: 0.0424Epoch 15/15: [=                             ] 4/63 batches, loss: 0.0372Epoch 15/15: [==                            ] 5/63 batches, loss: 0.0340Epoch 15/15: [==                            ] 6/63 batches, loss: 0.0325Epoch 15/15: [===                           ] 7/63 batches, loss: 0.0315Epoch 15/15: [===                           ] 8/63 batches, loss: 0.0301Epoch 15/15: [====                          ] 9/63 batches, loss: 0.0319Epoch 15/15: [====                          ] 10/63 batches, loss: 0.0309Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.0343Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.0338Epoch 15/15: [======                        ] 13/63 batches, loss: 0.0335Epoch 15/15: [======                        ] 14/63 batches, loss: 0.0348Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.0352Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.0347Epoch 15/15: [========                      ] 17/63 batches, loss: 0.0344Epoch 15/15: [========                      ] 18/63 batches, loss: 0.0344Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.0338Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.0328Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.0323Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.0331Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.0340Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.0341Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.0336Epoch 15/15: [============                  ] 26/63 batches, loss: 0.0336Epoch 15/15: [============                  ] 27/63 batches, loss: 0.0345Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.0351Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.0351Epoch 15/15: [==============                ] 30/63 batches, loss: 0.0349Epoch 15/15: [==============                ] 31/63 batches, loss: 0.0350Epoch 15/15: [===============               ] 32/63 batches, loss: 0.0348Epoch 15/15: [===============               ] 33/63 batches, loss: 0.0350Epoch 15/15: [================              ] 34/63 batches, loss: 0.0345Epoch 15/15: [================              ] 35/63 batches, loss: 0.0350Epoch 15/15: [=================             ] 36/63 batches, loss: 0.0352Epoch 15/15: [=================             ] 37/63 batches, loss: 0.0349Epoch 15/15: [==================            ] 38/63 batches, loss: 0.0348Epoch 15/15: [==================            ] 39/63 batches, loss: 0.0345Epoch 15/15: [===================           ] 40/63 batches, loss: 0.0351Epoch 15/15: [===================           ] 41/63 batches, loss: 0.0353Epoch 15/15: [====================          ] 42/63 batches, loss: 0.0354Epoch 15/15: [====================          ] 43/63 batches, loss: 0.0350Epoch 15/15: [====================          ] 44/63 batches, loss: 0.0355Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.0353Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.0353Epoch 15/15: [======================        ] 47/63 batches, loss: 0.0348Epoch 15/15: [======================        ] 48/63 batches, loss: 0.0347Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.0345Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.0349Epoch 15/15: [========================      ] 51/63 batches, loss: 0.0346Epoch 15/15: [========================      ] 52/63 batches, loss: 0.0345Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.0345Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.0341Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.0339Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.0339Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.0342Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.0341Epoch 15/15: [============================  ] 59/63 batches, loss: 0.0341Epoch 15/15: [============================  ] 60/63 batches, loss: 0.0341Epoch 15/15: [============================= ] 61/63 batches, loss: 0.0340Epoch 15/15: [============================= ] 62/63 batches, loss: 0.0342Epoch 15/15: [==============================] 63/63 batches, loss: 0.0337
[2025-05-03 16:37:55,526][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0337
[2025-05-03 16:37:56,025][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0339, Metrics: {'mse': 0.033467523753643036, 'rmse': 0.18294131232076322, 'r2': 0.4841541647911072}
[2025-05-03 16:37:56,026][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-03 16:37:56,026][src.training.lm_trainer][INFO] - Training completed in 41.35 seconds
[2025-05-03 16:37:56,026][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-03 16:37:59,029][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.01581970602273941, 'rmse': 0.12577641282346785, 'r2': 0.48465585708618164}
[2025-05-03 16:37:59,029][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03099973127245903, 'rmse': 0.17606740548000085, 'r2': 0.522191047668457}
[2025-05-03 16:37:59,029][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.030697615817189217, 'rmse': 0.17520735092224074, 'r2': 0.4707867503166199}
[2025-05-03 16:38:00,810][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/ar/ar/model.pt
[2025-05-03 16:38:00,811][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▃▃▃▂▂▂▁▁
wandb:     best_val_mse █▅▃▃▃▃▂▂▂▁▁
wandb:      best_val_r2 ▁▄▆▆▆▆▇▇▇██
wandb:    best_val_rmse █▆▄▄▄▃▃▃▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▄▃▅▅▅▅▅▅▆▆▆▆▇
wandb:       train_loss █▄▃▃▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▆▃▃▃▃▃▃▂▂▂▁▁▁
wandb:          val_mse █▅▆▃▃▃▃▃▃▂▂▂▁▁▁
wandb:           val_r2 ▁▄▃▆▆▆▆▆▆▇▇▇███
wandb:         val_rmse █▆▆▄▄▄▄▃▃▃▃▂▁▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03133
wandb:     best_val_mse 0.031
wandb:      best_val_r2 0.52219
wandb:    best_val_rmse 0.17607
wandb:            epoch 15
wandb:   final_test_mse 0.0307
wandb:    final_test_r2 0.47079
wandb:  final_test_rmse 0.17521
wandb:  final_train_mse 0.01582
wandb:   final_train_r2 0.48466
wandb: final_train_rmse 0.12578
wandb:    final_val_mse 0.031
wandb:     final_val_r2 0.52219
wandb:   final_val_rmse 0.17607
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03372
wandb:       train_time 41.35033
wandb:         val_loss 0.03394
wandb:          val_mse 0.03347
wandb:           val_r2 0.48415
wandb:         val_rmse 0.18294
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_163647-7bkovqdu
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_163647-7bkovqdu/logs
Experiment probe_layer3_complexity_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/ar/ar/results.json for layer 3
Running experiment: probe_layer3_question_type_en
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=3"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[en]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer3_question_type_en"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/layer3/en"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-03 16:38:46,978][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/en
experiment_name: probe_layer3_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-03 16:38:46,978][__main__][INFO] - Normalized task: question_type
[2025-05-03 16:38:46,978][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-03 16:38:46,978][__main__][INFO] - Determined Task Type: classification
[2025-05-03 16:38:46,982][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-05-03 16:38:46,982][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-03 16:38:53,390][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-03 16:38:55,735][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-03 16:38:55,736][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:38:56,218][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:38:56,372][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:38:56,796][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-05-03 16:38:56,804][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:38:56,805][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-05-03 16:38:56,806][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:38:56,931][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:38:57,081][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:38:57,115][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-05-03 16:38:57,116][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:38:57,116][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-05-03 16:38:57,117][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:38:57,150][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:38:57,271][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:38:57,295][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-05-03 16:38:57,297][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:38:57,297][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-05-03 16:38:57,298][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-05-03 16:38:57,299][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:38:57,299][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:38:57,299][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:38:57,299][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:38:57,299][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-05-03 16:38:57,299][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-05-03 16:38:57,299][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-05-03 16:38:57,300][src.data.datasets][INFO] - Sample label: 1
[2025-05-03 16:38:57,300][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:38:57,300][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:38:57,300][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:38:57,300][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:38:57,300][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-03 16:38:57,300][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-03 16:38:57,300][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-05-03 16:38:57,300][src.data.datasets][INFO] - Sample label: 0
[2025-05-03 16:38:57,300][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:38:57,301][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:38:57,301][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:38:57,301][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:38:57,301][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-03 16:38:57,301][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-03 16:38:57,301][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-05-03 16:38:57,301][src.data.datasets][INFO] - Sample label: 0
[2025-05-03 16:38:57,301][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-05-03 16:38:57,301][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-03 16:38:57,301][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-03 16:38:57,302][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-03 16:38:57,302][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-03 16:39:07,621][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-03 16:39:07,622][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-03 16:39:07,622][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=3, freeze_model=True
[2025-05-03 16:39:07,622][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-03 16:39:07,628][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-03 16:39:07,629][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-03 16:39:07,629][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-03 16:39:07,629][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-03 16:39:07,629][__main__][INFO] - Successfully created lm_probe model for en
[2025-05-03 16:39:07,630][__main__][INFO] - Total parameters: 394,568,839
[2025-05-03 16:39:07,630][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-03 16:39:07,630][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6814Epoch 1/15: [                              ] 2/75 batches, loss: 0.7018Epoch 1/15: [=                             ] 3/75 batches, loss: 0.6803Epoch 1/15: [=                             ] 4/75 batches, loss: 0.6861Epoch 1/15: [==                            ] 5/75 batches, loss: 0.6910Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6893Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6860Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6924Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6926Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6926Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6899Epoch 1/15: [====                          ] 12/75 batches, loss: 0.6893Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.6879Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.6869Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6857Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6855Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6842Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6845Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6847Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6850Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6867Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6836Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6854Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6850Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6847Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6852Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6843Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6837Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6832Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6826Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6828Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6834Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6822Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6828Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6833Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6815Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6837Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6844Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6838Epoch 1/15: [================              ] 40/75 batches, loss: 0.6833Epoch 1/15: [================              ] 41/75 batches, loss: 0.6841Epoch 1/15: [================              ] 42/75 batches, loss: 0.6843Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6846Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6849Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6848Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6840Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6833Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6831Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6833Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6826Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6824Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6825Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6822Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6811Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6809Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6801Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6801Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6807Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6792Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6778Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6789Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6799Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6809Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6801Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6797Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6785Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6777Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6766Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6764Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6759Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6754Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6745Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6743Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6740Epoch 1/15: [==============================] 75/75 batches, loss: 0.6731
[2025-05-03 16:39:15,851][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6731
[2025-05-03 16:39:16,321][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6614, Metrics: {'accuracy': 0.5416666666666666, 'f1': 0.23255813953488372, 'precision': 0.7142857142857143, 'recall': 0.1388888888888889}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.7459Epoch 2/15: [                              ] 2/75 batches, loss: 0.6539Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6465Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6530Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6330Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6263Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6283Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6235Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6180Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6228Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6210Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6221Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6210Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6207Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6199Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6290Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6246Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6253Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6236Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6264Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6283Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6286Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6269Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6250Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6260Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6253Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6246Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6217Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6205Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6191Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6172Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6170Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6185Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6196Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6177Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6176Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6171Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6170Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6165Epoch 2/15: [================              ] 40/75 batches, loss: 0.6178Epoch 2/15: [================              ] 41/75 batches, loss: 0.6167Epoch 2/15: [================              ] 42/75 batches, loss: 0.6175Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6171Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6179Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6186Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6184Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6172Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6164Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6157Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6164Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6166Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6167Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6155Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6155Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6161Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6173Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6156Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6151Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6141Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6133Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6139Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6133Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6122Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6125Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6120Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6117Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6102Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6087Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6075Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6081Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6064Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6062Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6062Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6040Epoch 2/15: [==============================] 75/75 batches, loss: 0.6050
[2025-05-03 16:39:19,073][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6050
[2025-05-03 16:39:19,574][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6205, Metrics: {'accuracy': 0.75, 'f1': 0.6785714285714286, 'precision': 0.95, 'recall': 0.5277777777777778}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.4856Epoch 3/15: [                              ] 2/75 batches, loss: 0.5148Epoch 3/15: [=                             ] 3/75 batches, loss: 0.5462Epoch 3/15: [=                             ] 4/75 batches, loss: 0.5446Epoch 3/15: [==                            ] 5/75 batches, loss: 0.5433Epoch 3/15: [==                            ] 6/75 batches, loss: 0.5496Epoch 3/15: [==                            ] 7/75 batches, loss: 0.5435Epoch 3/15: [===                           ] 8/75 batches, loss: 0.5475Epoch 3/15: [===                           ] 9/75 batches, loss: 0.5514Epoch 3/15: [====                          ] 10/75 batches, loss: 0.5520Epoch 3/15: [====                          ] 11/75 batches, loss: 0.5507Epoch 3/15: [====                          ] 12/75 batches, loss: 0.5492Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.5492Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.5505Epoch 3/15: [======                        ] 15/75 batches, loss: 0.5519Epoch 3/15: [======                        ] 16/75 batches, loss: 0.5552Epoch 3/15: [======                        ] 17/75 batches, loss: 0.5586Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.5607Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.5609Epoch 3/15: [========                      ] 20/75 batches, loss: 0.5655Epoch 3/15: [========                      ] 21/75 batches, loss: 0.5649Epoch 3/15: [========                      ] 22/75 batches, loss: 0.5656Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.5651Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.5623Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.5626Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.5625Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.5620Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.5619Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.5622Epoch 3/15: [============                  ] 30/75 batches, loss: 0.5631Epoch 3/15: [============                  ] 31/75 batches, loss: 0.5632Epoch 3/15: [============                  ] 32/75 batches, loss: 0.5632Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.5619Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.5625Epoch 3/15: [==============                ] 35/75 batches, loss: 0.5625Epoch 3/15: [==============                ] 36/75 batches, loss: 0.5647Epoch 3/15: [==============                ] 37/75 batches, loss: 0.5638Epoch 3/15: [===============               ] 38/75 batches, loss: 0.5645Epoch 3/15: [===============               ] 39/75 batches, loss: 0.5626Epoch 3/15: [================              ] 40/75 batches, loss: 0.5593Epoch 3/15: [================              ] 41/75 batches, loss: 0.5600Epoch 3/15: [================              ] 42/75 batches, loss: 0.5603Epoch 3/15: [=================             ] 43/75 batches, loss: 0.5609Epoch 3/15: [=================             ] 44/75 batches, loss: 0.5621Epoch 3/15: [==================            ] 45/75 batches, loss: 0.5620Epoch 3/15: [==================            ] 46/75 batches, loss: 0.5621Epoch 3/15: [==================            ] 47/75 batches, loss: 0.5622Epoch 3/15: [===================           ] 48/75 batches, loss: 0.5632Epoch 3/15: [===================           ] 49/75 batches, loss: 0.5632Epoch 3/15: [====================          ] 50/75 batches, loss: 0.5619Epoch 3/15: [====================          ] 51/75 batches, loss: 0.5624Epoch 3/15: [====================          ] 52/75 batches, loss: 0.5601Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.5621Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.5625Epoch 3/15: [======================        ] 55/75 batches, loss: 0.5639Epoch 3/15: [======================        ] 56/75 batches, loss: 0.5630Epoch 3/15: [======================        ] 57/75 batches, loss: 0.5612Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.5622Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.5611Epoch 3/15: [========================      ] 60/75 batches, loss: 0.5624Epoch 3/15: [========================      ] 61/75 batches, loss: 0.5608Epoch 3/15: [========================      ] 62/75 batches, loss: 0.5595Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.5605Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.5602Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.5601Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.5595Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.5597Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.5595Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.5589Epoch 3/15: [============================  ] 70/75 batches, loss: 0.5595Epoch 3/15: [============================  ] 71/75 batches, loss: 0.5601Epoch 3/15: [============================  ] 72/75 batches, loss: 0.5601Epoch 3/15: [============================= ] 73/75 batches, loss: 0.5596Epoch 3/15: [============================= ] 74/75 batches, loss: 0.5586Epoch 3/15: [==============================] 75/75 batches, loss: 0.5580
[2025-05-03 16:39:22,267][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.5580
[2025-05-03 16:39:22,748][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6201, Metrics: {'accuracy': 0.7083333333333334, 'f1': 0.6037735849056604, 'precision': 0.9411764705882353, 'recall': 0.4444444444444444}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.5342Epoch 4/15: [                              ] 2/75 batches, loss: 0.5272Epoch 4/15: [=                             ] 3/75 batches, loss: 0.5317Epoch 4/15: [=                             ] 4/75 batches, loss: 0.5332Epoch 4/15: [==                            ] 5/75 batches, loss: 0.5340Epoch 4/15: [==                            ] 6/75 batches, loss: 0.5165Epoch 4/15: [==                            ] 7/75 batches, loss: 0.5209Epoch 4/15: [===                           ] 8/75 batches, loss: 0.5227Epoch 4/15: [===                           ] 9/75 batches, loss: 0.5250Epoch 4/15: [====                          ] 10/75 batches, loss: 0.5236Epoch 4/15: [====                          ] 11/75 batches, loss: 0.5135Epoch 4/15: [====                          ] 12/75 batches, loss: 0.5196Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.5165Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.5233Epoch 4/15: [======                        ] 15/75 batches, loss: 0.5217Epoch 4/15: [======                        ] 16/75 batches, loss: 0.5259Epoch 4/15: [======                        ] 17/75 batches, loss: 0.5270Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.5315Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.5317Epoch 4/15: [========                      ] 20/75 batches, loss: 0.5324Epoch 4/15: [========                      ] 21/75 batches, loss: 0.5303Epoch 4/15: [========                      ] 22/75 batches, loss: 0.5362Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.5355Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.5348Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.5337Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.5358Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.5360Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.5374Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.5396Epoch 4/15: [============                  ] 30/75 batches, loss: 0.5403Epoch 4/15: [============                  ] 31/75 batches, loss: 0.5395Epoch 4/15: [============                  ] 32/75 batches, loss: 0.5423Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.5459Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.5447Epoch 4/15: [==============                ] 35/75 batches, loss: 0.5465Epoch 4/15: [==============                ] 36/75 batches, loss: 0.5449Epoch 4/15: [==============                ] 37/75 batches, loss: 0.5458Epoch 4/15: [===============               ] 38/75 batches, loss: 0.5469Epoch 4/15: [===============               ] 39/75 batches, loss: 0.5470Epoch 4/15: [================              ] 40/75 batches, loss: 0.5475Epoch 4/15: [================              ] 41/75 batches, loss: 0.5471Epoch 4/15: [================              ] 42/75 batches, loss: 0.5485Epoch 4/15: [=================             ] 43/75 batches, loss: 0.5499Epoch 4/15: [=================             ] 44/75 batches, loss: 0.5495Epoch 4/15: [==================            ] 45/75 batches, loss: 0.5489Epoch 4/15: [==================            ] 46/75 batches, loss: 0.5475Epoch 4/15: [==================            ] 47/75 batches, loss: 0.5476Epoch 4/15: [===================           ] 48/75 batches, loss: 0.5483Epoch 4/15: [===================           ] 49/75 batches, loss: 0.5483Epoch 4/15: [====================          ] 50/75 batches, loss: 0.5485Epoch 4/15: [====================          ] 51/75 batches, loss: 0.5485Epoch 4/15: [====================          ] 52/75 batches, loss: 0.5478Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.5472Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.5462Epoch 4/15: [======================        ] 55/75 batches, loss: 0.5455Epoch 4/15: [======================        ] 56/75 batches, loss: 0.5460Epoch 4/15: [======================        ] 57/75 batches, loss: 0.5468Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.5451Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.5451Epoch 4/15: [========================      ] 60/75 batches, loss: 0.5437Epoch 4/15: [========================      ] 61/75 batches, loss: 0.5443Epoch 4/15: [========================      ] 62/75 batches, loss: 0.5425Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.5413Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.5409Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.5394Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.5380Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.5375Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.5377Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.5386Epoch 4/15: [============================  ] 70/75 batches, loss: 0.5398Epoch 4/15: [============================  ] 71/75 batches, loss: 0.5399Epoch 4/15: [============================  ] 72/75 batches, loss: 0.5395Epoch 4/15: [============================= ] 73/75 batches, loss: 0.5393Epoch 4/15: [============================= ] 74/75 batches, loss: 0.5405Epoch 4/15: [==============================] 75/75 batches, loss: 0.5401
[2025-05-03 16:39:25,618][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5401
[2025-05-03 16:39:26,002][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5982, Metrics: {'accuracy': 0.7638888888888888, 'f1': 0.7017543859649122, 'precision': 0.9523809523809523, 'recall': 0.5555555555555556}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.4957Epoch 5/15: [                              ] 2/75 batches, loss: 0.4454Epoch 5/15: [=                             ] 3/75 batches, loss: 0.4634Epoch 5/15: [=                             ] 4/75 batches, loss: 0.4799Epoch 5/15: [==                            ] 5/75 batches, loss: 0.4879Epoch 5/15: [==                            ] 6/75 batches, loss: 0.4911Epoch 5/15: [==                            ] 7/75 batches, loss: 0.4862Epoch 5/15: [===                           ] 8/75 batches, loss: 0.4805Epoch 5/15: [===                           ] 9/75 batches, loss: 0.4942Epoch 5/15: [====                          ] 10/75 batches, loss: 0.4998Epoch 5/15: [====                          ] 11/75 batches, loss: 0.5036Epoch 5/15: [====                          ] 12/75 batches, loss: 0.5027Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.5009Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.5077Epoch 5/15: [======                        ] 15/75 batches, loss: 0.5096Epoch 5/15: [======                        ] 16/75 batches, loss: 0.5106Epoch 5/15: [======                        ] 17/75 batches, loss: 0.5172Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.5218Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.5237Epoch 5/15: [========                      ] 20/75 batches, loss: 0.5206Epoch 5/15: [========                      ] 21/75 batches, loss: 0.5202Epoch 5/15: [========                      ] 22/75 batches, loss: 0.5243Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.5288Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.5298Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.5288Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.5284Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.5307Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.5331Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.5326Epoch 5/15: [============                  ] 30/75 batches, loss: 0.5333Epoch 5/15: [============                  ] 31/75 batches, loss: 0.5315Epoch 5/15: [============                  ] 32/75 batches, loss: 0.5285Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.5276Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.5259Epoch 5/15: [==============                ] 35/75 batches, loss: 0.5240Epoch 5/15: [==============                ] 36/75 batches, loss: 0.5226Epoch 5/15: [==============                ] 37/75 batches, loss: 0.5207Epoch 5/15: [===============               ] 38/75 batches, loss: 0.5211Epoch 5/15: [===============               ] 39/75 batches, loss: 0.5231Epoch 5/15: [================              ] 40/75 batches, loss: 0.5227Epoch 5/15: [================              ] 41/75 batches, loss: 0.5235Epoch 5/15: [================              ] 42/75 batches, loss: 0.5238Epoch 5/15: [=================             ] 43/75 batches, loss: 0.5254Epoch 5/15: [=================             ] 44/75 batches, loss: 0.5266Epoch 5/15: [==================            ] 45/75 batches, loss: 0.5267Epoch 5/15: [==================            ] 46/75 batches, loss: 0.5268Epoch 5/15: [==================            ] 47/75 batches, loss: 0.5281Epoch 5/15: [===================           ] 48/75 batches, loss: 0.5273Epoch 5/15: [===================           ] 49/75 batches, loss: 0.5281Epoch 5/15: [====================          ] 50/75 batches, loss: 0.5276Epoch 5/15: [====================          ] 51/75 batches, loss: 0.5298Epoch 5/15: [====================          ] 52/75 batches, loss: 0.5300Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.5318Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.5311Epoch 5/15: [======================        ] 55/75 batches, loss: 0.5315Epoch 5/15: [======================        ] 56/75 batches, loss: 0.5325Epoch 5/15: [======================        ] 57/75 batches, loss: 0.5330Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.5322Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.5332Epoch 5/15: [========================      ] 60/75 batches, loss: 0.5337Epoch 5/15: [========================      ] 61/75 batches, loss: 0.5345Epoch 5/15: [========================      ] 62/75 batches, loss: 0.5338Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.5339Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.5333Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.5345Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.5353Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.5344Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.5340Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.5335Epoch 5/15: [============================  ] 70/75 batches, loss: 0.5328Epoch 5/15: [============================  ] 71/75 batches, loss: 0.5318Epoch 5/15: [============================  ] 72/75 batches, loss: 0.5327Epoch 5/15: [============================= ] 73/75 batches, loss: 0.5325Epoch 5/15: [============================= ] 74/75 batches, loss: 0.5320Epoch 5/15: [==============================] 75/75 batches, loss: 0.5331
[2025-05-03 16:39:28,633][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5331
[2025-05-03 16:39:28,903][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5872, Metrics: {'accuracy': 0.7916666666666666, 'f1': 0.7540983606557377, 'precision': 0.92, 'recall': 0.6388888888888888}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.5725Epoch 6/15: [                              ] 2/75 batches, loss: 0.5515Epoch 6/15: [=                             ] 3/75 batches, loss: 0.5254Epoch 6/15: [=                             ] 4/75 batches, loss: 0.5320Epoch 6/15: [==                            ] 5/75 batches, loss: 0.5258Epoch 6/15: [==                            ] 6/75 batches, loss: 0.5249Epoch 6/15: [==                            ] 7/75 batches, loss: 0.5273Epoch 6/15: [===                           ] 8/75 batches, loss: 0.5337Epoch 6/15: [===                           ] 9/75 batches, loss: 0.5354Epoch 6/15: [====                          ] 10/75 batches, loss: 0.5482Epoch 6/15: [====                          ] 11/75 batches, loss: 0.5436Epoch 6/15: [====                          ] 12/75 batches, loss: 0.5476Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.5490Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.5517Epoch 6/15: [======                        ] 15/75 batches, loss: 0.5498Epoch 6/15: [======                        ] 16/75 batches, loss: 0.5456Epoch 6/15: [======                        ] 17/75 batches, loss: 0.5510Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.5452Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.5446Epoch 6/15: [========                      ] 20/75 batches, loss: 0.5455Epoch 6/15: [========                      ] 21/75 batches, loss: 0.5449Epoch 6/15: [========                      ] 22/75 batches, loss: 0.5488Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.5486Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.5497Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.5460Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.5458Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.5446Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.5436Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.5455Epoch 6/15: [============                  ] 30/75 batches, loss: 0.5461Epoch 6/15: [============                  ] 31/75 batches, loss: 0.5457Epoch 6/15: [============                  ] 32/75 batches, loss: 0.5453Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.5433Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.5411Epoch 6/15: [==============                ] 35/75 batches, loss: 0.5417Epoch 6/15: [==============                ] 36/75 batches, loss: 0.5395Epoch 6/15: [==============                ] 37/75 batches, loss: 0.5401Epoch 6/15: [===============               ] 38/75 batches, loss: 0.5392Epoch 6/15: [===============               ] 39/75 batches, loss: 0.5359Epoch 6/15: [================              ] 40/75 batches, loss: 0.5336Epoch 6/15: [================              ] 41/75 batches, loss: 0.5322Epoch 6/15: [================              ] 42/75 batches, loss: 0.5313Epoch 6/15: [=================             ] 43/75 batches, loss: 0.5299Epoch 6/15: [=================             ] 44/75 batches, loss: 0.5312Epoch 6/15: [==================            ] 45/75 batches, loss: 0.5309Epoch 6/15: [==================            ] 46/75 batches, loss: 0.5308Epoch 6/15: [==================            ] 47/75 batches, loss: 0.5312Epoch 6/15: [===================           ] 48/75 batches, loss: 0.5301Epoch 6/15: [===================           ] 49/75 batches, loss: 0.5316Epoch 6/15: [====================          ] 50/75 batches, loss: 0.5313Epoch 6/15: [====================          ] 51/75 batches, loss: 0.5300Epoch 6/15: [====================          ] 52/75 batches, loss: 0.5290Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.5281Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.5283Epoch 6/15: [======================        ] 55/75 batches, loss: 0.5270Epoch 6/15: [======================        ] 56/75 batches, loss: 0.5254Epoch 6/15: [======================        ] 57/75 batches, loss: 0.5258Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.5259Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.5259Epoch 6/15: [========================      ] 60/75 batches, loss: 0.5258Epoch 6/15: [========================      ] 61/75 batches, loss: 0.5248Epoch 6/15: [========================      ] 62/75 batches, loss: 0.5264Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.5257Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.5257Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.5263Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.5257Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.5249Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.5250Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.5257Epoch 6/15: [============================  ] 70/75 batches, loss: 0.5266Epoch 6/15: [============================  ] 71/75 batches, loss: 0.5267Epoch 6/15: [============================  ] 72/75 batches, loss: 0.5271Epoch 6/15: [============================= ] 73/75 batches, loss: 0.5269Epoch 6/15: [============================= ] 74/75 batches, loss: 0.5270Epoch 6/15: [==============================] 75/75 batches, loss: 0.5262
[2025-05-03 16:39:31,725][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5262
[2025-05-03 16:39:32,284][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5762, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8181818181818182, 'precision': 0.9, 'recall': 0.75}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.6320Epoch 7/15: [                              ] 2/75 batches, loss: 0.6191Epoch 7/15: [=                             ] 3/75 batches, loss: 0.5994Epoch 7/15: [=                             ] 4/75 batches, loss: 0.6088Epoch 7/15: [==                            ] 5/75 batches, loss: 0.5858Epoch 7/15: [==                            ] 6/75 batches, loss: 0.5775Epoch 7/15: [==                            ] 7/75 batches, loss: 0.5654Epoch 7/15: [===                           ] 8/75 batches, loss: 0.5626Epoch 7/15: [===                           ] 9/75 batches, loss: 0.5598Epoch 7/15: [====                          ] 10/75 batches, loss: 0.5565Epoch 7/15: [====                          ] 11/75 batches, loss: 0.5512Epoch 7/15: [====                          ] 12/75 batches, loss: 0.5553Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.5459Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.5468Epoch 7/15: [======                        ] 15/75 batches, loss: 0.5391Epoch 7/15: [======                        ] 16/75 batches, loss: 0.5384Epoch 7/15: [======                        ] 17/75 batches, loss: 0.5354Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.5354Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.5309Epoch 7/15: [========                      ] 20/75 batches, loss: 0.5333Epoch 7/15: [========                      ] 21/75 batches, loss: 0.5323Epoch 7/15: [========                      ] 22/75 batches, loss: 0.5353Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.5352Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.5382Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.5406Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.5390Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.5413Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.5397Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.5416Epoch 7/15: [============                  ] 30/75 batches, loss: 0.5429Epoch 7/15: [============                  ] 31/75 batches, loss: 0.5425Epoch 7/15: [============                  ] 32/75 batches, loss: 0.5415Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.5401Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.5373Epoch 7/15: [==============                ] 35/75 batches, loss: 0.5349Epoch 7/15: [==============                ] 36/75 batches, loss: 0.5317Epoch 7/15: [==============                ] 37/75 batches, loss: 0.5309Epoch 7/15: [===============               ] 38/75 batches, loss: 0.5327Epoch 7/15: [===============               ] 39/75 batches, loss: 0.5322Epoch 7/15: [================              ] 40/75 batches, loss: 0.5315Epoch 7/15: [================              ] 41/75 batches, loss: 0.5319Epoch 7/15: [================              ] 42/75 batches, loss: 0.5308Epoch 7/15: [=================             ] 43/75 batches, loss: 0.5288Epoch 7/15: [=================             ] 44/75 batches, loss: 0.5284Epoch 7/15: [==================            ] 45/75 batches, loss: 0.5306Epoch 7/15: [==================            ] 46/75 batches, loss: 0.5302Epoch 7/15: [==================            ] 47/75 batches, loss: 0.5315Epoch 7/15: [===================           ] 48/75 batches, loss: 0.5311Epoch 7/15: [===================           ] 49/75 batches, loss: 0.5301Epoch 7/15: [====================          ] 50/75 batches, loss: 0.5282Epoch 7/15: [====================          ] 51/75 batches, loss: 0.5282Epoch 7/15: [====================          ] 52/75 batches, loss: 0.5286Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.5291Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.5300Epoch 7/15: [======================        ] 55/75 batches, loss: 0.5304Epoch 7/15: [======================        ] 56/75 batches, loss: 0.5294Epoch 7/15: [======================        ] 57/75 batches, loss: 0.5299Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.5302Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.5292Epoch 7/15: [========================      ] 60/75 batches, loss: 0.5279Epoch 7/15: [========================      ] 61/75 batches, loss: 0.5276Epoch 7/15: [========================      ] 62/75 batches, loss: 0.5276Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.5279Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.5278Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.5264Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.5273Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.5269Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.5273Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.5259Epoch 7/15: [============================  ] 70/75 batches, loss: 0.5254Epoch 7/15: [============================  ] 71/75 batches, loss: 0.5248Epoch 7/15: [============================  ] 72/75 batches, loss: 0.5249Epoch 7/15: [============================= ] 73/75 batches, loss: 0.5251Epoch 7/15: [============================= ] 74/75 batches, loss: 0.5249Epoch 7/15: [==============================] 75/75 batches, loss: 0.5240
[2025-05-03 16:39:35,137][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5240
[2025-05-03 16:39:35,554][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5727, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.8484848484848485, 'precision': 0.9333333333333333, 'recall': 0.7777777777777778}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.5476Epoch 8/15: [                              ] 2/75 batches, loss: 0.5538Epoch 8/15: [=                             ] 3/75 batches, loss: 0.5373Epoch 8/15: [=                             ] 4/75 batches, loss: 0.5435Epoch 8/15: [==                            ] 5/75 batches, loss: 0.5425Epoch 8/15: [==                            ] 6/75 batches, loss: 0.5337Epoch 8/15: [==                            ] 7/75 batches, loss: 0.5328Epoch 8/15: [===                           ] 8/75 batches, loss: 0.5320Epoch 8/15: [===                           ] 9/75 batches, loss: 0.5328Epoch 8/15: [====                          ] 10/75 batches, loss: 0.5279Epoch 8/15: [====                          ] 11/75 batches, loss: 0.5259Epoch 8/15: [====                          ] 12/75 batches, loss: 0.5291Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.5322Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.5315Epoch 8/15: [======                        ] 15/75 batches, loss: 0.5281Epoch 8/15: [======                        ] 16/75 batches, loss: 0.5268Epoch 8/15: [======                        ] 17/75 batches, loss: 0.5307Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.5313Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.5288Epoch 8/15: [========                      ] 20/75 batches, loss: 0.5324Epoch 8/15: [========                      ] 21/75 batches, loss: 0.5315Epoch 8/15: [========                      ] 22/75 batches, loss: 0.5274Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.5255Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.5276Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.5239Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.5234Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.5226Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.5214Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.5192Epoch 8/15: [============                  ] 30/75 batches, loss: 0.5208Epoch 8/15: [============                  ] 31/75 batches, loss: 0.5234Epoch 8/15: [============                  ] 32/75 batches, loss: 0.5244Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.5257Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.5261Epoch 8/15: [==============                ] 35/75 batches, loss: 0.5238Epoch 8/15: [==============                ] 36/75 batches, loss: 0.5213Epoch 8/15: [==============                ] 37/75 batches, loss: 0.5208Epoch 8/15: [===============               ] 38/75 batches, loss: 0.5192Epoch 8/15: [===============               ] 39/75 batches, loss: 0.5193Epoch 8/15: [================              ] 40/75 batches, loss: 0.5206Epoch 8/15: [================              ] 41/75 batches, loss: 0.5210Epoch 8/15: [================              ] 42/75 batches, loss: 0.5218Epoch 8/15: [=================             ] 43/75 batches, loss: 0.5216Epoch 8/15: [=================             ] 44/75 batches, loss: 0.5208Epoch 8/15: [==================            ] 45/75 batches, loss: 0.5190Epoch 8/15: [==================            ] 46/75 batches, loss: 0.5185Epoch 8/15: [==================            ] 47/75 batches, loss: 0.5188Epoch 8/15: [===================           ] 48/75 batches, loss: 0.5194Epoch 8/15: [===================           ] 49/75 batches, loss: 0.5211Epoch 8/15: [====================          ] 50/75 batches, loss: 0.5213Epoch 8/15: [====================          ] 51/75 batches, loss: 0.5209Epoch 8/15: [====================          ] 52/75 batches, loss: 0.5197Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.5196Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.5189Epoch 8/15: [======================        ] 55/75 batches, loss: 0.5195Epoch 8/15: [======================        ] 56/75 batches, loss: 0.5203Epoch 8/15: [======================        ] 57/75 batches, loss: 0.5213Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.5216Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.5205Epoch 8/15: [========================      ] 60/75 batches, loss: 0.5206Epoch 8/15: [========================      ] 61/75 batches, loss: 0.5201Epoch 8/15: [========================      ] 62/75 batches, loss: 0.5203Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.5195Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.5198Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.5200Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.5194Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.5192Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.5193Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.5184Epoch 8/15: [============================  ] 70/75 batches, loss: 0.5186Epoch 8/15: [============================  ] 71/75 batches, loss: 0.5174Epoch 8/15: [============================  ] 72/75 batches, loss: 0.5173Epoch 8/15: [============================= ] 73/75 batches, loss: 0.5178Epoch 8/15: [============================= ] 74/75 batches, loss: 0.5190Epoch 8/15: [==============================] 75/75 batches, loss: 0.5195
[2025-05-03 16:39:38,435][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5195
[2025-05-03 16:39:38,845][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5676, Metrics: {'accuracy': 0.875, 'f1': 0.8615384615384616, 'precision': 0.9655172413793104, 'recall': 0.7777777777777778}
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.4558Epoch 9/15: [                              ] 2/75 batches, loss: 0.5061Epoch 9/15: [=                             ] 3/75 batches, loss: 0.5004Epoch 9/15: [=                             ] 4/75 batches, loss: 0.5229Epoch 9/15: [==                            ] 5/75 batches, loss: 0.5166Epoch 9/15: [==                            ] 6/75 batches, loss: 0.5220Epoch 9/15: [==                            ] 7/75 batches, loss: 0.5241Epoch 9/15: [===                           ] 8/75 batches, loss: 0.5267Epoch 9/15: [===                           ] 9/75 batches, loss: 0.5251Epoch 9/15: [====                          ] 10/75 batches, loss: 0.5284Epoch 9/15: [====                          ] 11/75 batches, loss: 0.5286Epoch 9/15: [====                          ] 12/75 batches, loss: 0.5263Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.5249Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.5268Epoch 9/15: [======                        ] 15/75 batches, loss: 0.5238Epoch 9/15: [======                        ] 16/75 batches, loss: 0.5268Epoch 9/15: [======                        ] 17/75 batches, loss: 0.5273Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.5249Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.5305Epoch 9/15: [========                      ] 20/75 batches, loss: 0.5281Epoch 9/15: [========                      ] 21/75 batches, loss: 0.5294Epoch 9/15: [========                      ] 22/75 batches, loss: 0.5284Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.5278Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.5268Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.5234Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.5228Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.5195Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.5185Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.5172Epoch 9/15: [============                  ] 30/75 batches, loss: 0.5188Epoch 9/15: [============                  ] 31/75 batches, loss: 0.5215Epoch 9/15: [============                  ] 32/75 batches, loss: 0.5203Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.5210Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.5197Epoch 9/15: [==============                ] 35/75 batches, loss: 0.5202Epoch 9/15: [==============                ] 36/75 batches, loss: 0.5192Epoch 9/15: [==============                ] 37/75 batches, loss: 0.5220Epoch 9/15: [===============               ] 38/75 batches, loss: 0.5217Epoch 9/15: [===============               ] 39/75 batches, loss: 0.5230Epoch 9/15: [================              ] 40/75 batches, loss: 0.5237Epoch 9/15: [================              ] 41/75 batches, loss: 0.5223Epoch 9/15: [================              ] 42/75 batches, loss: 0.5213Epoch 9/15: [=================             ] 43/75 batches, loss: 0.5216Epoch 9/15: [=================             ] 44/75 batches, loss: 0.5224Epoch 9/15: [==================            ] 45/75 batches, loss: 0.5226Epoch 9/15: [==================            ] 46/75 batches, loss: 0.5248Epoch 9/15: [==================            ] 47/75 batches, loss: 0.5254Epoch 9/15: [===================           ] 48/75 batches, loss: 0.5256Epoch 9/15: [===================           ] 49/75 batches, loss: 0.5256Epoch 9/15: [====================          ] 50/75 batches, loss: 0.5250Epoch 9/15: [====================          ] 51/75 batches, loss: 0.5246Epoch 9/15: [====================          ] 52/75 batches, loss: 0.5259Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.5251Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.5245Epoch 9/15: [======================        ] 55/75 batches, loss: 0.5243Epoch 9/15: [======================        ] 56/75 batches, loss: 0.5241Epoch 9/15: [======================        ] 57/75 batches, loss: 0.5233Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.5234Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.5233Epoch 9/15: [========================      ] 60/75 batches, loss: 0.5222Epoch 9/15: [========================      ] 61/75 batches, loss: 0.5224Epoch 9/15: [========================      ] 62/75 batches, loss: 0.5212Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.5223Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.5227Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.5215Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.5209Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.5226Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.5214Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.5216Epoch 9/15: [============================  ] 70/75 batches, loss: 0.5204Epoch 9/15: [============================  ] 71/75 batches, loss: 0.5198Epoch 9/15: [============================  ] 72/75 batches, loss: 0.5190Epoch 9/15: [============================= ] 73/75 batches, loss: 0.5195Epoch 9/15: [============================= ] 74/75 batches, loss: 0.5195Epoch 9/15: [==============================] 75/75 batches, loss: 0.5209
[2025-05-03 16:39:41,619][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5209
[2025-05-03 16:39:42,091][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.5628, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8787878787878788, 'precision': 0.9666666666666667, 'recall': 0.8055555555555556}
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.5780Epoch 10/15: [                              ] 2/75 batches, loss: 0.5305Epoch 10/15: [=                             ] 3/75 batches, loss: 0.5303Epoch 10/15: [=                             ] 4/75 batches, loss: 0.5309Epoch 10/15: [==                            ] 5/75 batches, loss: 0.5264Epoch 10/15: [==                            ] 6/75 batches, loss: 0.5314Epoch 10/15: [==                            ] 7/75 batches, loss: 0.5182Epoch 10/15: [===                           ] 8/75 batches, loss: 0.5120Epoch 10/15: [===                           ] 9/75 batches, loss: 0.5176Epoch 10/15: [====                          ] 10/75 batches, loss: 0.5243Epoch 10/15: [====                          ] 11/75 batches, loss: 0.5183Epoch 10/15: [====                          ] 12/75 batches, loss: 0.5231Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.5235Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.5239Epoch 10/15: [======                        ] 15/75 batches, loss: 0.5217Epoch 10/15: [======                        ] 16/75 batches, loss: 0.5248Epoch 10/15: [======                        ] 17/75 batches, loss: 0.5198Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.5146Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.5197Epoch 10/15: [========                      ] 20/75 batches, loss: 0.5208Epoch 10/15: [========                      ] 21/75 batches, loss: 0.5224Epoch 10/15: [========                      ] 22/75 batches, loss: 0.5276Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.5287Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.5266Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.5245Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.5259Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.5232Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.5239Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.5227Epoch 10/15: [============                  ] 30/75 batches, loss: 0.5216Epoch 10/15: [============                  ] 31/75 batches, loss: 0.5220Epoch 10/15: [============                  ] 32/75 batches, loss: 0.5202Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.5230Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.5214Epoch 10/15: [==============                ] 35/75 batches, loss: 0.5218Epoch 10/15: [==============                ] 36/75 batches, loss: 0.5234Epoch 10/15: [==============                ] 37/75 batches, loss: 0.5269Epoch 10/15: [===============               ] 38/75 batches, loss: 0.5267Epoch 10/15: [===============               ] 39/75 batches, loss: 0.5286Epoch 10/15: [================              ] 40/75 batches, loss: 0.5292Epoch 10/15: [================              ] 41/75 batches, loss: 0.5275Epoch 10/15: [================              ] 42/75 batches, loss: 0.5248Epoch 10/15: [=================             ] 43/75 batches, loss: 0.5261Epoch 10/15: [=================             ] 44/75 batches, loss: 0.5252Epoch 10/15: [==================            ] 45/75 batches, loss: 0.5244Epoch 10/15: [==================            ] 46/75 batches, loss: 0.5242Epoch 10/15: [==================            ] 47/75 batches, loss: 0.5244Epoch 10/15: [===================           ] 48/75 batches, loss: 0.5241Epoch 10/15: [===================           ] 49/75 batches, loss: 0.5248Epoch 10/15: [====================          ] 50/75 batches, loss: 0.5258Epoch 10/15: [====================          ] 51/75 batches, loss: 0.5263Epoch 10/15: [====================          ] 52/75 batches, loss: 0.5250Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.5246Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.5247Epoch 10/15: [======================        ] 55/75 batches, loss: 0.5253Epoch 10/15: [======================        ] 56/75 batches, loss: 0.5250Epoch 10/15: [======================        ] 57/75 batches, loss: 0.5239Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.5248Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.5238Epoch 10/15: [========================      ] 60/75 batches, loss: 0.5216Epoch 10/15: [========================      ] 61/75 batches, loss: 0.5195Epoch 10/15: [========================      ] 62/75 batches, loss: 0.5204Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.5198Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.5191Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.5184Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.5194Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.5207Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.5198Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.5204Epoch 10/15: [============================  ] 70/75 batches, loss: 0.5202Epoch 10/15: [============================  ] 71/75 batches, loss: 0.5210Epoch 10/15: [============================  ] 72/75 batches, loss: 0.5204Epoch 10/15: [============================= ] 73/75 batches, loss: 0.5202Epoch 10/15: [============================= ] 74/75 batches, loss: 0.5203Epoch 10/15: [==============================] 75/75 batches, loss: 0.5207
[2025-05-03 16:39:44,995][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5207
[2025-05-03 16:39:45,411][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.5868, Metrics: {'accuracy': 0.7916666666666666, 'f1': 0.7368421052631579, 'precision': 1.0, 'recall': 0.5833333333333334}
[2025-05-03 16:39:45,411][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.4659Epoch 11/15: [                              ] 2/75 batches, loss: 0.4891Epoch 11/15: [=                             ] 3/75 batches, loss: 0.4967Epoch 11/15: [=                             ] 4/75 batches, loss: 0.5051Epoch 11/15: [==                            ] 5/75 batches, loss: 0.5190Epoch 11/15: [==                            ] 6/75 batches, loss: 0.5352Epoch 11/15: [==                            ] 7/75 batches, loss: 0.5274Epoch 11/15: [===                           ] 8/75 batches, loss: 0.5342Epoch 11/15: [===                           ] 9/75 batches, loss: 0.5288Epoch 11/15: [====                          ] 10/75 batches, loss: 0.5311Epoch 11/15: [====                          ] 11/75 batches, loss: 0.5308Epoch 11/15: [====                          ] 12/75 batches, loss: 0.5307Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.5299Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.5327Epoch 11/15: [======                        ] 15/75 batches, loss: 0.5291Epoch 11/15: [======                        ] 16/75 batches, loss: 0.5246Epoch 11/15: [======                        ] 17/75 batches, loss: 0.5235Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.5252Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.5259Epoch 11/15: [========                      ] 20/75 batches, loss: 0.5250Epoch 11/15: [========                      ] 21/75 batches, loss: 0.5228Epoch 11/15: [========                      ] 22/75 batches, loss: 0.5224Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.5247Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.5219Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.5215Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.5249Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.5258Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.5243Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.5220Epoch 11/15: [============                  ] 30/75 batches, loss: 0.5209Epoch 11/15: [============                  ] 31/75 batches, loss: 0.5173Epoch 11/15: [============                  ] 32/75 batches, loss: 0.5176Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.5161Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.5149Epoch 11/15: [==============                ] 35/75 batches, loss: 0.5152Epoch 11/15: [==============                ] 36/75 batches, loss: 0.5136Epoch 11/15: [==============                ] 37/75 batches, loss: 0.5129Epoch 11/15: [===============               ] 38/75 batches, loss: 0.5138Epoch 11/15: [===============               ] 39/75 batches, loss: 0.5146Epoch 11/15: [================              ] 40/75 batches, loss: 0.5155Epoch 11/15: [================              ] 41/75 batches, loss: 0.5170Epoch 11/15: [================              ] 42/75 batches, loss: 0.5174Epoch 11/15: [=================             ] 43/75 batches, loss: 0.5182Epoch 11/15: [=================             ] 44/75 batches, loss: 0.5157Epoch 11/15: [==================            ] 45/75 batches, loss: 0.5170Epoch 11/15: [==================            ] 46/75 batches, loss: 0.5157Epoch 11/15: [==================            ] 47/75 batches, loss: 0.5145Epoch 11/15: [===================           ] 48/75 batches, loss: 0.5160Epoch 11/15: [===================           ] 49/75 batches, loss: 0.5179Epoch 11/15: [====================          ] 50/75 batches, loss: 0.5179Epoch 11/15: [====================          ] 51/75 batches, loss: 0.5176Epoch 11/15: [====================          ] 52/75 batches, loss: 0.5155Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.5166Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.5172Epoch 11/15: [======================        ] 55/75 batches, loss: 0.5170Epoch 11/15: [======================        ] 56/75 batches, loss: 0.5166Epoch 11/15: [======================        ] 57/75 batches, loss: 0.5165Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.5172Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.5172Epoch 11/15: [========================      ] 60/75 batches, loss: 0.5165Epoch 11/15: [========================      ] 61/75 batches, loss: 0.5171Epoch 11/15: [========================      ] 62/75 batches, loss: 0.5170Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.5161Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.5159Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.5154Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.5160Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.5165Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.5159Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.5162Epoch 11/15: [============================  ] 70/75 batches, loss: 0.5163Epoch 11/15: [============================  ] 71/75 batches, loss: 0.5160Epoch 11/15: [============================  ] 72/75 batches, loss: 0.5162Epoch 11/15: [============================= ] 73/75 batches, loss: 0.5167Epoch 11/15: [============================= ] 74/75 batches, loss: 0.5169Epoch 11/15: [==============================] 75/75 batches, loss: 0.5154
[2025-05-03 16:39:47,789][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5154
[2025-05-03 16:39:48,155][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.5898, Metrics: {'accuracy': 0.7638888888888888, 'f1': 0.6909090909090909, 'precision': 1.0, 'recall': 0.5277777777777778}
[2025-05-03 16:39:48,155][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.4964Epoch 12/15: [                              ] 2/75 batches, loss: 0.4999Epoch 12/15: [=                             ] 3/75 batches, loss: 0.5171Epoch 12/15: [=                             ] 4/75 batches, loss: 0.5263Epoch 12/15: [==                            ] 5/75 batches, loss: 0.5301Epoch 12/15: [==                            ] 6/75 batches, loss: 0.5292Epoch 12/15: [==                            ] 7/75 batches, loss: 0.5329Epoch 12/15: [===                           ] 8/75 batches, loss: 0.5247Epoch 12/15: [===                           ] 9/75 batches, loss: 0.5210Epoch 12/15: [====                          ] 10/75 batches, loss: 0.5287Epoch 12/15: [====                          ] 11/75 batches, loss: 0.5275Epoch 12/15: [====                          ] 12/75 batches, loss: 0.5229Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.5177Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.5216Epoch 12/15: [======                        ] 15/75 batches, loss: 0.5162Epoch 12/15: [======                        ] 16/75 batches, loss: 0.5158Epoch 12/15: [======                        ] 17/75 batches, loss: 0.5147Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.5150Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.5149Epoch 12/15: [========                      ] 20/75 batches, loss: 0.5107Epoch 12/15: [========                      ] 21/75 batches, loss: 0.5093Epoch 12/15: [========                      ] 22/75 batches, loss: 0.5112Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.5129Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.5153Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.5146Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.5123Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.5095Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.5137Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.5156Epoch 12/15: [============                  ] 30/75 batches, loss: 0.5168Epoch 12/15: [============                  ] 31/75 batches, loss: 0.5156Epoch 12/15: [============                  ] 32/75 batches, loss: 0.5148Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.5154Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.5136Epoch 12/15: [==============                ] 35/75 batches, loss: 0.5147Epoch 12/15: [==============                ] 36/75 batches, loss: 0.5145Epoch 12/15: [==============                ] 37/75 batches, loss: 0.5132Epoch 12/15: [===============               ] 38/75 batches, loss: 0.5125Epoch 12/15: [===============               ] 39/75 batches, loss: 0.5127Epoch 12/15: [================              ] 40/75 batches, loss: 0.5135Epoch 12/15: [================              ] 41/75 batches, loss: 0.5166Epoch 12/15: [================              ] 42/75 batches, loss: 0.5181Epoch 12/15: [=================             ] 43/75 batches, loss: 0.5145Epoch 12/15: [=================             ] 44/75 batches, loss: 0.5156Epoch 12/15: [==================            ] 45/75 batches, loss: 0.5148Epoch 12/15: [==================            ] 46/75 batches, loss: 0.5161Epoch 12/15: [==================            ] 47/75 batches, loss: 0.5155Epoch 12/15: [===================           ] 48/75 batches, loss: 0.5153Epoch 12/15: [===================           ] 49/75 batches, loss: 0.5141Epoch 12/15: [====================          ] 50/75 batches, loss: 0.5135Epoch 12/15: [====================          ] 51/75 batches, loss: 0.5126Epoch 12/15: [====================          ] 52/75 batches, loss: 0.5117Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.5111Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.5110Epoch 12/15: [======================        ] 55/75 batches, loss: 0.5117Epoch 12/15: [======================        ] 56/75 batches, loss: 0.5116Epoch 12/15: [======================        ] 57/75 batches, loss: 0.5125Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.5129Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.5120Epoch 12/15: [========================      ] 60/75 batches, loss: 0.5131Epoch 12/15: [========================      ] 61/75 batches, loss: 0.5129Epoch 12/15: [========================      ] 62/75 batches, loss: 0.5147Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.5157Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.5158Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.5156Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.5155Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.5168Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.5164Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.5162Epoch 12/15: [============================  ] 70/75 batches, loss: 0.5169Epoch 12/15: [============================  ] 71/75 batches, loss: 0.5172Epoch 12/15: [============================  ] 72/75 batches, loss: 0.5181Epoch 12/15: [============================= ] 73/75 batches, loss: 0.5176Epoch 12/15: [============================= ] 74/75 batches, loss: 0.5174Epoch 12/15: [==============================] 75/75 batches, loss: 0.5172
[2025-05-03 16:39:50,444][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.5172
[2025-05-03 16:39:50,864][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.5539, Metrics: {'accuracy': 0.9027777777777778, 'f1': 0.8985507246376812, 'precision': 0.9393939393939394, 'recall': 0.8611111111111112}
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.5312Epoch 13/15: [                              ] 2/75 batches, loss: 0.5212Epoch 13/15: [=                             ] 3/75 batches, loss: 0.5076Epoch 13/15: [=                             ] 4/75 batches, loss: 0.5142Epoch 13/15: [==                            ] 5/75 batches, loss: 0.5318Epoch 13/15: [==                            ] 6/75 batches, loss: 0.5362Epoch 13/15: [==                            ] 7/75 batches, loss: 0.5381Epoch 13/15: [===                           ] 8/75 batches, loss: 0.5251Epoch 13/15: [===                           ] 9/75 batches, loss: 0.5227Epoch 13/15: [====                          ] 10/75 batches, loss: 0.5255Epoch 13/15: [====                          ] 11/75 batches, loss: 0.5228Epoch 13/15: [====                          ] 12/75 batches, loss: 0.5193Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.5199Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.5111Epoch 13/15: [======                        ] 15/75 batches, loss: 0.5112Epoch 13/15: [======                        ] 16/75 batches, loss: 0.5105Epoch 13/15: [======                        ] 17/75 batches, loss: 0.5102Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.5085Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.5120Epoch 13/15: [========                      ] 20/75 batches, loss: 0.5104Epoch 13/15: [========                      ] 21/75 batches, loss: 0.5137Epoch 13/15: [========                      ] 22/75 batches, loss: 0.5101Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.5138Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.5135Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.5094Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.5038Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.5036Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.5066Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.5089Epoch 13/15: [============                  ] 30/75 batches, loss: 0.5088Epoch 13/15: [============                  ] 31/75 batches, loss: 0.5082Epoch 13/15: [============                  ] 32/75 batches, loss: 0.5081Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.5073Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.5076Epoch 13/15: [==============                ] 35/75 batches, loss: 0.5096Epoch 13/15: [==============                ] 36/75 batches, loss: 0.5083Epoch 13/15: [==============                ] 37/75 batches, loss: 0.5086Epoch 13/15: [===============               ] 38/75 batches, loss: 0.5098Epoch 13/15: [===============               ] 39/75 batches, loss: 0.5144Epoch 13/15: [================              ] 40/75 batches, loss: 0.5133Epoch 13/15: [================              ] 41/75 batches, loss: 0.5136Epoch 13/15: [================              ] 42/75 batches, loss: 0.5123Epoch 13/15: [=================             ] 43/75 batches, loss: 0.5105Epoch 13/15: [=================             ] 44/75 batches, loss: 0.5120Epoch 13/15: [==================            ] 45/75 batches, loss: 0.5128Epoch 13/15: [==================            ] 46/75 batches, loss: 0.5132Epoch 13/15: [==================            ] 47/75 batches, loss: 0.5145Epoch 13/15: [===================           ] 48/75 batches, loss: 0.5138Epoch 13/15: [===================           ] 49/75 batches, loss: 0.5150Epoch 13/15: [====================          ] 50/75 batches, loss: 0.5143Epoch 13/15: [====================          ] 51/75 batches, loss: 0.5143Epoch 13/15: [====================          ] 52/75 batches, loss: 0.5147Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.5146Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.5158Epoch 13/15: [======================        ] 55/75 batches, loss: 0.5160Epoch 13/15: [======================        ] 56/75 batches, loss: 0.5154Epoch 13/15: [======================        ] 57/75 batches, loss: 0.5156Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.5154Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.5157Epoch 13/15: [========================      ] 60/75 batches, loss: 0.5155Epoch 13/15: [========================      ] 61/75 batches, loss: 0.5165Epoch 13/15: [========================      ] 62/75 batches, loss: 0.5159Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.5159Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.5157Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.5159Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.5165Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.5149Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.5144Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.5145Epoch 13/15: [============================  ] 70/75 batches, loss: 0.5136Epoch 13/15: [============================  ] 71/75 batches, loss: 0.5128Epoch 13/15: [============================  ] 72/75 batches, loss: 0.5126Epoch 13/15: [============================= ] 73/75 batches, loss: 0.5128Epoch 13/15: [============================= ] 74/75 batches, loss: 0.5143Epoch 13/15: [==============================] 75/75 batches, loss: 0.5154
[2025-05-03 16:39:53,810][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.5154
[2025-05-03 16:39:54,196][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.5584, Metrics: {'accuracy': 0.875, 'f1': 0.8656716417910447, 'precision': 0.9354838709677419, 'recall': 0.8055555555555556}
[2025-05-03 16:39:54,196][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.6244Epoch 14/15: [                              ] 2/75 batches, loss: 0.5570Epoch 14/15: [=                             ] 3/75 batches, loss: 0.5535Epoch 14/15: [=                             ] 4/75 batches, loss: 0.5433Epoch 14/15: [==                            ] 5/75 batches, loss: 0.5272Epoch 14/15: [==                            ] 6/75 batches, loss: 0.5234Epoch 14/15: [==                            ] 7/75 batches, loss: 0.5206Epoch 14/15: [===                           ] 8/75 batches, loss: 0.5217Epoch 14/15: [===                           ] 9/75 batches, loss: 0.5137Epoch 14/15: [====                          ] 10/75 batches, loss: 0.5059Epoch 14/15: [====                          ] 11/75 batches, loss: 0.5057Epoch 14/15: [====                          ] 12/75 batches, loss: 0.5056Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.5055Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.5139Epoch 14/15: [======                        ] 15/75 batches, loss: 0.5103Epoch 14/15: [======                        ] 16/75 batches, loss: 0.5116Epoch 14/15: [======                        ] 17/75 batches, loss: 0.5152Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.5201Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.5206Epoch 14/15: [========                      ] 20/75 batches, loss: 0.5221Epoch 14/15: [========                      ] 21/75 batches, loss: 0.5215Epoch 14/15: [========                      ] 22/75 batches, loss: 0.5187Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.5191Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.5175Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.5173Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.5196Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.5217Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.5184Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.5155Epoch 14/15: [============                  ] 30/75 batches, loss: 0.5135Epoch 14/15: [============                  ] 31/75 batches, loss: 0.5140Epoch 14/15: [============                  ] 32/75 batches, loss: 0.5123Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.5136Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.5150Epoch 14/15: [==============                ] 35/75 batches, loss: 0.5149Epoch 14/15: [==============                ] 36/75 batches, loss: 0.5147Epoch 14/15: [==============                ] 37/75 batches, loss: 0.5137Epoch 14/15: [===============               ] 38/75 batches, loss: 0.5142Epoch 14/15: [===============               ] 39/75 batches, loss: 0.5139Epoch 14/15: [================              ] 40/75 batches, loss: 0.5138Epoch 14/15: [================              ] 41/75 batches, loss: 0.5142Epoch 14/15: [================              ] 42/75 batches, loss: 0.5141Epoch 14/15: [=================             ] 43/75 batches, loss: 0.5161Epoch 14/15: [=================             ] 44/75 batches, loss: 0.5171Epoch 14/15: [==================            ] 45/75 batches, loss: 0.5182Epoch 14/15: [==================            ] 46/75 batches, loss: 0.5184Epoch 14/15: [==================            ] 47/75 batches, loss: 0.5181Epoch 14/15: [===================           ] 48/75 batches, loss: 0.5170Epoch 14/15: [===================           ] 49/75 batches, loss: 0.5166Epoch 14/15: [====================          ] 50/75 batches, loss: 0.5159Epoch 14/15: [====================          ] 51/75 batches, loss: 0.5153Epoch 14/15: [====================          ] 52/75 batches, loss: 0.5138Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.5138Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.5136Epoch 14/15: [======================        ] 55/75 batches, loss: 0.5141Epoch 14/15: [======================        ] 56/75 batches, loss: 0.5147Epoch 14/15: [======================        ] 57/75 batches, loss: 0.5142Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.5136Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.5131Epoch 14/15: [========================      ] 60/75 batches, loss: 0.5130Epoch 14/15: [========================      ] 61/75 batches, loss: 0.5117Epoch 14/15: [========================      ] 62/75 batches, loss: 0.5120Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.5136Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.5132Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.5127Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.5126Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.5128Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.5131Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.5113Epoch 14/15: [============================  ] 70/75 batches, loss: 0.5109Epoch 14/15: [============================  ] 71/75 batches, loss: 0.5111Epoch 14/15: [============================  ] 72/75 batches, loss: 0.5107Epoch 14/15: [============================= ] 73/75 batches, loss: 0.5119Epoch 14/15: [============================= ] 74/75 batches, loss: 0.5118Epoch 14/15: [==============================] 75/75 batches, loss: 0.5118
[2025-05-03 16:39:56,652][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.5118
[2025-05-03 16:39:57,073][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.5552, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8823529411764706, 'precision': 0.9375, 'recall': 0.8333333333333334}
[2025-05-03 16:39:57,074][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.5044Epoch 15/15: [                              ] 2/75 batches, loss: 0.5033Epoch 15/15: [=                             ] 3/75 batches, loss: 0.4877Epoch 15/15: [=                             ] 4/75 batches, loss: 0.4946Epoch 15/15: [==                            ] 5/75 batches, loss: 0.5059Epoch 15/15: [==                            ] 6/75 batches, loss: 0.5056Epoch 15/15: [==                            ] 7/75 batches, loss: 0.5034Epoch 15/15: [===                           ] 8/75 batches, loss: 0.5003Epoch 15/15: [===                           ] 9/75 batches, loss: 0.5137Epoch 15/15: [====                          ] 10/75 batches, loss: 0.5103Epoch 15/15: [====                          ] 11/75 batches, loss: 0.5101Epoch 15/15: [====                          ] 12/75 batches, loss: 0.5110Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.5102Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.5072Epoch 15/15: [======                        ] 15/75 batches, loss: 0.5022Epoch 15/15: [======                        ] 16/75 batches, loss: 0.5040Epoch 15/15: [======                        ] 17/75 batches, loss: 0.5026Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.5014Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.5003Epoch 15/15: [========                      ] 20/75 batches, loss: 0.4971Epoch 15/15: [========                      ] 21/75 batches, loss: 0.5031Epoch 15/15: [========                      ] 22/75 batches, loss: 0.5031Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.5069Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.5038Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.5009Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.4974Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.4985Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.4984Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.4970Epoch 15/15: [============                  ] 30/75 batches, loss: 0.5037Epoch 15/15: [============                  ] 31/75 batches, loss: 0.5044Epoch 15/15: [============                  ] 32/75 batches, loss: 0.5073Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.5072Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.5066Epoch 15/15: [==============                ] 35/75 batches, loss: 0.5055Epoch 15/15: [==============                ] 36/75 batches, loss: 0.5061Epoch 15/15: [==============                ] 37/75 batches, loss: 0.5067Epoch 15/15: [===============               ] 38/75 batches, loss: 0.5073Epoch 15/15: [===============               ] 39/75 batches, loss: 0.5078Epoch 15/15: [================              ] 40/75 batches, loss: 0.5072Epoch 15/15: [================              ] 41/75 batches, loss: 0.5074Epoch 15/15: [================              ] 42/75 batches, loss: 0.5073Epoch 15/15: [=================             ] 43/75 batches, loss: 0.5078Epoch 15/15: [=================             ] 44/75 batches, loss: 0.5096Epoch 15/15: [==================            ] 45/75 batches, loss: 0.5100Epoch 15/15: [==================            ] 46/75 batches, loss: 0.5115Epoch 15/15: [==================            ] 47/75 batches, loss: 0.5104Epoch 15/15: [===================           ] 48/75 batches, loss: 0.5107Epoch 15/15: [===================           ] 49/75 batches, loss: 0.5101Epoch 15/15: [====================          ] 50/75 batches, loss: 0.5120Epoch 15/15: [====================          ] 51/75 batches, loss: 0.5133Epoch 15/15: [====================          ] 52/75 batches, loss: 0.5131Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.5139Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.5145Epoch 15/15: [======================        ] 55/75 batches, loss: 0.5159Epoch 15/15: [======================        ] 56/75 batches, loss: 0.5140Epoch 15/15: [======================        ] 57/75 batches, loss: 0.5143Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.5144Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.5138Epoch 15/15: [========================      ] 60/75 batches, loss: 0.5136Epoch 15/15: [========================      ] 61/75 batches, loss: 0.5115Epoch 15/15: [========================      ] 62/75 batches, loss: 0.5118Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.5120Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.5123Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.5122Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.5117Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.5123Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.5133Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.5138Epoch 15/15: [============================  ] 70/75 batches, loss: 0.5142Epoch 15/15: [============================  ] 71/75 batches, loss: 0.5142Epoch 15/15: [============================  ] 72/75 batches, loss: 0.5153Epoch 15/15: [============================= ] 73/75 batches, loss: 0.5150Epoch 15/15: [============================= ] 74/75 batches, loss: 0.5149Epoch 15/15: [==============================] 75/75 batches, loss: 0.5136
[2025-05-03 16:39:59,633][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.5136
[2025-05-03 16:40:00,046][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.5510, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8823529411764706, 'precision': 0.9375, 'recall': 0.8333333333333334}
[2025-05-03 16:40:00,434][src.training.lm_trainer][INFO] - Training completed in 47.75 seconds
[2025-05-03 16:40:00,435][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-03 16:40:03,791][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9949664429530202, 'f1': 0.9949579831932773, 'precision': 0.9966329966329966, 'recall': 0.9932885906040269}
[2025-05-03 16:40:03,792][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8823529411764706, 'precision': 0.9375, 'recall': 0.8333333333333334}
[2025-05-03 16:40:03,792][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9019607843137255, 'precision': 0.9787234042553191, 'recall': 0.8363636363636363}
[2025-05-03 16:40:05,448][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/en/en/model.pt
[2025-05-03 16:40:05,449][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▅▄▅▆▇▇▇███
wandb:           best_val_f1 ▁▆▅▆▆▇▇████
wandb:         best_val_loss █▅▅▄▃▃▂▂▂▁▁
wandb:    best_val_precision ▁█▇█▇▆▇██▇▇
wandb:       best_val_recall ▁▅▄▅▆▇▇▇▇██
wandb:                 epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▂▂▂▃▃▃▃▂▂▃▃▃
wandb:            train_loss █▅▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▅▄▅▆▇▇▇█▆▅█▇██
wandb:                val_f1 ▁▆▅▆▆▇▇██▆▆████
wandb:              val_loss █▅▅▄▃▃▂▂▂▃▃▁▁▁▁
wandb:         val_precision ▁▇▇▇▆▆▆▇▇██▇▆▆▆
wandb:            val_recall ▁▅▄▅▆▇▇▇▇▅▅█▇██
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.88889
wandb:           best_val_f1 0.88235
wandb:         best_val_loss 0.55099
wandb:    best_val_precision 0.9375
wandb:       best_val_recall 0.83333
wandb:                 epoch 15
wandb:   final_test_accuracy 0.90909
wandb:         final_test_f1 0.90196
wandb:  final_test_precision 0.97872
wandb:     final_test_recall 0.83636
wandb:  final_train_accuracy 0.99497
wandb:        final_train_f1 0.99496
wandb: final_train_precision 0.99663
wandb:    final_train_recall 0.99329
wandb:    final_val_accuracy 0.88889
wandb:          final_val_f1 0.88235
wandb:   final_val_precision 0.9375
wandb:      final_val_recall 0.83333
wandb:         learning_rate 0.0001
wandb:            train_loss 0.51358
wandb:            train_time 47.74976
wandb:          val_accuracy 0.88889
wandb:                val_f1 0.88235
wandb:              val_loss 0.55099
wandb:         val_precision 0.9375
wandb:            val_recall 0.83333
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_163847-j3vkae1l
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_163847-j3vkae1l/logs
Experiment probe_layer3_question_type_en completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/en/en/results.json for layer 3
Running experiment: probe_layer3_complexity_en
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=3"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[en]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer3_complexity_en"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/layer3/en"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-03 16:40:51,266][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/en
experiment_name: probe_layer3_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-03 16:40:51,267][__main__][INFO] - Normalized task: complexity
[2025-05-03 16:40:51,267][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-03 16:40:51,267][__main__][INFO] - Determined Task Type: regression
[2025-05-03 16:40:51,271][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-05-03 16:40:51,271][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-03 16:40:57,695][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-03 16:41:00,153][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-03 16:41:00,153][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:41:00,582][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:41:00,681][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:41:01,184][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-05-03 16:41:01,192][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:41:01,193][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-05-03 16:41:01,194][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:41:01,471][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:41:01,607][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:41:01,642][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-05-03 16:41:01,643][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:41:01,643][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-05-03 16:41:01,644][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:41:01,701][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:41:01,910][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:41:01,945][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-05-03 16:41:01,947][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:41:01,947][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-05-03 16:41:01,948][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-05-03 16:41:01,948][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:41:01,948][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:41:01,949][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:41:01,949][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:41:01,949][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:41:01,949][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-05-03 16:41:01,949][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-05-03 16:41:01,949][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-05-03 16:41:01,949][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:41:01,949][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:41:01,949][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:41:01,950][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:41:01,950][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:41:01,950][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-05-03 16:41:01,950][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-05-03 16:41:01,950][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-05-03 16:41:01,950][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:41:01,950][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:41:01,950][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:41:01,950][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:41:01,951][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:41:01,951][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-05-03 16:41:01,951][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-05-03 16:41:01,951][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-05-03 16:41:01,951][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-05-03 16:41:01,951][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-03 16:41:01,951][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-03 16:41:01,951][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-03 16:41:01,952][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-03 16:41:13,139][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-03 16:41:13,140][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-03 16:41:13,140][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=3, freeze_model=True
[2025-05-03 16:41:13,140][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-03 16:41:13,143][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-03 16:41:13,144][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-03 16:41:13,144][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-03 16:41:13,144][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-03 16:41:13,144][__main__][INFO] - Successfully created lm_probe model for en
[2025-05-03 16:41:13,145][__main__][INFO] - Total parameters: 394,255,105
[2025-05-03 16:41:13,145][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.3815Epoch 1/15: [                              ] 2/75 batches, loss: 0.5845Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4905Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5083Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4859Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4606Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4563Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4523Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4449Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4279Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4101Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4207Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4000Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4157Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4148Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4227Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4226Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4173Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4134Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4192Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4157Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4216Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4093Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4046Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4008Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3967Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3970Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3915Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3913Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3895Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3833Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3794Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3774Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3730Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3708Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3688Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3644Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3638Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3605Epoch 1/15: [================              ] 40/75 batches, loss: 0.3580Epoch 1/15: [================              ] 41/75 batches, loss: 0.3570Epoch 1/15: [================              ] 42/75 batches, loss: 0.3516Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3476Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3469Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3461Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3415Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3369Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3336Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3299Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3256Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3228Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3189Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3167Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3164Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3141Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3101Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3069Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3036Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3009Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2975Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2945Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2930Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2896Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2874Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2846Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2819Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2796Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2778Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2788Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2763Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2743Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2717Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2695Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2670Epoch 1/15: [==============================] 75/75 batches, loss: 0.2640
[2025-05-03 16:41:22,220][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2640
[2025-05-03 16:41:22,567][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0552, Metrics: {'mse': 0.056709036231040955, 'rmse': 0.23813659154157926, 'r2': -0.35501551628112793}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.3633Epoch 2/15: [                              ] 2/75 batches, loss: 0.2701Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2353Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2071Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2082Epoch 2/15: [==                            ] 6/75 batches, loss: 0.2012Epoch 2/15: [==                            ] 7/75 batches, loss: 0.2126Epoch 2/15: [===                           ] 8/75 batches, loss: 0.2185Epoch 2/15: [===                           ] 9/75 batches, loss: 0.2157Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2174Epoch 2/15: [====                          ] 11/75 batches, loss: 0.2092Epoch 2/15: [====                          ] 12/75 batches, loss: 0.2012Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1964Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1899Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1837Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1792Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1713Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1686Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1661Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1647Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1653Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1606Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1599Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1597Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1562Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1568Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1589Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1568Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1550Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1525Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1528Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1534Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1509Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1486Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1480Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1486Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1503Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1495Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1502Epoch 2/15: [================              ] 40/75 batches, loss: 0.1506Epoch 2/15: [================              ] 41/75 batches, loss: 0.1486Epoch 2/15: [================              ] 42/75 batches, loss: 0.1476Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1466Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1459Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1472Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1495Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1472Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1498Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1484Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1481Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1470Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1462Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1457Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1448Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1440Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1473Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1465Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1457Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1442Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1435Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1430Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1446Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1438Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1436Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1421Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1425Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1417Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1407Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1399Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1397Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1393Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1390Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1392Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1388Epoch 2/15: [==============================] 75/75 batches, loss: 0.1375
[2025-05-03 16:41:25,487][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1375
[2025-05-03 16:41:26,012][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0462, Metrics: {'mse': 0.04948284849524498, 'rmse': 0.22244740613287667, 'r2': -0.1823517084121704}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1460Epoch 3/15: [                              ] 2/75 batches, loss: 0.1424Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1518Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1444Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1321Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1218Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1205Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1159Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1183Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1133Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1181Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1168Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1195Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1160Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1180Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1189Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1177Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1170Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1168Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1181Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1165Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1164Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1173Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1162Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1151Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1155Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1159Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1162Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1157Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1137Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1138Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1157Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1158Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1166Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1156Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1149Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1170Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1159Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1159Epoch 3/15: [================              ] 40/75 batches, loss: 0.1147Epoch 3/15: [================              ] 41/75 batches, loss: 0.1141Epoch 3/15: [================              ] 42/75 batches, loss: 0.1136Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1137Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1125Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1119Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1114Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1104Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1092Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1083Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1081Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1070Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1062Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1064Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1055Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1052Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1051Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1049Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1048Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1039Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1034Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1038Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1032Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1035Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1032Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1026Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1020Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1011Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1009Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1012Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1017Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1012Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1010Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1022Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1020Epoch 3/15: [==============================] 75/75 batches, loss: 0.1025
[2025-05-03 16:41:28,909][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1025
[2025-05-03 16:41:29,328][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0360, Metrics: {'mse': 0.0375586673617363, 'rmse': 0.19380058658769922, 'r2': 0.10256671905517578}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0772Epoch 4/15: [                              ] 2/75 batches, loss: 0.0712Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0660Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0626Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0582Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0638Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0722Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0753Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0734Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0774Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0832Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0840Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0812Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0795Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0773Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0795Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0809Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0842Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0846Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0868Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0872Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0858Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0834Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0840Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0841Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0845Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0841Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0837Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0826Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0830Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0824Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0815Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0808Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0811Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0809Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0797Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0787Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0785Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0783Epoch 4/15: [================              ] 40/75 batches, loss: 0.0790Epoch 4/15: [================              ] 41/75 batches, loss: 0.0791Epoch 4/15: [================              ] 42/75 batches, loss: 0.0793Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0796Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0795Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0796Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0799Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0790Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0788Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0781Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0772Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0766Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0767Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0770Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0774Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0779Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0781Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0777Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0782Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0779Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0778Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0779Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0782Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0782Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0778Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0779Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0773Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0777Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0775Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0768Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0769Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0766Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0768Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0765Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0767Epoch 4/15: [==============================] 75/75 batches, loss: 0.0763
[2025-05-03 16:41:32,048][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0763
[2025-05-03 16:41:32,535][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0344, Metrics: {'mse': 0.036467280238866806, 'rmse': 0.19096408101752227, 'r2': 0.12864452600479126}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0644Epoch 5/15: [                              ] 2/75 batches, loss: 0.0788Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0727Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0688Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0661Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0638Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0600Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0580Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0662Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0678Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0684Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0708Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0704Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0695Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0690Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0691Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0679Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0663Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0668Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0664Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0652Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0647Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0661Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0670Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0666Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0648Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0666Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0660Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0665Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0681Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0680Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0676Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0679Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0669Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0674Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0674Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0665Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0662Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0664Epoch 5/15: [================              ] 40/75 batches, loss: 0.0658Epoch 5/15: [================              ] 41/75 batches, loss: 0.0654Epoch 5/15: [================              ] 42/75 batches, loss: 0.0650Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0641Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0640Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0633Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0632Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0623Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0621Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0625Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0625Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0626Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0627Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0633Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0634Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0627Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0625Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0624Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0617Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0626Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0623Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0629Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0627Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0628Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0625Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0627Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0629Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0625Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0621Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0620Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0621Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0622Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0622Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0618Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0620Epoch 5/15: [==============================] 75/75 batches, loss: 0.0619
[2025-05-03 16:41:35,185][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0619
[2025-05-03 16:41:35,604][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0313, Metrics: {'mse': 0.03273588418960571, 'rmse': 0.18093060600574384, 'r2': 0.21780312061309814}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0349Epoch 6/15: [                              ] 2/75 batches, loss: 0.0545Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0471Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0484Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0549Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0497Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0497Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0524Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0515Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0539Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0526Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0525Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0502Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0491Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0498Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0524Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0526Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0529Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0516Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0507Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0501Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0490Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0482Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0481Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0501Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0493Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0502Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0498Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0508Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0509Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0517Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0523Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0523Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0523Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0521Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0517Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0515Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0519Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0522Epoch 6/15: [================              ] 40/75 batches, loss: 0.0530Epoch 6/15: [================              ] 41/75 batches, loss: 0.0536Epoch 6/15: [================              ] 42/75 batches, loss: 0.0537Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0536Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0534Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0531Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0529Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0530Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0532Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0532Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0536Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0543Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0536Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0532Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0533Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0530Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0527Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0523Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0524Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0529Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0526Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0524Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0521Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0524Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0530Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0533Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0532Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0530Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0530Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0527Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0527Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0526Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0523Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0522Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0521Epoch 6/15: [==============================] 75/75 batches, loss: 0.0528
[2025-05-03 16:41:38,434][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0528
[2025-05-03 16:41:38,894][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0306, Metrics: {'mse': 0.03140323609113693, 'rmse': 0.1772095823908429, 'r2': 0.2496456503868103}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0614Epoch 7/15: [                              ] 2/75 batches, loss: 0.0635Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0560Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0483Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0518Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0469Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0473Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0474Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0445Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0479Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0476Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0484Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0488Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0489Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0482Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0477Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0485Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0467Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0506Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0523Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0511Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0505Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0494Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0499Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0488Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0483Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0481Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0478Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0483Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0487Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0499Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0492Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0487Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0493Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0487Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0487Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0492Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0486Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0491Epoch 7/15: [================              ] 40/75 batches, loss: 0.0487Epoch 7/15: [================              ] 41/75 batches, loss: 0.0487Epoch 7/15: [================              ] 42/75 batches, loss: 0.0487Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0494Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0500Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0498Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0496Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0494Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0497Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0494Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0489Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0493Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0493Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0498Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0496Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0491Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0489Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0488Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0488Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0487Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0485Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0483Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0481Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0483Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0487Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0487Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0487Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0485Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0486Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0484Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0484Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0481Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0479Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0475Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0478Epoch 7/15: [==============================] 75/75 batches, loss: 0.0474
[2025-05-03 16:41:41,748][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0474
[2025-05-03 16:41:42,179][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0289, Metrics: {'mse': 0.03008570522069931, 'rmse': 0.1734523139675551, 'r2': 0.2811269760131836}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.1210Epoch 8/15: [                              ] 2/75 batches, loss: 0.0799Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0623Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0680Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0689Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0648Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0643Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0583Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0561Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0562Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0546Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0542Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0556Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0553Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0541Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0524Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0526Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0529Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0535Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0546Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0546Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0538Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0529Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0537Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0527Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0519Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0523Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0518Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0516Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0523Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0523Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0527Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0525Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0520Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0527Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0524Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0528Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0528Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0527Epoch 8/15: [================              ] 40/75 batches, loss: 0.0522Epoch 8/15: [================              ] 41/75 batches, loss: 0.0519Epoch 8/15: [================              ] 42/75 batches, loss: 0.0529Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0525Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0523Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0525Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0526Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0521Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0520Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0514Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0512Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0505Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0503Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0497Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0494Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0493Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0495Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0507Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0507Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0507Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0503Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0504Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0503Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0500Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0496Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0497Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0496Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0493Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0490Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0485Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0487Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0486Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0482Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0483Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0485Epoch 8/15: [==============================] 75/75 batches, loss: 0.0479
[2025-05-03 16:41:45,053][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0479
[2025-05-03 16:41:45,494][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0294, Metrics: {'mse': 0.031082311645150185, 'rmse': 0.17630176302337475, 'r2': 0.25731390714645386}
[2025-05-03 16:41:45,495][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0591Epoch 9/15: [                              ] 2/75 batches, loss: 0.0474Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0397Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0361Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0367Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0386Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0414Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0458Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0430Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0424Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0458Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0463Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0445Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0437Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0447Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0468Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0490Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0493Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0499Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0492Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0484Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0489Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0494Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0489Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0483Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0479Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0483Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0482Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0478Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0474Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0477Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0474Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0477Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0477Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0475Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0469Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0474Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0476Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0474Epoch 9/15: [================              ] 40/75 batches, loss: 0.0467Epoch 9/15: [================              ] 41/75 batches, loss: 0.0471Epoch 9/15: [================              ] 42/75 batches, loss: 0.0472Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0474Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0470Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0468Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0469Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0464Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0463Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0465Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0461Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0464Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0458Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0457Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0457Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0457Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0455Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0454Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0452Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0462Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0458Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0454Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0451Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0447Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0445Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0443Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0443Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0445Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0445Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0444Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0443Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0442Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0444Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0442Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0441Epoch 9/15: [==============================] 75/75 batches, loss: 0.0441
[2025-05-03 16:41:47,890][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0441
[2025-05-03 16:41:48,316][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0294, Metrics: {'mse': 0.02967524155974388, 'rmse': 0.17226503289914608, 'r2': 0.290934681892395}
[2025-05-03 16:41:48,317][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0555Epoch 10/15: [                              ] 2/75 batches, loss: 0.0522Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0467Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0444Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0388Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0431Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0459Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0433Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0406Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0392Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0386Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0380Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0376Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0388Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0387Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0379Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0380Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0395Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0408Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0408Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0413Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0416Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0418Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0417Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0417Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0424Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0428Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0424Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0432Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0432Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0430Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0427Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0431Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0427Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0427Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0425Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0427Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0424Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0420Epoch 10/15: [================              ] 40/75 batches, loss: 0.0416Epoch 10/15: [================              ] 41/75 batches, loss: 0.0419Epoch 10/15: [================              ] 42/75 batches, loss: 0.0417Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0417Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0413Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0411Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0413Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0419Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0422Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0417Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0416Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0416Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0414Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0417Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0418Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0416Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0420Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0427Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0426Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0424Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0425Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0422Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0422Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0424Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0421Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0420Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0418Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0417Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0416Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0418Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0417Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0418Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0415Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0415Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0415Epoch 10/15: [==============================] 75/75 batches, loss: 0.0416
[2025-05-03 16:41:50,661][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0416
[2025-05-03 16:41:50,957][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0273, Metrics: {'mse': 0.02773258276283741, 'rmse': 0.16653102642702172, 'r2': 0.3373528718948364}
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0443Epoch 11/15: [                              ] 2/75 batches, loss: 0.0438Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0437Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0405Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0386Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0342Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0351Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0358Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0361Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0375Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0394Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0385Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0384Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0387Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0366Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0357Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0359Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0357Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0351Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0354Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0352Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0353Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0353Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0347Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0344Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0347Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0346Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0351Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0357Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0363Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0359Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0358Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0356Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0359Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0354Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0351Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0351Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0346Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0342Epoch 11/15: [================              ] 40/75 batches, loss: 0.0340Epoch 11/15: [================              ] 41/75 batches, loss: 0.0342Epoch 11/15: [================              ] 42/75 batches, loss: 0.0342Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0344Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0345Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0346Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0342Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0341Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0337Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0335Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0332Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0331Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0328Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0332Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0329Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0336Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0338Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0339Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0345Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0345Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0349Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0346Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0346Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0349Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0348Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0351Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0349Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0347Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0345Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0346Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0346Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0347Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0344Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0343Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0343Epoch 11/15: [==============================] 75/75 batches, loss: 0.0340
[2025-05-03 16:41:53,846][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0340
[2025-05-03 16:41:54,300][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0264, Metrics: {'mse': 0.026872653514146805, 'rmse': 0.16392880623656966, 'r2': 0.3579002022743225}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0634Epoch 12/15: [                              ] 2/75 batches, loss: 0.0571Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0469Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0435Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0433Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0420Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0404Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0387Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0373Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0369Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0361Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0357Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0377Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0374Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0369Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0361Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0353Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0350Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0356Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0355Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0353Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0356Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0362Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0358Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0357Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0364Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0363Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0372Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0381Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0379Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0381Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0384Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0386Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0387Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0385Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0386Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0382Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0382Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0377Epoch 12/15: [================              ] 40/75 batches, loss: 0.0376Epoch 12/15: [================              ] 41/75 batches, loss: 0.0376Epoch 12/15: [================              ] 42/75 batches, loss: 0.0379Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0378Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0378Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0383Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0382Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0380Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0384Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0380Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0378Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0382Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0385Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0387Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0386Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0383Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0382Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0385Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0382Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0383Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0383Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0384Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0389Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0388Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0387Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0384Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0384Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0384Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0382Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0381Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0379Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0376Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0376Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0374Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0373Epoch 12/15: [==============================] 75/75 batches, loss: 0.0370
[2025-05-03 16:41:57,084][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0370
[2025-05-03 16:41:57,635][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0253, Metrics: {'mse': 0.025943441316485405, 'rmse': 0.16106967845154904, 'r2': 0.3801029324531555}
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0250Epoch 13/15: [                              ] 2/75 batches, loss: 0.0250Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0260Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0261Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0238Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0259Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0287Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0294Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0309Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0319Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0326Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0334Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0320Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0314Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0318Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0326Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0324Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0319Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0321Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0320Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0320Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0325Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0321Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0322Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0316Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0327Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0327Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0323Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0321Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0325Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0327Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0326Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0324Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0323Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0323Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0322Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0320Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0318Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0322Epoch 13/15: [================              ] 40/75 batches, loss: 0.0322Epoch 13/15: [================              ] 41/75 batches, loss: 0.0325Epoch 13/15: [================              ] 42/75 batches, loss: 0.0323Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0325Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0331Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0337Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0333Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0333Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0335Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0333Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0329Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0325Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0323Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0321Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0322Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0322Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0320Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0318Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0319Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0323Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0323Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0322Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0323Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0323Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0324Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0326Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0325Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0323Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0325Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0325Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0325Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0325Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0324Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0323Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0322Epoch 13/15: [==============================] 75/75 batches, loss: 0.0325
[2025-05-03 16:42:00,621][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0325
[2025-05-03 16:42:00,961][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0257, Metrics: {'mse': 0.026026912033557892, 'rmse': 0.16132858405613648, 'r2': 0.37810850143432617}
[2025-05-03 16:42:00,962][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0126Epoch 14/15: [                              ] 2/75 batches, loss: 0.0246Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0233Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0215Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0239Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0236Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0261Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0278Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0305Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0301Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0301Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0310Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0308Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0300Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0297Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0289Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0289Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0285Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0292Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0289Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0290Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0291Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0292Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0287Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0301Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0307Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0304Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0303Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0301Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0297Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0298Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0301Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0306Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0307Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0307Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0302Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0301Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0301Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0297Epoch 14/15: [================              ] 40/75 batches, loss: 0.0295Epoch 14/15: [================              ] 41/75 batches, loss: 0.0293Epoch 14/15: [================              ] 42/75 batches, loss: 0.0296Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0302Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0305Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0306Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0305Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0307Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0305Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0306Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0310Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0313Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0313Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0318Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0316Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0314Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0313Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0312Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0312Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0311Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0311Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0310Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0314Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0314Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0311Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0311Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0310Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0311Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0311Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0308Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0309Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0312Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0312Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0311Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0310Epoch 14/15: [==============================] 75/75 batches, loss: 0.0313
[2025-05-03 16:42:03,405][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0313
[2025-05-03 16:42:03,895][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0251, Metrics: {'mse': 0.025347793474793434, 'rmse': 0.15920990382131833, 'r2': 0.39433544874191284}
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0169Epoch 15/15: [                              ] 2/75 batches, loss: 0.0218Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0214Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0238Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0253Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0287Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0292Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0303Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0282Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0276Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0279Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0286Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0321Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0314Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0307Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0312Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0314Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0304Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0305Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0303Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0303Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0298Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0297Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0297Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0295Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0293Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0291Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0287Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0287Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0289Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0290Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0298Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0297Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0292Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0296Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0300Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0302Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0300Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0302Epoch 15/15: [================              ] 40/75 batches, loss: 0.0299Epoch 15/15: [================              ] 41/75 batches, loss: 0.0298Epoch 15/15: [================              ] 42/75 batches, loss: 0.0300Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0299Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0302Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0299Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0300Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0299Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0297Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0293Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0296Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0297Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0299Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0301Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0301Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0299Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0299Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0301Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0299Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0296Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0297Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0300Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0301Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0300Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0299Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0299Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0301Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0301Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0300Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0300Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0298Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0297Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0302Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0299Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0299Epoch 15/15: [==============================] 75/75 batches, loss: 0.0300
[2025-05-03 16:42:06,736][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0300
[2025-05-03 16:42:07,173][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0232, Metrics: {'mse': 0.023563994094729424, 'rmse': 0.15350568098519815, 'r2': 0.43695783615112305}
[2025-05-03 16:42:07,552][src.training.lm_trainer][INFO] - Training completed in 48.49 seconds
[2025-05-03 16:42:07,553][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-03 16:42:10,878][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.020849667489528656, 'rmse': 0.1443941393877489, 'r2': 0.22285860776901245}
[2025-05-03 16:42:10,878][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.023563994094729424, 'rmse': 0.15350568098519815, 'r2': 0.43695783615112305}
[2025-05-03 16:42:10,878][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.022300826385617256, 'rmse': 0.14933461214874888, 'r2': 0.4213380813598633}
[2025-05-03 16:42:12,522][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/en/en/model.pt
[2025-05-03 16:42:12,523][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▄▃▃▃▂▂▂▁▁▁
wandb:     best_val_mse █▆▄▄▃▃▂▂▂▂▁▁
wandb:      best_val_r2 ▁▃▅▅▆▆▇▇▇▇██
wandb:    best_val_rmse █▇▄▄▃▃▃▂▂▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▅▅▆▆▆▆▆▆▆▇▆▇
wandb:       train_loss █▄▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▄▃▃▃▂▂▂▂▂▁▂▁▁
wandb:          val_mse █▆▄▄▃▃▂▃▂▂▂▂▂▁▁
wandb:           val_r2 ▁▃▅▅▆▆▇▆▇▇▇▇▇██
wandb:         val_rmse █▇▄▄▃▃▃▃▃▂▂▂▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0232
wandb:     best_val_mse 0.02356
wandb:      best_val_r2 0.43696
wandb:    best_val_rmse 0.15351
wandb:            epoch 15
wandb:   final_test_mse 0.0223
wandb:    final_test_r2 0.42134
wandb:  final_test_rmse 0.14933
wandb:  final_train_mse 0.02085
wandb:   final_train_r2 0.22286
wandb: final_train_rmse 0.14439
wandb:    final_val_mse 0.02356
wandb:     final_val_r2 0.43696
wandb:   final_val_rmse 0.15351
wandb:    learning_rate 0.0001
wandb:       train_loss 0.02997
wandb:       train_time 48.48893
wandb:         val_loss 0.0232
wandb:          val_mse 0.02356
wandb:           val_r2 0.43696
wandb:         val_rmse 0.15351
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_164051-qzcxr6jy
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_164051-qzcxr6jy/logs
Experiment probe_layer3_complexity_en completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/en/en/results.json for layer 3
Running experiment: probe_layer3_question_type_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=3"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer3_question_type_fi"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/layer3/fi"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-03 16:42:59,070][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/fi
experiment_name: probe_layer3_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-03 16:42:59,070][__main__][INFO] - Normalized task: question_type
[2025-05-03 16:42:59,070][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-03 16:42:59,070][__main__][INFO] - Determined Task Type: classification
[2025-05-03 16:42:59,115][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-05-03 16:42:59,115][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-03 16:43:05,074][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-03 16:43:07,719][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-03 16:43:07,719][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:43:08,279][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:43:08,482][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:43:08,860][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-03 16:43:08,871][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:43:08,872][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-03 16:43:08,873][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:43:08,955][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:43:09,096][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:43:09,111][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-03 16:43:09,113][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:43:09,113][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-03 16:43:09,114][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:43:09,347][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:43:09,511][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:43:09,560][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-03 16:43:09,562][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:43:09,563][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-03 16:43:09,574][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-03 16:43:09,575][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:43:09,575][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:43:09,575][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:43:09,575][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:43:09,575][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-05-03 16:43:09,575][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-05-03 16:43:09,576][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-03 16:43:09,576][src.data.datasets][INFO] - Sample label: 1
[2025-05-03 16:43:09,576][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:43:09,576][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:43:09,576][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:43:09,576][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:43:09,576][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-05-03 16:43:09,576][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-05-03 16:43:09,576][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-03 16:43:09,576][src.data.datasets][INFO] - Sample label: 1
[2025-05-03 16:43:09,577][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:43:09,577][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:43:09,577][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:43:09,577][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:43:09,577][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-03 16:43:09,577][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-03 16:43:09,577][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-03 16:43:09,577][src.data.datasets][INFO] - Sample label: 0
[2025-05-03 16:43:09,577][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-03 16:43:09,577][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-03 16:43:09,578][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-03 16:43:09,578][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-03 16:43:09,578][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-03 16:43:20,531][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-03 16:43:20,532][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-03 16:43:20,532][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=3, freeze_model=True
[2025-05-03 16:43:20,532][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-03 16:43:20,538][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-03 16:43:20,538][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-03 16:43:20,538][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-03 16:43:20,539][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-03 16:43:20,539][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-03 16:43:20,539][__main__][INFO] - Total parameters: 394,568,839
[2025-05-03 16:43:20,540][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-03 16:43:20,540][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7029Epoch 1/15: [                              ] 2/75 batches, loss: 0.7112Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7218Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7216Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7128Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7044Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7020Epoch 1/15: [===                           ] 8/75 batches, loss: 0.7014Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6995Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6985Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6997Epoch 1/15: [====                          ] 12/75 batches, loss: 0.6982Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.6984Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.6979Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6980Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6979Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6981Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6975Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6973Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6971Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6965Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6959Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6958Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6957Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6955Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6954Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6953Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6954Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6953Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6951Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6948Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6947Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6947Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6945Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6940Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6937Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6938Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6938Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6937Epoch 1/15: [================              ] 40/75 batches, loss: 0.6935Epoch 1/15: [================              ] 41/75 batches, loss: 0.6939Epoch 1/15: [================              ] 42/75 batches, loss: 0.6937Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6939Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6938Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6938Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6936Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6937Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6935Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6934Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6937Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6934Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6932Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6935Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6933Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6933Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6933Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6932Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6927Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6926Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6923Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6924Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6921Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6911Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6912Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6903Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6902Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6899Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6883Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6880Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6885Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6879Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6879Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6877Epoch 1/15: [==============================] 75/75 batches, loss: 0.6864
[2025-05-03 16:43:28,770][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6864
[2025-05-03 16:43:29,074][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6687, Metrics: {'accuracy': 0.5555555555555556, 'f1': 0.17647058823529413, 'precision': 0.75, 'recall': 0.1}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6336Epoch 2/15: [                              ] 2/75 batches, loss: 0.6376Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6595Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6673Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6681Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6632Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6620Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6631Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6614Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6644Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6615Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6578Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6549Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6479Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6472Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6427Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6349Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6359Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6443Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6497Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6502Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6458Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6468Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6442Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6454Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6461Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6451Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6442Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6436Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6425Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6433Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6416Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6427Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6443Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6444Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6442Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6439Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6428Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6425Epoch 2/15: [================              ] 40/75 batches, loss: 0.6416Epoch 2/15: [================              ] 41/75 batches, loss: 0.6420Epoch 2/15: [================              ] 42/75 batches, loss: 0.6418Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6409Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6396Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6373Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6381Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6380Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6381Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6387Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6378Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6381Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6367Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6367Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6360Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6357Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6345Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6354Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6341Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6347Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6331Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6333Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6337Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6344Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6356Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6345Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6344Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6335Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6328Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6324Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6317Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6317Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6298Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6293Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6284Epoch 2/15: [==============================] 75/75 batches, loss: 0.6281
[2025-05-03 16:43:31,946][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6281
[2025-05-03 16:43:32,290][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6076, Metrics: {'accuracy': 0.8253968253968254, 'f1': 0.8, 'precision': 0.88, 'recall': 0.7333333333333333}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.5404Epoch 3/15: [                              ] 2/75 batches, loss: 0.5384Epoch 3/15: [=                             ] 3/75 batches, loss: 0.5539Epoch 3/15: [=                             ] 4/75 batches, loss: 0.5842Epoch 3/15: [==                            ] 5/75 batches, loss: 0.5752Epoch 3/15: [==                            ] 6/75 batches, loss: 0.5737Epoch 3/15: [==                            ] 7/75 batches, loss: 0.5705Epoch 3/15: [===                           ] 8/75 batches, loss: 0.5763Epoch 3/15: [===                           ] 9/75 batches, loss: 0.5709Epoch 3/15: [====                          ] 10/75 batches, loss: 0.5844Epoch 3/15: [====                          ] 11/75 batches, loss: 0.5845Epoch 3/15: [====                          ] 12/75 batches, loss: 0.5809Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.5745Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.5766Epoch 3/15: [======                        ] 15/75 batches, loss: 0.5738Epoch 3/15: [======                        ] 16/75 batches, loss: 0.5718Epoch 3/15: [======                        ] 17/75 batches, loss: 0.5692Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.5632Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.5689Epoch 3/15: [========                      ] 20/75 batches, loss: 0.5637Epoch 3/15: [========                      ] 21/75 batches, loss: 0.5655Epoch 3/15: [========                      ] 22/75 batches, loss: 0.5685Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.5659Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.5691Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.5745Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.5776Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.5761Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.5773Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.5787Epoch 3/15: [============                  ] 30/75 batches, loss: 0.5799Epoch 3/15: [============                  ] 31/75 batches, loss: 0.5794Epoch 3/15: [============                  ] 32/75 batches, loss: 0.5796Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.5794Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.5773Epoch 3/15: [==============                ] 35/75 batches, loss: 0.5771Epoch 3/15: [==============                ] 36/75 batches, loss: 0.5755Epoch 3/15: [==============                ] 37/75 batches, loss: 0.5757Epoch 3/15: [===============               ] 38/75 batches, loss: 0.5750Epoch 3/15: [===============               ] 39/75 batches, loss: 0.5765Epoch 3/15: [================              ] 40/75 batches, loss: 0.5755Epoch 3/15: [================              ] 41/75 batches, loss: 0.5748Epoch 3/15: [================              ] 42/75 batches, loss: 0.5714Epoch 3/15: [=================             ] 43/75 batches, loss: 0.5698Epoch 3/15: [=================             ] 44/75 batches, loss: 0.5713Epoch 3/15: [==================            ] 45/75 batches, loss: 0.5709Epoch 3/15: [==================            ] 46/75 batches, loss: 0.5696Epoch 3/15: [==================            ] 47/75 batches, loss: 0.5704Epoch 3/15: [===================           ] 48/75 batches, loss: 0.5716Epoch 3/15: [===================           ] 49/75 batches, loss: 0.5701Epoch 3/15: [====================          ] 50/75 batches, loss: 0.5713Epoch 3/15: [====================          ] 51/75 batches, loss: 0.5713Epoch 3/15: [====================          ] 52/75 batches, loss: 0.5710Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.5719Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.5712Epoch 3/15: [======================        ] 55/75 batches, loss: 0.5723Epoch 3/15: [======================        ] 56/75 batches, loss: 0.5720Epoch 3/15: [======================        ] 57/75 batches, loss: 0.5700Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.5698Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.5716Epoch 3/15: [========================      ] 60/75 batches, loss: 0.5722Epoch 3/15: [========================      ] 61/75 batches, loss: 0.5721Epoch 3/15: [========================      ] 62/75 batches, loss: 0.5731Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.5744Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.5736Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.5737Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.5729Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.5716Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.5721Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.5720Epoch 3/15: [============================  ] 70/75 batches, loss: 0.5713Epoch 3/15: [============================  ] 71/75 batches, loss: 0.5716Epoch 3/15: [============================  ] 72/75 batches, loss: 0.5720Epoch 3/15: [============================= ] 73/75 batches, loss: 0.5721Epoch 3/15: [============================= ] 74/75 batches, loss: 0.5727Epoch 3/15: [==============================] 75/75 batches, loss: 0.5726
[2025-05-03 16:43:35,107][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.5726
[2025-05-03 16:43:35,620][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6117, Metrics: {'accuracy': 0.746031746031746, 'f1': 0.6521739130434783, 'precision': 0.9375, 'recall': 0.5}
[2025-05-03 16:43:35,621][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.5899Epoch 4/15: [                              ] 2/75 batches, loss: 0.5387Epoch 4/15: [=                             ] 3/75 batches, loss: 0.5617Epoch 4/15: [=                             ] 4/75 batches, loss: 0.5710Epoch 4/15: [==                            ] 5/75 batches, loss: 0.5696Epoch 4/15: [==                            ] 6/75 batches, loss: 0.5859Epoch 4/15: [==                            ] 7/75 batches, loss: 0.5807Epoch 4/15: [===                           ] 8/75 batches, loss: 0.5839Epoch 4/15: [===                           ] 9/75 batches, loss: 0.5825Epoch 4/15: [====                          ] 10/75 batches, loss: 0.5782Epoch 4/15: [====                          ] 11/75 batches, loss: 0.5775Epoch 4/15: [====                          ] 12/75 batches, loss: 0.5768Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.5694Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.5726Epoch 4/15: [======                        ] 15/75 batches, loss: 0.5730Epoch 4/15: [======                        ] 16/75 batches, loss: 0.5714Epoch 4/15: [======                        ] 17/75 batches, loss: 0.5699Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.5682Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.5651Epoch 4/15: [========                      ] 20/75 batches, loss: 0.5637Epoch 4/15: [========                      ] 21/75 batches, loss: 0.5650Epoch 4/15: [========                      ] 22/75 batches, loss: 0.5693Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.5719Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.5728Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.5717Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.5712Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.5699Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.5678Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.5702Epoch 4/15: [============                  ] 30/75 batches, loss: 0.5662Epoch 4/15: [============                  ] 31/75 batches, loss: 0.5662Epoch 4/15: [============                  ] 32/75 batches, loss: 0.5666Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.5678Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.5681Epoch 4/15: [==============                ] 35/75 batches, loss: 0.5677Epoch 4/15: [==============                ] 36/75 batches, loss: 0.5635Epoch 4/15: [==============                ] 37/75 batches, loss: 0.5648Epoch 4/15: [===============               ] 38/75 batches, loss: 0.5644Epoch 4/15: [===============               ] 39/75 batches, loss: 0.5613Epoch 4/15: [================              ] 40/75 batches, loss: 0.5609Epoch 4/15: [================              ] 41/75 batches, loss: 0.5609Epoch 4/15: [================              ] 42/75 batches, loss: 0.5603Epoch 4/15: [=================             ] 43/75 batches, loss: 0.5606Epoch 4/15: [=================             ] 44/75 batches, loss: 0.5580Epoch 4/15: [==================            ] 45/75 batches, loss: 0.5573Epoch 4/15: [==================            ] 46/75 batches, loss: 0.5577Epoch 4/15: [==================            ] 47/75 batches, loss: 0.5571Epoch 4/15: [===================           ] 48/75 batches, loss: 0.5556Epoch 4/15: [===================           ] 49/75 batches, loss: 0.5578Epoch 4/15: [====================          ] 50/75 batches, loss: 0.5587Epoch 4/15: [====================          ] 51/75 batches, loss: 0.5597Epoch 4/15: [====================          ] 52/75 batches, loss: 0.5596Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.5588Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.5579Epoch 4/15: [======================        ] 55/75 batches, loss: 0.5596Epoch 4/15: [======================        ] 56/75 batches, loss: 0.5588Epoch 4/15: [======================        ] 57/75 batches, loss: 0.5578Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.5580Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.5575Epoch 4/15: [========================      ] 60/75 batches, loss: 0.5580Epoch 4/15: [========================      ] 61/75 batches, loss: 0.5589Epoch 4/15: [========================      ] 62/75 batches, loss: 0.5587Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.5586Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.5587Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.5587Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.5588Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.5583Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.5582Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.5602Epoch 4/15: [============================  ] 70/75 batches, loss: 0.5596Epoch 4/15: [============================  ] 71/75 batches, loss: 0.5587Epoch 4/15: [============================  ] 72/75 batches, loss: 0.5584Epoch 4/15: [============================= ] 73/75 batches, loss: 0.5586Epoch 4/15: [============================= ] 74/75 batches, loss: 0.5579Epoch 4/15: [==============================] 75/75 batches, loss: 0.5584
[2025-05-03 16:43:37,917][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5584
[2025-05-03 16:43:38,232][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5794, Metrics: {'accuracy': 0.8253968253968254, 'f1': 0.7924528301886793, 'precision': 0.9130434782608695, 'recall': 0.7}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.4983Epoch 5/15: [                              ] 2/75 batches, loss: 0.5216Epoch 5/15: [=                             ] 3/75 batches, loss: 0.5545Epoch 5/15: [=                             ] 4/75 batches, loss: 0.5558Epoch 5/15: [==                            ] 5/75 batches, loss: 0.5414Epoch 5/15: [==                            ] 6/75 batches, loss: 0.5461Epoch 5/15: [==                            ] 7/75 batches, loss: 0.5432Epoch 5/15: [===                           ] 8/75 batches, loss: 0.5447Epoch 5/15: [===                           ] 9/75 batches, loss: 0.5503Epoch 5/15: [====                          ] 10/75 batches, loss: 0.5452Epoch 5/15: [====                          ] 11/75 batches, loss: 0.5521Epoch 5/15: [====                          ] 12/75 batches, loss: 0.5523Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.5478Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.5398Epoch 5/15: [======                        ] 15/75 batches, loss: 0.5342Epoch 5/15: [======                        ] 16/75 batches, loss: 0.5335Epoch 5/15: [======                        ] 17/75 batches, loss: 0.5331Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.5362Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.5325Epoch 5/15: [========                      ] 20/75 batches, loss: 0.5324Epoch 5/15: [========                      ] 21/75 batches, loss: 0.5322Epoch 5/15: [========                      ] 22/75 batches, loss: 0.5337Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.5377Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.5335Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.5308Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.5323Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.5378Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.5393Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.5435Epoch 5/15: [============                  ] 30/75 batches, loss: 0.5454Epoch 5/15: [============                  ] 31/75 batches, loss: 0.5462Epoch 5/15: [============                  ] 32/75 batches, loss: 0.5476Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.5474Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.5465Epoch 5/15: [==============                ] 35/75 batches, loss: 0.5484Epoch 5/15: [==============                ] 36/75 batches, loss: 0.5505Epoch 5/15: [==============                ] 37/75 batches, loss: 0.5496Epoch 5/15: [===============               ] 38/75 batches, loss: 0.5497Epoch 5/15: [===============               ] 39/75 batches, loss: 0.5476Epoch 5/15: [================              ] 40/75 batches, loss: 0.5478Epoch 5/15: [================              ] 41/75 batches, loss: 0.5489Epoch 5/15: [================              ] 42/75 batches, loss: 0.5486Epoch 5/15: [=================             ] 43/75 batches, loss: 0.5461Epoch 5/15: [=================             ] 44/75 batches, loss: 0.5454Epoch 5/15: [==================            ] 45/75 batches, loss: 0.5463Epoch 5/15: [==================            ] 46/75 batches, loss: 0.5444Epoch 5/15: [==================            ] 47/75 batches, loss: 0.5425Epoch 5/15: [===================           ] 48/75 batches, loss: 0.5390Epoch 5/15: [===================           ] 49/75 batches, loss: 0.5406Epoch 5/15: [====================          ] 50/75 batches, loss: 0.5414Epoch 5/15: [====================          ] 51/75 batches, loss: 0.5413Epoch 5/15: [====================          ] 52/75 batches, loss: 0.5412Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.5436Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.5427Epoch 5/15: [======================        ] 55/75 batches, loss: 0.5419Epoch 5/15: [======================        ] 56/75 batches, loss: 0.5420Epoch 5/15: [======================        ] 57/75 batches, loss: 0.5427Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.5426Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.5433Epoch 5/15: [========================      ] 60/75 batches, loss: 0.5432Epoch 5/15: [========================      ] 61/75 batches, loss: 0.5423Epoch 5/15: [========================      ] 62/75 batches, loss: 0.5422Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.5437Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.5451Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.5448Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.5450Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.5457Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.5444Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.5452Epoch 5/15: [============================  ] 70/75 batches, loss: 0.5450Epoch 5/15: [============================  ] 71/75 batches, loss: 0.5446Epoch 5/15: [============================  ] 72/75 batches, loss: 0.5455Epoch 5/15: [============================= ] 73/75 batches, loss: 0.5453Epoch 5/15: [============================= ] 74/75 batches, loss: 0.5441Epoch 5/15: [==============================] 75/75 batches, loss: 0.5443
[2025-05-03 16:43:41,062][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5443
[2025-05-03 16:43:41,455][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5859, Metrics: {'accuracy': 0.7936507936507936, 'f1': 0.7346938775510204, 'precision': 0.9473684210526315, 'recall': 0.6}
[2025-05-03 16:43:41,455][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.5863Epoch 6/15: [                              ] 2/75 batches, loss: 0.5861Epoch 6/15: [=                             ] 3/75 batches, loss: 0.5880Epoch 6/15: [=                             ] 4/75 batches, loss: 0.5934Epoch 6/15: [==                            ] 5/75 batches, loss: 0.5705Epoch 6/15: [==                            ] 6/75 batches, loss: 0.5719Epoch 6/15: [==                            ] 7/75 batches, loss: 0.5720Epoch 6/15: [===                           ] 8/75 batches, loss: 0.5586Epoch 6/15: [===                           ] 9/75 batches, loss: 0.5547Epoch 6/15: [====                          ] 10/75 batches, loss: 0.5423Epoch 6/15: [====                          ] 11/75 batches, loss: 0.5408Epoch 6/15: [====                          ] 12/75 batches, loss: 0.5412Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.5465Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.5467Epoch 6/15: [======                        ] 15/75 batches, loss: 0.5425Epoch 6/15: [======                        ] 16/75 batches, loss: 0.5388Epoch 6/15: [======                        ] 17/75 batches, loss: 0.5347Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.5352Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.5353Epoch 6/15: [========                      ] 20/75 batches, loss: 0.5382Epoch 6/15: [========                      ] 21/75 batches, loss: 0.5403Epoch 6/15: [========                      ] 22/75 batches, loss: 0.5381Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.5379Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.5355Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.5416Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.5437Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.5417Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.5432Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.5428Epoch 6/15: [============                  ] 30/75 batches, loss: 0.5437Epoch 6/15: [============                  ] 31/75 batches, loss: 0.5431Epoch 6/15: [============                  ] 32/75 batches, loss: 0.5418Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.5390Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.5387Epoch 6/15: [==============                ] 35/75 batches, loss: 0.5386Epoch 6/15: [==============                ] 36/75 batches, loss: 0.5387Epoch 6/15: [==============                ] 37/75 batches, loss: 0.5373Epoch 6/15: [===============               ] 38/75 batches, loss: 0.5401Epoch 6/15: [===============               ] 39/75 batches, loss: 0.5417Epoch 6/15: [================              ] 40/75 batches, loss: 0.5412Epoch 6/15: [================              ] 41/75 batches, loss: 0.5399Epoch 6/15: [================              ] 42/75 batches, loss: 0.5400Epoch 6/15: [=================             ] 43/75 batches, loss: 0.5403Epoch 6/15: [=================             ] 44/75 batches, loss: 0.5411Epoch 6/15: [==================            ] 45/75 batches, loss: 0.5398Epoch 6/15: [==================            ] 46/75 batches, loss: 0.5392Epoch 6/15: [==================            ] 47/75 batches, loss: 0.5388Epoch 6/15: [===================           ] 48/75 batches, loss: 0.5391Epoch 6/15: [===================           ] 49/75 batches, loss: 0.5408Epoch 6/15: [====================          ] 50/75 batches, loss: 0.5406Epoch 6/15: [====================          ] 51/75 batches, loss: 0.5418Epoch 6/15: [====================          ] 52/75 batches, loss: 0.5406Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.5401Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.5394Epoch 6/15: [======================        ] 55/75 batches, loss: 0.5393Epoch 6/15: [======================        ] 56/75 batches, loss: 0.5406Epoch 6/15: [======================        ] 57/75 batches, loss: 0.5408Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.5396Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.5396Epoch 6/15: [========================      ] 60/75 batches, loss: 0.5398Epoch 6/15: [========================      ] 61/75 batches, loss: 0.5397Epoch 6/15: [========================      ] 62/75 batches, loss: 0.5397Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.5400Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.5410Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.5420Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.5429Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.5421Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.5418Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.5411Epoch 6/15: [============================  ] 70/75 batches, loss: 0.5409Epoch 6/15: [============================  ] 71/75 batches, loss: 0.5406Epoch 6/15: [============================  ] 72/75 batches, loss: 0.5409Epoch 6/15: [============================= ] 73/75 batches, loss: 0.5396Epoch 6/15: [============================= ] 74/75 batches, loss: 0.5395Epoch 6/15: [==============================] 75/75 batches, loss: 0.5397
[2025-05-03 16:43:43,957][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5397
[2025-05-03 16:43:44,280][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5884, Metrics: {'accuracy': 0.7777777777777778, 'f1': 0.7083333333333334, 'precision': 0.9444444444444444, 'recall': 0.5666666666666667}
[2025-05-03 16:43:44,281][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.5765Epoch 7/15: [                              ] 2/75 batches, loss: 0.5548Epoch 7/15: [=                             ] 3/75 batches, loss: 0.5621Epoch 7/15: [=                             ] 4/75 batches, loss: 0.5419Epoch 7/15: [==                            ] 5/75 batches, loss: 0.5494Epoch 7/15: [==                            ] 6/75 batches, loss: 0.5466Epoch 7/15: [==                            ] 7/75 batches, loss: 0.5561Epoch 7/15: [===                           ] 8/75 batches, loss: 0.5558Epoch 7/15: [===                           ] 9/75 batches, loss: 0.5568Epoch 7/15: [====                          ] 10/75 batches, loss: 0.5572Epoch 7/15: [====                          ] 11/75 batches, loss: 0.5442Epoch 7/15: [====                          ] 12/75 batches, loss: 0.5413Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.5446Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.5376Epoch 7/15: [======                        ] 15/75 batches, loss: 0.5378Epoch 7/15: [======                        ] 16/75 batches, loss: 0.5379Epoch 7/15: [======                        ] 17/75 batches, loss: 0.5364Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.5363Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.5406Epoch 7/15: [========                      ] 20/75 batches, loss: 0.5406Epoch 7/15: [========                      ] 21/75 batches, loss: 0.5379Epoch 7/15: [========                      ] 22/75 batches, loss: 0.5397Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.5404Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.5414Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.5462Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.5462Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.5471Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.5458Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.5449Epoch 7/15: [============                  ] 30/75 batches, loss: 0.5433Epoch 7/15: [============                  ] 31/75 batches, loss: 0.5415Epoch 7/15: [============                  ] 32/75 batches, loss: 0.5401Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.5405Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.5412Epoch 7/15: [==============                ] 35/75 batches, loss: 0.5422Epoch 7/15: [==============                ] 36/75 batches, loss: 0.5407Epoch 7/15: [==============                ] 37/75 batches, loss: 0.5405Epoch 7/15: [===============               ] 38/75 batches, loss: 0.5387Epoch 7/15: [===============               ] 39/75 batches, loss: 0.5367Epoch 7/15: [================              ] 40/75 batches, loss: 0.5344Epoch 7/15: [================              ] 41/75 batches, loss: 0.5345Epoch 7/15: [================              ] 42/75 batches, loss: 0.5348Epoch 7/15: [=================             ] 43/75 batches, loss: 0.5334Epoch 7/15: [=================             ] 44/75 batches, loss: 0.5318Epoch 7/15: [==================            ] 45/75 batches, loss: 0.5316Epoch 7/15: [==================            ] 46/75 batches, loss: 0.5297Epoch 7/15: [==================            ] 47/75 batches, loss: 0.5316Epoch 7/15: [===================           ] 48/75 batches, loss: 0.5327Epoch 7/15: [===================           ] 49/75 batches, loss: 0.5342Epoch 7/15: [====================          ] 50/75 batches, loss: 0.5342Epoch 7/15: [====================          ] 51/75 batches, loss: 0.5337Epoch 7/15: [====================          ] 52/75 batches, loss: 0.5340Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.5343Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.5329Epoch 7/15: [======================        ] 55/75 batches, loss: 0.5350Epoch 7/15: [======================        ] 56/75 batches, loss: 0.5350Epoch 7/15: [======================        ] 57/75 batches, loss: 0.5353Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.5368Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.5362Epoch 7/15: [========================      ] 60/75 batches, loss: 0.5365Epoch 7/15: [========================      ] 61/75 batches, loss: 0.5367Epoch 7/15: [========================      ] 62/75 batches, loss: 0.5365Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.5360Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.5372Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.5375Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.5364Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.5364Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.5363Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.5363Epoch 7/15: [============================  ] 70/75 batches, loss: 0.5367Epoch 7/15: [============================  ] 71/75 batches, loss: 0.5381Epoch 7/15: [============================  ] 72/75 batches, loss: 0.5380Epoch 7/15: [============================= ] 73/75 batches, loss: 0.5371Epoch 7/15: [============================= ] 74/75 batches, loss: 0.5363Epoch 7/15: [==============================] 75/75 batches, loss: 0.5354
[2025-05-03 16:43:46,704][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5354
[2025-05-03 16:43:47,144][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5810, Metrics: {'accuracy': 0.8253968253968254, 'f1': 0.7924528301886793, 'precision': 0.9130434782608695, 'recall': 0.7}
[2025-05-03 16:43:47,145][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-03 16:43:47,145][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 7
[2025-05-03 16:43:47,145][src.training.lm_trainer][INFO] - Training completed in 21.42 seconds
[2025-05-03 16:43:47,145][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-03 16:43:50,351][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9472803347280335, 'f1': 0.9457364341085271, 'precision': 0.9751332149200711, 'recall': 0.9180602006688964}
[2025-05-03 16:43:50,351][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.8253968253968254, 'f1': 0.7924528301886793, 'precision': 0.9130434782608695, 'recall': 0.7}
[2025-05-03 16:43:50,352][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.8363636363636363, 'f1': 0.8333333333333334, 'precision': 0.8490566037735849, 'recall': 0.8181818181818182}
[2025-05-03 16:43:52,035][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/fi/fi/model.pt
[2025-05-03 16:43:52,037][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁██
wandb:           best_val_f1 ▁██
wandb:         best_val_loss █▃▁
wandb:    best_val_precision ▁▇█
wandb:       best_val_recall ▁██
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▂▃▃▃
wandb:            train_loss █▅▃▂▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁█▆█▇▇█
wandb:                val_f1 ▁█▆█▇▇█
wandb:              val_loss █▃▄▁▂▂▁
wandb:         val_precision ▁▆█▇██▇
wandb:            val_recall ▁█▅█▇▆█
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.8254
wandb:           best_val_f1 0.79245
wandb:         best_val_loss 0.57944
wandb:    best_val_precision 0.91304
wandb:       best_val_recall 0.7
wandb:      early_stop_epoch 7
wandb:                 epoch 7
wandb:   final_test_accuracy 0.83636
wandb:         final_test_f1 0.83333
wandb:  final_test_precision 0.84906
wandb:     final_test_recall 0.81818
wandb:  final_train_accuracy 0.94728
wandb:        final_train_f1 0.94574
wandb: final_train_precision 0.97513
wandb:    final_train_recall 0.91806
wandb:    final_val_accuracy 0.8254
wandb:          final_val_f1 0.79245
wandb:   final_val_precision 0.91304
wandb:      final_val_recall 0.7
wandb:         learning_rate 0.0001
wandb:            train_loss 0.53539
wandb:            train_time 21.41722
wandb:          val_accuracy 0.8254
wandb:                val_f1 0.79245
wandb:              val_loss 0.58104
wandb:         val_precision 0.91304
wandb:            val_recall 0.7
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_164259-ajv3fo94
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_164259-ajv3fo94/logs
Experiment probe_layer3_question_type_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/fi/fi/results.json for layer 3
Running experiment: probe_layer3_complexity_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=3"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer3_complexity_fi"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/layer3/fi"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-03 16:44:36,543][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/fi
experiment_name: probe_layer3_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-03 16:44:36,543][__main__][INFO] - Normalized task: complexity
[2025-05-03 16:44:36,543][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-03 16:44:36,543][__main__][INFO] - Determined Task Type: regression
[2025-05-03 16:44:36,547][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-05-03 16:44:36,548][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-03 16:44:42,376][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-03 16:44:44,840][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-03 16:44:44,840][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:44:45,355][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:44:45,386][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:44:45,778][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-03 16:44:45,788][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:44:45,789][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-03 16:44:45,801][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:44:45,912][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:44:46,161][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:44:46,185][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-03 16:44:46,186][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:44:46,187][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-03 16:44:46,187][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:44:46,384][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:44:46,613][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:44:46,716][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-03 16:44:46,718][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:44:46,718][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-03 16:44:46,719][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-03 16:44:46,720][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:44:46,721][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:44:46,721][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:44:46,721][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:44:46,721][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:44:46,721][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-05-03 16:44:46,721][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-03 16:44:46,721][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-05-03 16:44:46,722][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:44:46,722][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:44:46,722][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:44:46,722][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:44:46,722][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:44:46,722][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-05-03 16:44:46,722][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-03 16:44:46,722][src.data.datasets][INFO] - Sample label: 1.0
[2025-05-03 16:44:46,722][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:44:46,722][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:44:46,723][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:44:46,723][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:44:46,723][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:44:46,723][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-05-03 16:44:46,723][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-03 16:44:46,723][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-05-03 16:44:46,723][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-03 16:44:46,723][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-03 16:44:46,723][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-03 16:44:46,724][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-03 16:44:46,724][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-03 16:44:58,384][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-03 16:44:58,385][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-03 16:44:58,385][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=3, freeze_model=True
[2025-05-03 16:44:58,385][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-03 16:44:58,388][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-03 16:44:58,388][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-03 16:44:58,388][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-03 16:44:58,389][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-03 16:44:58,389][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-03 16:44:58,389][__main__][INFO] - Total parameters: 394,255,105
[2025-05-03 16:44:58,390][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.3871Epoch 1/15: [                              ] 2/75 batches, loss: 0.5294Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4093Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4228Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4052Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3694Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3582Epoch 1/15: [===                           ] 8/75 batches, loss: 0.3820Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3821Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3627Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3528Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3497Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3364Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3364Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3329Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3507Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3465Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3485Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3561Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3599Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3611Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3692Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3642Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3565Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3519Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3471Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3451Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3378Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3359Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3349Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3303Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3264Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3238Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3263Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3227Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3236Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3198Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3189Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3189Epoch 1/15: [================              ] 40/75 batches, loss: 0.3162Epoch 1/15: [================              ] 41/75 batches, loss: 0.3123Epoch 1/15: [================              ] 42/75 batches, loss: 0.3081Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3053Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3042Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3069Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3024Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3007Epoch 1/15: [===================           ] 48/75 batches, loss: 0.2953Epoch 1/15: [===================           ] 49/75 batches, loss: 0.2937Epoch 1/15: [====================          ] 50/75 batches, loss: 0.2931Epoch 1/15: [====================          ] 51/75 batches, loss: 0.2927Epoch 1/15: [====================          ] 52/75 batches, loss: 0.2928Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.2908Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.2929Epoch 1/15: [======================        ] 55/75 batches, loss: 0.2909Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2889Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2896Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2889Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2888Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2862Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2833Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2821Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2798Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2784Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2763Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2749Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2719Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2693Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2701Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2688Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2682Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2667Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2641Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2626Epoch 1/15: [==============================] 75/75 batches, loss: 0.2606
[2025-05-03 16:45:06,419][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2606
[2025-05-03 16:45:06,716][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0979, Metrics: {'mse': 0.09768663346767426, 'rmse': 0.3125486097676236, 'r2': -0.49002158641815186}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1717Epoch 2/15: [                              ] 2/75 batches, loss: 0.1676Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1533Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1886Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1817Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1584Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1707Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1651Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1619Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1629Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1587Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1537Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1493Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1452Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1535Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1511Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1479Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1450Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1436Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1474Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1460Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1435Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1419Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1406Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1411Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1473Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1478Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1470Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1483Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1488Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1480Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1482Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1479Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1509Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1503Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1496Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1481Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1470Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1468Epoch 2/15: [================              ] 40/75 batches, loss: 0.1462Epoch 2/15: [================              ] 41/75 batches, loss: 0.1463Epoch 2/15: [================              ] 42/75 batches, loss: 0.1444Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1443Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1426Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1426Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1451Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1438Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1434Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1434Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1420Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1421Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1415Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1419Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1407Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1400Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1421Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1403Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1392Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1387Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1375Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1364Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1358Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1351Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1349Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1345Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1340Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1331Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1337Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1330Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1320Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1319Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1316Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1317Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1310Epoch 2/15: [==============================] 75/75 batches, loss: 0.1305
[2025-05-03 16:45:09,468][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1305
[2025-05-03 16:45:09,845][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1315, Metrics: {'mse': 0.13161858916282654, 'rmse': 0.36279276338265976, 'r2': -1.0075883865356445}
[2025-05-03 16:45:09,846][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1000Epoch 3/15: [                              ] 2/75 batches, loss: 0.1088Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1059Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0982Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0884Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0918Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0948Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0919Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0887Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0885Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0846Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0819Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0878Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0912Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0902Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0891Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0886Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0891Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0889Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0885Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0878Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0876Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0865Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0912Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0916Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0920Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0907Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0903Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0908Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0896Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0892Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0903Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0909Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0909Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0900Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0898Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0889Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0895Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0888Epoch 3/15: [================              ] 40/75 batches, loss: 0.0901Epoch 3/15: [================              ] 41/75 batches, loss: 0.0896Epoch 3/15: [================              ] 42/75 batches, loss: 0.0888Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0881Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0880Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0883Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0870Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0874Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0889Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0897Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0889Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0889Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0896Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0888Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0898Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0889Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0889Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0882Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0884Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0877Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0875Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0869Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0872Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0876Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0871Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0866Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0864Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0861Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0856Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0852Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0852Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0850Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0845Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0840Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0836Epoch 3/15: [==============================] 75/75 batches, loss: 0.0843
[2025-05-03 16:45:12,250][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0843
[2025-05-03 16:45:12,643][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0959, Metrics: {'mse': 0.09592651575803757, 'rmse': 0.3097200603093664, 'r2': -0.4631744623184204}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0304Epoch 4/15: [                              ] 2/75 batches, loss: 0.0335Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0416Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0562Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0516Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0520Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0555Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0570Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0595Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0590Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0610Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0619Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0608Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0587Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0607Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0612Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0627Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0627Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0644Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0642Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0628Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0650Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0648Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0655Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0661Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0659Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0648Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0644Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0646Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0642Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0640Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0648Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0652Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0668Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0663Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0661Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0655Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0662Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0654Epoch 4/15: [================              ] 40/75 batches, loss: 0.0649Epoch 4/15: [================              ] 41/75 batches, loss: 0.0650Epoch 4/15: [================              ] 42/75 batches, loss: 0.0648Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0651Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0647Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0650Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0643Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0642Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0638Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0648Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0645Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0648Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0658Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0659Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0668Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0664Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0673Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0673Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0674Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0676Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0676Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0681Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0675Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0680Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0680Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0681Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0679Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0675Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0675Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0675Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0671Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0667Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0666Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0666Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0666Epoch 4/15: [==============================] 75/75 batches, loss: 0.0663
[2025-05-03 16:45:15,511][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0663
[2025-05-03 16:45:16,010][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0871, Metrics: {'mse': 0.08718114346265793, 'rmse': 0.29526453133191927, 'r2': -0.3297806978225708}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1014Epoch 5/15: [                              ] 2/75 batches, loss: 0.0646Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0702Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0696Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0610Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0676Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0682Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0660Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0630Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0595Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0576Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0567Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0573Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0569Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0601Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0596Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0580Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0590Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0599Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0583Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0618Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0609Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0611Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0596Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0614Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0611Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0603Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0596Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0592Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0585Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0575Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0580Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0580Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0580Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0587Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0583Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0580Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0579Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0574Epoch 5/15: [================              ] 40/75 batches, loss: 0.0582Epoch 5/15: [================              ] 41/75 batches, loss: 0.0580Epoch 5/15: [================              ] 42/75 batches, loss: 0.0577Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0579Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0577Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0576Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0580Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0575Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0579Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0585Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0588Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0582Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0576Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0574Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0584Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0582Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0581Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0579Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0578Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0576Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0573Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0572Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0574Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0574Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0577Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0579Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0582Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0579Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0579Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0578Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0577Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0578Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0580Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0577Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0577Epoch 5/15: [==============================] 75/75 batches, loss: 0.0577
[2025-05-03 16:45:18,756][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0577
[2025-05-03 16:45:19,211][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0778, Metrics: {'mse': 0.07782787829637527, 'rmse': 0.2789764834110131, 'r2': -0.18711459636688232}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0317Epoch 6/15: [                              ] 2/75 batches, loss: 0.0556Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0501Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0455Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0598Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0566Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0553Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0566Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0545Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0549Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0534Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0520Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0519Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0518Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0517Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0506Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0488Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0526Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0521Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0520Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0520Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0518Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0524Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0524Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0536Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0539Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0546Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0532Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0526Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0524Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0522Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0516Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0508Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0504Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0501Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0495Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0490Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0482Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0492Epoch 6/15: [================              ] 40/75 batches, loss: 0.0492Epoch 6/15: [================              ] 41/75 batches, loss: 0.0493Epoch 6/15: [================              ] 42/75 batches, loss: 0.0490Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0491Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0504Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0501Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0498Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0499Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0501Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0508Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0506Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0507Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0508Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0506Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0504Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0503Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0500Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0500Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0506Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0504Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0504Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0511Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0512Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0516Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0519Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0522Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0520Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0516Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0521Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0522Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0520Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0523Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0521Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0519Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0521Epoch 6/15: [==============================] 75/75 batches, loss: 0.0517
[2025-05-03 16:45:21,917][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0517
[2025-05-03 16:45:22,365][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0838, Metrics: {'mse': 0.08384472876787186, 'rmse': 0.2895595426986855, 'r2': -0.2788900136947632}
[2025-05-03 16:45:22,366][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0582Epoch 7/15: [                              ] 2/75 batches, loss: 0.0470Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0501Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0531Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0467Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0455Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0440Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0459Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0463Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0472Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0466Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0461Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0468Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0480Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0472Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0463Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0468Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0458Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0439Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0441Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0440Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0433Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0437Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0437Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0430Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0439Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0433Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0435Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0432Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0421Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0421Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0420Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0417Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0419Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0413Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0407Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0413Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0412Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0410Epoch 7/15: [================              ] 40/75 batches, loss: 0.0409Epoch 7/15: [================              ] 41/75 batches, loss: 0.0414Epoch 7/15: [================              ] 42/75 batches, loss: 0.0415Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0418Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0420Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0419Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0421Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0424Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0421Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0422Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0418Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0416Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0415Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0415Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0423Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0426Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0425Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0425Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0427Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0425Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0426Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0430Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0427Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0429Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0427Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0429Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0426Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0423Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0420Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0419Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0421Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0419Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0420Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0421Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0419Epoch 7/15: [==============================] 75/75 batches, loss: 0.0419
[2025-05-03 16:45:24,754][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0419
[2025-05-03 16:45:25,268][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0856, Metrics: {'mse': 0.08562304079532623, 'rmse': 0.29261415002580826, 'r2': -0.3060147762298584}
[2025-05-03 16:45:25,269][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0357Epoch 8/15: [                              ] 2/75 batches, loss: 0.0348Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0356Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0373Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0390Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0408Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0368Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0370Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0376Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0402Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0405Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0399Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0404Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0402Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0405Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0396Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0388Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0389Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0392Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0401Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0389Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0397Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0393Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0396Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0397Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0397Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0388Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0389Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0392Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0395Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0397Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0395Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0398Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0401Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0399Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0397Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0392Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0390Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0388Epoch 8/15: [================              ] 40/75 batches, loss: 0.0394Epoch 8/15: [================              ] 41/75 batches, loss: 0.0392Epoch 8/15: [================              ] 42/75 batches, loss: 0.0392Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0397Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0395Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0395Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0394Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0397Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0396Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0392Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0391Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0393Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0401Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0399Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0395Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0395Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0393Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0395Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0391Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0393Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0395Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0393Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0394Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0393Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0394Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0393Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0391Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0392Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0389Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0388Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0385Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0387Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0385Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0386Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0387Epoch 8/15: [==============================] 75/75 batches, loss: 0.0396
[2025-05-03 16:45:27,712][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0396
[2025-05-03 16:45:28,228][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0927, Metrics: {'mse': 0.09277486056089401, 'rmse': 0.3045896593137955, 'r2': -0.4151020050048828}
[2025-05-03 16:45:28,229][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0337Epoch 9/15: [                              ] 2/75 batches, loss: 0.0509Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0505Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0447Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0412Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0388Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0373Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0353Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0342Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0333Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0321Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0307Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0297Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0286Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0281Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0290Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0289Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0292Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0293Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0295Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0298Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0302Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0303Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0312Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0316Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0316Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0310Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0312Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0311Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0308Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0307Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0308Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0311Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0321Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0317Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0315Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0322Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0321Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0320Epoch 9/15: [================              ] 40/75 batches, loss: 0.0324Epoch 9/15: [================              ] 41/75 batches, loss: 0.0324Epoch 9/15: [================              ] 42/75 batches, loss: 0.0321Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0322Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0319Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0319Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0319Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0321Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0319Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0320Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0322Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0323Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0323Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0322Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0319Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0318Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0318Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0317Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0321Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0324Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0324Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0325Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0327Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0328Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0331Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0329Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0328Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0329Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0328Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0329Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0328Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0327Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0324Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0322Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0319Epoch 9/15: [==============================] 75/75 batches, loss: 0.0321
[2025-05-03 16:45:30,633][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0321
[2025-05-03 16:45:31,080][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0799, Metrics: {'mse': 0.0799516811966896, 'rmse': 0.2827572831894691, 'r2': -0.21950912475585938}
[2025-05-03 16:45:31,080][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-03 16:45:31,081][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 9
[2025-05-03 16:45:31,081][src.training.lm_trainer][INFO] - Training completed in 27.76 seconds
[2025-05-03 16:45:31,081][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-03 16:45:34,426][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.014995574951171875, 'rmse': 0.12245642062044715, 'r2': 0.2579403519630432}
[2025-05-03 16:45:34,426][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07782787829637527, 'rmse': 0.2789764834110131, 'r2': -0.18711459636688232}
[2025-05-03 16:45:34,426][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03400088846683502, 'rmse': 0.18439329832408505, 'r2': 0.13893473148345947}
[2025-05-03 16:45:36,146][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/fi/fi/model.pt
[2025-05-03 16:45:36,147][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▄▁
wandb:     best_val_mse █▇▄▁
wandb:      best_val_r2 ▁▂▅█
wandb:    best_val_rmse █▇▄▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▄▁▄▅▆▅▅▅
wandb:       train_loss █▄▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▄█▃▂▁▂▂▃▁
wandb:          val_mse ▄█▃▂▁▂▂▃▁
wandb:           val_r2 ▅▁▆▇█▇▇▆█
wandb:         val_rmse ▄█▄▂▁▂▂▃▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07784
wandb:     best_val_mse 0.07783
wandb:      best_val_r2 -0.18711
wandb:    best_val_rmse 0.27898
wandb: early_stop_epoch 9
wandb:            epoch 9
wandb:   final_test_mse 0.034
wandb:    final_test_r2 0.13893
wandb:  final_test_rmse 0.18439
wandb:  final_train_mse 0.015
wandb:   final_train_r2 0.25794
wandb: final_train_rmse 0.12246
wandb:    final_val_mse 0.07783
wandb:     final_val_r2 -0.18711
wandb:   final_val_rmse 0.27898
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03212
wandb:       train_time 27.76034
wandb:         val_loss 0.07989
wandb:          val_mse 0.07995
wandb:           val_r2 -0.21951
wandb:         val_rmse 0.28276
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_164436-rws5xj1l
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_164436-rws5xj1l/logs
Experiment probe_layer3_complexity_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/fi/fi/results.json for layer 3
Running experiment: probe_layer3_question_type_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=3"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer3_question_type_id"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/layer3/id"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-03 16:46:20,721][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/id
experiment_name: probe_layer3_question_type_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-03 16:46:20,721][__main__][INFO] - Normalized task: question_type
[2025-05-03 16:46:20,721][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-03 16:46:20,721][__main__][INFO] - Determined Task Type: classification
[2025-05-03 16:46:20,725][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-05-03 16:46:20,725][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-03 16:46:27,548][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-03 16:46:30,137][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-03 16:46:30,138][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:46:30,701][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:46:30,882][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:46:31,364][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-03 16:46:31,371][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:46:31,372][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-03 16:46:31,373][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:46:31,464][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:46:31,615][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:46:31,651][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-03 16:46:31,652][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:46:31,652][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-03 16:46:31,653][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:46:31,748][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:46:31,957][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:46:31,975][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-03 16:46:31,976][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:46:31,976][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-03 16:46:31,977][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-03 16:46:31,977][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:46:31,977][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:46:31,978][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:46:31,978][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:46:31,978][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-05-03 16:46:31,978][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-05-03 16:46:31,978][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-03 16:46:31,978][src.data.datasets][INFO] - Sample label: 1
[2025-05-03 16:46:31,978][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:46:31,978][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:46:31,978][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:46:31,979][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:46:31,979][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-03 16:46:31,979][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-03 16:46:31,979][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-03 16:46:31,979][src.data.datasets][INFO] - Sample label: 1
[2025-05-03 16:46:31,979][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:46:31,979][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:46:31,979][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:46:31,979][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:46:31,979][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-03 16:46:31,980][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-03 16:46:31,980][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-03 16:46:31,980][src.data.datasets][INFO] - Sample label: 1
[2025-05-03 16:46:31,980][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-03 16:46:31,980][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-03 16:46:31,980][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-03 16:46:31,980][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-03 16:46:31,981][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-03 16:46:44,150][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-03 16:46:44,151][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-03 16:46:44,151][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=3, freeze_model=True
[2025-05-03 16:46:44,151][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-03 16:46:44,157][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-03 16:46:44,157][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-03 16:46:44,157][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-03 16:46:44,157][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-03 16:46:44,157][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-03 16:46:44,158][__main__][INFO] - Total parameters: 394,568,839
[2025-05-03 16:46:44,158][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-03 16:46:44,159][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.7761Epoch 1/15: [=                             ] 2/60 batches, loss: 0.7317Epoch 1/15: [=                             ] 3/60 batches, loss: 0.7126Epoch 1/15: [==                            ] 4/60 batches, loss: 0.7228Epoch 1/15: [==                            ] 5/60 batches, loss: 0.7201Epoch 1/15: [===                           ] 6/60 batches, loss: 0.7156Epoch 1/15: [===                           ] 7/60 batches, loss: 0.7119Epoch 1/15: [====                          ] 8/60 batches, loss: 0.7095Epoch 1/15: [====                          ] 9/60 batches, loss: 0.7076Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.7061Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.7040Epoch 1/15: [======                        ] 12/60 batches, loss: 0.7028Epoch 1/15: [======                        ] 13/60 batches, loss: 0.7015Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.7012Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.7007Epoch 1/15: [========                      ] 16/60 batches, loss: 0.6999Epoch 1/15: [========                      ] 17/60 batches, loss: 0.6994Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.6994Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.6989Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.6986Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.6985Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.6989Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.6986Epoch 1/15: [============                  ] 24/60 batches, loss: 0.6985Epoch 1/15: [============                  ] 25/60 batches, loss: 0.6984Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.6984Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.6983Epoch 1/15: [==============                ] 28/60 batches, loss: 0.6980Epoch 1/15: [==============                ] 29/60 batches, loss: 0.6979Epoch 1/15: [===============               ] 30/60 batches, loss: 0.6979Epoch 1/15: [===============               ] 31/60 batches, loss: 0.6977Epoch 1/15: [================              ] 32/60 batches, loss: 0.6975Epoch 1/15: [================              ] 33/60 batches, loss: 0.6972Epoch 1/15: [=================             ] 34/60 batches, loss: 0.6970Epoch 1/15: [=================             ] 35/60 batches, loss: 0.6970Epoch 1/15: [==================            ] 36/60 batches, loss: 0.6970Epoch 1/15: [==================            ] 37/60 batches, loss: 0.6969Epoch 1/15: [===================           ] 38/60 batches, loss: 0.6968Epoch 1/15: [===================           ] 39/60 batches, loss: 0.6967Epoch 1/15: [====================          ] 40/60 batches, loss: 0.6965Epoch 1/15: [====================          ] 41/60 batches, loss: 0.6963Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.6963Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.6962Epoch 1/15: [======================        ] 44/60 batches, loss: 0.6962Epoch 1/15: [======================        ] 45/60 batches, loss: 0.6960Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.6959Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.6958Epoch 1/15: [========================      ] 48/60 batches, loss: 0.6958Epoch 1/15: [========================      ] 49/60 batches, loss: 0.6957Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.6956Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.6955Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.6954Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.6954Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.6954Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.6953Epoch 1/15: [============================  ] 56/60 batches, loss: 0.6953Epoch 1/15: [============================  ] 57/60 batches, loss: 0.6952Epoch 1/15: [============================= ] 58/60 batches, loss: 0.6951Epoch 1/15: [============================= ] 59/60 batches, loss: 0.6951Epoch 1/15: [==============================] 60/60 batches, loss: 0.6951
[2025-05-03 16:46:53,195][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6951
[2025-05-03 16:46:53,685][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6916, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.6858Epoch 2/15: [=                             ] 2/60 batches, loss: 0.6915Epoch 2/15: [=                             ] 3/60 batches, loss: 0.6910Epoch 2/15: [==                            ] 4/60 batches, loss: 0.6907Epoch 2/15: [==                            ] 5/60 batches, loss: 0.6907Epoch 2/15: [===                           ] 6/60 batches, loss: 0.6903Epoch 2/15: [===                           ] 7/60 batches, loss: 0.6898Epoch 2/15: [====                          ] 8/60 batches, loss: 0.6899Epoch 2/15: [====                          ] 9/60 batches, loss: 0.6903Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.6902Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.6909Epoch 2/15: [======                        ] 12/60 batches, loss: 0.6904Epoch 2/15: [======                        ] 13/60 batches, loss: 0.6899Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.6896Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.6892Epoch 2/15: [========                      ] 16/60 batches, loss: 0.6898Epoch 2/15: [========                      ] 17/60 batches, loss: 0.6893Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.6886Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.6890Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.6902Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.6894Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.6900Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.6889Epoch 2/15: [============                  ] 24/60 batches, loss: 0.6890Epoch 2/15: [============                  ] 25/60 batches, loss: 0.6886Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.6877Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.6882Epoch 2/15: [==============                ] 28/60 batches, loss: 0.6879Epoch 2/15: [==============                ] 29/60 batches, loss: 0.6868Epoch 2/15: [===============               ] 30/60 batches, loss: 0.6867Epoch 2/15: [===============               ] 31/60 batches, loss: 0.6860Epoch 2/15: [================              ] 32/60 batches, loss: 0.6852Epoch 2/15: [================              ] 33/60 batches, loss: 0.6863Epoch 2/15: [=================             ] 34/60 batches, loss: 0.6862Epoch 2/15: [=================             ] 35/60 batches, loss: 0.6856Epoch 2/15: [==================            ] 36/60 batches, loss: 0.6846Epoch 2/15: [==================            ] 37/60 batches, loss: 0.6850Epoch 2/15: [===================           ] 38/60 batches, loss: 0.6844Epoch 2/15: [===================           ] 39/60 batches, loss: 0.6847Epoch 2/15: [====================          ] 40/60 batches, loss: 0.6843Epoch 2/15: [====================          ] 41/60 batches, loss: 0.6846Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.6844Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.6828Epoch 2/15: [======================        ] 44/60 batches, loss: 0.6809Epoch 2/15: [======================        ] 45/60 batches, loss: 0.6801Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.6796Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.6777Epoch 2/15: [========================      ] 48/60 batches, loss: 0.6759Epoch 2/15: [========================      ] 49/60 batches, loss: 0.6739Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.6717Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.6701Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.6683Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.6672Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.6670Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.6646Epoch 2/15: [============================  ] 56/60 batches, loss: 0.6639Epoch 2/15: [============================  ] 57/60 batches, loss: 0.6639Epoch 2/15: [============================= ] 58/60 batches, loss: 0.6626Epoch 2/15: [============================= ] 59/60 batches, loss: 0.6617Epoch 2/15: [==============================] 60/60 batches, loss: 0.6618
[2025-05-03 16:46:56,191][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6618
[2025-05-03 16:46:56,536][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6413, Metrics: {'accuracy': 0.7222222222222222, 'f1': 0.6875, 'precision': 0.7857142857142857, 'recall': 0.6111111111111112}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.6629Epoch 3/15: [=                             ] 2/60 batches, loss: 0.6168Epoch 3/15: [=                             ] 3/60 batches, loss: 0.6086Epoch 3/15: [==                            ] 4/60 batches, loss: 0.6117Epoch 3/15: [==                            ] 5/60 batches, loss: 0.5965Epoch 3/15: [===                           ] 6/60 batches, loss: 0.5953Epoch 3/15: [===                           ] 7/60 batches, loss: 0.5854Epoch 3/15: [====                          ] 8/60 batches, loss: 0.5846Epoch 3/15: [====                          ] 9/60 batches, loss: 0.5910Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.5917Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.5902Epoch 3/15: [======                        ] 12/60 batches, loss: 0.5887Epoch 3/15: [======                        ] 13/60 batches, loss: 0.5883Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.5885Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.5876Epoch 3/15: [========                      ] 16/60 batches, loss: 0.5896Epoch 3/15: [========                      ] 17/60 batches, loss: 0.5895Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.5930Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.5935Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.5951Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.5951Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.5942Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.5894Epoch 3/15: [============                  ] 24/60 batches, loss: 0.5880Epoch 3/15: [============                  ] 25/60 batches, loss: 0.5872Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.5911Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.5914Epoch 3/15: [==============                ] 28/60 batches, loss: 0.5893Epoch 3/15: [==============                ] 29/60 batches, loss: 0.5876Epoch 3/15: [===============               ] 30/60 batches, loss: 0.5871Epoch 3/15: [===============               ] 31/60 batches, loss: 0.5848Epoch 3/15: [================              ] 32/60 batches, loss: 0.5841Epoch 3/15: [================              ] 33/60 batches, loss: 0.5807Epoch 3/15: [=================             ] 34/60 batches, loss: 0.5820Epoch 3/15: [=================             ] 35/60 batches, loss: 0.5842Epoch 3/15: [==================            ] 36/60 batches, loss: 0.5841Epoch 3/15: [==================            ] 37/60 batches, loss: 0.5840Epoch 3/15: [===================           ] 38/60 batches, loss: 0.5837Epoch 3/15: [===================           ] 39/60 batches, loss: 0.5843Epoch 3/15: [====================          ] 40/60 batches, loss: 0.5845Epoch 3/15: [====================          ] 41/60 batches, loss: 0.5847Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.5866Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.5866Epoch 3/15: [======================        ] 44/60 batches, loss: 0.5868Epoch 3/15: [======================        ] 45/60 batches, loss: 0.5858Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.5864Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.5876Epoch 3/15: [========================      ] 48/60 batches, loss: 0.5874Epoch 3/15: [========================      ] 49/60 batches, loss: 0.5857Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.5864Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.5868Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.5862Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.5872Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.5872Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.5847Epoch 3/15: [============================  ] 56/60 batches, loss: 0.5829Epoch 3/15: [============================  ] 57/60 batches, loss: 0.5837Epoch 3/15: [============================= ] 58/60 batches, loss: 0.5826Epoch 3/15: [============================= ] 59/60 batches, loss: 0.5828Epoch 3/15: [==============================] 60/60 batches, loss: 0.5828
[2025-05-03 16:46:59,101][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.5828
[2025-05-03 16:46:59,480][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6108, Metrics: {'accuracy': 0.7916666666666666, 'f1': 0.7692307692307693, 'precision': 0.8620689655172413, 'recall': 0.6944444444444444}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.5172Epoch 4/15: [=                             ] 2/60 batches, loss: 0.6121Epoch 4/15: [=                             ] 3/60 batches, loss: 0.5862Epoch 4/15: [==                            ] 4/60 batches, loss: 0.5769Epoch 4/15: [==                            ] 5/60 batches, loss: 0.5887Epoch 4/15: [===                           ] 6/60 batches, loss: 0.5769Epoch 4/15: [===                           ] 7/60 batches, loss: 0.5802Epoch 4/15: [====                          ] 8/60 batches, loss: 0.5730Epoch 4/15: [====                          ] 9/60 batches, loss: 0.5590Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.5529Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.5472Epoch 4/15: [======                        ] 12/60 batches, loss: 0.5504Epoch 4/15: [======                        ] 13/60 batches, loss: 0.5515Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.5516Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.5495Epoch 4/15: [========                      ] 16/60 batches, loss: 0.5450Epoch 4/15: [========                      ] 17/60 batches, loss: 0.5463Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.5430Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.5408Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.5398Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.5421Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.5421Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.5392Epoch 4/15: [============                  ] 24/60 batches, loss: 0.5404Epoch 4/15: [============                  ] 25/60 batches, loss: 0.5434Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.5460Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.5467Epoch 4/15: [==============                ] 28/60 batches, loss: 0.5480Epoch 4/15: [==============                ] 29/60 batches, loss: 0.5500Epoch 4/15: [===============               ] 30/60 batches, loss: 0.5485Epoch 4/15: [===============               ] 31/60 batches, loss: 0.5487Epoch 4/15: [================              ] 32/60 batches, loss: 0.5459Epoch 4/15: [================              ] 33/60 batches, loss: 0.5440Epoch 4/15: [=================             ] 34/60 batches, loss: 0.5456Epoch 4/15: [=================             ] 35/60 batches, loss: 0.5449Epoch 4/15: [==================            ] 36/60 batches, loss: 0.5443Epoch 4/15: [==================            ] 37/60 batches, loss: 0.5440Epoch 4/15: [===================           ] 38/60 batches, loss: 0.5453Epoch 4/15: [===================           ] 39/60 batches, loss: 0.5439Epoch 4/15: [====================          ] 40/60 batches, loss: 0.5424Epoch 4/15: [====================          ] 41/60 batches, loss: 0.5418Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.5433Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.5462Epoch 4/15: [======================        ] 44/60 batches, loss: 0.5473Epoch 4/15: [======================        ] 45/60 batches, loss: 0.5483Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.5489Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.5507Epoch 4/15: [========================      ] 48/60 batches, loss: 0.5501Epoch 4/15: [========================      ] 49/60 batches, loss: 0.5513Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.5516Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.5526Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.5523Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.5545Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.5550Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.5540Epoch 4/15: [============================  ] 56/60 batches, loss: 0.5543Epoch 4/15: [============================  ] 57/60 batches, loss: 0.5539Epoch 4/15: [============================= ] 58/60 batches, loss: 0.5556Epoch 4/15: [============================= ] 59/60 batches, loss: 0.5545Epoch 4/15: [==============================] 60/60 batches, loss: 0.5572
[2025-05-03 16:47:01,692][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5572
[2025-05-03 16:47:02,137][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6058, Metrics: {'accuracy': 0.8055555555555556, 'f1': 0.8055555555555556, 'precision': 0.8055555555555556, 'recall': 0.8055555555555556}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.6609Epoch 5/15: [=                             ] 2/60 batches, loss: 0.6428Epoch 5/15: [=                             ] 3/60 batches, loss: 0.6088Epoch 5/15: [==                            ] 4/60 batches, loss: 0.5978Epoch 5/15: [==                            ] 5/60 batches, loss: 0.5900Epoch 5/15: [===                           ] 6/60 batches, loss: 0.5763Epoch 5/15: [===                           ] 7/60 batches, loss: 0.5749Epoch 5/15: [====                          ] 8/60 batches, loss: 0.5925Epoch 5/15: [====                          ] 9/60 batches, loss: 0.5925Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.5866Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.5782Epoch 5/15: [======                        ] 12/60 batches, loss: 0.5753Epoch 5/15: [======                        ] 13/60 batches, loss: 0.5785Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.5817Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.5748Epoch 5/15: [========                      ] 16/60 batches, loss: 0.5753Epoch 5/15: [========                      ] 17/60 batches, loss: 0.5748Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.5675Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.5651Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.5661Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.5624Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.5606Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.5585Epoch 5/15: [============                  ] 24/60 batches, loss: 0.5570Epoch 5/15: [============                  ] 25/60 batches, loss: 0.5544Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.5565Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.5574Epoch 5/15: [==============                ] 28/60 batches, loss: 0.5577Epoch 5/15: [==============                ] 29/60 batches, loss: 0.5569Epoch 5/15: [===============               ] 30/60 batches, loss: 0.5572Epoch 5/15: [===============               ] 31/60 batches, loss: 0.5565Epoch 5/15: [================              ] 32/60 batches, loss: 0.5551Epoch 5/15: [================              ] 33/60 batches, loss: 0.5537Epoch 5/15: [=================             ] 34/60 batches, loss: 0.5535Epoch 5/15: [=================             ] 35/60 batches, loss: 0.5532Epoch 5/15: [==================            ] 36/60 batches, loss: 0.5505Epoch 5/15: [==================            ] 37/60 batches, loss: 0.5503Epoch 5/15: [===================           ] 38/60 batches, loss: 0.5501Epoch 5/15: [===================           ] 39/60 batches, loss: 0.5496Epoch 5/15: [====================          ] 40/60 batches, loss: 0.5481Epoch 5/15: [====================          ] 41/60 batches, loss: 0.5468Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.5467Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.5461Epoch 5/15: [======================        ] 44/60 batches, loss: 0.5464Epoch 5/15: [======================        ] 45/60 batches, loss: 0.5460Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.5436Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.5446Epoch 5/15: [========================      ] 48/60 batches, loss: 0.5437Epoch 5/15: [========================      ] 49/60 batches, loss: 0.5440Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.5459Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.5455Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.5457Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.5462Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.5453Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.5463Epoch 5/15: [============================  ] 56/60 batches, loss: 0.5470Epoch 5/15: [============================  ] 57/60 batches, loss: 0.5460Epoch 5/15: [============================= ] 58/60 batches, loss: 0.5451Epoch 5/15: [============================= ] 59/60 batches, loss: 0.5450Epoch 5/15: [==============================] 60/60 batches, loss: 0.5470
[2025-05-03 16:47:04,527][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5470
[2025-05-03 16:47:04,898][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6030, Metrics: {'accuracy': 0.8055555555555556, 'f1': 0.8055555555555556, 'precision': 0.8055555555555556, 'recall': 0.8055555555555556}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.5814Epoch 6/15: [=                             ] 2/60 batches, loss: 0.5693Epoch 6/15: [=                             ] 3/60 batches, loss: 0.5575Epoch 6/15: [==                            ] 4/60 batches, loss: 0.5444Epoch 6/15: [==                            ] 5/60 batches, loss: 0.5537Epoch 6/15: [===                           ] 6/60 batches, loss: 0.5409Epoch 6/15: [===                           ] 7/60 batches, loss: 0.5407Epoch 6/15: [====                          ] 8/60 batches, loss: 0.5393Epoch 6/15: [====                          ] 9/60 batches, loss: 0.5354Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.5400Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.5320Epoch 6/15: [======                        ] 12/60 batches, loss: 0.5349Epoch 6/15: [======                        ] 13/60 batches, loss: 0.5332Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.5374Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.5414Epoch 6/15: [========                      ] 16/60 batches, loss: 0.5367Epoch 6/15: [========                      ] 17/60 batches, loss: 0.5351Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.5314Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.5301Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.5276Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.5290Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.5328Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.5297Epoch 6/15: [============                  ] 24/60 batches, loss: 0.5312Epoch 6/15: [============                  ] 25/60 batches, loss: 0.5288Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.5273Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.5273Epoch 6/15: [==============                ] 28/60 batches, loss: 0.5266Epoch 6/15: [==============                ] 29/60 batches, loss: 0.5275Epoch 6/15: [===============               ] 30/60 batches, loss: 0.5277Epoch 6/15: [===============               ] 31/60 batches, loss: 0.5260Epoch 6/15: [================              ] 32/60 batches, loss: 0.5272Epoch 6/15: [================              ] 33/60 batches, loss: 0.5279Epoch 6/15: [=================             ] 34/60 batches, loss: 0.5274Epoch 6/15: [=================             ] 35/60 batches, loss: 0.5279Epoch 6/15: [==================            ] 36/60 batches, loss: 0.5273Epoch 6/15: [==================            ] 37/60 batches, loss: 0.5254Epoch 6/15: [===================           ] 38/60 batches, loss: 0.5263Epoch 6/15: [===================           ] 39/60 batches, loss: 0.5273Epoch 6/15: [====================          ] 40/60 batches, loss: 0.5284Epoch 6/15: [====================          ] 41/60 batches, loss: 0.5268Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.5265Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.5304Epoch 6/15: [======================        ] 44/60 batches, loss: 0.5311Epoch 6/15: [======================        ] 45/60 batches, loss: 0.5328Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.5334Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.5347Epoch 6/15: [========================      ] 48/60 batches, loss: 0.5368Epoch 6/15: [========================      ] 49/60 batches, loss: 0.5369Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.5375Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.5381Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.5380Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.5394Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.5398Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.5408Epoch 6/15: [============================  ] 56/60 batches, loss: 0.5416Epoch 6/15: [============================  ] 57/60 batches, loss: 0.5416Epoch 6/15: [============================= ] 58/60 batches, loss: 0.5418Epoch 6/15: [============================= ] 59/60 batches, loss: 0.5413Epoch 6/15: [==============================] 60/60 batches, loss: 0.5417
[2025-05-03 16:47:07,283][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5417
[2025-05-03 16:47:07,680][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5959, Metrics: {'accuracy': 0.8055555555555556, 'f1': 0.78125, 'precision': 0.8928571428571429, 'recall': 0.6944444444444444}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.5505Epoch 7/15: [=                             ] 2/60 batches, loss: 0.5515Epoch 7/15: [=                             ] 3/60 batches, loss: 0.5304Epoch 7/15: [==                            ] 4/60 batches, loss: 0.5180Epoch 7/15: [==                            ] 5/60 batches, loss: 0.5160Epoch 7/15: [===                           ] 6/60 batches, loss: 0.5088Epoch 7/15: [===                           ] 7/60 batches, loss: 0.5226Epoch 7/15: [====                          ] 8/60 batches, loss: 0.5177Epoch 7/15: [====                          ] 9/60 batches, loss: 0.5187Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.5272Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.5318Epoch 7/15: [======                        ] 12/60 batches, loss: 0.5307Epoch 7/15: [======                        ] 13/60 batches, loss: 0.5364Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.5276Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.5279Epoch 7/15: [========                      ] 16/60 batches, loss: 0.5319Epoch 7/15: [========                      ] 17/60 batches, loss: 0.5320Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.5323Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.5322Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.5304Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.5342Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.5360Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.5398Epoch 7/15: [============                  ] 24/60 batches, loss: 0.5386Epoch 7/15: [============                  ] 25/60 batches, loss: 0.5380Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.5385Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.5395Epoch 7/15: [==============                ] 28/60 batches, loss: 0.5381Epoch 7/15: [==============                ] 29/60 batches, loss: 0.5345Epoch 7/15: [===============               ] 30/60 batches, loss: 0.5381Epoch 7/15: [===============               ] 31/60 batches, loss: 0.5380Epoch 7/15: [================              ] 32/60 batches, loss: 0.5377Epoch 7/15: [================              ] 33/60 batches, loss: 0.5375Epoch 7/15: [=================             ] 34/60 batches, loss: 0.5379Epoch 7/15: [=================             ] 35/60 batches, loss: 0.5384Epoch 7/15: [==================            ] 36/60 batches, loss: 0.5408Epoch 7/15: [==================            ] 37/60 batches, loss: 0.5413Epoch 7/15: [===================           ] 38/60 batches, loss: 0.5397Epoch 7/15: [===================           ] 39/60 batches, loss: 0.5397Epoch 7/15: [====================          ] 40/60 batches, loss: 0.5410Epoch 7/15: [====================          ] 41/60 batches, loss: 0.5419Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.5406Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.5398Epoch 7/15: [======================        ] 44/60 batches, loss: 0.5405Epoch 7/15: [======================        ] 45/60 batches, loss: 0.5400Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.5401Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.5399Epoch 7/15: [========================      ] 48/60 batches, loss: 0.5389Epoch 7/15: [========================      ] 49/60 batches, loss: 0.5376Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.5360Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.5371Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.5357Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.5338Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.5345Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.5332Epoch 7/15: [============================  ] 56/60 batches, loss: 0.5344Epoch 7/15: [============================  ] 57/60 batches, loss: 0.5343Epoch 7/15: [============================= ] 58/60 batches, loss: 0.5347Epoch 7/15: [============================= ] 59/60 batches, loss: 0.5369Epoch 7/15: [==============================] 60/60 batches, loss: 0.5374
[2025-05-03 16:47:10,121][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5374
[2025-05-03 16:47:10,642][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5930, Metrics: {'accuracy': 0.8194444444444444, 'f1': 0.8059701492537313, 'precision': 0.8709677419354839, 'recall': 0.75}
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.5672Epoch 8/15: [=                             ] 2/60 batches, loss: 0.5237Epoch 8/15: [=                             ] 3/60 batches, loss: 0.5376Epoch 8/15: [==                            ] 4/60 batches, loss: 0.5493Epoch 8/15: [==                            ] 5/60 batches, loss: 0.5563Epoch 8/15: [===                           ] 6/60 batches, loss: 0.5542Epoch 8/15: [===                           ] 7/60 batches, loss: 0.5556Epoch 8/15: [====                          ] 8/60 batches, loss: 0.5590Epoch 8/15: [====                          ] 9/60 batches, loss: 0.5416Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.5344Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.5349Epoch 8/15: [======                        ] 12/60 batches, loss: 0.5378Epoch 8/15: [======                        ] 13/60 batches, loss: 0.5379Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.5438Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.5525Epoch 8/15: [========                      ] 16/60 batches, loss: 0.5452Epoch 8/15: [========                      ] 17/60 batches, loss: 0.5458Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.5439Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.5415Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.5446Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.5427Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.5392Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.5400Epoch 8/15: [============                  ] 24/60 batches, loss: 0.5396Epoch 8/15: [============                  ] 25/60 batches, loss: 0.5422Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.5407Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.5380Epoch 8/15: [==============                ] 28/60 batches, loss: 0.5394Epoch 8/15: [==============                ] 29/60 batches, loss: 0.5382Epoch 8/15: [===============               ] 30/60 batches, loss: 0.5378Epoch 8/15: [===============               ] 31/60 batches, loss: 0.5354Epoch 8/15: [================              ] 32/60 batches, loss: 0.5364Epoch 8/15: [================              ] 33/60 batches, loss: 0.5370Epoch 8/15: [=================             ] 34/60 batches, loss: 0.5379Epoch 8/15: [=================             ] 35/60 batches, loss: 0.5405Epoch 8/15: [==================            ] 36/60 batches, loss: 0.5409Epoch 8/15: [==================            ] 37/60 batches, loss: 0.5411Epoch 8/15: [===================           ] 38/60 batches, loss: 0.5400Epoch 8/15: [===================           ] 39/60 batches, loss: 0.5385Epoch 8/15: [====================          ] 40/60 batches, loss: 0.5370Epoch 8/15: [====================          ] 41/60 batches, loss: 0.5357Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.5355Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.5353Epoch 8/15: [======================        ] 44/60 batches, loss: 0.5347Epoch 8/15: [======================        ] 45/60 batches, loss: 0.5350Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.5352Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.5330Epoch 8/15: [========================      ] 48/60 batches, loss: 0.5320Epoch 8/15: [========================      ] 49/60 batches, loss: 0.5326Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.5313Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.5336Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.5334Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.5329Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.5333Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.5337Epoch 8/15: [============================  ] 56/60 batches, loss: 0.5336Epoch 8/15: [============================  ] 57/60 batches, loss: 0.5331Epoch 8/15: [============================= ] 58/60 batches, loss: 0.5332Epoch 8/15: [============================= ] 59/60 batches, loss: 0.5337Epoch 8/15: [==============================] 60/60 batches, loss: 0.5338
[2025-05-03 16:47:13,086][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5338
[2025-05-03 16:47:13,538][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5919, Metrics: {'accuracy': 0.8055555555555556, 'f1': 0.7741935483870968, 'precision': 0.9230769230769231, 'recall': 0.6666666666666666}
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.6109Epoch 9/15: [=                             ] 2/60 batches, loss: 0.6103Epoch 9/15: [=                             ] 3/60 batches, loss: 0.5841Epoch 9/15: [==                            ] 4/60 batches, loss: 0.5564Epoch 9/15: [==                            ] 5/60 batches, loss: 0.5728Epoch 9/15: [===                           ] 6/60 batches, loss: 0.5660Epoch 9/15: [===                           ] 7/60 batches, loss: 0.5687Epoch 9/15: [====                          ] 8/60 batches, loss: 0.5577Epoch 9/15: [====                          ] 9/60 batches, loss: 0.5473Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.5411Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.5429Epoch 9/15: [======                        ] 12/60 batches, loss: 0.5434Epoch 9/15: [======                        ] 13/60 batches, loss: 0.5417Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.5402Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.5378Epoch 9/15: [========                      ] 16/60 batches, loss: 0.5390Epoch 9/15: [========                      ] 17/60 batches, loss: 0.5390Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.5347Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.5368Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.5351Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.5363Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.5366Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.5346Epoch 9/15: [============                  ] 24/60 batches, loss: 0.5354Epoch 9/15: [============                  ] 25/60 batches, loss: 0.5356Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.5339Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.5361Epoch 9/15: [==============                ] 28/60 batches, loss: 0.5348Epoch 9/15: [==============                ] 29/60 batches, loss: 0.5322Epoch 9/15: [===============               ] 30/60 batches, loss: 0.5313Epoch 9/15: [===============               ] 31/60 batches, loss: 0.5327Epoch 9/15: [================              ] 32/60 batches, loss: 0.5337Epoch 9/15: [================              ] 33/60 batches, loss: 0.5314Epoch 9/15: [=================             ] 34/60 batches, loss: 0.5288Epoch 9/15: [=================             ] 35/60 batches, loss: 0.5326Epoch 9/15: [==================            ] 36/60 batches, loss: 0.5325Epoch 9/15: [==================            ] 37/60 batches, loss: 0.5338Epoch 9/15: [===================           ] 38/60 batches, loss: 0.5318Epoch 9/15: [===================           ] 39/60 batches, loss: 0.5311Epoch 9/15: [====================          ] 40/60 batches, loss: 0.5322Epoch 9/15: [====================          ] 41/60 batches, loss: 0.5310Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.5312Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.5322Epoch 9/15: [======================        ] 44/60 batches, loss: 0.5316Epoch 9/15: [======================        ] 45/60 batches, loss: 0.5310Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.5308Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.5298Epoch 9/15: [========================      ] 48/60 batches, loss: 0.5305Epoch 9/15: [========================      ] 49/60 batches, loss: 0.5294Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.5302Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.5298Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.5313Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.5314Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.5314Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.5327Epoch 9/15: [============================  ] 56/60 batches, loss: 0.5328Epoch 9/15: [============================  ] 57/60 batches, loss: 0.5319Epoch 9/15: [============================= ] 58/60 batches, loss: 0.5306Epoch 9/15: [============================= ] 59/60 batches, loss: 0.5309Epoch 9/15: [==============================] 60/60 batches, loss: 0.5305
[2025-05-03 16:47:16,068][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5305
[2025-05-03 16:47:16,508][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.5913, Metrics: {'accuracy': 0.8194444444444444, 'f1': 0.8115942028985508, 'precision': 0.8484848484848485, 'recall': 0.7777777777777778}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.5277Epoch 10/15: [=                             ] 2/60 batches, loss: 0.5182Epoch 10/15: [=                             ] 3/60 batches, loss: 0.5069Epoch 10/15: [==                            ] 4/60 batches, loss: 0.5105Epoch 10/15: [==                            ] 5/60 batches, loss: 0.5228Epoch 10/15: [===                           ] 6/60 batches, loss: 0.5196Epoch 10/15: [===                           ] 7/60 batches, loss: 0.5211Epoch 10/15: [====                          ] 8/60 batches, loss: 0.5189Epoch 10/15: [====                          ] 9/60 batches, loss: 0.5203Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.5299Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.5379Epoch 10/15: [======                        ] 12/60 batches, loss: 0.5342Epoch 10/15: [======                        ] 13/60 batches, loss: 0.5283Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.5303Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.5317Epoch 10/15: [========                      ] 16/60 batches, loss: 0.5302Epoch 10/15: [========                      ] 17/60 batches, loss: 0.5300Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.5342Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.5339Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.5336Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.5319Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.5299Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.5236Epoch 10/15: [============                  ] 24/60 batches, loss: 0.5276Epoch 10/15: [============                  ] 25/60 batches, loss: 0.5282Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.5304Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.5296Epoch 10/15: [==============                ] 28/60 batches, loss: 0.5287Epoch 10/15: [==============                ] 29/60 batches, loss: 0.5281Epoch 10/15: [===============               ] 30/60 batches, loss: 0.5266Epoch 10/15: [===============               ] 31/60 batches, loss: 0.5268Epoch 10/15: [================              ] 32/60 batches, loss: 0.5308Epoch 10/15: [================              ] 33/60 batches, loss: 0.5271Epoch 10/15: [=================             ] 34/60 batches, loss: 0.5246Epoch 10/15: [=================             ] 35/60 batches, loss: 0.5244Epoch 10/15: [==================            ] 36/60 batches, loss: 0.5251Epoch 10/15: [==================            ] 37/60 batches, loss: 0.5235Epoch 10/15: [===================           ] 38/60 batches, loss: 0.5251Epoch 10/15: [===================           ] 39/60 batches, loss: 0.5261Epoch 10/15: [====================          ] 40/60 batches, loss: 0.5282Epoch 10/15: [====================          ] 41/60 batches, loss: 0.5253Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.5269Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.5274Epoch 10/15: [======================        ] 44/60 batches, loss: 0.5281Epoch 10/15: [======================        ] 45/60 batches, loss: 0.5271Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.5260Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.5262Epoch 10/15: [========================      ] 48/60 batches, loss: 0.5274Epoch 10/15: [========================      ] 49/60 batches, loss: 0.5267Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.5265Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.5262Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.5287Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.5290Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.5289Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.5285Epoch 10/15: [============================  ] 56/60 batches, loss: 0.5292Epoch 10/15: [============================  ] 57/60 batches, loss: 0.5299Epoch 10/15: [============================= ] 58/60 batches, loss: 0.5317Epoch 10/15: [============================= ] 59/60 batches, loss: 0.5308Epoch 10/15: [==============================] 60/60 batches, loss: 0.5317
[2025-05-03 16:47:19,078][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5317
[2025-05-03 16:47:19,510][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.5843, Metrics: {'accuracy': 0.8194444444444444, 'f1': 0.8059701492537313, 'precision': 0.8709677419354839, 'recall': 0.75}
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.4336Epoch 11/15: [=                             ] 2/60 batches, loss: 0.4762Epoch 11/15: [=                             ] 3/60 batches, loss: 0.4863Epoch 11/15: [==                            ] 4/60 batches, loss: 0.5063Epoch 11/15: [==                            ] 5/60 batches, loss: 0.5031Epoch 11/15: [===                           ] 6/60 batches, loss: 0.5082Epoch 11/15: [===                           ] 7/60 batches, loss: 0.5144Epoch 11/15: [====                          ] 8/60 batches, loss: 0.5250Epoch 11/15: [====                          ] 9/60 batches, loss: 0.5268Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.5201Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.5170Epoch 11/15: [======                        ] 12/60 batches, loss: 0.5147Epoch 11/15: [======                        ] 13/60 batches, loss: 0.5122Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.5073Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.5009Epoch 11/15: [========                      ] 16/60 batches, loss: 0.4983Epoch 11/15: [========                      ] 17/60 batches, loss: 0.4974Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.5032Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.5033Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.5038Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.5067Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.5072Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.5074Epoch 11/15: [============                  ] 24/60 batches, loss: 0.5069Epoch 11/15: [============                  ] 25/60 batches, loss: 0.5049Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.5059Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.5075Epoch 11/15: [==============                ] 28/60 batches, loss: 0.5072Epoch 11/15: [==============                ] 29/60 batches, loss: 0.5077Epoch 11/15: [===============               ] 30/60 batches, loss: 0.5084Epoch 11/15: [===============               ] 31/60 batches, loss: 0.5108Epoch 11/15: [================              ] 32/60 batches, loss: 0.5129Epoch 11/15: [================              ] 33/60 batches, loss: 0.5113Epoch 11/15: [=================             ] 34/60 batches, loss: 0.5139Epoch 11/15: [=================             ] 35/60 batches, loss: 0.5138Epoch 11/15: [==================            ] 36/60 batches, loss: 0.5150Epoch 11/15: [==================            ] 37/60 batches, loss: 0.5149Epoch 11/15: [===================           ] 38/60 batches, loss: 0.5147Epoch 11/15: [===================           ] 39/60 batches, loss: 0.5156Epoch 11/15: [====================          ] 40/60 batches, loss: 0.5166Epoch 11/15: [====================          ] 41/60 batches, loss: 0.5176Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.5193Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.5199Epoch 11/15: [======================        ] 44/60 batches, loss: 0.5201Epoch 11/15: [======================        ] 45/60 batches, loss: 0.5201Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.5209Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.5196Epoch 11/15: [========================      ] 48/60 batches, loss: 0.5189Epoch 11/15: [========================      ] 49/60 batches, loss: 0.5195Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.5189Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.5191Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.5209Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.5222Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.5247Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.5252Epoch 11/15: [============================  ] 56/60 batches, loss: 0.5274Epoch 11/15: [============================  ] 57/60 batches, loss: 0.5258Epoch 11/15: [============================= ] 58/60 batches, loss: 0.5267Epoch 11/15: [============================= ] 59/60 batches, loss: 0.5278Epoch 11/15: [==============================] 60/60 batches, loss: 0.5293
[2025-05-03 16:47:22,021][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5293
[2025-05-03 16:47:22,502][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.5936, Metrics: {'accuracy': 0.7777777777777778, 'f1': 0.7241379310344828, 'precision': 0.9545454545454546, 'recall': 0.5833333333333334}
[2025-05-03 16:47:22,503][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.5612Epoch 12/15: [=                             ] 2/60 batches, loss: 0.5453Epoch 12/15: [=                             ] 3/60 batches, loss: 0.5246Epoch 12/15: [==                            ] 4/60 batches, loss: 0.5084Epoch 12/15: [==                            ] 5/60 batches, loss: 0.5157Epoch 12/15: [===                           ] 6/60 batches, loss: 0.5113Epoch 12/15: [===                           ] 7/60 batches, loss: 0.5272Epoch 12/15: [====                          ] 8/60 batches, loss: 0.5287Epoch 12/15: [====                          ] 9/60 batches, loss: 0.5313Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.5341Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.5344Epoch 12/15: [======                        ] 12/60 batches, loss: 0.5280Epoch 12/15: [======                        ] 13/60 batches, loss: 0.5319Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.5339Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.5289Epoch 12/15: [========                      ] 16/60 batches, loss: 0.5341Epoch 12/15: [========                      ] 17/60 batches, loss: 0.5361Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.5384Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.5418Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.5424Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.5428Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.5444Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.5432Epoch 12/15: [============                  ] 24/60 batches, loss: 0.5428Epoch 12/15: [============                  ] 25/60 batches, loss: 0.5404Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.5393Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.5395Epoch 12/15: [==============                ] 28/60 batches, loss: 0.5383Epoch 12/15: [==============                ] 29/60 batches, loss: 0.5360Epoch 12/15: [===============               ] 30/60 batches, loss: 0.5347Epoch 12/15: [===============               ] 31/60 batches, loss: 0.5342Epoch 12/15: [================              ] 32/60 batches, loss: 0.5303Epoch 12/15: [================              ] 33/60 batches, loss: 0.5317Epoch 12/15: [=================             ] 34/60 batches, loss: 0.5309Epoch 12/15: [=================             ] 35/60 batches, loss: 0.5324Epoch 12/15: [==================            ] 36/60 batches, loss: 0.5296Epoch 12/15: [==================            ] 37/60 batches, loss: 0.5297Epoch 12/15: [===================           ] 38/60 batches, loss: 0.5306Epoch 12/15: [===================           ] 39/60 batches, loss: 0.5309Epoch 12/15: [====================          ] 40/60 batches, loss: 0.5306Epoch 12/15: [====================          ] 41/60 batches, loss: 0.5310Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.5287Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.5285Epoch 12/15: [======================        ] 44/60 batches, loss: 0.5280Epoch 12/15: [======================        ] 45/60 batches, loss: 0.5259Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.5279Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.5277Epoch 12/15: [========================      ] 48/60 batches, loss: 0.5283Epoch 12/15: [========================      ] 49/60 batches, loss: 0.5281Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.5281Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.5292Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.5292Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.5287Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.5292Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.5289Epoch 12/15: [============================  ] 56/60 batches, loss: 0.5293Epoch 12/15: [============================  ] 57/60 batches, loss: 0.5284Epoch 12/15: [============================= ] 58/60 batches, loss: 0.5280Epoch 12/15: [============================= ] 59/60 batches, loss: 0.5280Epoch 12/15: [==============================] 60/60 batches, loss: 0.5270
[2025-05-03 16:47:24,504][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.5270
[2025-05-03 16:47:24,894][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.5840, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8235294117647058, 'precision': 0.875, 'recall': 0.7777777777777778}
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.5280Epoch 13/15: [=                             ] 2/60 batches, loss: 0.4944Epoch 13/15: [=                             ] 3/60 batches, loss: 0.4903Epoch 13/15: [==                            ] 4/60 batches, loss: 0.4998Epoch 13/15: [==                            ] 5/60 batches, loss: 0.4864Epoch 13/15: [===                           ] 6/60 batches, loss: 0.4895Epoch 13/15: [===                           ] 7/60 batches, loss: 0.5137Epoch 13/15: [====                          ] 8/60 batches, loss: 0.5143Epoch 13/15: [====                          ] 9/60 batches, loss: 0.5082Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.5041Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.5075Epoch 13/15: [======                        ] 12/60 batches, loss: 0.5011Epoch 13/15: [======                        ] 13/60 batches, loss: 0.5090Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.5162Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.5112Epoch 13/15: [========                      ] 16/60 batches, loss: 0.5166Epoch 13/15: [========                      ] 17/60 batches, loss: 0.5158Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.5153Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.5190Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.5206Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.5222Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.5243Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.5245Epoch 13/15: [============                  ] 24/60 batches, loss: 0.5236Epoch 13/15: [============                  ] 25/60 batches, loss: 0.5266Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.5242Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.5273Epoch 13/15: [==============                ] 28/60 batches, loss: 0.5265Epoch 13/15: [==============                ] 29/60 batches, loss: 0.5265Epoch 13/15: [===============               ] 30/60 batches, loss: 0.5274Epoch 13/15: [===============               ] 31/60 batches, loss: 0.5290Epoch 13/15: [================              ] 32/60 batches, loss: 0.5282Epoch 13/15: [================              ] 33/60 batches, loss: 0.5304Epoch 13/15: [=================             ] 34/60 batches, loss: 0.5314Epoch 13/15: [=================             ] 35/60 batches, loss: 0.5279Epoch 13/15: [==================            ] 36/60 batches, loss: 0.5279Epoch 13/15: [==================            ] 37/60 batches, loss: 0.5274Epoch 13/15: [===================           ] 38/60 batches, loss: 0.5274Epoch 13/15: [===================           ] 39/60 batches, loss: 0.5275Epoch 13/15: [====================          ] 40/60 batches, loss: 0.5276Epoch 13/15: [====================          ] 41/60 batches, loss: 0.5293Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.5299Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.5282Epoch 13/15: [======================        ] 44/60 batches, loss: 0.5282Epoch 13/15: [======================        ] 45/60 batches, loss: 0.5293Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.5307Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.5301Epoch 13/15: [========================      ] 48/60 batches, loss: 0.5291Epoch 13/15: [========================      ] 49/60 batches, loss: 0.5282Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.5259Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.5269Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.5277Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.5284Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.5285Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.5275Epoch 13/15: [============================  ] 56/60 batches, loss: 0.5272Epoch 13/15: [============================  ] 57/60 batches, loss: 0.5265Epoch 13/15: [============================= ] 58/60 batches, loss: 0.5266Epoch 13/15: [============================= ] 59/60 batches, loss: 0.5259Epoch 13/15: [==============================] 60/60 batches, loss: 0.5291
[2025-05-03 16:47:27,260][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.5291
[2025-05-03 16:47:27,741][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.5824, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8235294117647058, 'precision': 0.875, 'recall': 0.7777777777777778}
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.6322Epoch 14/15: [=                             ] 2/60 batches, loss: 0.5708Epoch 14/15: [=                             ] 3/60 batches, loss: 0.5752Epoch 14/15: [==                            ] 4/60 batches, loss: 0.5595Epoch 14/15: [==                            ] 5/60 batches, loss: 0.5419Epoch 14/15: [===                           ] 6/60 batches, loss: 0.5317Epoch 14/15: [===                           ] 7/60 batches, loss: 0.5413Epoch 14/15: [====                          ] 8/60 batches, loss: 0.5455Epoch 14/15: [====                          ] 9/60 batches, loss: 0.5469Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.5408Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.5327Epoch 14/15: [======                        ] 12/60 batches, loss: 0.5343Epoch 14/15: [======                        ] 13/60 batches, loss: 0.5355Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.5428Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.5389Epoch 14/15: [========                      ] 16/60 batches, loss: 0.5354Epoch 14/15: [========                      ] 17/60 batches, loss: 0.5337Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.5282Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.5258Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.5274Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.5273Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.5241Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.5212Epoch 14/15: [============                  ] 24/60 batches, loss: 0.5201Epoch 14/15: [============                  ] 25/60 batches, loss: 0.5235Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.5228Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.5199Epoch 14/15: [==============                ] 28/60 batches, loss: 0.5193Epoch 14/15: [==============                ] 29/60 batches, loss: 0.5219Epoch 14/15: [===============               ] 30/60 batches, loss: 0.5222Epoch 14/15: [===============               ] 31/60 batches, loss: 0.5219Epoch 14/15: [================              ] 32/60 batches, loss: 0.5221Epoch 14/15: [================              ] 33/60 batches, loss: 0.5209Epoch 14/15: [=================             ] 34/60 batches, loss: 0.5232Epoch 14/15: [=================             ] 35/60 batches, loss: 0.5228Epoch 14/15: [==================            ] 36/60 batches, loss: 0.5221Epoch 14/15: [==================            ] 37/60 batches, loss: 0.5210Epoch 14/15: [===================           ] 38/60 batches, loss: 0.5193Epoch 14/15: [===================           ] 39/60 batches, loss: 0.5189Epoch 14/15: [====================          ] 40/60 batches, loss: 0.5215Epoch 14/15: [====================          ] 41/60 batches, loss: 0.5224Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.5240Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.5249Epoch 14/15: [======================        ] 44/60 batches, loss: 0.5252Epoch 14/15: [======================        ] 45/60 batches, loss: 0.5236Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.5237Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.5252Epoch 14/15: [========================      ] 48/60 batches, loss: 0.5267Epoch 14/15: [========================      ] 49/60 batches, loss: 0.5272Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.5272Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.5294Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.5293Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.5284Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.5297Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.5285Epoch 14/15: [============================  ] 56/60 batches, loss: 0.5263Epoch 14/15: [============================  ] 57/60 batches, loss: 0.5266Epoch 14/15: [============================= ] 58/60 batches, loss: 0.5266Epoch 14/15: [============================= ] 59/60 batches, loss: 0.5260Epoch 14/15: [==============================] 60/60 batches, loss: 0.5257
[2025-05-03 16:47:30,020][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.5257
[2025-05-03 16:47:30,589][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.5840, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8333333333333334, 'precision': 0.8333333333333334, 'recall': 0.8333333333333334}
[2025-05-03 16:47:30,590][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.5471Epoch 15/15: [=                             ] 2/60 batches, loss: 0.5373Epoch 15/15: [=                             ] 3/60 batches, loss: 0.4948Epoch 15/15: [==                            ] 4/60 batches, loss: 0.5037Epoch 15/15: [==                            ] 5/60 batches, loss: 0.5239Epoch 15/15: [===                           ] 6/60 batches, loss: 0.5086Epoch 15/15: [===                           ] 7/60 batches, loss: 0.5113Epoch 15/15: [====                          ] 8/60 batches, loss: 0.5192Epoch 15/15: [====                          ] 9/60 batches, loss: 0.5282Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.5303Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.5284Epoch 15/15: [======                        ] 12/60 batches, loss: 0.5343Epoch 15/15: [======                        ] 13/60 batches, loss: 0.5379Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.5372Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.5287Epoch 15/15: [========                      ] 16/60 batches, loss: 0.5313Epoch 15/15: [========                      ] 17/60 batches, loss: 0.5286Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.5272Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.5268Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.5198Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.5191Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.5244Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.5259Epoch 15/15: [============                  ] 24/60 batches, loss: 0.5209Epoch 15/15: [============                  ] 25/60 batches, loss: 0.5234Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.5227Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.5211Epoch 15/15: [==============                ] 28/60 batches, loss: 0.5233Epoch 15/15: [==============                ] 29/60 batches, loss: 0.5273Epoch 15/15: [===============               ] 30/60 batches, loss: 0.5305Epoch 15/15: [===============               ] 31/60 batches, loss: 0.5315Epoch 15/15: [================              ] 32/60 batches, loss: 0.5299Epoch 15/15: [================              ] 33/60 batches, loss: 0.5308Epoch 15/15: [=================             ] 34/60 batches, loss: 0.5280Epoch 15/15: [=================             ] 35/60 batches, loss: 0.5293Epoch 15/15: [==================            ] 36/60 batches, loss: 0.5274Epoch 15/15: [==================            ] 37/60 batches, loss: 0.5280Epoch 15/15: [===================           ] 38/60 batches, loss: 0.5274Epoch 15/15: [===================           ] 39/60 batches, loss: 0.5246Epoch 15/15: [====================          ] 40/60 batches, loss: 0.5253Epoch 15/15: [====================          ] 41/60 batches, loss: 0.5254Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.5237Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.5240Epoch 15/15: [======================        ] 44/60 batches, loss: 0.5233Epoch 15/15: [======================        ] 45/60 batches, loss: 0.5252Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.5247Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.5259Epoch 15/15: [========================      ] 48/60 batches, loss: 0.5263Epoch 15/15: [========================      ] 49/60 batches, loss: 0.5259Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.5246Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.5233Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.5230Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.5229Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.5248Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.5239Epoch 15/15: [============================  ] 56/60 batches, loss: 0.5244Epoch 15/15: [============================  ] 57/60 batches, loss: 0.5254Epoch 15/15: [============================= ] 58/60 batches, loss: 0.5271Epoch 15/15: [============================= ] 59/60 batches, loss: 0.5273Epoch 15/15: [==============================] 60/60 batches, loss: 0.5277
[2025-05-03 16:47:32,632][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.5277
[2025-05-03 16:47:32,983][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.5942, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8333333333333334, 'precision': 0.8333333333333334, 'recall': 0.8333333333333334}
[2025-05-03 16:47:32,984][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
[2025-05-03 16:47:32,984][src.training.lm_trainer][INFO] - Training completed in 42.58 seconds
[2025-05-03 16:47:32,984][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-03 16:47:35,955][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9832285115303984, 'f1': 0.982532751091703, 'precision': 0.9803921568627451, 'recall': 0.9846827133479212}
[2025-05-03 16:47:35,955][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8235294117647058, 'precision': 0.875, 'recall': 0.7777777777777778}
[2025-05-03 16:47:35,955][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7090909090909091, 'f1': 0.7142857142857143, 'precision': 0.7017543859649122, 'recall': 0.7272727272727273}
[2025-05-03 16:47:37,622][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/id/id/model.pt
[2025-05-03 16:47:37,623][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▆▇▇▇▇█▇████
wandb:           best_val_f1 ▁▇██████████
wandb:         best_val_loss █▅▃▃▂▂▂▂▂▁▁▁
wandb:    best_val_precision ▁▇█▇▇███▇███
wandb:       best_val_recall ▁▆▇██▇█▇████
wandb:                 epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▃▃▃▃▃▃▃▃▃▃▃▃
wandb:            train_loss █▇▃▂▂▂▁▁▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▆▇▇▇▇█▇██▇████
wandb:                val_f1 ▁▇▇███████▇████
wandb:              val_loss █▅▃▃▂▂▂▂▂▁▂▁▁▁▂
wandb:         val_precision ▁▇▇▇▇█▇█▇▇█▇▇▇▇
wandb:            val_recall ▁▆▇██▇▇▇█▇▆████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.83333
wandb:           best_val_f1 0.82353
wandb:         best_val_loss 0.5824
wandb:    best_val_precision 0.875
wandb:       best_val_recall 0.77778
wandb:                 epoch 15
wandb:   final_test_accuracy 0.70909
wandb:         final_test_f1 0.71429
wandb:  final_test_precision 0.70175
wandb:     final_test_recall 0.72727
wandb:  final_train_accuracy 0.98323
wandb:        final_train_f1 0.98253
wandb: final_train_precision 0.98039
wandb:    final_train_recall 0.98468
wandb:    final_val_accuracy 0.83333
wandb:          final_val_f1 0.82353
wandb:   final_val_precision 0.875
wandb:      final_val_recall 0.77778
wandb:         learning_rate 0.0001
wandb:            train_loss 0.52769
wandb:            train_time 42.58148
wandb:          val_accuracy 0.83333
wandb:                val_f1 0.83333
wandb:              val_loss 0.59425
wandb:         val_precision 0.83333
wandb:            val_recall 0.83333
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_164620-59gbd3vk
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_164620-59gbd3vk/logs
Experiment probe_layer3_question_type_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/id/id/results.json for layer 3
Running experiment: probe_layer3_complexity_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=3"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer3_complexity_id"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/layer3/id"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-03 16:48:24,059][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/id
experiment_name: probe_layer3_complexity_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-03 16:48:24,059][__main__][INFO] - Normalized task: complexity
[2025-05-03 16:48:24,060][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-03 16:48:24,060][__main__][INFO] - Determined Task Type: regression
[2025-05-03 16:48:24,064][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-05-03 16:48:24,064][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-03 16:48:30,324][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-03 16:48:32,798][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-03 16:48:32,799][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:48:33,318][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:48:33,466][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:48:34,018][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-03 16:48:34,026][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:48:34,026][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-03 16:48:34,028][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:48:34,191][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:48:34,291][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:48:34,328][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-03 16:48:34,330][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:48:34,330][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-03 16:48:34,331][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:48:34,512][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:48:34,657][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:48:34,716][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-03 16:48:34,717][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:48:34,717][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-03 16:48:34,718][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-03 16:48:34,719][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:48:34,719][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:48:34,719][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:48:34,720][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:48:34,720][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:48:34,720][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-05-03 16:48:34,720][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-03 16:48:34,720][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-05-03 16:48:34,720][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:48:34,720][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:48:34,720][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:48:34,720][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:48:34,721][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:48:34,721][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-05-03 16:48:34,721][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-03 16:48:34,721][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-03 16:48:34,721][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-03 16:48:34,721][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-03 16:48:34,721][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-03 16:48:34,721][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-03 16:48:34,721][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-03 16:48:34,721][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-05-03 16:48:34,722][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-03 16:48:34,722][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-05-03 16:48:34,722][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-03 16:48:34,722][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-03 16:48:34,722][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-03 16:48:34,722][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-03 16:48:34,722][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-03 16:48:45,844][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-03 16:48:45,845][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-03 16:48:45,845][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=3, freeze_model=True
[2025-05-03 16:48:45,845][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-03 16:48:45,848][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-03 16:48:45,849][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-03 16:48:45,849][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-03 16:48:45,849][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-03 16:48:45,849][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-03 16:48:45,850][__main__][INFO] - Total parameters: 394,255,105
[2025-05-03 16:48:45,850][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.3377Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4819Epoch 1/15: [=                             ] 3/60 batches, loss: 0.5040Epoch 1/15: [==                            ] 4/60 batches, loss: 0.4982Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4773Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4731Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4906Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4895Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4868Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4784Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4571Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4467Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4341Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4331Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4183Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4133Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4192Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4208Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4116Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4073Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4026Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4122Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4081Epoch 1/15: [============                  ] 24/60 batches, loss: 0.3980Epoch 1/15: [============                  ] 25/60 batches, loss: 0.3908Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.3878Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3839Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3778Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3748Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3764Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3752Epoch 1/15: [================              ] 32/60 batches, loss: 0.3706Epoch 1/15: [================              ] 33/60 batches, loss: 0.3660Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3680Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3630Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3597Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3558Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3519Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3482Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3442Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3397Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3367Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3328Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3299Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3284Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3241Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3206Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3188Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3173Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3139Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3114Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3085Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3064Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3069Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3049Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3035Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3044Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3013Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3003Epoch 1/15: [==============================] 60/60 batches, loss: 0.2986
[2025-05-03 16:48:54,314][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2986
[2025-05-03 16:48:54,703][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0900, Metrics: {'mse': 0.08863360434770584, 'rmse': 0.297713963978356, 'r2': -1.1199567317962646}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.2739Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2020Epoch 2/15: [=                             ] 3/60 batches, loss: 0.1743Epoch 2/15: [==                            ] 4/60 batches, loss: 0.1883Epoch 2/15: [==                            ] 5/60 batches, loss: 0.1741Epoch 2/15: [===                           ] 6/60 batches, loss: 0.1841Epoch 2/15: [===                           ] 7/60 batches, loss: 0.1846Epoch 2/15: [====                          ] 8/60 batches, loss: 0.1909Epoch 2/15: [====                          ] 9/60 batches, loss: 0.1941Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.1848Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.1853Epoch 2/15: [======                        ] 12/60 batches, loss: 0.1838Epoch 2/15: [======                        ] 13/60 batches, loss: 0.1877Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.1957Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.1901Epoch 2/15: [========                      ] 16/60 batches, loss: 0.1909Epoch 2/15: [========                      ] 17/60 batches, loss: 0.1910Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.1934Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.1990Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1973Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1930Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.2003Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.2021Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1970Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1990Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1969Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1942Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1913Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1881Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1855Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1831Epoch 2/15: [================              ] 32/60 batches, loss: 0.1808Epoch 2/15: [================              ] 33/60 batches, loss: 0.1801Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1791Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1817Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1802Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1801Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1801Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1807Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1799Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1804Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1799Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1784Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1800Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1790Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1782Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1762Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1739Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1749Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1743Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1731Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1716Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1705Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1702Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1691Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1678Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1674Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1672Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1660Epoch 2/15: [==============================] 60/60 batches, loss: 0.1642
[2025-05-03 16:48:57,042][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1642
[2025-05-03 16:48:57,445][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0983, Metrics: {'mse': 0.09670546650886536, 'rmse': 0.31097502553881295, 'r2': -1.313020944595337}
[2025-05-03 16:48:57,446][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.1009Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1110Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1098Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1248Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1251Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1215Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1269Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1333Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1263Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1203Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1197Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1205Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1206Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1258Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1229Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1217Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1204Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1194Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1170Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1162Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1134Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1118Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1100Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1088Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1098Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1102Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1093Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1070Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1051Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1034Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1064Epoch 3/15: [================              ] 32/60 batches, loss: 0.1076Epoch 3/15: [================              ] 33/60 batches, loss: 0.1107Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1110Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1113Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1123Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1131Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1132Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1134Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1134Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1138Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1128Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1127Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1122Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1113Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1110Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1107Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1116Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1114Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1107Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1100Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1107Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1106Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1115Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1121Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1115Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1113Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1103Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1095Epoch 3/15: [==============================] 60/60 batches, loss: 0.1125
[2025-05-03 16:48:59,465][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1125
[2025-05-03 16:48:59,966][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0720, Metrics: {'mse': 0.07088659703731537, 'rmse': 0.2662453699828701, 'r2': -0.695480227470398}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0584Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0846Epoch 4/15: [=                             ] 3/60 batches, loss: 0.1016Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1157Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1048Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1067Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1172Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1083Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1037Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.0968Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1016Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1011Epoch 4/15: [======                        ] 13/60 batches, loss: 0.0982Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.0958Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.0945Epoch 4/15: [========                      ] 16/60 batches, loss: 0.0944Epoch 4/15: [========                      ] 17/60 batches, loss: 0.0969Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.0952Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.0952Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.0964Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.0978Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.0987Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.0985Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1006Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1002Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1032Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1019Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1017Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1032Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1028Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1017Epoch 4/15: [================              ] 32/60 batches, loss: 0.1015Epoch 4/15: [================              ] 33/60 batches, loss: 0.0998Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1005Epoch 4/15: [=================             ] 35/60 batches, loss: 0.0994Epoch 4/15: [==================            ] 36/60 batches, loss: 0.0977Epoch 4/15: [==================            ] 37/60 batches, loss: 0.0967Epoch 4/15: [===================           ] 38/60 batches, loss: 0.0968Epoch 4/15: [===================           ] 39/60 batches, loss: 0.0967Epoch 4/15: [====================          ] 40/60 batches, loss: 0.0970Epoch 4/15: [====================          ] 41/60 batches, loss: 0.0983Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.0995Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.0991Epoch 4/15: [======================        ] 44/60 batches, loss: 0.0982Epoch 4/15: [======================        ] 45/60 batches, loss: 0.0969Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.0959Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.0946Epoch 4/15: [========================      ] 48/60 batches, loss: 0.0937Epoch 4/15: [========================      ] 49/60 batches, loss: 0.0936Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.0941Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.0944Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.0940Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.0938Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.0933Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.0930Epoch 4/15: [============================  ] 56/60 batches, loss: 0.0943Epoch 4/15: [============================  ] 57/60 batches, loss: 0.0936Epoch 4/15: [============================= ] 58/60 batches, loss: 0.0932Epoch 4/15: [============================= ] 59/60 batches, loss: 0.0929Epoch 4/15: [==============================] 60/60 batches, loss: 0.0920
[2025-05-03 16:49:02,440][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0920
[2025-05-03 16:49:02,952][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0731, Metrics: {'mse': 0.07178628444671631, 'rmse': 0.2679296259220251, 'r2': -0.7169990539550781}
[2025-05-03 16:49:02,953][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.0995Epoch 5/15: [=                             ] 2/60 batches, loss: 0.1001Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0861Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0971Epoch 5/15: [==                            ] 5/60 batches, loss: 0.1039Epoch 5/15: [===                           ] 6/60 batches, loss: 0.1087Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1097Epoch 5/15: [====                          ] 8/60 batches, loss: 0.1071Epoch 5/15: [====                          ] 9/60 batches, loss: 0.1046Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.1014Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.0985Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1026Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1048Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1003Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.0996Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1000Epoch 5/15: [========                      ] 17/60 batches, loss: 0.0996Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.0985Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.0961Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.0978Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.0969Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.0959Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.0960Epoch 5/15: [============                  ] 24/60 batches, loss: 0.0949Epoch 5/15: [============                  ] 25/60 batches, loss: 0.0948Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.0952Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.0944Epoch 5/15: [==============                ] 28/60 batches, loss: 0.0941Epoch 5/15: [==============                ] 29/60 batches, loss: 0.0956Epoch 5/15: [===============               ] 30/60 batches, loss: 0.0970Epoch 5/15: [===============               ] 31/60 batches, loss: 0.0976Epoch 5/15: [================              ] 32/60 batches, loss: 0.0969Epoch 5/15: [================              ] 33/60 batches, loss: 0.0958Epoch 5/15: [=================             ] 34/60 batches, loss: 0.0953Epoch 5/15: [=================             ] 35/60 batches, loss: 0.0967Epoch 5/15: [==================            ] 36/60 batches, loss: 0.0960Epoch 5/15: [==================            ] 37/60 batches, loss: 0.0961Epoch 5/15: [===================           ] 38/60 batches, loss: 0.0945Epoch 5/15: [===================           ] 39/60 batches, loss: 0.0933Epoch 5/15: [====================          ] 40/60 batches, loss: 0.0933Epoch 5/15: [====================          ] 41/60 batches, loss: 0.0928Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.0927Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.0929Epoch 5/15: [======================        ] 44/60 batches, loss: 0.0924Epoch 5/15: [======================        ] 45/60 batches, loss: 0.0926Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.0916Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0910Epoch 5/15: [========================      ] 48/60 batches, loss: 0.0908Epoch 5/15: [========================      ] 49/60 batches, loss: 0.0906Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.0904Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0897Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0892Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0891Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0888Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0881Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0876Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0871Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0878Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0870Epoch 5/15: [==============================] 60/60 batches, loss: 0.0864
[2025-05-03 16:49:04,937][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0864
[2025-05-03 16:49:05,364][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0711, Metrics: {'mse': 0.06985342502593994, 'rmse': 0.26429798528543486, 'r2': -0.6707684993743896}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.0383Epoch 6/15: [=                             ] 2/60 batches, loss: 0.0556Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0691Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0634Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0702Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0642Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0759Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0831Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0820Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0830Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0800Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0791Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0826Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0835Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0839Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0835Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0805Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0779Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0784Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0781Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0770Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0780Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0773Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0755Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0754Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0751Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0751Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0745Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0749Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0762Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0754Epoch 6/15: [================              ] 32/60 batches, loss: 0.0751Epoch 6/15: [================              ] 33/60 batches, loss: 0.0765Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0772Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0777Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0769Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0767Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0765Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0767Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0770Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0766Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0768Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0761Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0762Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0766Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0760Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0760Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0758Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0760Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0762Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0779Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0777Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0774Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0769Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0774Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0773Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0771Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0766Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0757Epoch 6/15: [==============================] 60/60 batches, loss: 0.0757
[2025-05-03 16:49:07,623][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0757
[2025-05-03 16:49:07,973][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0512, Metrics: {'mse': 0.05070050433278084, 'rmse': 0.22516772489142586, 'r2': -0.21266508102416992}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0378Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0336Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0586Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0561Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0557Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0599Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0628Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0631Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0636Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0596Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0574Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0586Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0585Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0613Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0628Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0605Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0592Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0587Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0580Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0574Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0571Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0579Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0597Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0611Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0616Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0622Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0620Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0627Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0638Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0641Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0647Epoch 7/15: [================              ] 32/60 batches, loss: 0.0638Epoch 7/15: [================              ] 33/60 batches, loss: 0.0645Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0645Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0645Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0639Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0646Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0635Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0640Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0637Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0640Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0653Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0641Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0634Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0642Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0646Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0641Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0646Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0652Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0648Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0650Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0648Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0649Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0644Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0642Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0636Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0635Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0632Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0636Epoch 7/15: [==============================] 60/60 batches, loss: 0.0639
[2025-05-03 16:49:10,411][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0639
[2025-05-03 16:49:10,743][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0638, Metrics: {'mse': 0.06284873932600021, 'rmse': 0.2506965084040865, 'r2': -0.503229022026062}
[2025-05-03 16:49:10,743][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0328Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0550Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0608Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0553Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0567Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0566Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0535Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0580Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0566Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0567Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0541Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0540Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0522Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0540Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0549Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0552Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0573Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0577Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0571Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0561Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0566Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0586Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0587Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0596Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0581Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0575Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0582Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0577Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0575Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0571Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0574Epoch 8/15: [================              ] 32/60 batches, loss: 0.0572Epoch 8/15: [================              ] 33/60 batches, loss: 0.0571Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0569Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0575Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0572Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0584Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0585Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0592Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0588Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0587Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0586Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0580Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0577Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0575Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0570Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0569Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0567Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0572Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0574Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0575Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0579Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0574Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0579Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0585Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0587Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0589Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0591Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0589Epoch 8/15: [==============================] 60/60 batches, loss: 0.0583
[2025-05-03 16:49:12,687][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0583
[2025-05-03 16:49:13,148][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0520, Metrics: {'mse': 0.05154126137495041, 'rmse': 0.2270270058273914, 'r2': -0.2327744960784912}
[2025-05-03 16:49:13,149][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.0579Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0468Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0414Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0383Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0442Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0427Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0416Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0466Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0476Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0474Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0538Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0509Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0505Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0518Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0508Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0521Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0519Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0520Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0517Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0514Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0510Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0523Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0520Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0508Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0529Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0532Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0553Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0561Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0566Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0580Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0577Epoch 9/15: [================              ] 32/60 batches, loss: 0.0567Epoch 9/15: [================              ] 33/60 batches, loss: 0.0571Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0569Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0580Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0588Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0591Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0585Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0581Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0575Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0574Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0584Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0580Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0581Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0579Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0579Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0575Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0569Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0570Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0567Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0565Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0566Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0566Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0560Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0557Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0556Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0554Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0554Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0557Epoch 9/15: [==============================] 60/60 batches, loss: 0.0551
[2025-05-03 16:49:15,084][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0551
[2025-05-03 16:49:15,519][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0471, Metrics: {'mse': 0.04659842327237129, 'rmse': 0.21586667939348883, 'r2': -0.11455070972442627}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0427Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0446Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0575Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0598Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0577Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0527Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0527Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0540Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0552Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0587Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0578Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0554Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0578Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0567Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0589Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0577Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0563Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0565Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0564Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0570Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0560Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0562Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0564Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0562Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0566Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0556Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0554Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0545Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0549Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0539Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0536Epoch 10/15: [================              ] 32/60 batches, loss: 0.0553Epoch 10/15: [================              ] 33/60 batches, loss: 0.0550Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0554Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0554Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0557Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0554Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0553Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0555Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0555Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0559Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0557Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0551Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0547Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0546Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0542Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0538Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0532Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0533Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0530Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0524Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0521Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0517Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0515Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0511Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0509Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0514Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0519Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0519Epoch 10/15: [==============================] 60/60 batches, loss: 0.0524
[2025-05-03 16:49:18,065][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0524
[2025-05-03 16:49:18,511][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0405, Metrics: {'mse': 0.04010562598705292, 'rmse': 0.20026389087165195, 'r2': 0.04074537754058838}
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0592Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0605Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0535Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0531Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0514Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0490Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0521Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0510Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0514Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0502Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0506Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0556Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0541Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0534Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0525Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0539Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0526Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0561Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0562Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0553Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0540Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0532Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0539Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0534Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0525Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0523Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0520Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0525Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0521Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0519Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0514Epoch 11/15: [================              ] 32/60 batches, loss: 0.0515Epoch 11/15: [================              ] 33/60 batches, loss: 0.0518Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0517Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0524Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0520Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0512Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0506Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0505Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0508Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0509Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0510Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0506Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0510Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0509Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0510Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0507Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0507Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0508Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0501Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0497Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0497Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0499Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0502Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0504Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0505Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0502Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0504Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0508Epoch 11/15: [==============================] 60/60 batches, loss: 0.0509
[2025-05-03 16:49:21,070][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0509
[2025-05-03 16:49:21,397][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0582, Metrics: {'mse': 0.05755963549017906, 'rmse': 0.23991589253356907, 'r2': -0.37672317028045654}
[2025-05-03 16:49:21,398][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0447Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0522Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0478Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0469Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0431Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0457Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0434Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0451Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0448Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0448Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0428Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0436Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0423Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0415Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0434Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0446Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0444Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0463Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0465Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0462Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0457Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0453Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0448Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0460Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0467Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0479Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0478Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0476Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0495Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0489Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0485Epoch 12/15: [================              ] 32/60 batches, loss: 0.0479Epoch 12/15: [================              ] 33/60 batches, loss: 0.0473Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0482Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0478Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0477Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0473Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0475Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0474Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0469Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0480Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0481Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0481Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0487Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0485Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0486Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0485Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0481Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0479Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0473Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0481Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0476Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0477Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0475Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0475Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0475Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0477Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0475Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0474Epoch 12/15: [==============================] 60/60 batches, loss: 0.0469
[2025-05-03 16:49:23,477][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0469
[2025-05-03 16:49:23,983][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0640, Metrics: {'mse': 0.06333405524492264, 'rmse': 0.25166258213115955, 'r2': -0.5148369073867798}
[2025-05-03 16:49:23,984][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0378Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0542Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0497Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0541Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0522Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0486Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0489Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0491Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0492Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0478Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0459Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0473Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0474Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0483Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0494Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0488Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0497Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0488Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0472Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0491Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0485Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0488Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0483Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0482Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0484Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0475Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0474Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0469Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0459Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0457Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0448Epoch 13/15: [================              ] 32/60 batches, loss: 0.0443Epoch 13/15: [================              ] 33/60 batches, loss: 0.0450Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0454Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0457Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0455Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0458Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0457Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0455Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0450Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0455Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0449Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0448Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0453Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0452Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0456Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0455Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0456Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0455Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0456Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0453Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0456Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0451Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0451Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0454Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0453Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0453Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0456Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0452Epoch 13/15: [==============================] 60/60 batches, loss: 0.0450
[2025-05-03 16:49:25,984][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0450
[2025-05-03 16:49:26,345][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0379, Metrics: {'mse': 0.03774316608905792, 'rmse': 0.19427600492355696, 'r2': 0.097251296043396}
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0276Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0293Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0391Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0362Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0343Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0353Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0334Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0361Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0361Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0387Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0377Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0423Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0425Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0435Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0424Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0422Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0428Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0428Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0414Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0412Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0430Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0435Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0428Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0431Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0437Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0427Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0430Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0425Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0426Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0426Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0431Epoch 14/15: [================              ] 32/60 batches, loss: 0.0429Epoch 14/15: [================              ] 33/60 batches, loss: 0.0428Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0425Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0427Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0437Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0439Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0438Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0438Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0437Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0435Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0435Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0434Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0439Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0436Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0442Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0444Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0445Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0455Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0467Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0464Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0460Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0456Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0462Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0461Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0456Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0458Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0457Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0452Epoch 14/15: [==============================] 60/60 batches, loss: 0.0453
[2025-05-03 16:49:28,715][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0453
[2025-05-03 16:49:29,167][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0513, Metrics: {'mse': 0.051129117608070374, 'rmse': 0.22611748629433853, 'r2': -0.2229166030883789}
[2025-05-03 16:49:29,168][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0549Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0458Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0516Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0425Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0455Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0441Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0488Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0478Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0475Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0462Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0467Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0468Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0451Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0448Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0440Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0434Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0434Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0433Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0438Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0433Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0424Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0414Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0417Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0418Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0418Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0437Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0431Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0424Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0415Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0417Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0417Epoch 15/15: [================              ] 32/60 batches, loss: 0.0413Epoch 15/15: [================              ] 33/60 batches, loss: 0.0408Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0407Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0404Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0403Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0402Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0398Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0403Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0409Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0401Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0399Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0397Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0399Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0401Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0405Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0402Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0397Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0394Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0400Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0402Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0400Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0401Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0400Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0409Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0410Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0410Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0411Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0414Epoch 15/15: [==============================] 60/60 batches, loss: 0.0410
[2025-05-03 16:49:31,056][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0410
[2025-05-03 16:49:31,522][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0369, Metrics: {'mse': 0.03681319206953049, 'rmse': 0.19186764205965134, 'r2': 0.1194944977760315}
[2025-05-03 16:49:31,951][src.training.lm_trainer][INFO] - Training completed in 40.30 seconds
[2025-05-03 16:49:31,951][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-03 16:49:34,737][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.02335006557404995, 'rmse': 0.1528072824640565, 'r2': 0.3565123677253723}
[2025-05-03 16:49:34,737][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03681319206953049, 'rmse': 0.19186764205965134, 'r2': 0.1194944977760315}
[2025-05-03 16:49:34,737][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.023815622553229332, 'rmse': 0.15432311088501727, 'r2': 0.41591304540634155}
[2025-05-03 16:49:36,410][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/id/id/model.pt
[2025-05-03 16:49:36,412][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▆▃▂▁▁▁
wandb:     best_val_mse █▆▅▃▂▁▁▁
wandb:      best_val_r2 ▁▃▄▆▇███
wandb:    best_val_rmse █▆▆▃▃▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▄▄▄▆▅▆▆▇▆▅▇▆
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▅▅▅▃▄▃▂▁▃▄▁▃▁
wandb:          val_mse ▇█▅▅▅▃▄▃▂▁▃▄▁▃▁
wandb:           val_r2 ▂▁▄▄▄▆▅▆▇█▆▅█▆█
wandb:         val_rmse ▇█▅▅▅▃▄▃▂▁▄▅▁▃▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03687
wandb:     best_val_mse 0.03681
wandb:      best_val_r2 0.11949
wandb:    best_val_rmse 0.19187
wandb:            epoch 15
wandb:   final_test_mse 0.02382
wandb:    final_test_r2 0.41591
wandb:  final_test_rmse 0.15432
wandb:  final_train_mse 0.02335
wandb:   final_train_r2 0.35651
wandb: final_train_rmse 0.15281
wandb:    final_val_mse 0.03681
wandb:     final_val_r2 0.11949
wandb:   final_val_rmse 0.19187
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04105
wandb:       train_time 40.30212
wandb:         val_loss 0.03687
wandb:          val_mse 0.03681
wandb:           val_r2 0.11949
wandb:         val_rmse 0.19187
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_164824-nqdt7y6q
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_164824-nqdt7y6q/logs
Experiment probe_layer3_complexity_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/id/id/results.json for layer 3
Running experiment: probe_layer3_question_type_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=3"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer3_question_type_ja"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/layer3/ja"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-03 16:50:21,936][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/ja
experiment_name: probe_layer3_question_type_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-03 16:50:21,936][__main__][INFO] - Normalized task: question_type
[2025-05-03 16:50:21,936][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-03 16:50:21,936][__main__][INFO] - Determined Task Type: classification
[2025-05-03 16:50:21,940][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ja']
[2025-05-03 16:50:21,941][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-03 16:50:28,229][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-03 16:50:30,615][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-03 16:50:30,615][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:50:31,055][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:50:31,288][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:50:31,616][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-03 16:50:31,625][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:50:31,625][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-03 16:50:31,628][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:50:31,782][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:50:31,902][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:50:31,972][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-03 16:50:31,973][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:50:31,973][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-03 16:50:31,974][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-03 16:50:32,212][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:50:32,278][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-03 16:50:32,388][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-03 16:50:32,389][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-03 16:50:32,389][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-03 16:50:32,400][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-03 16:50:32,401][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:50:32,401][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:50:32,401][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:50:32,401][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:50:32,401][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-05-03 16:50:32,402][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-05-03 16:50:32,402][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-03 16:50:32,402][src.data.datasets][INFO] - Sample label: 1
[2025-05-03 16:50:32,402][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:50:32,402][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:50:32,402][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:50:32,402][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:50:32,402][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-05-03 16:50:32,402][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-05-03 16:50:32,402][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-03 16:50:32,402][src.data.datasets][INFO] - Sample label: 0
[2025-05-03 16:50:32,403][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-03 16:50:32,403][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-03 16:50:32,403][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-03 16:50:32,403][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-03 16:50:32,403][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-05-03 16:50:32,403][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-05-03 16:50:32,403][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-03 16:50:32,403][src.data.datasets][INFO] - Sample label: 1
[2025-05-03 16:50:32,403][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-03 16:50:32,403][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-03 16:50:32,404][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-03 16:50:32,404][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-03 16:50:32,404][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-03 16:50:44,381][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-03 16:50:44,382][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-03 16:50:44,382][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=3, freeze_model=True
[2025-05-03 16:50:44,382][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-03 16:50:44,388][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-03 16:50:44,388][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-03 16:50:44,388][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-03 16:50:44,389][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-03 16:50:44,389][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-03 16:50:44,389][__main__][INFO] - Total parameters: 394,568,839
[2025-05-03 16:50:44,390][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-03 16:50:44,390][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7114Epoch 1/15: [                              ] 2/75 batches, loss: 0.7285Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7239Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7099Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7038Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7047Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7042Epoch 1/15: [===                           ] 8/75 batches, loss: 0.7027Epoch 1/15: [===                           ] 9/75 batches, loss: 0.7020Epoch 1/15: [====                          ] 10/75 batches, loss: 0.7004Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6994Epoch 1/15: [====                          ] 12/75 batches, loss: 0.6984Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.6981Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.6979Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6972Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6965Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6967Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6964Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6962Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6957Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6952Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6945Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6947Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6942Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6942Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6941Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6936Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6941Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6939Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6936Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6935Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6926Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6927Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6923Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6913Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6902Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6889Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6891Epoch 1/15: [================              ] 40/75 batches, loss: 0.6893Epoch 1/15: [================              ] 41/75 batches, loss: 0.6887Epoch 1/15: [================              ] 42/75 batches, loss: 0.6893Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6888Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6886Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6883Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6884Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6873Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6857Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6851Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6850Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6848Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6845Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6839Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6831Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6822Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6808Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6816Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6816Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6809Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6806Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6783Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6775Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6779Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6775Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6754Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6755Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6756Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6754Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6743Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6737Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6724Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6722Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6718Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6704Epoch 1/15: [==============================] 75/75 batches, loss: 0.6708
[2025-05-03 16:50:53,042][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6708
[2025-05-03 16:50:53,379][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6057, Metrics: {'accuracy': 0.8043478260869565, 'f1': 0.7804878048780488, 'precision': 0.9411764705882353, 'recall': 0.6666666666666666}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.5559Epoch 2/15: [                              ] 2/75 batches, loss: 0.5748Epoch 2/15: [=                             ] 3/75 batches, loss: 0.5906Epoch 2/15: [=                             ] 4/75 batches, loss: 0.5788Epoch 2/15: [==                            ] 5/75 batches, loss: 0.5768Epoch 2/15: [==                            ] 6/75 batches, loss: 0.5919Epoch 2/15: [==                            ] 7/75 batches, loss: 0.5955Epoch 2/15: [===                           ] 8/75 batches, loss: 0.5976Epoch 2/15: [===                           ] 9/75 batches, loss: 0.5830Epoch 2/15: [====                          ] 10/75 batches, loss: 0.5835Epoch 2/15: [====                          ] 11/75 batches, loss: 0.5830Epoch 2/15: [====                          ] 12/75 batches, loss: 0.5845Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.5825Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.5849Epoch 2/15: [======                        ] 15/75 batches, loss: 0.5863Epoch 2/15: [======                        ] 16/75 batches, loss: 0.5812Epoch 2/15: [======                        ] 17/75 batches, loss: 0.5816Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.5833Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.5793Epoch 2/15: [========                      ] 20/75 batches, loss: 0.5818Epoch 2/15: [========                      ] 21/75 batches, loss: 0.5819Epoch 2/15: [========                      ] 22/75 batches, loss: 0.5802Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.5806Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.5826Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.5809Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.5809Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.5790Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.5810Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.5776Epoch 2/15: [============                  ] 30/75 batches, loss: 0.5752Epoch 2/15: [============                  ] 31/75 batches, loss: 0.5746Epoch 2/15: [============                  ] 32/75 batches, loss: 0.5772Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.5772Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.5751Epoch 2/15: [==============                ] 35/75 batches, loss: 0.5760Epoch 2/15: [==============                ] 36/75 batches, loss: 0.5757Epoch 2/15: [==============                ] 37/75 batches, loss: 0.5762Epoch 2/15: [===============               ] 38/75 batches, loss: 0.5759Epoch 2/15: [===============               ] 39/75 batches, loss: 0.5750Epoch 2/15: [================              ] 40/75 batches, loss: 0.5750Epoch 2/15: [================              ] 41/75 batches, loss: 0.5752Epoch 2/15: [================              ] 42/75 batches, loss: 0.5775Epoch 2/15: [=================             ] 43/75 batches, loss: 0.5758Epoch 2/15: [=================             ] 44/75 batches, loss: 0.5757Epoch 2/15: [==================            ] 45/75 batches, loss: 0.5769Epoch 2/15: [==================            ] 46/75 batches, loss: 0.5745Epoch 2/15: [==================            ] 47/75 batches, loss: 0.5745Epoch 2/15: [===================           ] 48/75 batches, loss: 0.5748Epoch 2/15: [===================           ] 49/75 batches, loss: 0.5751Epoch 2/15: [====================          ] 50/75 batches, loss: 0.5748Epoch 2/15: [====================          ] 51/75 batches, loss: 0.5735Epoch 2/15: [====================          ] 52/75 batches, loss: 0.5732Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.5734Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.5743Epoch 2/15: [======================        ] 55/75 batches, loss: 0.5736Epoch 2/15: [======================        ] 56/75 batches, loss: 0.5731Epoch 2/15: [======================        ] 57/75 batches, loss: 0.5718Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.5720Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.5715Epoch 2/15: [========================      ] 60/75 batches, loss: 0.5719Epoch 2/15: [========================      ] 61/75 batches, loss: 0.5725Epoch 2/15: [========================      ] 62/75 batches, loss: 0.5734Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.5737Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.5730Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.5721Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.5714Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.5705Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.5706Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.5700Epoch 2/15: [============================  ] 70/75 batches, loss: 0.5699Epoch 2/15: [============================  ] 71/75 batches, loss: 0.5692Epoch 2/15: [============================  ] 72/75 batches, loss: 0.5688Epoch 2/15: [============================= ] 73/75 batches, loss: 0.5693Epoch 2/15: [============================= ] 74/75 batches, loss: 0.5680Epoch 2/15: [==============================] 75/75 batches, loss: 0.5674
[2025-05-03 16:50:56,203][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.5674
[2025-05-03 16:50:56,597][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.5557, Metrics: {'accuracy': 0.9130434782608695, 'f1': 0.9230769230769231, 'precision': 0.8571428571428571, 'recall': 1.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.5247Epoch 3/15: [                              ] 2/75 batches, loss: 0.5305Epoch 3/15: [=                             ] 3/75 batches, loss: 0.5177Epoch 3/15: [=                             ] 4/75 batches, loss: 0.5254Epoch 3/15: [==                            ] 5/75 batches, loss: 0.5172Epoch 3/15: [==                            ] 6/75 batches, loss: 0.5168Epoch 3/15: [==                            ] 7/75 batches, loss: 0.5193Epoch 3/15: [===                           ] 8/75 batches, loss: 0.5145Epoch 3/15: [===                           ] 9/75 batches, loss: 0.5145Epoch 3/15: [====                          ] 10/75 batches, loss: 0.5079Epoch 3/15: [====                          ] 11/75 batches, loss: 0.5107Epoch 3/15: [====                          ] 12/75 batches, loss: 0.5116Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.5091Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.5079Epoch 3/15: [======                        ] 15/75 batches, loss: 0.5066Epoch 3/15: [======                        ] 16/75 batches, loss: 0.5054Epoch 3/15: [======                        ] 17/75 batches, loss: 0.5065Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.5060Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.5052Epoch 3/15: [========                      ] 20/75 batches, loss: 0.5123Epoch 3/15: [========                      ] 21/75 batches, loss: 0.5096Epoch 3/15: [========                      ] 22/75 batches, loss: 0.5089Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.5089Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.5079Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.5122Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.5132Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.5143Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.5147Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.5140Epoch 3/15: [============                  ] 30/75 batches, loss: 0.5166Epoch 3/15: [============                  ] 31/75 batches, loss: 0.5159Epoch 3/15: [============                  ] 32/75 batches, loss: 0.5159Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.5139Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.5161Epoch 3/15: [==============                ] 35/75 batches, loss: 0.5167Epoch 3/15: [==============                ] 36/75 batches, loss: 0.5190Epoch 3/15: [==============                ] 37/75 batches, loss: 0.5201Epoch 3/15: [===============               ] 38/75 batches, loss: 0.5205Epoch 3/15: [===============               ] 39/75 batches, loss: 0.5218Epoch 3/15: [================              ] 40/75 batches, loss: 0.5226Epoch 3/15: [================              ] 41/75 batches, loss: 0.5242Epoch 3/15: [================              ] 42/75 batches, loss: 0.5257Epoch 3/15: [=================             ] 43/75 batches, loss: 0.5268Epoch 3/15: [=================             ] 44/75 batches, loss: 0.5278Epoch 3/15: [==================            ] 45/75 batches, loss: 0.5288Epoch 3/15: [==================            ] 46/75 batches, loss: 0.5307Epoch 3/15: [==================            ] 47/75 batches, loss: 0.5295Epoch 3/15: [===================           ] 48/75 batches, loss: 0.5309Epoch 3/15: [===================           ] 49/75 batches, loss: 0.5329Epoch 3/15: [====================          ] 50/75 batches, loss: 0.5330Epoch 3/15: [====================          ] 51/75 batches, loss: 0.5327Epoch 3/15: [====================          ] 52/75 batches, loss: 0.5332Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.5319Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.5324Epoch 3/15: [======================        ] 55/75 batches, loss: 0.5329Epoch 3/15: [======================        ] 56/75 batches, loss: 0.5322Epoch 3/15: [======================        ] 57/75 batches, loss: 0.5312Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.5310Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.5319Epoch 3/15: [========================      ] 60/75 batches, loss: 0.5316Epoch 3/15: [========================      ] 61/75 batches, loss: 0.5321Epoch 3/15: [========================      ] 62/75 batches, loss: 0.5317Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.5319Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.5314Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.5321Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.5327Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.5329Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.5340Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.5328Epoch 3/15: [============================  ] 70/75 batches, loss: 0.5325Epoch 3/15: [============================  ] 71/75 batches, loss: 0.5322Epoch 3/15: [============================  ] 72/75 batches, loss: 0.5323Epoch 3/15: [============================= ] 73/75 batches, loss: 0.5319Epoch 3/15: [============================= ] 74/75 batches, loss: 0.5323Epoch 3/15: [==============================] 75/75 batches, loss: 0.5346
[2025-05-03 16:50:59,479][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.5346
[2025-05-03 16:50:59,835][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.5321, Metrics: {'accuracy': 0.9130434782608695, 'f1': 0.9130434782608695, 'precision': 0.9545454545454546, 'recall': 0.875}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6141Epoch 4/15: [                              ] 2/75 batches, loss: 0.5881Epoch 4/15: [=                             ] 3/75 batches, loss: 0.5794Epoch 4/15: [=                             ] 4/75 batches, loss: 0.5572Epoch 4/15: [==                            ] 5/75 batches, loss: 0.5620Epoch 4/15: [==                            ] 6/75 batches, loss: 0.5613Epoch 4/15: [==                            ] 7/75 batches, loss: 0.5569Epoch 4/15: [===                           ] 8/75 batches, loss: 0.5515Epoch 4/15: [===                           ] 9/75 batches, loss: 0.5479Epoch 4/15: [====                          ] 10/75 batches, loss: 0.5439Epoch 4/15: [====                          ] 11/75 batches, loss: 0.5333Epoch 4/15: [====                          ] 12/75 batches, loss: 0.5279Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.5221Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.5166Epoch 4/15: [======                        ] 15/75 batches, loss: 0.5200Epoch 4/15: [======                        ] 16/75 batches, loss: 0.5231Epoch 4/15: [======                        ] 17/75 batches, loss: 0.5225Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.5242Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.5247Epoch 4/15: [========                      ] 20/75 batches, loss: 0.5263Epoch 4/15: [========                      ] 21/75 batches, loss: 0.5291Epoch 4/15: [========                      ] 22/75 batches, loss: 0.5317Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.5305Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.5296Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.5266Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.5262Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.5265Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.5275Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.5301Epoch 4/15: [============                  ] 30/75 batches, loss: 0.5309Epoch 4/15: [============                  ] 31/75 batches, loss: 0.5295Epoch 4/15: [============                  ] 32/75 batches, loss: 0.5285Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.5251Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.5246Epoch 4/15: [==============                ] 35/75 batches, loss: 0.5237Epoch 4/15: [==============                ] 36/75 batches, loss: 0.5253Epoch 4/15: [==============                ] 37/75 batches, loss: 0.5257Epoch 4/15: [===============               ] 38/75 batches, loss: 0.5287Epoch 4/15: [===============               ] 39/75 batches, loss: 0.5303Epoch 4/15: [================              ] 40/75 batches, loss: 0.5296Epoch 4/15: [================              ] 41/75 batches, loss: 0.5301Epoch 4/15: [================              ] 42/75 batches, loss: 0.5287Epoch 4/15: [=================             ] 43/75 batches, loss: 0.5281Epoch 4/15: [=================             ] 44/75 batches, loss: 0.5250Epoch 4/15: [==================            ] 45/75 batches, loss: 0.5226Epoch 4/15: [==================            ] 46/75 batches, loss: 0.5229Epoch 4/15: [==================            ] 47/75 batches, loss: 0.5230Epoch 4/15: [===================           ] 48/75 batches, loss: 0.5237Epoch 4/15: [===================           ] 49/75 batches, loss: 0.5243Epoch 4/15: [====================          ] 50/75 batches, loss: 0.5244Epoch 4/15: [====================          ] 51/75 batches, loss: 0.5247Epoch 4/15: [====================          ] 52/75 batches, loss: 0.5254Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.5261Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.5250Epoch 4/15: [======================        ] 55/75 batches, loss: 0.5247Epoch 4/15: [======================        ] 56/75 batches, loss: 0.5257Epoch 4/15: [======================        ] 57/75 batches, loss: 0.5265Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.5271Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.5273Epoch 4/15: [========================      ] 60/75 batches, loss: 0.5263Epoch 4/15: [========================      ] 61/75 batches, loss: 0.5268Epoch 4/15: [========================      ] 62/75 batches, loss: 0.5277Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.5275Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.5278Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.5269Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.5255Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.5245Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.5251Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.5269Epoch 4/15: [============================  ] 70/75 batches, loss: 0.5263Epoch 4/15: [============================  ] 71/75 batches, loss: 0.5270Epoch 4/15: [============================  ] 72/75 batches, loss: 0.5268Epoch 4/15: [============================= ] 73/75 batches, loss: 0.5274Epoch 4/15: [============================= ] 74/75 batches, loss: 0.5255Epoch 4/15: [==============================] 75/75 batches, loss: 0.5264
[2025-05-03 16:51:02,674][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5264
[2025-05-03 16:51:02,933][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5239, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.96, 'precision': 0.9230769230769231, 'recall': 1.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.4837Epoch 5/15: [                              ] 2/75 batches, loss: 0.5058Epoch 5/15: [=                             ] 3/75 batches, loss: 0.5015Epoch 5/15: [=                             ] 4/75 batches, loss: 0.4961Epoch 5/15: [==                            ] 5/75 batches, loss: 0.5081Epoch 5/15: [==                            ] 6/75 batches, loss: 0.5038Epoch 5/15: [==                            ] 7/75 batches, loss: 0.5079Epoch 5/15: [===                           ] 8/75 batches, loss: 0.5164Epoch 5/15: [===                           ] 9/75 batches, loss: 0.5236Epoch 5/15: [====                          ] 10/75 batches, loss: 0.5177Epoch 5/15: [====                          ] 11/75 batches, loss: 0.5178Epoch 5/15: [====                          ] 12/75 batches, loss: 0.5162Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.5229Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.5204Epoch 5/15: [======                        ] 15/75 batches, loss: 0.5228Epoch 5/15: [======                        ] 16/75 batches, loss: 0.5245Epoch 5/15: [======                        ] 17/75 batches, loss: 0.5216Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.5234Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.5263Epoch 5/15: [========                      ] 20/75 batches, loss: 0.5248Epoch 5/15: [========                      ] 21/75 batches, loss: 0.5224Epoch 5/15: [========                      ] 22/75 batches, loss: 0.5234Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.5243Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.5229Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.5223Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.5228Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.5221Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.5242Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.5239Epoch 5/15: [============                  ] 30/75 batches, loss: 0.5281Epoch 5/15: [============                  ] 31/75 batches, loss: 0.5263Epoch 5/15: [============                  ] 32/75 batches, loss: 0.5249Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.5253Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.5263Epoch 5/15: [==============                ] 35/75 batches, loss: 0.5262Epoch 5/15: [==============                ] 36/75 batches, loss: 0.5233Epoch 5/15: [==============                ] 37/75 batches, loss: 0.5222Epoch 5/15: [===============               ] 38/75 batches, loss: 0.5227Epoch 5/15: [===============               ] 39/75 batches, loss: 0.5223Epoch 5/15: [================              ] 40/75 batches, loss: 0.5228Epoch 5/15: [================              ] 41/75 batches, loss: 0.5219Epoch 5/15: [================              ] 42/75 batches, loss: 0.5226Epoch 5/15: [=================             ] 43/75 batches, loss: 0.5222Epoch 5/15: [=================             ] 44/75 batches, loss: 0.5221Epoch 5/15: [==================            ] 45/75 batches, loss: 0.5219Epoch 5/15: [==================            ] 46/75 batches, loss: 0.5216Epoch 5/15: [==================            ] 47/75 batches, loss: 0.5214Epoch 5/15: [===================           ] 48/75 batches, loss: 0.5213Epoch 5/15: [===================           ] 49/75 batches, loss: 0.5210Epoch 5/15: [====================          ] 50/75 batches, loss: 0.5202Epoch 5/15: [====================          ] 51/75 batches, loss: 0.5190Epoch 5/15: [====================          ] 52/75 batches, loss: 0.5195Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.5178Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.5172Epoch 5/15: [======================        ] 55/75 batches, loss: 0.5170Epoch 5/15: [======================        ] 56/75 batches, loss: 0.5172Epoch 5/15: [======================        ] 57/75 batches, loss: 0.5173Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.5173Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.5159Epoch 5/15: [========================      ] 60/75 batches, loss: 0.5158Epoch 5/15: [========================      ] 61/75 batches, loss: 0.5145Epoch 5/15: [========================      ] 62/75 batches, loss: 0.5150Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.5149Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.5139Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.5156Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.5148Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.5158Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.5154Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.5154Epoch 5/15: [============================  ] 70/75 batches, loss: 0.5165Epoch 5/15: [============================  ] 71/75 batches, loss: 0.5180Epoch 5/15: [============================  ] 72/75 batches, loss: 0.5191Epoch 5/15: [============================= ] 73/75 batches, loss: 0.5190Epoch 5/15: [============================= ] 74/75 batches, loss: 0.5186Epoch 5/15: [==============================] 75/75 batches, loss: 0.5188
[2025-05-03 16:51:05,766][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5188
[2025-05-03 16:51:06,123][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5209, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.96, 'precision': 0.9230769230769231, 'recall': 1.0}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.6007Epoch 6/15: [                              ] 2/75 batches, loss: 0.5968Epoch 6/15: [=                             ] 3/75 batches, loss: 0.5742Epoch 6/15: [=                             ] 4/75 batches, loss: 0.5814Epoch 6/15: [==                            ] 5/75 batches, loss: 0.5729Epoch 6/15: [==                            ] 6/75 batches, loss: 0.5500Epoch 6/15: [==                            ] 7/75 batches, loss: 0.5381Epoch 6/15: [===                           ] 8/75 batches, loss: 0.5308Epoch 6/15: [===                           ] 9/75 batches, loss: 0.5305Epoch 6/15: [====                          ] 10/75 batches, loss: 0.5321Epoch 6/15: [====                          ] 11/75 batches, loss: 0.5305Epoch 6/15: [====                          ] 12/75 batches, loss: 0.5272Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.5273Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.5275Epoch 6/15: [======                        ] 15/75 batches, loss: 0.5258Epoch 6/15: [======                        ] 16/75 batches, loss: 0.5260Epoch 6/15: [======                        ] 17/75 batches, loss: 0.5234Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.5227Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.5195Epoch 6/15: [========                      ] 20/75 batches, loss: 0.5164Epoch 6/15: [========                      ] 21/75 batches, loss: 0.5191Epoch 6/15: [========                      ] 22/75 batches, loss: 0.5201Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.5207Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.5201Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.5186Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.5182Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.5218Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.5227Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.5216Epoch 6/15: [============                  ] 30/75 batches, loss: 0.5204Epoch 6/15: [============                  ] 31/75 batches, loss: 0.5222Epoch 6/15: [============                  ] 32/75 batches, loss: 0.5232Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.5224Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.5227Epoch 6/15: [==============                ] 35/75 batches, loss: 0.5228Epoch 6/15: [==============                ] 36/75 batches, loss: 0.5237Epoch 6/15: [==============                ] 37/75 batches, loss: 0.5244Epoch 6/15: [===============               ] 38/75 batches, loss: 0.5233Epoch 6/15: [===============               ] 39/75 batches, loss: 0.5251Epoch 6/15: [================              ] 40/75 batches, loss: 0.5262Epoch 6/15: [================              ] 41/75 batches, loss: 0.5245Epoch 6/15: [================              ] 42/75 batches, loss: 0.5260Epoch 6/15: [=================             ] 43/75 batches, loss: 0.5261Epoch 6/15: [=================             ] 44/75 batches, loss: 0.5239Epoch 6/15: [==================            ] 45/75 batches, loss: 0.5230Epoch 6/15: [==================            ] 46/75 batches, loss: 0.5239Epoch 6/15: [==================            ] 47/75 batches, loss: 0.5235Epoch 6/15: [===================           ] 48/75 batches, loss: 0.5232Epoch 6/15: [===================           ] 49/75 batches, loss: 0.5232Epoch 6/15: [====================          ] 50/75 batches, loss: 0.5232Epoch 6/15: [====================          ] 51/75 batches, loss: 0.5215Epoch 6/15: [====================          ] 52/75 batches, loss: 0.5202Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.5189Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.5185Epoch 6/15: [======================        ] 55/75 batches, loss: 0.5189Epoch 6/15: [======================        ] 56/75 batches, loss: 0.5183Epoch 6/15: [======================        ] 57/75 batches, loss: 0.5185Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.5190Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.5177Epoch 6/15: [========================      ] 60/75 batches, loss: 0.5175Epoch 6/15: [========================      ] 61/75 batches, loss: 0.5177Epoch 6/15: [========================      ] 62/75 batches, loss: 0.5176Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.5180Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.5175Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.5180Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.5182Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.5177Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.5193Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.5186Epoch 6/15: [============================  ] 70/75 batches, loss: 0.5181Epoch 6/15: [============================  ] 71/75 batches, loss: 0.5182Epoch 6/15: [============================  ] 72/75 batches, loss: 0.5199Epoch 6/15: [============================= ] 73/75 batches, loss: 0.5195Epoch 6/15: [============================= ] 74/75 batches, loss: 0.5195Epoch 6/15: [==============================] 75/75 batches, loss: 0.5175
[2025-05-03 16:51:08,862][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5175
[2025-05-03 16:51:09,255][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5177, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9795918367346939, 'precision': 0.96, 'recall': 1.0}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.5121Epoch 7/15: [                              ] 2/75 batches, loss: 0.4723Epoch 7/15: [=                             ] 3/75 batches, loss: 0.5013Epoch 7/15: [=                             ] 4/75 batches, loss: 0.4859Epoch 7/15: [==                            ] 5/75 batches, loss: 0.4707Epoch 7/15: [==                            ] 6/75 batches, loss: 0.4683Epoch 7/15: [==                            ] 7/75 batches, loss: 0.4679Epoch 7/15: [===                           ] 8/75 batches, loss: 0.4740Epoch 7/15: [===                           ] 9/75 batches, loss: 0.4827Epoch 7/15: [====                          ] 10/75 batches, loss: 0.4805Epoch 7/15: [====                          ] 11/75 batches, loss: 0.4783Epoch 7/15: [====                          ] 12/75 batches, loss: 0.4835Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.4850Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.4902Epoch 7/15: [======                        ] 15/75 batches, loss: 0.4897Epoch 7/15: [======                        ] 16/75 batches, loss: 0.4878Epoch 7/15: [======                        ] 17/75 batches, loss: 0.4894Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.4899Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.4932Epoch 7/15: [========                      ] 20/75 batches, loss: 0.4974Epoch 7/15: [========                      ] 21/75 batches, loss: 0.4979Epoch 7/15: [========                      ] 22/75 batches, loss: 0.4994Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.5029Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.5025Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.5035Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.5063Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.5098Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.5105Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.5095Epoch 7/15: [============                  ] 30/75 batches, loss: 0.5079Epoch 7/15: [============                  ] 31/75 batches, loss: 0.5102Epoch 7/15: [============                  ] 32/75 batches, loss: 0.5100Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.5098Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.5118Epoch 7/15: [==============                ] 35/75 batches, loss: 0.5122Epoch 7/15: [==============                ] 36/75 batches, loss: 0.5113Epoch 7/15: [==============                ] 37/75 batches, loss: 0.5101Epoch 7/15: [===============               ] 38/75 batches, loss: 0.5118Epoch 7/15: [===============               ] 39/75 batches, loss: 0.5118Epoch 7/15: [================              ] 40/75 batches, loss: 0.5120Epoch 7/15: [================              ] 41/75 batches, loss: 0.5110Epoch 7/15: [================              ] 42/75 batches, loss: 0.5106Epoch 7/15: [=================             ] 43/75 batches, loss: 0.5125Epoch 7/15: [=================             ] 44/75 batches, loss: 0.5140Epoch 7/15: [==================            ] 45/75 batches, loss: 0.5146Epoch 7/15: [==================            ] 46/75 batches, loss: 0.5134Epoch 7/15: [==================            ] 47/75 batches, loss: 0.5136Epoch 7/15: [===================           ] 48/75 batches, loss: 0.5135Epoch 7/15: [===================           ] 49/75 batches, loss: 0.5155Epoch 7/15: [====================          ] 50/75 batches, loss: 0.5159Epoch 7/15: [====================          ] 51/75 batches, loss: 0.5161Epoch 7/15: [====================          ] 52/75 batches, loss: 0.5156Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.5150Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.5153Epoch 7/15: [======================        ] 55/75 batches, loss: 0.5166Epoch 7/15: [======================        ] 56/75 batches, loss: 0.5168Epoch 7/15: [======================        ] 57/75 batches, loss: 0.5166Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.5178Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.5173Epoch 7/15: [========================      ] 60/75 batches, loss: 0.5191Epoch 7/15: [========================      ] 61/75 batches, loss: 0.5181Epoch 7/15: [========================      ] 62/75 batches, loss: 0.5172Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.5193Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.5205Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.5213Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.5199Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.5183Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.5175Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.5169Epoch 7/15: [============================  ] 70/75 batches, loss: 0.5161Epoch 7/15: [============================  ] 71/75 batches, loss: 0.5160Epoch 7/15: [============================  ] 72/75 batches, loss: 0.5155Epoch 7/15: [============================= ] 73/75 batches, loss: 0.5151Epoch 7/15: [============================= ] 74/75 batches, loss: 0.5153Epoch 7/15: [==============================] 75/75 batches, loss: 0.5141
[2025-05-03 16:51:12,077][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5141
[2025-05-03 16:51:12,417][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5161, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9795918367346939, 'precision': 0.96, 'recall': 1.0}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.5542Epoch 8/15: [                              ] 2/75 batches, loss: 0.5294Epoch 8/15: [=                             ] 3/75 batches, loss: 0.5457Epoch 8/15: [=                             ] 4/75 batches, loss: 0.5365Epoch 8/15: [==                            ] 5/75 batches, loss: 0.5411Epoch 8/15: [==                            ] 6/75 batches, loss: 0.5395Epoch 8/15: [==                            ] 7/75 batches, loss: 0.5310Epoch 8/15: [===                           ] 8/75 batches, loss: 0.5298Epoch 8/15: [===                           ] 9/75 batches, loss: 0.5167Epoch 8/15: [====                          ] 10/75 batches, loss: 0.5178Epoch 8/15: [====                          ] 11/75 batches, loss: 0.5192Epoch 8/15: [====                          ] 12/75 batches, loss: 0.5140Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.5097Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.5049Epoch 8/15: [======                        ] 15/75 batches, loss: 0.5054Epoch 8/15: [======                        ] 16/75 batches, loss: 0.5084Epoch 8/15: [======                        ] 17/75 batches, loss: 0.5082Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.5120Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.5079Epoch 8/15: [========                      ] 20/75 batches, loss: 0.5077Epoch 8/15: [========                      ] 21/75 batches, loss: 0.5089Epoch 8/15: [========                      ] 22/75 batches, loss: 0.5065Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.5055Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.5055Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.5060Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.5053Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.5072Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.5038Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.5031Epoch 8/15: [============                  ] 30/75 batches, loss: 0.5042Epoch 8/15: [============                  ] 31/75 batches, loss: 0.5064Epoch 8/15: [============                  ] 32/75 batches, loss: 0.5051Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.5072Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.5091Epoch 8/15: [==============                ] 35/75 batches, loss: 0.5101Epoch 8/15: [==============                ] 36/75 batches, loss: 0.5100Epoch 8/15: [==============                ] 37/75 batches, loss: 0.5073Epoch 8/15: [===============               ] 38/75 batches, loss: 0.5079Epoch 8/15: [===============               ] 39/75 batches, loss: 0.5059Epoch 8/15: [================              ] 40/75 batches, loss: 0.5060Epoch 8/15: [================              ] 41/75 batches, loss: 0.5059Epoch 8/15: [================              ] 42/75 batches, loss: 0.5076Epoch 8/15: [=================             ] 43/75 batches, loss: 0.5085Epoch 8/15: [=================             ] 44/75 batches, loss: 0.5084Epoch 8/15: [==================            ] 45/75 batches, loss: 0.5073Epoch 8/15: [==================            ] 46/75 batches, loss: 0.5064Epoch 8/15: [==================            ] 47/75 batches, loss: 0.5076Epoch 8/15: [===================           ] 48/75 batches, loss: 0.5099Epoch 8/15: [===================           ] 49/75 batches, loss: 0.5108Epoch 8/15: [====================          ] 50/75 batches, loss: 0.5105Epoch 8/15: [====================          ] 51/75 batches, loss: 0.5090Epoch 8/15: [====================          ] 52/75 batches, loss: 0.5104Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.5112Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.5119Epoch 8/15: [======================        ] 55/75 batches, loss: 0.5127Epoch 8/15: [======================        ] 56/75 batches, loss: 0.5121Epoch 8/15: [======================        ] 57/75 batches, loss: 0.5139Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.5129Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.5131Epoch 8/15: [========================      ] 60/75 batches, loss: 0.5126Epoch 8/15: [========================      ] 61/75 batches, loss: 0.5121Epoch 8/15: [========================      ] 62/75 batches, loss: 0.5127Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.5126Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.5121Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.5126Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.5110Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.5109Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.5112Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.5118Epoch 8/15: [============================  ] 70/75 batches, loss: 0.5119Epoch 8/15: [============================  ] 71/75 batches, loss: 0.5124Epoch 8/15: [============================  ] 72/75 batches, loss: 0.5126Epoch 8/15: [============================= ] 73/75 batches, loss: 0.5122Epoch 8/15: [============================= ] 74/75 batches, loss: 0.5124Epoch 8/15: [==============================] 75/75 batches, loss: 0.5127
[2025-05-03 16:51:15,247][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5127
[2025-05-03 16:51:15,617][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5149, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9795918367346939, 'precision': 0.96, 'recall': 1.0}
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.5033Epoch 9/15: [                              ] 2/75 batches, loss: 0.5321Epoch 9/15: [=                             ] 3/75 batches, loss: 0.5413Epoch 9/15: [=                             ] 4/75 batches, loss: 0.5442Epoch 9/15: [==                            ] 5/75 batches, loss: 0.5229Epoch 9/15: [==                            ] 6/75 batches, loss: 0.5276Epoch 9/15: [==                            ] 7/75 batches, loss: 0.5276Epoch 9/15: [===                           ] 8/75 batches, loss: 0.5193Epoch 9/15: [===                           ] 9/75 batches, loss: 0.5301Epoch 9/15: [====                          ] 10/75 batches, loss: 0.5319Epoch 9/15: [====                          ] 11/75 batches, loss: 0.5185Epoch 9/15: [====                          ] 12/75 batches, loss: 0.5252Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.5225Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.5195Epoch 9/15: [======                        ] 15/75 batches, loss: 0.5184Epoch 9/15: [======                        ] 16/75 batches, loss: 0.5138Epoch 9/15: [======                        ] 17/75 batches, loss: 0.5132Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.5147Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.5141Epoch 9/15: [========                      ] 20/75 batches, loss: 0.5162Epoch 9/15: [========                      ] 21/75 batches, loss: 0.5173Epoch 9/15: [========                      ] 22/75 batches, loss: 0.5201Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.5215Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.5190Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.5202Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.5197Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.5217Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.5194Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.5235Epoch 9/15: [============                  ] 30/75 batches, loss: 0.5247Epoch 9/15: [============                  ] 31/75 batches, loss: 0.5242Epoch 9/15: [============                  ] 32/75 batches, loss: 0.5251Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.5259Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.5253Epoch 9/15: [==============                ] 35/75 batches, loss: 0.5248Epoch 9/15: [==============                ] 36/75 batches, loss: 0.5239Epoch 9/15: [==============                ] 37/75 batches, loss: 0.5215Epoch 9/15: [===============               ] 38/75 batches, loss: 0.5216Epoch 9/15: [===============               ] 39/75 batches, loss: 0.5218Epoch 9/15: [================              ] 40/75 batches, loss: 0.5202Epoch 9/15: [================              ] 41/75 batches, loss: 0.5204Epoch 9/15: [================              ] 42/75 batches, loss: 0.5179Epoch 9/15: [=================             ] 43/75 batches, loss: 0.5181Epoch 9/15: [=================             ] 44/75 batches, loss: 0.5167Epoch 9/15: [==================            ] 45/75 batches, loss: 0.5159Epoch 9/15: [==================            ] 46/75 batches, loss: 0.5157Epoch 9/15: [==================            ] 47/75 batches, loss: 0.5166Epoch 9/15: [===================           ] 48/75 batches, loss: 0.5167Epoch 9/15: [===================           ] 49/75 batches, loss: 0.5177Epoch 9/15: [====================          ] 50/75 batches, loss: 0.5170Epoch 9/15: [====================          ] 51/75 batches, loss: 0.5174Epoch 9/15: [====================          ] 52/75 batches, loss: 0.5182Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.5179Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.5182Epoch 9/15: [======================        ] 55/75 batches, loss: 0.5183Epoch 9/15: [======================        ] 56/75 batches, loss: 0.5186Epoch 9/15: [======================        ] 57/75 batches, loss: 0.5177Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.5167Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.5169Epoch 9/15: [========================      ] 60/75 batches, loss: 0.5167Epoch 9/15: [========================      ] 61/75 batches, loss: 0.5177Epoch 9/15: [========================      ] 62/75 batches, loss: 0.5175Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.5169Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.5161Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.5163Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.5147Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.5142Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.5137Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.5144Epoch 9/15: [============================  ] 70/75 batches, loss: 0.5131Epoch 9/15: [============================  ] 71/75 batches, loss: 0.5123Epoch 9/15: [============================  ] 72/75 batches, loss: 0.5122Epoch 9/15: [============================= ] 73/75 batches, loss: 0.5114Epoch 9/15: [============================= ] 74/75 batches, loss: 0.5110Epoch 9/15: [==============================] 75/75 batches, loss: 0.5107
[2025-05-03 16:51:18,419][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5107
[2025-05-03 16:51:18,785][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.5199, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.96, 'precision': 0.9230769230769231, 'recall': 1.0}
[2025-05-03 16:51:18,786][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.4400Epoch 10/15: [                              ] 2/75 batches, loss: 0.4364Epoch 10/15: [=                             ] 3/75 batches, loss: 0.4599Epoch 10/15: [=                             ] 4/75 batches, loss: 0.4707Epoch 10/15: [==                            ] 5/75 batches, loss: 0.4727Epoch 10/15: [==                            ] 6/75 batches, loss: 0.4911Epoch 10/15: [==                            ] 7/75 batches, loss: 0.4996Epoch 10/15: [===                           ] 8/75 batches, loss: 0.5062Epoch 10/15: [===                           ] 9/75 batches, loss: 0.5006Epoch 10/15: [====                          ] 10/75 batches, loss: 0.5011Epoch 10/15: [====                          ] 11/75 batches, loss: 0.5056Epoch 10/15: [====                          ] 12/75 batches, loss: 0.5116Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.5111Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.5099Epoch 10/15: [======                        ] 15/75 batches, loss: 0.5079Epoch 10/15: [======                        ] 16/75 batches, loss: 0.5063Epoch 10/15: [======                        ] 17/75 batches, loss: 0.5048Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.5023Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.5026Epoch 10/15: [========                      ] 20/75 batches, loss: 0.5051Epoch 10/15: [========                      ] 21/75 batches, loss: 0.5087Epoch 10/15: [========                      ] 22/75 batches, loss: 0.5086Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.5085Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.5103Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.5144Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.5149Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.5145Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.5108Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.5123Epoch 10/15: [============                  ] 30/75 batches, loss: 0.5169Epoch 10/15: [============                  ] 31/75 batches, loss: 0.5144Epoch 10/15: [============                  ] 32/75 batches, loss: 0.5140Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.5137Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.5155Epoch 10/15: [==============                ] 35/75 batches, loss: 0.5164Epoch 10/15: [==============                ] 36/75 batches, loss: 0.5169Epoch 10/15: [==============                ] 37/75 batches, loss: 0.5153Epoch 10/15: [===============               ] 38/75 batches, loss: 0.5150Epoch 10/15: [===============               ] 39/75 batches, loss: 0.5147Epoch 10/15: [================              ] 40/75 batches, loss: 0.5150Epoch 10/15: [================              ] 41/75 batches, loss: 0.5153Epoch 10/15: [================              ] 42/75 batches, loss: 0.5139Epoch 10/15: [=================             ] 43/75 batches, loss: 0.5126Epoch 10/15: [=================             ] 44/75 batches, loss: 0.5129Epoch 10/15: [==================            ] 45/75 batches, loss: 0.5118Epoch 10/15: [==================            ] 46/75 batches, loss: 0.5129Epoch 10/15: [==================            ] 47/75 batches, loss: 0.5128Epoch 10/15: [===================           ] 48/75 batches, loss: 0.5126Epoch 10/15: [===================           ] 49/75 batches, loss: 0.5126Epoch 10/15: [====================          ] 50/75 batches, loss: 0.5129Epoch 10/15: [====================          ] 51/75 batches, loss: 0.5128Epoch 10/15: [====================          ] 52/75 batches, loss: 0.5128Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.5135Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.5121Epoch 10/15: [======================        ] 55/75 batches, loss: 0.5119Epoch 10/15: [======================        ] 56/75 batches, loss: 0.5117Epoch 10/15: [======================        ] 57/75 batches, loss: 0.5125Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.5109Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.5104Epoch 10/15: [========================      ] 60/75 batches, loss: 0.5095Epoch 10/15: [========================      ] 61/75 batches, loss: 0.5099Epoch 10/15: [========================      ] 62/75 batches, loss: 0.5118Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.5117Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.5113Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.5112Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.5104Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.5108Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.5103Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.5092Epoch 10/15: [============================  ] 70/75 batches, loss: 0.5101Epoch 10/15: [============================  ] 71/75 batches, loss: 0.5099Epoch 10/15: [============================  ] 72/75 batches, loss: 0.5112Epoch 10/15: [============================= ] 73/75 batches, loss: 0.5101Epoch 10/15: [============================= ] 74/75 batches, loss: 0.5106Epoch 10/15: [==============================] 75/75 batches, loss: 0.5116
[2025-05-03 16:51:21,275][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5116
[2025-05-03 16:51:21,607][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.5237, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.96, 'precision': 0.9230769230769231, 'recall': 1.0}
[2025-05-03 16:51:21,608][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.4516Epoch 11/15: [                              ] 2/75 batches, loss: 0.4966Epoch 11/15: [=                             ] 3/75 batches, loss: 0.5436Epoch 11/15: [=                             ] 4/75 batches, loss: 0.5394Epoch 11/15: [==                            ] 5/75 batches, loss: 0.5371Epoch 11/15: [==                            ] 6/75 batches, loss: 0.5239Epoch 11/15: [==                            ] 7/75 batches, loss: 0.5211Epoch 11/15: [===                           ] 8/75 batches, loss: 0.5129Epoch 11/15: [===                           ] 9/75 batches, loss: 0.5139Epoch 11/15: [====                          ] 10/75 batches, loss: 0.5106Epoch 11/15: [====                          ] 11/75 batches, loss: 0.5121Epoch 11/15: [====                          ] 12/75 batches, loss: 0.5133Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.5146Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.5195Epoch 11/15: [======                        ] 15/75 batches, loss: 0.5216Epoch 11/15: [======                        ] 16/75 batches, loss: 0.5161Epoch 11/15: [======                        ] 17/75 batches, loss: 0.5147Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.5156Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.5108Epoch 11/15: [========                      ] 20/75 batches, loss: 0.5119Epoch 11/15: [========                      ] 21/75 batches, loss: 0.5115Epoch 11/15: [========                      ] 22/75 batches, loss: 0.5122Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.5108Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.5047Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.5039Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.5079Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.5108Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.5140Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.5139Epoch 11/15: [============                  ] 30/75 batches, loss: 0.5143Epoch 11/15: [============                  ] 31/75 batches, loss: 0.5163Epoch 11/15: [============                  ] 32/75 batches, loss: 0.5162Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.5147Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.5160Epoch 11/15: [==============                ] 35/75 batches, loss: 0.5151Epoch 11/15: [==============                ] 36/75 batches, loss: 0.5145Epoch 11/15: [==============                ] 37/75 batches, loss: 0.5124Epoch 11/15: [===============               ] 38/75 batches, loss: 0.5123Epoch 11/15: [===============               ] 39/75 batches, loss: 0.5114Epoch 11/15: [================              ] 40/75 batches, loss: 0.5124Epoch 11/15: [================              ] 41/75 batches, loss: 0.5110Epoch 11/15: [================              ] 42/75 batches, loss: 0.5125Epoch 11/15: [=================             ] 43/75 batches, loss: 0.5124Epoch 11/15: [=================             ] 44/75 batches, loss: 0.5100Epoch 11/15: [==================            ] 45/75 batches, loss: 0.5098Epoch 11/15: [==================            ] 46/75 batches, loss: 0.5094Epoch 11/15: [==================            ] 47/75 batches, loss: 0.5093Epoch 11/15: [===================           ] 48/75 batches, loss: 0.5082Epoch 11/15: [===================           ] 49/75 batches, loss: 0.5081Epoch 11/15: [====================          ] 50/75 batches, loss: 0.5089Epoch 11/15: [====================          ] 51/75 batches, loss: 0.5080Epoch 11/15: [====================          ] 52/75 batches, loss: 0.5090Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.5103Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.5098Epoch 11/15: [======================        ] 55/75 batches, loss: 0.5088Epoch 11/15: [======================        ] 56/75 batches, loss: 0.5079Epoch 11/15: [======================        ] 57/75 batches, loss: 0.5090Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.5077Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.5076Epoch 11/15: [========================      ] 60/75 batches, loss: 0.5078Epoch 11/15: [========================      ] 61/75 batches, loss: 0.5099Epoch 11/15: [========================      ] 62/75 batches, loss: 0.5106Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.5109Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.5115Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.5110Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.5109Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.5101Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.5105Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.5100Epoch 11/15: [============================  ] 70/75 batches, loss: 0.5096Epoch 11/15: [============================  ] 71/75 batches, loss: 0.5096Epoch 11/15: [============================  ] 72/75 batches, loss: 0.5095Epoch 11/15: [============================= ] 73/75 batches, loss: 0.5103Epoch 11/15: [============================= ] 74/75 batches, loss: 0.5112Epoch 11/15: [==============================] 75/75 batches, loss: 0.5114
[2025-05-03 16:51:24,049][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5114
[2025-05-03 16:51:24,444][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.5117, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9795918367346939, 'precision': 0.96, 'recall': 1.0}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.4582Epoch 12/15: [                              ] 2/75 batches, loss: 0.4218Epoch 12/15: [=                             ] 3/75 batches, loss: 0.4419Epoch 12/15: [=                             ] 4/75 batches, loss: 0.4701Epoch 12/15: [==                            ] 5/75 batches, loss: 0.4863Epoch 12/15: [==                            ] 6/75 batches, loss: 0.4814Epoch 12/15: [==                            ] 7/75 batches, loss: 0.5052Epoch 12/15: [===                           ] 8/75 batches, loss: 0.5042Epoch 12/15: [===                           ] 9/75 batches, loss: 0.5081Epoch 12/15: [====                          ] 10/75 batches, loss: 0.5101Epoch 12/15: [====                          ] 11/75 batches, loss: 0.5095Epoch 12/15: [====                          ] 12/75 batches, loss: 0.5167Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.5175Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.5134Epoch 12/15: [======                        ] 15/75 batches, loss: 0.5176Epoch 12/15: [======                        ] 16/75 batches, loss: 0.5177Epoch 12/15: [======                        ] 17/75 batches, loss: 0.5155Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.5122Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.5131Epoch 12/15: [========                      ] 20/75 batches, loss: 0.5115Epoch 12/15: [========                      ] 21/75 batches, loss: 0.5112Epoch 12/15: [========                      ] 22/75 batches, loss: 0.5079Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.5100Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.5129Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.5135Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.5104Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.5119Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.5107Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.5105Epoch 12/15: [============                  ] 30/75 batches, loss: 0.5106Epoch 12/15: [============                  ] 31/75 batches, loss: 0.5149Epoch 12/15: [============                  ] 32/75 batches, loss: 0.5131Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.5137Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.5121Epoch 12/15: [==============                ] 35/75 batches, loss: 0.5138Epoch 12/15: [==============                ] 36/75 batches, loss: 0.5109Epoch 12/15: [==============                ] 37/75 batches, loss: 0.5097Epoch 12/15: [===============               ] 38/75 batches, loss: 0.5117Epoch 12/15: [===============               ] 39/75 batches, loss: 0.5117Epoch 12/15: [================              ] 40/75 batches, loss: 0.5121Epoch 12/15: [================              ] 41/75 batches, loss: 0.5119Epoch 12/15: [================              ] 42/75 batches, loss: 0.5118Epoch 12/15: [=================             ] 43/75 batches, loss: 0.5111Epoch 12/15: [=================             ] 44/75 batches, loss: 0.5106Epoch 12/15: [==================            ] 45/75 batches, loss: 0.5095Epoch 12/15: [==================            ] 46/75 batches, loss: 0.5073Epoch 12/15: [==================            ] 47/75 batches, loss: 0.5072Epoch 12/15: [===================           ] 48/75 batches, loss: 0.5066Epoch 12/15: [===================           ] 49/75 batches, loss: 0.5102Epoch 12/15: [====================          ] 50/75 batches, loss: 0.5109Epoch 12/15: [====================          ] 51/75 batches, loss: 0.5103Epoch 12/15: [====================          ] 52/75 batches, loss: 0.5111Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.5110Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.5114Epoch 12/15: [======================        ] 55/75 batches, loss: 0.5097Epoch 12/15: [======================        ] 56/75 batches, loss: 0.5096Epoch 12/15: [======================        ] 57/75 batches, loss: 0.5091Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.5082Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.5074Epoch 12/15: [========================      ] 60/75 batches, loss: 0.5089Epoch 12/15: [========================      ] 61/75 batches, loss: 0.5106Epoch 12/15: [========================      ] 62/75 batches, loss: 0.5111Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.5106Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.5098Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.5104Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.5109Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.5104Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.5093Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.5099Epoch 12/15: [============================  ] 70/75 batches, loss: 0.5105Epoch 12/15: [============================  ] 71/75 batches, loss: 0.5118Epoch 12/15: [============================  ] 72/75 batches, loss: 0.5107Epoch 12/15: [============================= ] 73/75 batches, loss: 0.5106Epoch 12/15: [============================= ] 74/75 batches, loss: 0.5108Epoch 12/15: [==============================] 75/75 batches, loss: 0.5111
[2025-05-03 16:51:27,260][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.5111
[2025-05-03 16:51:27,623][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.5128, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9795918367346939, 'precision': 0.96, 'recall': 1.0}
[2025-05-03 16:51:27,624][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.5033Epoch 13/15: [                              ] 2/75 batches, loss: 0.5172Epoch 13/15: [=                             ] 3/75 batches, loss: 0.4824Epoch 13/15: [=                             ] 4/75 batches, loss: 0.4639Epoch 13/15: [==                            ] 5/75 batches, loss: 0.4893Epoch 13/15: [==                            ] 6/75 batches, loss: 0.5085Epoch 13/15: [==                            ] 7/75 batches, loss: 0.5223Epoch 13/15: [===                           ] 8/75 batches, loss: 0.5203Epoch 13/15: [===                           ] 9/75 batches, loss: 0.5131Epoch 13/15: [====                          ] 10/75 batches, loss: 0.5193Epoch 13/15: [====                          ] 11/75 batches, loss: 0.5157Epoch 13/15: [====                          ] 12/75 batches, loss: 0.5128Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.5161Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.5207Epoch 13/15: [======                        ] 15/75 batches, loss: 0.5179Epoch 13/15: [======                        ] 16/75 batches, loss: 0.5178Epoch 13/15: [======                        ] 17/75 batches, loss: 0.5198Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.5212Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.5242Epoch 13/15: [========                      ] 20/75 batches, loss: 0.5239Epoch 13/15: [========                      ] 21/75 batches, loss: 0.5227Epoch 13/15: [========                      ] 22/75 batches, loss: 0.5231Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.5243Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.5244Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.5217Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.5238Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.5232Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.5224Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.5225Epoch 13/15: [============                  ] 30/75 batches, loss: 0.5212Epoch 13/15: [============                  ] 31/75 batches, loss: 0.5199Epoch 13/15: [============                  ] 32/75 batches, loss: 0.5195Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.5212Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.5185Epoch 13/15: [==============                ] 35/75 batches, loss: 0.5185Epoch 13/15: [==============                ] 36/75 batches, loss: 0.5153Epoch 13/15: [==============                ] 37/75 batches, loss: 0.5105Epoch 13/15: [===============               ] 38/75 batches, loss: 0.5097Epoch 13/15: [===============               ] 39/75 batches, loss: 0.5105Epoch 13/15: [================              ] 40/75 batches, loss: 0.5121Epoch 13/15: [================              ] 41/75 batches, loss: 0.5101Epoch 13/15: [================              ] 42/75 batches, loss: 0.5111Epoch 13/15: [=================             ] 43/75 batches, loss: 0.5124Epoch 13/15: [=================             ] 44/75 batches, loss: 0.5111Epoch 13/15: [==================            ] 45/75 batches, loss: 0.5115Epoch 13/15: [==================            ] 46/75 batches, loss: 0.5130Epoch 13/15: [==================            ] 47/75 batches, loss: 0.5128Epoch 13/15: [===================           ] 48/75 batches, loss: 0.5146Epoch 13/15: [===================           ] 49/75 batches, loss: 0.5149Epoch 13/15: [====================          ] 50/75 batches, loss: 0.5142Epoch 13/15: [====================          ] 51/75 batches, loss: 0.5144Epoch 13/15: [====================          ] 52/75 batches, loss: 0.5151Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.5153Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.5140Epoch 13/15: [======================        ] 55/75 batches, loss: 0.5134Epoch 13/15: [======================        ] 56/75 batches, loss: 0.5124Epoch 13/15: [======================        ] 57/75 batches, loss: 0.5115Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.5114Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.5136Epoch 13/15: [========================      ] 60/75 batches, loss: 0.5124Epoch 13/15: [========================      ] 61/75 batches, loss: 0.5119Epoch 13/15: [========================      ] 62/75 batches, loss: 0.5114Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.5125Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.5124Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.5116Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.5130Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.5125Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.5128Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.5134Epoch 13/15: [============================  ] 70/75 batches, loss: 0.5122Epoch 13/15: [============================  ] 71/75 batches, loss: 0.5121Epoch 13/15: [============================  ] 72/75 batches, loss: 0.5120Epoch 13/15: [============================= ] 73/75 batches, loss: 0.5123Epoch 13/15: [============================= ] 74/75 batches, loss: 0.5112Epoch 13/15: [==============================] 75/75 batches, loss: 0.5100
[2025-05-03 16:51:30,109][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.5100
[2025-05-03 16:51:30,331][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.5107, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9795918367346939, 'precision': 0.96, 'recall': 1.0}
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.4565Epoch 14/15: [                              ] 2/75 batches, loss: 0.4680Epoch 14/15: [=                             ] 3/75 batches, loss: 0.4970Epoch 14/15: [=                             ] 4/75 batches, loss: 0.4698Epoch 14/15: [==                            ] 5/75 batches, loss: 0.4767Epoch 14/15: [==                            ] 6/75 batches, loss: 0.4852Epoch 14/15: [==                            ] 7/75 batches, loss: 0.4849Epoch 14/15: [===                           ] 8/75 batches, loss: 0.4902Epoch 14/15: [===                           ] 9/75 batches, loss: 0.4900Epoch 14/15: [====                          ] 10/75 batches, loss: 0.4890Epoch 14/15: [====                          ] 11/75 batches, loss: 0.4930Epoch 14/15: [====                          ] 12/75 batches, loss: 0.4939Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.4948Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.5030Epoch 14/15: [======                        ] 15/75 batches, loss: 0.5062Epoch 14/15: [======                        ] 16/75 batches, loss: 0.5011Epoch 14/15: [======                        ] 17/75 batches, loss: 0.4984Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.4961Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.4965Epoch 14/15: [========                      ] 20/75 batches, loss: 0.4969Epoch 14/15: [========                      ] 21/75 batches, loss: 0.4970Epoch 14/15: [========                      ] 22/75 batches, loss: 0.4973Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.5015Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.5052Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.5032Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.5043Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.5061Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.5042Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.5035Epoch 14/15: [============                  ] 30/75 batches, loss: 0.5044Epoch 14/15: [============                  ] 31/75 batches, loss: 0.5021Epoch 14/15: [============                  ] 32/75 batches, loss: 0.5024Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.5018Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.5050Epoch 14/15: [==============                ] 35/75 batches, loss: 0.5031Epoch 14/15: [==============                ] 36/75 batches, loss: 0.5051Epoch 14/15: [==============                ] 37/75 batches, loss: 0.5039Epoch 14/15: [===============               ] 38/75 batches, loss: 0.5032Epoch 14/15: [===============               ] 39/75 batches, loss: 0.5039Epoch 14/15: [================              ] 40/75 batches, loss: 0.5051Epoch 14/15: [================              ] 41/75 batches, loss: 0.5066Epoch 14/15: [================              ] 42/75 batches, loss: 0.5083Epoch 14/15: [=================             ] 43/75 batches, loss: 0.5097Epoch 14/15: [=================             ] 44/75 batches, loss: 0.5097Epoch 14/15: [==================            ] 45/75 batches, loss: 0.5102Epoch 14/15: [==================            ] 46/75 batches, loss: 0.5106Epoch 14/15: [==================            ] 47/75 batches, loss: 0.5120Epoch 14/15: [===================           ] 48/75 batches, loss: 0.5119Epoch 14/15: [===================           ] 49/75 batches, loss: 0.5127Epoch 14/15: [====================          ] 50/75 batches, loss: 0.5139Epoch 14/15: [====================          ] 51/75 batches, loss: 0.5133Epoch 14/15: [====================          ] 52/75 batches, loss: 0.5126Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.5125Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.5123Epoch 14/15: [======================        ] 55/75 batches, loss: 0.5130Epoch 14/15: [======================        ] 56/75 batches, loss: 0.5124Epoch 14/15: [======================        ] 57/75 batches, loss: 0.5114Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.5113Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.5122Epoch 14/15: [========================      ] 60/75 batches, loss: 0.5117Epoch 14/15: [========================      ] 61/75 batches, loss: 0.5108Epoch 14/15: [========================      ] 62/75 batches, loss: 0.5092Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.5087Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.5089Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.5081Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.5074Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.5088Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.5094Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.5097Epoch 14/15: [============================  ] 70/75 batches, loss: 0.5092Epoch 14/15: [============================  ] 71/75 batches, loss: 0.5087Epoch 14/15: [============================  ] 72/75 batches, loss: 0.5086Epoch 14/15: [============================= ] 73/75 batches, loss: 0.5089Epoch 14/15: [============================= ] 74/75 batches, loss: 0.5088Epoch 14/15: [==============================] 75/75 batches, loss: 0.5091
[2025-05-03 16:51:33,338][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.5091
[2025-05-03 16:51:33,683][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.5115, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9795918367346939, 'precision': 0.96, 'recall': 1.0}
[2025-05-03 16:51:33,684][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.5276Epoch 15/15: [                              ] 2/75 batches, loss: 0.5273Epoch 15/15: [=                             ] 3/75 batches, loss: 0.4962Epoch 15/15: [=                             ] 4/75 batches, loss: 0.4920Epoch 15/15: [==                            ] 5/75 batches, loss: 0.4800Epoch 15/15: [==                            ] 6/75 batches, loss: 0.4887Epoch 15/15: [==                            ] 7/75 batches, loss: 0.4881Epoch 15/15: [===                           ] 8/75 batches, loss: 0.4906Epoch 15/15: [===                           ] 9/75 batches, loss: 0.4953Epoch 15/15: [====                          ] 10/75 batches, loss: 0.4992Epoch 15/15: [====                          ] 11/75 batches, loss: 0.4974Epoch 15/15: [====                          ] 12/75 batches, loss: 0.5039Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.5077Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.5056Epoch 15/15: [======                        ] 15/75 batches, loss: 0.5041Epoch 15/15: [======                        ] 16/75 batches, loss: 0.5079Epoch 15/15: [======                        ] 17/75 batches, loss: 0.5106Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.5090Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.5087Epoch 15/15: [========                      ] 20/75 batches, loss: 0.5178Epoch 15/15: [========                      ] 21/75 batches, loss: 0.5194Epoch 15/15: [========                      ] 22/75 batches, loss: 0.5197Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.5180Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.5204Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.5208Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.5197Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.5208Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.5220Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.5223Epoch 15/15: [============                  ] 30/75 batches, loss: 0.5208Epoch 15/15: [============                  ] 31/75 batches, loss: 0.5219Epoch 15/15: [============                  ] 32/75 batches, loss: 0.5221Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.5201Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.5192Epoch 15/15: [==============                ] 35/75 batches, loss: 0.5170Epoch 15/15: [==============                ] 36/75 batches, loss: 0.5187Epoch 15/15: [==============                ] 37/75 batches, loss: 0.5182Epoch 15/15: [===============               ] 38/75 batches, loss: 0.5172Epoch 15/15: [===============               ] 39/75 batches, loss: 0.5151Epoch 15/15: [================              ] 40/75 batches, loss: 0.5142Epoch 15/15: [================              ] 41/75 batches, loss: 0.5152Epoch 15/15: [================              ] 42/75 batches, loss: 0.5138Epoch 15/15: [=================             ] 43/75 batches, loss: 0.5141Epoch 15/15: [=================             ] 44/75 batches, loss: 0.5139Epoch 15/15: [==================            ] 45/75 batches, loss: 0.5143Epoch 15/15: [==================            ] 46/75 batches, loss: 0.5158Epoch 15/15: [==================            ] 47/75 batches, loss: 0.5155Epoch 15/15: [===================           ] 48/75 batches, loss: 0.5148Epoch 15/15: [===================           ] 49/75 batches, loss: 0.5154Epoch 15/15: [====================          ] 50/75 batches, loss: 0.5150Epoch 15/15: [====================          ] 51/75 batches, loss: 0.5157Epoch 15/15: [====================          ] 52/75 batches, loss: 0.5154Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.5148Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.5141Epoch 15/15: [======================        ] 55/75 batches, loss: 0.5153Epoch 15/15: [======================        ] 56/75 batches, loss: 0.5146Epoch 15/15: [======================        ] 57/75 batches, loss: 0.5144Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.5147Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.5149Epoch 15/15: [========================      ] 60/75 batches, loss: 0.5121Epoch 15/15: [========================      ] 61/75 batches, loss: 0.5108Epoch 15/15: [========================      ] 62/75 batches, loss: 0.5105Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.5110Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.5109Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.5109Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.5101Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.5097Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.5092Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.5092Epoch 15/15: [============================  ] 70/75 batches, loss: 0.5087Epoch 15/15: [============================  ] 71/75 batches, loss: 0.5090Epoch 15/15: [============================  ] 72/75 batches, loss: 0.5086Epoch 15/15: [============================= ] 73/75 batches, loss: 0.5083Epoch 15/15: [============================= ] 74/75 batches, loss: 0.5091Epoch 15/15: [==============================] 75/75 batches, loss: 0.5103
[2025-05-03 16:51:36,000][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.5103
[2025-05-03 16:51:36,378][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.5289, Metrics: {'accuracy': 0.9347826086956522, 'f1': 0.9411764705882353, 'precision': 0.8888888888888888, 'recall': 1.0}
[2025-05-03 16:51:36,379][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
[2025-05-03 16:51:36,379][src.training.lm_trainer][INFO] - Training completed in 46.44 seconds
[2025-05-03 16:51:36,379][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-03 16:51:39,827][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9958018471872376, 'f1': 0.9958088851634534, 'precision': 0.9949748743718593, 'recall': 0.9966442953020134}
[2025-05-03 16:51:39,827][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9795918367346939, 'precision': 0.96, 'recall': 1.0}
[2025-05-03 16:51:39,828][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.6739130434782609, 'f1': 0.7857142857142857, 'precision': 0.6470588235294118, 'recall': 1.0}
[2025-05-03 16:51:41,454][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/ja/ja/model.pt
[2025-05-03 16:51:41,456][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▅▅▇▇█████
wandb:           best_val_f1 ▁▆▆▇▇█████
wandb:         best_val_loss █▄▃▂▂▂▁▁▁▁
wandb:    best_val_precision ▇▁█▅▅█████
wandb:       best_val_recall ▁█▅███████
wandb:                 epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▃▃▃▃▃▃▃▃▃▃▃▃
wandb:            train_loss █▄▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▅▅▇▇███▇▇████▆
wandb:                val_f1 ▁▆▆▇▇███▇▇████▇
wandb:              val_loss █▄▃▂▂▂▁▁▂▂▁▁▁▁▂
wandb:         val_precision ▇▁█▅▅███▅▅████▃
wandb:            val_recall ▁█▅████████████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.97826
wandb:           best_val_f1 0.97959
wandb:         best_val_loss 0.51074
wandb:    best_val_precision 0.96
wandb:       best_val_recall 1
wandb:                 epoch 15
wandb:   final_test_accuracy 0.67391
wandb:         final_test_f1 0.78571
wandb:  final_test_precision 0.64706
wandb:     final_test_recall 1
wandb:  final_train_accuracy 0.9958
wandb:        final_train_f1 0.99581
wandb: final_train_precision 0.99497
wandb:    final_train_recall 0.99664
wandb:    final_val_accuracy 0.97826
wandb:          final_val_f1 0.97959
wandb:   final_val_precision 0.96
wandb:      final_val_recall 1
wandb:         learning_rate 0.0001
wandb:            train_loss 0.51031
wandb:            train_time 46.44039
wandb:          val_accuracy 0.93478
wandb:                val_f1 0.94118
wandb:              val_loss 0.52892
wandb:         val_precision 0.88889
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_165021-ryybcb7s
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250503_165021-ryybcb7s/logs
Experiment probe_layer3_question_type_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/ja/ja/results.json for layer 3
Running experiment: probe_layer3_complexity_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=3"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer3_complexity_ja"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/layer3/ja"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-03 16:52:28,747][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/ja
experiment_name: probe_layer3_complexity_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-03 16:52:28,747][__main__][INFO] - Normalized task: complexity
[2025-05-03 16:52:28,747][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-03 16:52:28,747][__main__][INFO] - Determined Task Type: regression
[2025-05-03 16:52:28,752][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-05-03 16:52:28,752][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
slurmstepd: error: *** JOB 64442817 ON k28i22 CANCELLED AT 2025-05-03T16:52:33 DUE TO TIME LIMIT ***
