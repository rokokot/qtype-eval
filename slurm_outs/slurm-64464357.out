SLURM_JOB_ID: 64464357
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: qtype_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Wed May  7 13:39:48 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main probing experiments (non-control)...
=======================
PROBING LAYER 2
=======================
Experiment probe_layer2_complexity_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/fi/fi/results.json for layer 2
=======================
PROBING LAYER 10
=======================
Experiment probe_layer10_complexity_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer10/fi/fi/results.json for layer 10
Running control probing experiments...
=======================
PROBING LAYER 2 (CONTROL EXPERIMENTS)
=======================
Experiment probe_layer2_complexity_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_complexity_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_complexity_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/fi/fi/results.json for layer 2
=======================
PROBING LAYER 10 (CONTROL EXPERIMENTS)
=======================
Experiment probe_layer10_complexity_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer10/fi/fi/results.json for layer 10
Experiment probe_layer10_complexity_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer10/fi/fi/results.json for layer 10
Experiment probe_layer10_complexity_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer10/fi/fi/results.json for layer 10
Running submetric probing experiments...
=======================
PROBING LAYER 2 (SUBMETRIC EXPERIMENTS)
=======================
Experiment probe_layer2_avg_max_depth_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_avg_subordinate_chain_len_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/fi/fi/results.json for layer 2
=======================
PROBING LAYER 10 (SUBMETRIC EXPERIMENTS)
=======================
Experiment probe_layer10_avg_max_depth_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer10/fi/fi/results.json for layer 10
Experiment probe_layer10_avg_subordinate_chain_len_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer10/fi/fi/results.json for layer 10
Running control submetric probing experiments...
=======================
PROBING LAYER 2 (SUBMETRIC CONTROL EXPERIMENTS)
=======================
Running experiment: probe_layer2_avg_max_depth_control1_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_max_depth_control1_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:40:28,630][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer2/fi
experiment_name: probe_layer2_avg_max_depth_control1_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:40:28,630][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:40:28,630][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 13:40:28,630][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:40:28,630][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:40:28,635][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:40:28,635][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 13:40:28,635][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:40:32,230][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:40:34,489][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:40:34,489][src.data.datasets][INFO] - Loading 'control_avg_max_depth_seed1' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:40:34,759][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_max_depth_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:19:23 2025).
[2025-05-07 13:40:34,899][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_max_depth_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:19:23 2025).
[2025-05-07 13:40:35,289][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:40:35,297][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:40:35,298][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:40:35,309][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:40:35,445][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:40:35,582][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:40:35,600][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:40:35,602][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:40:35,602][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:40:35,603][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:40:35,684][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:40:35,775][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:40:35,794][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:40:35,796][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:40:35,796][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:40:35,798][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:40:35,798][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:40:35,798][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:40:35,798][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:40:35,798][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:40:35,799][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:40:35,799][src.data.datasets][INFO] -   Mean: 0.1934, Std: 0.1761
[2025-05-07 13:40:35,799][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:40:35,799][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 13:40:35,799][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:40:35,799][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:40:35,799][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:40:35,799][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:40:35,799][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-05-07 13:40:35,800][src.data.datasets][INFO] -   Mean: 0.2487, Std: 0.2137
[2025-05-07 13:40:35,800][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:40:35,800][src.data.datasets][INFO] - Sample label: 0.5
[2025-05-07 13:40:35,800][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:40:35,800][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:40:35,800][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:40:35,800][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:40:35,800][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-05-07 13:40:35,800][src.data.datasets][INFO] -   Mean: 0.1785, Std: 0.1746
[2025-05-07 13:40:35,800][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:40:35,801][src.data.datasets][INFO] - Sample label: 0.125
[2025-05-07 13:40:35,801][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:40:35,801][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:40:35,801][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:40:35,801][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 13:40:35,801][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:40:43,342][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:40:43,344][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:40:43,344][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 13:40:43,344][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:40:43,347][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:40:43,347][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:40:43,347][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:40:43,347][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:40:43,347][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:40:43,348][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:40:43,348][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6014Epoch 1/15: [                              ] 2/75 batches, loss: 0.6386Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5490Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5238Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4936Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4519Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4397Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4653Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4862Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4702Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4527Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4535Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4339Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4367Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4346Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4443Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4337Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4339Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4338Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4295Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4405Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4434Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4381Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4244Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4173Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4119Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4059Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4004Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3993Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4066Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4018Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3960Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3919Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3893Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3856Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3848Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3807Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3774Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3755Epoch 1/15: [================              ] 40/75 batches, loss: 0.3698Epoch 1/15: [================              ] 41/75 batches, loss: 0.3662Epoch 1/15: [================              ] 42/75 batches, loss: 0.3622Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3590Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3576Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3591Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3544Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3554Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3512Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3485Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3459Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3452Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3432Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3391Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3393Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3375Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3355Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3360Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3351Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3331Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3306Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3267Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3253Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3240Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3222Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3204Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3184Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3166Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3131Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3125Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3131Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3103Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3086Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3054Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3031Epoch 1/15: [==============================] 75/75 batches, loss: 0.3026
[2025-05-07 13:40:50,054][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3026
[2025-05-07 13:40:50,324][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0783, Metrics: {'mse': 0.0780661329627037, 'rmse': 0.2794031727856785, 'r2': -0.7090235948562622}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1823Epoch 2/15: [                              ] 2/75 batches, loss: 0.1577Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1248Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1631Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1556Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1462Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1534Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1535Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1508Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1601Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1534Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1513Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1479Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1433Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1455Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1432Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1434Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1419Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1424Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1437Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1416Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1420Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1459Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1436Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1458Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1522Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1513Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1489Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1499Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1505Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1482Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1465Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1448Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1460Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1471Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1467Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1470Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1456Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1463Epoch 2/15: [================              ] 40/75 batches, loss: 0.1463Epoch 2/15: [================              ] 41/75 batches, loss: 0.1451Epoch 2/15: [================              ] 42/75 batches, loss: 0.1458Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1442Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1419Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1426Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1447Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1449Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1452Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1493Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1482Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1470Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1485Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1491Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1488Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1478Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1483Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1474Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1466Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1458Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1451Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1450Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1442Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1432Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1424Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1424Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1421Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1418Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1420Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1413Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1403Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1395Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1401Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1412Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1402Epoch 2/15: [==============================] 75/75 batches, loss: 0.1398
[2025-05-07 13:40:53,034][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1398
[2025-05-07 13:40:53,263][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0812, Metrics: {'mse': 0.08107658475637436, 'rmse': 0.2847395033295773, 'r2': -0.7749284505844116}
[2025-05-07 13:40:53,263][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1520Epoch 3/15: [                              ] 2/75 batches, loss: 0.1212Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1186Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1101Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1111Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1141Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1143Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1057Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1017Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1053Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1081Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1133Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1164Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1158Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1140Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1168Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1184Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1187Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1179Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1156Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1170Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1158Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1142Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1163Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1148Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1130Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1126Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1121Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1124Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1116Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1119Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1125Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1120Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1131Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1115Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1098Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1089Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1090Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1085Epoch 3/15: [================              ] 40/75 batches, loss: 0.1081Epoch 3/15: [================              ] 41/75 batches, loss: 0.1079Epoch 3/15: [================              ] 42/75 batches, loss: 0.1069Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1056Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1056Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1053Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1051Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1052Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1073Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1079Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1069Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1059Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1057Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1049Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1064Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1056Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1048Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1040Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1035Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1027Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1027Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1020Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1012Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1018Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1022Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1015Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1009Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1023Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1018Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1021Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1032Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1032Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1030Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1028Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1026Epoch 3/15: [==============================] 75/75 batches, loss: 0.1030
[2025-05-07 13:40:55,524][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1030
[2025-05-07 13:40:55,844][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0632, Metrics: {'mse': 0.06307581812143326, 'rmse': 0.2511489958598944, 'r2': -0.3808556795120239}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1136Epoch 4/15: [                              ] 2/75 batches, loss: 0.0944Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0949Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1117Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1053Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1102Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1056Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1090Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1072Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1009Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1017Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0994Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0960Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0922Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0964Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0945Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0927Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0932Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0915Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0919Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0921Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0929Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0949Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0954Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1002Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0989Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0980Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0970Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0976Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0962Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0950Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0943Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0940Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0924Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0922Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0913Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0902Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0907Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0897Epoch 4/15: [================              ] 40/75 batches, loss: 0.0913Epoch 4/15: [================              ] 41/75 batches, loss: 0.0919Epoch 4/15: [================              ] 42/75 batches, loss: 0.0912Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0921Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0914Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0918Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0913Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0904Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0902Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0906Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0895Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0889Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0889Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0890Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0903Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0897Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0890Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0884Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0884Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0882Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0882Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0875Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0870Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0869Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0867Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0864Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0860Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0856Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0856Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0860Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0855Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0858Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0853Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0848Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0853Epoch 4/15: [==============================] 75/75 batches, loss: 0.0856
[2025-05-07 13:40:58,566][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0856
[2025-05-07 13:40:58,869][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0478, Metrics: {'mse': 0.04770667478442192, 'rmse': 0.21841857701308723, 'r2': -0.044394493103027344}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1707Epoch 5/15: [                              ] 2/75 batches, loss: 0.1521Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1277Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1215Epoch 5/15: [==                            ] 5/75 batches, loss: 0.1148Epoch 5/15: [==                            ] 6/75 batches, loss: 0.1034Epoch 5/15: [==                            ] 7/75 batches, loss: 0.1039Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0960Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0911Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0880Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0882Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0888Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0870Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0851Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0891Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0884Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0881Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0913Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0897Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0880Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0882Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0877Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0867Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0863Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0904Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0896Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0880Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0885Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0874Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0851Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0837Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0847Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0840Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0832Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0831Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0826Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0824Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0821Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0813Epoch 5/15: [================              ] 40/75 batches, loss: 0.0816Epoch 5/15: [================              ] 41/75 batches, loss: 0.0815Epoch 5/15: [================              ] 42/75 batches, loss: 0.0810Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0801Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0805Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0803Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0802Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0794Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0790Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0812Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0809Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0805Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0805Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0799Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0791Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0784Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0782Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0778Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0771Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0769Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0771Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0774Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0772Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0775Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0770Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0767Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0776Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0780Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0781Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0780Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0784Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0790Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0785Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0783Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0779Epoch 5/15: [==============================] 75/75 batches, loss: 0.0779
[2025-05-07 13:41:01,532][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0779
[2025-05-07 13:41:01,799][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0480, Metrics: {'mse': 0.047934576869010925, 'rmse': 0.21893966490567882, 'r2': -0.04938375949859619}
[2025-05-07 13:41:01,800][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0517Epoch 6/15: [                              ] 2/75 batches, loss: 0.0442Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0679Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0664Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0678Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0683Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0651Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0662Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0689Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0688Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0689Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0683Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0695Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0687Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0712Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0733Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0712Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0720Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0704Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0703Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0697Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0678Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0677Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0678Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0675Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0707Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0716Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0710Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0697Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0686Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0686Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0686Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0687Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0682Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0695Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0689Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0683Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0676Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0695Epoch 6/15: [================              ] 40/75 batches, loss: 0.0685Epoch 6/15: [================              ] 41/75 batches, loss: 0.0690Epoch 6/15: [================              ] 42/75 batches, loss: 0.0687Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0692Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0700Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0711Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0704Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0707Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0704Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0694Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0692Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0699Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0701Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0697Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0695Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0705Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0698Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0692Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0688Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0685Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0683Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0685Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0681Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0681Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0682Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0680Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0673Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0668Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0673Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0672Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0674Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0671Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0667Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0665Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0667Epoch 6/15: [==============================] 75/75 batches, loss: 0.0668
[2025-05-07 13:41:04,105][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0668
[2025-05-07 13:41:04,337][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0547, Metrics: {'mse': 0.05465947091579437, 'rmse': 0.2337936502897253, 'r2': -0.19660508632659912}
[2025-05-07 13:41:04,337][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0514Epoch 7/15: [                              ] 2/75 batches, loss: 0.0560Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0514Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0534Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0533Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0571Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0588Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0648Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0687Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0676Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0678Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0663Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0644Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0618Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0613Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0602Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0611Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0613Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0625Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0624Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0608Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0597Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0593Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0597Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0600Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0600Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0601Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0598Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0600Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0588Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0578Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0580Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0581Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0584Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0580Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0574Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0580Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0576Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0567Epoch 7/15: [================              ] 40/75 batches, loss: 0.0574Epoch 7/15: [================              ] 41/75 batches, loss: 0.0577Epoch 7/15: [================              ] 42/75 batches, loss: 0.0578Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0588Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0589Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0590Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0585Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0592Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0592Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0590Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0585Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0586Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0587Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0588Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0594Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0600Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0596Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0595Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0596Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0601Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0601Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0596Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0591Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0587Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0587Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0591Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0590Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0592Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0590Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0596Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0592Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0589Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0589Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0585Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0581Epoch 7/15: [==============================] 75/75 batches, loss: 0.0582
[2025-05-07 13:41:06,599][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0582
[2025-05-07 13:41:06,882][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0561, Metrics: {'mse': 0.0560288280248642, 'rmse': 0.2367040938067278, 'r2': -0.22658300399780273}
[2025-05-07 13:41:06,882][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0488Epoch 8/15: [                              ] 2/75 batches, loss: 0.0567Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0529Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0596Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0529Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0548Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0529Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0550Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0543Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0566Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0556Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0567Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0547Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0566Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0579Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0580Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0573Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0570Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0566Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0564Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0557Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0567Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0569Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0569Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0578Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0582Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0577Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0572Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0559Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0558Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0565Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0560Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0564Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0563Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0559Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0552Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0544Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0545Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0557Epoch 8/15: [================              ] 40/75 batches, loss: 0.0558Epoch 8/15: [================              ] 41/75 batches, loss: 0.0566Epoch 8/15: [================              ] 42/75 batches, loss: 0.0573Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0570Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0570Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0577Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0569Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0567Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0562Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0560Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0560Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0560Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0560Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0558Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0557Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0563Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0563Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0560Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0557Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0559Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0564Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0560Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0560Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0556Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0558Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0555Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0553Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0554Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0550Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0550Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0551Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0549Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0547Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0545Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0543Epoch 8/15: [==============================] 75/75 batches, loss: 0.0543
[2025-05-07 13:41:09,250][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0543
[2025-05-07 13:41:09,483][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0563, Metrics: {'mse': 0.056260086596012115, 'rmse': 0.23719208797093574, 'r2': -0.23164570331573486}
[2025-05-07 13:41:09,484][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:41:09,484][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-07 13:41:09,484][src.training.lm_trainer][INFO] - Training completed in 22.46 seconds
[2025-05-07 13:41:09,484][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:41:12,393][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03228867053985596, 'rmse': 0.1796904853904512, 'r2': -0.04131937026977539}
[2025-05-07 13:41:12,394][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.04770667478442192, 'rmse': 0.21841857701308723, 'r2': -0.044394493103027344}
[2025-05-07 13:41:12,394][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.031033512204885483, 'rmse': 0.17616331117711623, 'r2': -0.017722368240356445}
[2025-05-07 13:41:14,067][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer2/fi/fi/model.pt
[2025-05-07 13:41:14,068][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▁
wandb:     best_val_mse █▅▁
wandb:      best_val_r2 ▁▄█
wandb:    best_val_rmse █▅▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▁▄▆▆▅▅
wandb:       train_loss █▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▄▁▁▂▃▃
wandb:          val_mse ▇█▄▁▁▂▃▃
wandb:           val_r2 ▂▁▅██▇▆▆
wandb:         val_rmse ▇█▄▁▁▃▃▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04782
wandb:     best_val_mse 0.04771
wandb:      best_val_r2 -0.04439
wandb:    best_val_rmse 0.21842
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.03103
wandb:    final_test_r2 -0.01772
wandb:  final_test_rmse 0.17616
wandb:  final_train_mse 0.03229
wandb:   final_train_r2 -0.04132
wandb: final_train_rmse 0.17969
wandb:    final_val_mse 0.04771
wandb:     final_val_r2 -0.04439
wandb:   final_val_rmse 0.21842
wandb:    learning_rate 0.0001
wandb:       train_loss 0.05428
wandb:       train_time 22.45514
wandb:         val_loss 0.05632
wandb:          val_mse 0.05626
wandb:           val_r2 -0.23165
wandb:         val_rmse 0.23719
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_134028-h2dnkeju
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_134028-h2dnkeju/logs
Experiment probe_layer2_avg_max_depth_control1_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_avg_max_depth_control2_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_max_depth_control2_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:41:42,178][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer2/fi
experiment_name: probe_layer2_avg_max_depth_control2_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:41:42,178][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:41:42,178][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 13:41:42,178][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:41:42,178][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:41:42,183][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:41:42,183][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 13:41:42,183][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:41:45,543][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:41:47,828][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:41:47,828][src.data.datasets][INFO] - Loading 'control_avg_max_depth_seed2' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:41:48,060][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_max_depth_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:21:04 2025).
[2025-05-07 13:41:48,120][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_max_depth_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:21:04 2025).
[2025-05-07 13:41:48,378][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:41:48,389][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:41:48,390][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:41:48,391][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:41:48,468][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:41:48,599][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:41:48,611][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:41:48,613][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:41:48,613][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:41:48,614][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:41:48,645][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:41:48,711][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:41:48,724][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:41:48,727][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:41:48,727][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:41:48,728][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:41:48,729][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:41:48,729][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:41:48,729][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:41:48,729][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:41:48,730][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:41:48,730][src.data.datasets][INFO] -   Mean: 0.1934, Std: 0.1761
[2025-05-07 13:41:48,730][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:41:48,730][src.data.datasets][INFO] - Sample label: 0.75
[2025-05-07 13:41:48,730][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:41:48,730][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:41:48,730][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:41:48,730][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:41:48,730][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-05-07 13:41:48,731][src.data.datasets][INFO] -   Mean: 0.2487, Std: 0.2137
[2025-05-07 13:41:48,731][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:41:48,731][src.data.datasets][INFO] - Sample label: 0.5
[2025-05-07 13:41:48,731][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:41:48,731][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:41:48,731][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:41:48,731][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:41:48,731][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-05-07 13:41:48,731][src.data.datasets][INFO] -   Mean: 0.1785, Std: 0.1746
[2025-05-07 13:41:48,732][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:41:48,732][src.data.datasets][INFO] - Sample label: 0.125
[2025-05-07 13:41:48,732][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:41:48,732][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:41:48,732][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:41:48,732][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 13:41:48,732][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:41:56,051][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:41:56,052][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:41:56,053][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 13:41:56,053][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:41:56,056][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:41:56,056][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:41:56,056][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:41:56,056][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:41:56,056][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:41:56,057][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:41:56,057][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4879Epoch 1/15: [                              ] 2/75 batches, loss: 0.5790Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5222Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4664Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4805Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4396Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4191Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4524Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4655Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4508Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4424Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4349Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4233Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4244Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4195Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4430Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4362Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4323Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4297Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4256Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4338Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4343Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4279Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4160Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4100Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4052Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3984Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3924Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3946Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3926Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3845Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3793Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3755Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3748Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3714Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3738Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3706Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3646Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3644Epoch 1/15: [================              ] 40/75 batches, loss: 0.3592Epoch 1/15: [================              ] 41/75 batches, loss: 0.3545Epoch 1/15: [================              ] 42/75 batches, loss: 0.3528Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3506Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3484Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3503Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3456Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3451Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3409Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3398Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3378Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3370Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3354Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3323Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3340Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3309Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3287Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3316Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3316Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3300Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3284Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3246Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3236Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3216Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3209Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3198Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3182Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3157Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3119Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3134Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3124Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3101Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3075Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3053Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3031Epoch 1/15: [==============================] 75/75 batches, loss: 0.3017
[2025-05-07 13:42:02,969][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3017
[2025-05-07 13:42:03,274][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0956, Metrics: {'mse': 0.0954703837633133, 'rmse': 0.3089828211459551, 'r2': -1.0900375843048096}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1683Epoch 2/15: [                              ] 2/75 batches, loss: 0.1382Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1177Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1404Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1387Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1335Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1458Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1404Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1426Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1470Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1433Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1371Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1360Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1290Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1294Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1266Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1257Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1255Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1239Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1240Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1265Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1303Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1299Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1288Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1271Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1315Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1330Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1334Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1365Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1409Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1392Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1374Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1364Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1400Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1456Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1442Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1439Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1433Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1430Epoch 2/15: [================              ] 40/75 batches, loss: 0.1426Epoch 2/15: [================              ] 41/75 batches, loss: 0.1431Epoch 2/15: [================              ] 42/75 batches, loss: 0.1420Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1421Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1412Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1396Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1429Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1435Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1434Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1453Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1441Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1438Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1443Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1440Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1447Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1441Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1462Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1453Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1445Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1443Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1424Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1425Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1423Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1420Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1420Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1409Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1407Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1403Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1406Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1402Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1393Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1398Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1398Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1398Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1395Epoch 2/15: [==============================] 75/75 batches, loss: 0.1391
[2025-05-07 13:42:05,938][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1391
[2025-05-07 13:42:06,214][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0687, Metrics: {'mse': 0.06857751309871674, 'rmse': 0.261873085861676, 'r2': -0.5012989044189453}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.0989Epoch 3/15: [                              ] 2/75 batches, loss: 0.0882Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0916Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0848Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0815Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0997Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0930Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0880Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0850Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0882Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0859Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0872Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0940Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0970Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0948Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0946Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0948Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0946Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0937Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0924Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0937Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0948Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0921Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1038Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1041Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1021Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1013Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1032Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1047Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1028Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1027Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1025Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1032Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1029Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1027Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1013Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1007Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1007Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1012Epoch 3/15: [================              ] 40/75 batches, loss: 0.1009Epoch 3/15: [================              ] 41/75 batches, loss: 0.1004Epoch 3/15: [================              ] 42/75 batches, loss: 0.1001Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0999Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1014Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1011Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1003Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1008Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1016Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1016Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1014Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1010Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1003Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0994Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1000Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0991Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0984Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0981Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0977Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0980Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0970Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0967Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0967Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0975Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0980Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0978Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0973Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0971Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0972Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0976Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0979Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0978Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0975Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0969Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0968Epoch 3/15: [==============================] 75/75 batches, loss: 0.0977
[2025-05-07 13:42:08,888][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0977
[2025-05-07 13:42:09,125][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0615, Metrics: {'mse': 0.06137189269065857, 'rmse': 0.2477335114405368, 'r2': -0.34355342388153076}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1159Epoch 4/15: [                              ] 2/75 batches, loss: 0.0881Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0979Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0997Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0935Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0970Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0956Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0938Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0967Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0917Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0916Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0936Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0897Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0882Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0896Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0897Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0877Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0874Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0856Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0877Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0882Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0912Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0932Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0931Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0961Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0948Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0943Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0931Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0923Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0913Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0916Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0915Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0898Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0889Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0907Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0899Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0894Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0895Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0889Epoch 4/15: [================              ] 40/75 batches, loss: 0.0883Epoch 4/15: [================              ] 41/75 batches, loss: 0.0878Epoch 4/15: [================              ] 42/75 batches, loss: 0.0870Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0870Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0859Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0855Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0845Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0839Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0850Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0847Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0843Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0835Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0840Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0841Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0836Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0833Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0840Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0837Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0850Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0848Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0851Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0849Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0841Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0842Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0840Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0842Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0838Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0838Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0840Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0836Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0838Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0833Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0831Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0828Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0828Epoch 4/15: [==============================] 75/75 batches, loss: 0.0828
[2025-05-07 13:42:11,759][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0828
[2025-05-07 13:42:12,018][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0565, Metrics: {'mse': 0.05644235014915466, 'rmse': 0.23757598815779904, 'r2': -0.2356358766555786}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0870Epoch 5/15: [                              ] 2/75 batches, loss: 0.1054Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1085Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1003Epoch 5/15: [==                            ] 5/75 batches, loss: 0.1023Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0948Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0929Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0893Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0885Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0848Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0833Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0846Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0863Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0889Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0876Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0868Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0861Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0878Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0865Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0857Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0860Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0838Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0829Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0815Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0824Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0838Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0836Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0835Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0841Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0829Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0811Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0804Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0800Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0793Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0802Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0808Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0810Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0803Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0816Epoch 5/15: [================              ] 40/75 batches, loss: 0.0821Epoch 5/15: [================              ] 41/75 batches, loss: 0.0818Epoch 5/15: [================              ] 42/75 batches, loss: 0.0812Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0812Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0814Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0806Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0811Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0799Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0800Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0807Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0801Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0794Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0788Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0785Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0784Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0778Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0771Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0765Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0768Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0765Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0767Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0765Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0764Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0764Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0766Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0762Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0760Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0757Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0754Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0747Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0749Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0749Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0745Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0740Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0737Epoch 5/15: [==============================] 75/75 batches, loss: 0.0732
[2025-05-07 13:42:14,696][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0732
[2025-05-07 13:42:14,944][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0662, Metrics: {'mse': 0.06618359684944153, 'rmse': 0.25726172830299016, 'r2': -0.44889116287231445}
[2025-05-07 13:42:14,945][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0592Epoch 6/15: [                              ] 2/75 batches, loss: 0.0549Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0555Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0462Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0570Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0546Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0592Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0676Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0624Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0626Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0641Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0634Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0664Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0652Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0657Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0672Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0651Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0692Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0691Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0678Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0662Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0646Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0650Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0658Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0664Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0669Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0671Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0661Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0656Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0664Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0657Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0651Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0657Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0657Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0665Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0656Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0652Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0643Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0665Epoch 6/15: [================              ] 40/75 batches, loss: 0.0665Epoch 6/15: [================              ] 41/75 batches, loss: 0.0660Epoch 6/15: [================              ] 42/75 batches, loss: 0.0652Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0647Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0655Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0663Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0663Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0658Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0666Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0660Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0655Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0661Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0665Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0659Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0656Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0656Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0654Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0653Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0652Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0646Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0648Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0653Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0654Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0660Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0657Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0657Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0653Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0654Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0656Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0654Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0652Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0652Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0650Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0645Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0644Epoch 6/15: [==============================] 75/75 batches, loss: 0.0640
[2025-05-07 13:42:17,297][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0640
[2025-05-07 13:42:17,606][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0621, Metrics: {'mse': 0.062046460807323456, 'rmse': 0.24909127003434595, 'r2': -0.358320951461792}
[2025-05-07 13:42:17,607][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0754Epoch 7/15: [                              ] 2/75 batches, loss: 0.0633Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0633Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0635Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0607Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0585Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0610Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0573Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0570Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0627Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0619Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0606Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0634Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0643Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0652Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0642Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0639Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0661Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0640Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0630Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0645Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0633Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0639Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0641Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0639Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0639Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0632Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0627Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0624Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0611Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0600Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0604Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0597Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0591Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0586Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0583Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0590Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0605Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0601Epoch 7/15: [================              ] 40/75 batches, loss: 0.0593Epoch 7/15: [================              ] 41/75 batches, loss: 0.0600Epoch 7/15: [================              ] 42/75 batches, loss: 0.0602Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0599Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0601Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0595Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0592Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0586Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0582Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0583Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0584Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0580Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0586Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0584Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0588Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0598Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0597Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0594Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0601Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0596Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0599Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0597Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0594Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0598Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0600Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0600Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0598Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0596Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0593Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0595Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0596Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0596Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0594Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0591Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0591Epoch 7/15: [==============================] 75/75 batches, loss: 0.0590
[2025-05-07 13:42:19,874][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0590
[2025-05-07 13:42:20,153][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0617, Metrics: {'mse': 0.06168666109442711, 'rmse': 0.24836799531023943, 'r2': -0.35044431686401367}
[2025-05-07 13:42:20,154][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0274Epoch 8/15: [                              ] 2/75 batches, loss: 0.0254Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0375Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0444Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0422Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0406Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0430Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0438Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0491Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0499Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0512Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0499Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0534Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0521Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0501Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0499Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0502Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0505Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0519Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0517Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0512Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0504Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0499Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0499Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0506Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0500Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0513Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0512Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0507Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0516Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0518Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0521Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0521Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0522Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0527Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0521Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0530Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0528Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0533Epoch 8/15: [================              ] 40/75 batches, loss: 0.0536Epoch 8/15: [================              ] 41/75 batches, loss: 0.0530Epoch 8/15: [================              ] 42/75 batches, loss: 0.0527Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0526Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0531Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0533Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0528Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0541Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0536Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0530Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0532Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0544Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0550Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0544Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0541Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0539Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0544Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0542Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0535Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0538Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0549Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0543Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0542Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0546Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0545Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0541Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0539Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0543Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0541Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0540Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0537Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0535Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0535Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0532Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0529Epoch 8/15: [==============================] 75/75 batches, loss: 0.0529
[2025-05-07 13:42:22,419][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0529
[2025-05-07 13:42:22,708][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0617, Metrics: {'mse': 0.06165167689323425, 'rmse': 0.24829755716324367, 'r2': -0.3496783971786499}
[2025-05-07 13:42:22,709][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:42:22,709][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-07 13:42:22,709][src.training.lm_trainer][INFO] - Training completed in 22.89 seconds
[2025-05-07 13:42:22,709][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:42:25,668][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.032215800136327744, 'rmse': 0.1794876044085712, 'r2': -0.03896915912628174}
[2025-05-07 13:42:25,668][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05644235014915466, 'rmse': 0.23757598815779904, 'r2': -0.2356358766555786}
[2025-05-07 13:42:25,668][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03526831418275833, 'rmse': 0.18779860005537402, 'r2': -0.15659964084625244}
[2025-05-07 13:42:27,338][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer2/fi/fi/model.pt
[2025-05-07 13:42:27,339][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▂▁
wandb:     best_val_mse █▃▂▁
wandb:      best_val_r2 ▁▆▇█
wandb:    best_val_rmse █▃▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▄▅▆▅▅▅
wandb:       train_loss █▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▂▁▃▂▂▂
wandb:          val_mse █▃▂▁▃▂▂▂
wandb:           val_r2 ▁▆▇█▆▇▇▇
wandb:         val_rmse █▃▂▁▃▂▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05652
wandb:     best_val_mse 0.05644
wandb:      best_val_r2 -0.23564
wandb:    best_val_rmse 0.23758
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.03527
wandb:    final_test_r2 -0.1566
wandb:  final_test_rmse 0.1878
wandb:  final_train_mse 0.03222
wandb:   final_train_r2 -0.03897
wandb: final_train_rmse 0.17949
wandb:    final_val_mse 0.05644
wandb:     final_val_r2 -0.23564
wandb:   final_val_rmse 0.23758
wandb:    learning_rate 0.0001
wandb:       train_loss 0.05289
wandb:       train_time 22.88854
wandb:         val_loss 0.06169
wandb:          val_mse 0.06165
wandb:           val_r2 -0.34968
wandb:         val_rmse 0.2483
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_134142-la7p35qs
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_134142-la7p35qs/logs
Experiment probe_layer2_avg_max_depth_control2_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_avg_max_depth_control3_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_max_depth_control3_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:42:57,564][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer2/fi
experiment_name: probe_layer2_avg_max_depth_control3_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:42:57,564][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:42:57,564][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 13:42:57,564][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:42:57,564][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:42:57,569][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:42:57,569][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 13:42:57,569][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:43:00,648][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:43:02,857][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:43:02,858][src.data.datasets][INFO] - Loading 'control_avg_max_depth_seed3' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:43:03,104][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_max_depth_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:22:13 2025).
[2025-05-07 13:43:03,243][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_max_depth_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:22:13 2025).
[2025-05-07 13:43:03,625][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:43:03,635][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:43:03,636][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:43:03,638][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:43:03,694][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:43:03,790][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:43:03,803][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:43:03,804][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:43:03,804][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:43:03,805][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:43:03,877][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:43:03,964][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:43:04,009][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:43:04,011][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:43:04,012][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:43:04,026][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:43:04,027][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:43:04,027][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:43:04,027][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:43:04,027][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:43:04,027][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:43:04,027][src.data.datasets][INFO] -   Mean: 0.1934, Std: 0.1761
[2025-05-07 13:43:04,027][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:43:04,028][src.data.datasets][INFO] - Sample label: 0.25
[2025-05-07 13:43:04,028][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:43:04,028][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:43:04,028][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:43:04,028][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:43:04,028][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-05-07 13:43:04,028][src.data.datasets][INFO] -   Mean: 0.2487, Std: 0.2137
[2025-05-07 13:43:04,028][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:43:04,028][src.data.datasets][INFO] - Sample label: 0.5
[2025-05-07 13:43:04,029][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:43:04,029][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:43:04,029][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:43:04,029][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:43:04,029][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-05-07 13:43:04,029][src.data.datasets][INFO] -   Mean: 0.1785, Std: 0.1746
[2025-05-07 13:43:04,029][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:43:04,029][src.data.datasets][INFO] - Sample label: 0.125
[2025-05-07 13:43:04,029][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:43:04,029][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:43:04,030][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:43:04,030][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 13:43:04,030][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:43:10,662][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:43:10,663][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:43:10,663][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 13:43:10,663][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:43:10,666][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:43:10,666][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:43:10,667][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:43:10,667][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:43:10,667][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:43:10,668][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:43:10,668][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6649Epoch 1/15: [                              ] 2/75 batches, loss: 0.6286Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5427Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4916Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4696Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4594Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4441Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4698Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4856Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4784Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4666Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4590Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4413Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4413Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4314Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4476Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4318Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4297Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4283Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4215Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4334Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4371Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4282Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4164Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4113Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4050Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3987Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3939Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3962Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3937Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3900Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3845Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3812Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3787Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3744Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3768Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3701Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3657Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3649Epoch 1/15: [================              ] 40/75 batches, loss: 0.3617Epoch 1/15: [================              ] 41/75 batches, loss: 0.3575Epoch 1/15: [================              ] 42/75 batches, loss: 0.3536Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3484Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3486Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3511Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3462Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3464Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3431Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3416Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3392Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3376Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3354Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3322Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3327Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3307Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3292Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3303Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3286Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3258Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3236Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3198Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3188Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3166Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3152Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3138Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3128Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3103Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3071Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3063Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3054Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3026Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3018Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2985Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2963Epoch 1/15: [==============================] 75/75 batches, loss: 0.2940
[2025-05-07 13:43:17,034][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2940
[2025-05-07 13:43:17,287][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0845, Metrics: {'mse': 0.08435126394033432, 'rmse': 0.2904328905966649, 'r2': -0.8466178178787231}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1676Epoch 2/15: [                              ] 2/75 batches, loss: 0.1318Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1097Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1345Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1348Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1351Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1494Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1513Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1605Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1692Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1629Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1576Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1531Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1472Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1482Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1446Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1444Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1454Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1467Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1462Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1437Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1470Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1467Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1457Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1435Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1470Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1471Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1475Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1490Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1535Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1523Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1503Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1510Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1525Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1541Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1531Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1509Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1492Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1504Epoch 2/15: [================              ] 40/75 batches, loss: 0.1485Epoch 2/15: [================              ] 41/75 batches, loss: 0.1466Epoch 2/15: [================              ] 42/75 batches, loss: 0.1456Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1449Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1428Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1414Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1434Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1426Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1421Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1456Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1439Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1436Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1435Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1435Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1431Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1424Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1431Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1424Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1419Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1401Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1389Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1389Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1388Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1380Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1387Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1384Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1377Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1376Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1399Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1396Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1386Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1390Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1399Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1396Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1383Epoch 2/15: [==============================] 75/75 batches, loss: 0.1381
[2025-05-07 13:43:19,968][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1381
[2025-05-07 13:43:20,210][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0726, Metrics: {'mse': 0.07246337831020355, 'rmse': 0.2691902269960846, 'r2': -0.5863680839538574}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.0993Epoch 3/15: [                              ] 2/75 batches, loss: 0.0932Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1037Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1090Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0996Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0997Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0957Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0901Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0891Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0977Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0996Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1037Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1084Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1079Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1080Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1076Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1060Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1064Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1065Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1066Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1075Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1075Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1060Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1115Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1114Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1104Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1095Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1088Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1103Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1096Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1101Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1099Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1104Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1111Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1098Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1111Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1112Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1107Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1111Epoch 3/15: [================              ] 40/75 batches, loss: 0.1111Epoch 3/15: [================              ] 41/75 batches, loss: 0.1103Epoch 3/15: [================              ] 42/75 batches, loss: 0.1090Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1080Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1104Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1096Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1091Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1102Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1104Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1105Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1097Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1091Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1082Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1070Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1076Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1066Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1057Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1051Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1047Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1037Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1027Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1023Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1019Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1024Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1036Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1029Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1027Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1027Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1019Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1016Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1016Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1008Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1007Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1003Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1003Epoch 3/15: [==============================] 75/75 batches, loss: 0.1010
[2025-05-07 13:43:22,941][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1010
[2025-05-07 13:43:23,221][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0593, Metrics: {'mse': 0.05914286524057388, 'rmse': 0.24319306166207513, 'r2': -0.29475557804107666}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1170Epoch 4/15: [                              ] 2/75 batches, loss: 0.1058Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0874Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1025Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0923Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0923Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0899Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0875Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0893Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0834Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0878Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0968Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0928Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0919Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0963Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0970Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0939Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0916Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0916Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0905Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0899Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0913Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0920Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0952Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0978Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0979Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0982Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0967Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0954Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0952Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0946Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0935Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0925Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0927Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0931Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0919Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0906Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0922Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0910Epoch 4/15: [================              ] 40/75 batches, loss: 0.0900Epoch 4/15: [================              ] 41/75 batches, loss: 0.0905Epoch 4/15: [================              ] 42/75 batches, loss: 0.0900Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0905Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0911Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0910Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0903Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0901Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0904Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0894Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0888Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0882Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0885Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0886Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0885Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0881Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0882Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0877Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0879Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0878Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0878Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0889Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0885Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0881Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0881Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0874Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0870Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0866Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0865Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0859Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0859Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0851Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0851Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0851Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0849Epoch 4/15: [==============================] 75/75 batches, loss: 0.0844
[2025-05-07 13:43:25,956][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0844
[2025-05-07 13:43:26,224][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0620, Metrics: {'mse': 0.06187472119927406, 'rmse': 0.24874629886547872, 'r2': -0.35456132888793945}
[2025-05-07 13:43:26,224][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1277Epoch 5/15: [                              ] 2/75 batches, loss: 0.1002Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1120Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1090Epoch 5/15: [==                            ] 5/75 batches, loss: 0.1001Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0945Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0921Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0886Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0826Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0818Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0876Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0858Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0882Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0885Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0895Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0873Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0863Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0879Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0853Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0836Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0840Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0852Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0854Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0841Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0862Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0857Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0870Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0853Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0845Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0825Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0833Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0830Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0828Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0826Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0827Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0819Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0824Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0821Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0818Epoch 5/15: [================              ] 40/75 batches, loss: 0.0815Epoch 5/15: [================              ] 41/75 batches, loss: 0.0816Epoch 5/15: [================              ] 42/75 batches, loss: 0.0807Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0797Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0792Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0783Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0780Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0772Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0776Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0776Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0770Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0763Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0766Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0766Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0770Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0762Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0756Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0758Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0752Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0757Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0765Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0760Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0762Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0763Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0762Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0761Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0759Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0754Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0749Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0745Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0750Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0752Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0749Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0747Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0750Epoch 5/15: [==============================] 75/75 batches, loss: 0.0741
[2025-05-07 13:43:28,535][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0741
[2025-05-07 13:43:28,812][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0574, Metrics: {'mse': 0.05727324262261391, 'rmse': 0.23931828727160384, 'r2': -0.2538257837295532}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.1207Epoch 6/15: [                              ] 2/75 batches, loss: 0.0986Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0816Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0732Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0740Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0680Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0645Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0696Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0665Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0642Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0644Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0644Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0703Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0683Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0699Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0700Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0684Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0706Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0687Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0669Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0658Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0649Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0679Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0696Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0707Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0700Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0699Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0688Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0675Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0685Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0687Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0682Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0670Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0667Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0676Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0678Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0670Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0662Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0688Epoch 6/15: [================              ] 40/75 batches, loss: 0.0685Epoch 6/15: [================              ] 41/75 batches, loss: 0.0682Epoch 6/15: [================              ] 42/75 batches, loss: 0.0676Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0668Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0672Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0678Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0671Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0665Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0659Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0659Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0658Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0661Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0659Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0659Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0655Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0650Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0646Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0647Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0647Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0642Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0647Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0648Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0651Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0650Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0649Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0653Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0649Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0647Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0651Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0649Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0644Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0647Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0646Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0644Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0644Epoch 6/15: [==============================] 75/75 batches, loss: 0.0646
[2025-05-07 13:43:31,460][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0646
[2025-05-07 13:43:31,734][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0599, Metrics: {'mse': 0.059803884476423264, 'rmse': 0.2445483274864567, 'r2': -0.30922651290893555}
[2025-05-07 13:43:31,734][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0686Epoch 7/15: [                              ] 2/75 batches, loss: 0.0727Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0729Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0751Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0694Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0641Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0621Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0602Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0591Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0598Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0592Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0592Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0585Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0579Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0565Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0584Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0586Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0587Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0587Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0595Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0602Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0612Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0606Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0624Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0620Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0622Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0616Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0622Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0616Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0611Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0604Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0599Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0590Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0586Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0585Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0586Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0582Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0587Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0580Epoch 7/15: [================              ] 40/75 batches, loss: 0.0582Epoch 7/15: [================              ] 41/75 batches, loss: 0.0588Epoch 7/15: [================              ] 42/75 batches, loss: 0.0593Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0602Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0599Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0597Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0592Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0592Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0587Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0585Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0582Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0581Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0586Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0586Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0590Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0594Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0593Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0589Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0591Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0588Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0593Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0592Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0591Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0601Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0601Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0598Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0600Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0595Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0593Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0591Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0590Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0591Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0588Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0585Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0584Epoch 7/15: [==============================] 75/75 batches, loss: 0.0581
[2025-05-07 13:43:34,031][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0581
[2025-05-07 13:43:34,335][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0633, Metrics: {'mse': 0.06323880702257156, 'rmse': 0.25147327297860417, 'r2': -0.3844238519668579}
[2025-05-07 13:43:34,336][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0507Epoch 8/15: [                              ] 2/75 batches, loss: 0.0398Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0451Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0442Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0396Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0421Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0426Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0435Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0507Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0503Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0492Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0474Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0508Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0543Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0546Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0539Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0525Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0538Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0550Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0550Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0535Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0550Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0534Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0528Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0527Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0526Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0539Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0536Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0526Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0523Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0521Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0527Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0523Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0518Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0519Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0517Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0509Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0503Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0506Epoch 8/15: [================              ] 40/75 batches, loss: 0.0509Epoch 8/15: [================              ] 41/75 batches, loss: 0.0512Epoch 8/15: [================              ] 42/75 batches, loss: 0.0507Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0503Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0511Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0517Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0522Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0524Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0523Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0520Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0525Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0528Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0536Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0533Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0530Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0527Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0525Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0523Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0519Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0523Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0521Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0522Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0523Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0522Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0523Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0527Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0525Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0527Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0528Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0527Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0523Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0525Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0526Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0528Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0527Epoch 8/15: [==============================] 75/75 batches, loss: 0.0538
[2025-05-07 13:43:36,673][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0538
[2025-05-07 13:43:36,929][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0580, Metrics: {'mse': 0.05793127790093422, 'rmse': 0.2406891727954006, 'r2': -0.26823151111602783}
[2025-05-07 13:43:36,930][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0623Epoch 9/15: [                              ] 2/75 batches, loss: 0.0600Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0513Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0483Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0515Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0510Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0509Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0505Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0504Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0535Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0514Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0498Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0486Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0490Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0488Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0483Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0486Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0508Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0523Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0520Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0533Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0526Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0520Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0533Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0531Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0529Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0520Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0513Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0510Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0509Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0503Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0499Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0503Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0505Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0500Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0494Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0507Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0500Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0495Epoch 9/15: [================              ] 40/75 batches, loss: 0.0500Epoch 9/15: [================              ] 41/75 batches, loss: 0.0499Epoch 9/15: [================              ] 42/75 batches, loss: 0.0497Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0492Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0488Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0485Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0480Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0483Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0483Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0489Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0489Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0492Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0494Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0490Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0484Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0485Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0484Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0485Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0484Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0480Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0479Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0480Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0480Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0482Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0483Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0484Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0480Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0485Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0482Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0482Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0483Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0485Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0484Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0486Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0484Epoch 9/15: [==============================] 75/75 batches, loss: 0.0485
[2025-05-07 13:43:39,277][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0485
[2025-05-07 13:43:39,583][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0579, Metrics: {'mse': 0.057860855013132095, 'rmse': 0.24054283405067817, 'r2': -0.2666897773742676}
[2025-05-07 13:43:39,583][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:43:39,584][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 9
[2025-05-07 13:43:39,584][src.training.lm_trainer][INFO] - Training completed in 25.81 seconds
[2025-05-07 13:43:39,584][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:43:42,485][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03298254311084747, 'rmse': 0.18161096638377175, 'r2': -0.06369686126708984}
[2025-05-07 13:43:42,485][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05727324262261391, 'rmse': 0.23931828727160384, 'r2': -0.2538257837295532}
[2025-05-07 13:43:42,485][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03461148217320442, 'rmse': 0.18604161408997832, 'r2': -0.13505935668945312}
[2025-05-07 13:43:44,152][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer2/fi/fi/model.pt
[2025-05-07 13:43:44,154][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▁▁
wandb:     best_val_mse █▅▁▁
wandb:      best_val_r2 ▁▄██
wandb:    best_val_rmse █▅▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▅▄▅▄▄▅
wandb:       train_loss █▄▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▁▂▁▂▃▁▁
wandb:          val_mse █▅▁▂▁▂▃▁▁
wandb:           val_r2 ▁▄█▇█▇▆██
wandb:         val_rmse █▅▂▂▁▂▃▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05735
wandb:     best_val_mse 0.05727
wandb:      best_val_r2 -0.25383
wandb:    best_val_rmse 0.23932
wandb: early_stop_epoch 9
wandb:            epoch 9
wandb:   final_test_mse 0.03461
wandb:    final_test_r2 -0.13506
wandb:  final_test_rmse 0.18604
wandb:  final_train_mse 0.03298
wandb:   final_train_r2 -0.0637
wandb: final_train_rmse 0.18161
wandb:    final_val_mse 0.05727
wandb:     final_val_r2 -0.25383
wandb:   final_val_rmse 0.23932
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04846
wandb:       train_time 25.81277
wandb:         val_loss 0.05792
wandb:          val_mse 0.05786
wandb:           val_r2 -0.26669
wandb:         val_rmse 0.24054
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_134257-4keu0ozj
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_134257-4keu0ozj/logs
Experiment probe_layer2_avg_max_depth_control3_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_avg_subordinate_chain_len_control1_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_subordinate_chain_len_control1_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_subordinate_chain_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:44:14,078][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer2/fi
experiment_name: probe_layer2_avg_subordinate_chain_len_control1_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:44:14,078][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:44:14,078][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 13:44:14,078][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:44:14,078][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:44:14,083][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:44:14,083][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 13:44:14,083][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:44:18,050][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:44:20,368][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:44:20,369][src.data.datasets][INFO] - Loading 'control_avg_subordinate_chain_len_seed1' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:44:20,597][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:23:31 2025).
[2025-05-07 13:44:20,713][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:23:31 2025).
[2025-05-07 13:44:21,230][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:44:21,238][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:44:21,239][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:44:21,241][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:44:21,362][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:44:21,576][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:44:21,631][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:44:21,632][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:44:21,632][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:44:21,633][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:44:21,649][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:44:21,707][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:44:21,758][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:44:21,759][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:44:21,759][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:44:21,760][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:44:21,761][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:44:21,761][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:44:21,761][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:44:21,761][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:44:21,761][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:44:21,761][src.data.datasets][INFO] -   Mean: 0.0268, Std: 0.1180
[2025-05-07 13:44:21,761][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:44:21,761][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 13:44:21,762][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:44:21,762][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:44:21,762][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:44:21,762][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:44:21,762][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:44:21,762][src.data.datasets][INFO] -   Mean: 0.1217, Std: 0.2206
[2025-05-07 13:44:21,762][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:44:21,762][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 13:44:21,762][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:44:21,762][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:44:21,763][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:44:21,763][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:44:21,763][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:44:21,763][src.data.datasets][INFO] -   Mean: 0.1272, Std: 0.2107
[2025-05-07 13:44:21,763][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:44:21,763][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 13:44:21,763][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:44:21,763][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:44:21,763][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:44:21,764][__main__][INFO] - Using model type: lm_probe for submetric avg_subordinate_chain_len
[2025-05-07 13:44:21,764][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:44:28,984][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:44:28,985][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:44:28,985][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 13:44:28,985][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:44:28,988][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:44:28,989][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:44:28,989][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:44:28,989][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:44:28,989][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:44:28,990][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:44:28,990][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6425Epoch 1/15: [                              ] 2/75 batches, loss: 0.6533Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5602Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5097Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4877Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4587Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4385Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4727Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4821Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4638Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4587Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4554Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4364Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4392Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4296Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4394Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4271Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4255Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4235Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4186Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4341Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4372Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4298Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4166Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4088Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4021Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3958Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3912Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3909Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3916Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3842Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3792Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3744Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3723Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3682Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3685Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3634Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3601Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3577Epoch 1/15: [================              ] 40/75 batches, loss: 0.3521Epoch 1/15: [================              ] 41/75 batches, loss: 0.3477Epoch 1/15: [================              ] 42/75 batches, loss: 0.3439Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3399Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3383Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3408Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3362Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3371Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3329Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3305Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3285Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3285Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3259Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3219Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3224Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3196Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3182Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3192Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3195Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3187Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3168Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3132Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3119Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3100Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3085Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3066Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3055Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3030Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3009Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3010Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3023Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2995Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2977Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2943Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2915Epoch 1/15: [==============================] 75/75 batches, loss: 0.2903
[2025-05-07 13:44:35,190][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2903
[2025-05-07 13:44:35,404][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1063, Metrics: {'mse': 0.10614807903766632, 'rmse': 0.32580374313022603, 'r2': -1.1808857917785645}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2485Epoch 2/15: [                              ] 2/75 batches, loss: 0.1717Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1296Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1564Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1450Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1356Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1432Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1388Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1368Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1460Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1397Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1360Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1330Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1273Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1238Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1225Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1248Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1234Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1243Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1257Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1260Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1271Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1281Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1266Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1269Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1314Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1312Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1296Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1306Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1316Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1306Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1286Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1274Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1296Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1302Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1304Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1298Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1281Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1280Epoch 2/15: [================              ] 40/75 batches, loss: 0.1270Epoch 2/15: [================              ] 41/75 batches, loss: 0.1259Epoch 2/15: [================              ] 42/75 batches, loss: 0.1255Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1249Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1231Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1214Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1224Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1219Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1214Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1243Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1228Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1219Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1222Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1222Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1221Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1220Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1236Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1227Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1223Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1213Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1213Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1221Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1222Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1215Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1211Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1202Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1200Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1195Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1190Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1186Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1176Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1172Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1177Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1177Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1177Epoch 2/15: [==============================] 75/75 batches, loss: 0.1169
[2025-05-07 13:44:38,106][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1169
[2025-05-07 13:44:38,351][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1084, Metrics: {'mse': 0.10837330669164658, 'rmse': 0.32920101259207357, 'r2': -1.226604700088501}
[2025-05-07 13:44:38,352][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1181Epoch 3/15: [                              ] 2/75 batches, loss: 0.1076Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1032Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0949Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0840Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0920Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0878Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0842Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0815Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0835Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0846Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0837Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0863Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0897Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0892Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0882Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0887Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0870Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0863Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0851Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0848Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0847Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0856Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0901Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0888Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0883Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0871Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0864Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0885Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0884Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0900Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0901Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0905Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0914Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0898Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0899Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0884Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0880Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0895Epoch 3/15: [================              ] 40/75 batches, loss: 0.0892Epoch 3/15: [================              ] 41/75 batches, loss: 0.0886Epoch 3/15: [================              ] 42/75 batches, loss: 0.0882Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0877Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0877Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0874Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0869Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0876Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0881Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0888Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0887Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0879Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0881Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0889Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0890Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0882Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0873Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0863Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0860Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0855Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0849Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0841Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0839Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0847Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0853Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0849Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0842Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0842Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0837Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0838Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0844Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0838Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0831Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0825Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0824Epoch 3/15: [==============================] 75/75 batches, loss: 0.0830
[2025-05-07 13:44:40,661][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0830
[2025-05-07 13:44:40,885][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0890, Metrics: {'mse': 0.08898327499628067, 'rmse': 0.29830064531656764, 'r2': -0.8282231092453003}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1227Epoch 4/15: [                              ] 2/75 batches, loss: 0.0884Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0733Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0782Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0724Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0771Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0780Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0793Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0787Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0732Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0742Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0743Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0702Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0685Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0711Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0748Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0728Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0728Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0725Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0711Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0702Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0712Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0713Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0723Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0738Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0738Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0736Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0724Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0719Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0719Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0722Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0715Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0717Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0707Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0706Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0710Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0699Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0703Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0693Epoch 4/15: [================              ] 40/75 batches, loss: 0.0698Epoch 4/15: [================              ] 41/75 batches, loss: 0.0730Epoch 4/15: [================              ] 42/75 batches, loss: 0.0722Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0727Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0718Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0714Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0711Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0704Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0704Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0704Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0698Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0694Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0695Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0706Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0706Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0703Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0706Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0701Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0696Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0701Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0711Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0709Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0709Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0708Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0712Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0715Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0708Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0706Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0708Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0708Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0707Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0708Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0704Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0701Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0706Epoch 4/15: [==============================] 75/75 batches, loss: 0.0708
[2025-05-07 13:44:43,589][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0708
[2025-05-07 13:44:43,857][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0612, Metrics: {'mse': 0.06117764115333557, 'rmse': 0.24734114326843315, 'r2': -0.25693702697753906}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1731Epoch 5/15: [                              ] 2/75 batches, loss: 0.1117Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1121Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1091Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0967Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0887Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0890Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0834Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0774Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0733Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0717Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0705Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0683Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0673Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0705Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0686Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0700Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0725Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0709Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0690Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0698Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0677Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0668Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0665Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0680Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0677Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0667Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0670Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0660Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0641Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0630Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0651Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0643Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0642Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0648Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0647Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0641Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0639Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0636Epoch 5/15: [================              ] 40/75 batches, loss: 0.0633Epoch 5/15: [================              ] 41/75 batches, loss: 0.0648Epoch 5/15: [================              ] 42/75 batches, loss: 0.0639Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0642Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0640Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0636Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0635Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0626Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0624Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0646Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0641Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0636Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0636Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0631Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0631Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0624Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0621Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0625Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0620Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0615Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0612Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0607Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0606Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0604Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0602Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0615Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0610Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0609Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0605Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0602Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0602Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0602Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0598Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0594Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0591Epoch 5/15: [==============================] 75/75 batches, loss: 0.0586
[2025-05-07 13:44:46,502][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0586
[2025-05-07 13:44:46,737][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0722, Metrics: {'mse': 0.0721658319234848, 'rmse': 0.268636989120048, 'r2': -0.4826970100402832}
[2025-05-07 13:44:46,738][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0279Epoch 6/15: [                              ] 2/75 batches, loss: 0.0345Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0499Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0434Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0474Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0489Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0456Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0466Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0456Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0460Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0452Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0464Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0506Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0498Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0521Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0546Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0532Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0539Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0522Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0520Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0509Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0496Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0495Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0505Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0504Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0497Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0496Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0488Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0483Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0482Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0475Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0479Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0476Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0482Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0481Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0473Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0467Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0466Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0486Epoch 6/15: [================              ] 40/75 batches, loss: 0.0479Epoch 6/15: [================              ] 41/75 batches, loss: 0.0495Epoch 6/15: [================              ] 42/75 batches, loss: 0.0509Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0511Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0517Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0519Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0513Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0509Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0504Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0498Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0492Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0497Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0495Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0495Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0503Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0499Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0498Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0495Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0497Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0492Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0491Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0492Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0489Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0489Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0487Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0486Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0482Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0480Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0485Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0483Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0482Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0482Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0478Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0480Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0479Epoch 6/15: [==============================] 75/75 batches, loss: 0.0478
[2025-05-07 13:44:49,021][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0478
[2025-05-07 13:44:49,291][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0665, Metrics: {'mse': 0.06643031537532806, 'rmse': 0.25774079105824144, 'r2': -0.3648569583892822}
[2025-05-07 13:44:49,292][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0383Epoch 7/15: [                              ] 2/75 batches, loss: 0.0366Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0331Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0342Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0391Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0397Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0393Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0370Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0360Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0401Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0419Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0410Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0413Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0407Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0397Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0394Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0393Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0390Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0374Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0382Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0375Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0369Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0360Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0368Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0374Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0377Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0386Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0391Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0385Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0385Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0379Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0383Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0378Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0377Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0372Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0369Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0372Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0372Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0375Epoch 7/15: [================              ] 40/75 batches, loss: 0.0373Epoch 7/15: [================              ] 41/75 batches, loss: 0.0372Epoch 7/15: [================              ] 42/75 batches, loss: 0.0373Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0374Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0399Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0393Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0391Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0403Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0400Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0399Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0396Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0395Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0399Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0400Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0397Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0402Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0403Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0401Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0402Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0399Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0402Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0406Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0401Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0401Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0403Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0400Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0398Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0400Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0399Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0411Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0410Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0410Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0410Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0406Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0405Epoch 7/15: [==============================] 75/75 batches, loss: 0.0403
[2025-05-07 13:44:51,572][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0403
[2025-05-07 13:44:51,844][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0673, Metrics: {'mse': 0.06731949001550674, 'rmse': 0.2594599969465558, 'r2': -0.38312554359436035}
[2025-05-07 13:44:51,845][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0257Epoch 8/15: [                              ] 2/75 batches, loss: 0.0293Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0263Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0295Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0289Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0310Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0339Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0372Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0369Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0355Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0342Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0362Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0358Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0352Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0374Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0406Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0403Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0392Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0383Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0375Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0366Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0372Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0366Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0379Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0393Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0385Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0382Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0378Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0371Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0366Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0360Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0361Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0357Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0367Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0361Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0355Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0352Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0350Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0349Epoch 8/15: [================              ] 40/75 batches, loss: 0.0356Epoch 8/15: [================              ] 41/75 batches, loss: 0.0358Epoch 8/15: [================              ] 42/75 batches, loss: 0.0360Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0358Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0359Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0357Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0361Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0365Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0367Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0366Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0363Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0360Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0361Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0356Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0355Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0365Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0364Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0366Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0375Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0375Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0381Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0377Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0377Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0376Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0379Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0375Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0374Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0372Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0371Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0369Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0366Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0364Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0360Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0361Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0358Epoch 8/15: [==============================] 75/75 batches, loss: 0.0359
[2025-05-07 13:44:54,141][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0359
[2025-05-07 13:44:54,468][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0765, Metrics: {'mse': 0.07647297531366348, 'rmse': 0.2765374754236096, 'r2': -0.571190357208252}
[2025-05-07 13:44:54,469][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:44:54,469][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-07 13:44:54,469][src.training.lm_trainer][INFO] - Training completed in 22.22 seconds
[2025-05-07 13:44:54,469][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:44:57,505][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.0149746248498559, 'rmse': 0.12237084967366983, 'r2': -0.07520115375518799}
[2025-05-07 13:44:57,505][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06117764115333557, 'rmse': 0.24734114326843315, 'r2': -0.25693702697753906}
[2025-05-07 13:44:57,505][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.061339836567640305, 'rmse': 0.2476688041874477, 'r2': -0.38172614574432373}
[2025-05-07 13:44:59,135][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer2/fi/fi/model.pt
[2025-05-07 13:44:59,137][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▁
wandb:     best_val_mse █▅▁
wandb:      best_val_r2 ▁▄█
wandb:    best_val_rmse █▆▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▁▃▆▅▅▅
wandb:       train_loss █▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss ██▅▁▃▂▂▃
wandb:          val_mse ██▅▁▃▂▂▃
wandb:           val_r2 ▁▁▄█▆▇▇▆
wandb:         val_rmse ██▅▁▃▂▂▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06121
wandb:     best_val_mse 0.06118
wandb:      best_val_r2 -0.25694
wandb:    best_val_rmse 0.24734
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.06134
wandb:    final_test_r2 -0.38173
wandb:  final_test_rmse 0.24767
wandb:  final_train_mse 0.01497
wandb:   final_train_r2 -0.0752
wandb: final_train_rmse 0.12237
wandb:    final_val_mse 0.06118
wandb:     final_val_r2 -0.25694
wandb:   final_val_rmse 0.24734
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03588
wandb:       train_time 22.22335
wandb:         val_loss 0.07647
wandb:          val_mse 0.07647
wandb:           val_r2 -0.57119
wandb:         val_rmse 0.27654
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_134414-1q5wrrzy
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_134414-1q5wrrzy/logs
Experiment probe_layer2_avg_subordinate_chain_len_control1_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_avg_subordinate_chain_len_control2_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_subordinate_chain_len_control2_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=avg_subordinate_chain_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:45:27,261][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer2/fi
experiment_name: probe_layer2_avg_subordinate_chain_len_control2_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:45:27,261][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:45:27,261][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 13:45:27,261][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:45:27,261][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:45:27,265][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:45:27,265][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 13:45:27,266][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:45:31,155][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:45:33,567][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:45:33,567][src.data.datasets][INFO] - Loading 'control_avg_subordinate_chain_len_seed2' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:45:33,820][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:25:20 2025).
[2025-05-07 13:45:33,940][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:25:20 2025).
[2025-05-07 13:45:34,506][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:45:34,519][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:45:34,520][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:45:34,522][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:45:34,675][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:45:34,807][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:45:34,820][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:45:34,821][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:45:34,821][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:45:34,832][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:45:34,898][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:45:35,020][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:45:35,056][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:45:35,057][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:45:35,058][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:45:35,058][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:45:35,059][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:45:35,059][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:45:35,059][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:45:35,059][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:45:35,060][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:45:35,060][src.data.datasets][INFO] -   Mean: 0.0268, Std: 0.1180
[2025-05-07 13:45:35,060][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:45:35,060][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 13:45:35,060][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:45:35,060][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:45:35,060][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:45:35,060][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:45:35,061][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:45:35,061][src.data.datasets][INFO] -   Mean: 0.1217, Std: 0.2206
[2025-05-07 13:45:35,061][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:45:35,061][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 13:45:35,061][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:45:35,061][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:45:35,061][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:45:35,061][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:45:35,061][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:45:35,062][src.data.datasets][INFO] -   Mean: 0.1272, Std: 0.2107
[2025-05-07 13:45:35,062][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:45:35,062][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 13:45:35,062][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:45:35,062][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:45:35,062][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:45:35,062][__main__][INFO] - Using model type: lm_probe for submetric avg_subordinate_chain_len
[2025-05-07 13:45:35,063][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:45:42,872][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:45:42,873][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:45:42,873][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 13:45:42,873][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:45:42,876][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:45:42,877][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:45:42,877][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:45:42,877][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:45:42,877][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:45:42,878][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:45:42,878][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7286Epoch 1/15: [                              ] 2/75 batches, loss: 0.6923Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5869Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5164Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4907Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4535Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4344Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4693Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4955Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4757Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4694Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4626Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4426Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4495Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4399Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4546Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4430Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4406Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4362Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4321Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4397Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4396Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4324Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4192Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4099Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4012Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3935Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3891Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3892Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3911Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3844Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3794Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3745Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3714Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3674Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3673Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3621Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3573Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3549Epoch 1/15: [================              ] 40/75 batches, loss: 0.3495Epoch 1/15: [================              ] 41/75 batches, loss: 0.3457Epoch 1/15: [================              ] 42/75 batches, loss: 0.3419Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3380Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3353Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3395Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3351Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3346Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3299Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3271Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3263Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3262Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3245Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3205Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3207Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3184Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3167Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3182Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3185Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3159Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3137Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3106Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3092Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3078Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3062Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3043Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3040Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3015Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2978Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2991Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2978Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2952Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2933Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2910Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2885Epoch 1/15: [==============================] 75/75 batches, loss: 0.2865
[2025-05-07 13:45:49,553][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2865
[2025-05-07 13:45:49,833][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1064, Metrics: {'mse': 0.10624133050441742, 'rmse': 0.3259468215896842, 'r2': -1.1828017234802246}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1390Epoch 2/15: [                              ] 2/75 batches, loss: 0.1138Epoch 2/15: [=                             ] 3/75 batches, loss: 0.0919Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1191Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1132Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1125Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1223Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1186Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1182Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1267Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1235Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1211Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1230Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1174Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1185Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1174Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1148Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1146Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1121Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1127Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1124Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1141Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1168Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1151Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1160Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1203Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1212Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1209Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1227Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1237Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1228Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1212Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1201Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1214Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1234Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1231Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1245Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1232Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1218Epoch 2/15: [================              ] 40/75 batches, loss: 0.1210Epoch 2/15: [================              ] 41/75 batches, loss: 0.1209Epoch 2/15: [================              ] 42/75 batches, loss: 0.1201Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1198Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1185Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1170Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1191Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1187Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1188Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1205Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1191Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1190Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1191Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1192Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1192Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1186Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1195Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1185Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1176Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1175Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1164Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1168Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1167Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1163Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1162Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1155Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1158Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1156Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1158Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1154Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1148Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1150Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1156Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1155Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1165Epoch 2/15: [==============================] 75/75 batches, loss: 0.1159
[2025-05-07 13:45:52,549][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1159
[2025-05-07 13:45:52,867][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0950, Metrics: {'mse': 0.094913050532341, 'rmse': 0.3080796171971476, 'r2': -0.950054407119751}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1274Epoch 3/15: [                              ] 2/75 batches, loss: 0.0979Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0914Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0952Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0896Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0890Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0842Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0781Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0756Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0783Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0763Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0758Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0826Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0843Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0814Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0832Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0845Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0828Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0832Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0823Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0842Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0855Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0848Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0919Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0916Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0900Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0889Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0890Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0895Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0892Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0892Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0915Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0918Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0919Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0902Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0889Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0877Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0867Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0866Epoch 3/15: [================              ] 40/75 batches, loss: 0.0864Epoch 3/15: [================              ] 41/75 batches, loss: 0.0862Epoch 3/15: [================              ] 42/75 batches, loss: 0.0857Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0853Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0850Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0846Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0851Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0861Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0868Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0875Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0868Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0860Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0856Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0851Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0855Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0845Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0838Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0833Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0835Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0826Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0817Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0813Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0811Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0814Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0821Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0819Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0812Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0807Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0804Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0806Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0808Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0808Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0804Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0798Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0801Epoch 3/15: [==============================] 75/75 batches, loss: 0.0813
[2025-05-07 13:45:55,575][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0813
[2025-05-07 13:45:55,812][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0881, Metrics: {'mse': 0.08806267380714417, 'rmse': 0.29675355736223985, 'r2': -0.809308648109436}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0714Epoch 4/15: [                              ] 2/75 batches, loss: 0.0774Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0810Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0804Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0741Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0722Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0746Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0729Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0727Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0697Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0743Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0806Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0761Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0744Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0752Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0761Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0754Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0741Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0733Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0761Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0750Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0759Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0764Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0761Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0809Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0814Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0813Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0798Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0791Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0778Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0782Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0777Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0773Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0767Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0775Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0784Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0779Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0781Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0769Epoch 4/15: [================              ] 40/75 batches, loss: 0.0760Epoch 4/15: [================              ] 41/75 batches, loss: 0.0762Epoch 4/15: [================              ] 42/75 batches, loss: 0.0768Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0774Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0764Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0758Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0751Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0744Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0742Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0742Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0733Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0728Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0729Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0725Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0724Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0719Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0725Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0718Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0718Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0715Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0720Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0719Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0712Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0706Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0702Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0709Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0702Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0699Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0701Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0698Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0699Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0693Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0688Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0685Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0692Epoch 4/15: [==============================] 75/75 batches, loss: 0.0691
[2025-05-07 13:45:58,453][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0691
[2025-05-07 13:45:58,715][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0711, Metrics: {'mse': 0.07109694182872772, 'rmse': 0.26664009793864035, 'r2': -0.46073591709136963}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1249Epoch 5/15: [                              ] 2/75 batches, loss: 0.1088Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1180Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1046Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0931Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0870Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0857Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0808Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0748Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0716Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0737Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0738Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0726Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0759Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0769Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0745Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0727Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0725Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0712Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0698Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0707Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0687Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0683Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0680Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0683Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0681Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0678Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0668Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0658Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0650Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0645Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0640Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0646Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0643Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0649Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0638Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0649Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0647Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0647Epoch 5/15: [================              ] 40/75 batches, loss: 0.0654Epoch 5/15: [================              ] 41/75 batches, loss: 0.0653Epoch 5/15: [================              ] 42/75 batches, loss: 0.0648Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0649Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0640Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0636Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0634Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0626Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0620Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0636Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0633Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0625Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0619Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0614Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0610Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0604Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0599Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0596Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0594Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0594Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0591Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0585Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0585Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0587Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0589Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0588Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0589Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0584Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0582Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0582Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0580Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0580Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0576Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0575Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0574Epoch 5/15: [==============================] 75/75 batches, loss: 0.0571
[2025-05-07 13:46:01,362][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0571
[2025-05-07 13:46:01,629][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0718, Metrics: {'mse': 0.07178246974945068, 'rmse': 0.2679225069856034, 'r2': -0.47482049465179443}
[2025-05-07 13:46:01,630][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0682Epoch 6/15: [                              ] 2/75 batches, loss: 0.0746Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0629Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0601Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0811Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0719Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0672Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0642Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0588Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0589Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0566Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0568Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0574Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0565Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0579Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0568Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0559Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0636Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0629Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0611Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0605Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0589Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0585Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0582Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0594Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0584Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0578Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0576Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0566Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0563Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0556Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0547Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0556Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0547Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0553Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0558Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0552Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0554Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0570Epoch 6/15: [================              ] 40/75 batches, loss: 0.0566Epoch 6/15: [================              ] 41/75 batches, loss: 0.0562Epoch 6/15: [================              ] 42/75 batches, loss: 0.0558Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0550Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0547Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0557Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0550Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0546Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0540Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0533Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0527Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0529Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0526Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0526Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0521Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0521Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0516Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0517Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0517Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0512Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0521Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0521Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0521Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0521Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0519Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0518Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0516Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0512Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0511Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0511Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0507Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0506Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0501Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0497Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0493Epoch 6/15: [==============================] 75/75 batches, loss: 0.0492
[2025-05-07 13:46:03,916][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0492
[2025-05-07 13:46:04,226][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0726, Metrics: {'mse': 0.07262284308671951, 'rmse': 0.269486257695489, 'r2': -0.49208664894104004}
[2025-05-07 13:46:04,227][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0405Epoch 7/15: [                              ] 2/75 batches, loss: 0.0361Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0316Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0395Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0429Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0398Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0457Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0452Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0432Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0445Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0433Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0422Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0490Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0503Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0503Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0515Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0504Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0508Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0508Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0500Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0490Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0486Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0472Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0481Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0476Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0472Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0467Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0459Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0450Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0448Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0438Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0429Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0430Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0428Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0420Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0414Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0416Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0413Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0407Epoch 7/15: [================              ] 40/75 batches, loss: 0.0404Epoch 7/15: [================              ] 41/75 batches, loss: 0.0408Epoch 7/15: [================              ] 42/75 batches, loss: 0.0411Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0416Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0419Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0413Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0414Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0413Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0410Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0406Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0406Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0414Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0421Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0423Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0428Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0432Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0429Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0428Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0430Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0430Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0438Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0435Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0431Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0434Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0434Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0441Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0436Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0441Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0435Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0435Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0436Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0432Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0429Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0428Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0426Epoch 7/15: [==============================] 75/75 batches, loss: 0.0425
[2025-05-07 13:46:06,508][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0425
[2025-05-07 13:46:06,834][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0786, Metrics: {'mse': 0.07862551510334015, 'rmse': 0.2804024163650166, 'r2': -0.6154155731201172}
[2025-05-07 13:46:06,835][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0283Epoch 8/15: [                              ] 2/75 batches, loss: 0.0296Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0375Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0405Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0358Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0348Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0320Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0321Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0321Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0313Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0309Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0325Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0342Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0342Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0332Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0324Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0332Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0325Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0357Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0406Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0409Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0411Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0399Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0397Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0409Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0402Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0399Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0401Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0394Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0389Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0385Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0393Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0389Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0383Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0377Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0372Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0383Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0380Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0376Epoch 8/15: [================              ] 40/75 batches, loss: 0.0376Epoch 8/15: [================              ] 41/75 batches, loss: 0.0371Epoch 8/15: [================              ] 42/75 batches, loss: 0.0373Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0370Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0382Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0384Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0381Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0378Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0375Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0378Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0374Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0379Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0379Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0379Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0374Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0372Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0378Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0375Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0370Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0372Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0374Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0371Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0372Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0374Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0374Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0373Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0370Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0367Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0365Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0367Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0363Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0361Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0360Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0356Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0356Epoch 8/15: [==============================] 75/75 batches, loss: 0.0357
[2025-05-07 13:46:09,140][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0357
[2025-05-07 13:46:09,441][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0843, Metrics: {'mse': 0.08435345441102982, 'rmse': 0.2904366616166592, 'r2': -0.733100175857544}
[2025-05-07 13:46:09,441][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:46:09,441][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-07 13:46:09,441][src.training.lm_trainer][INFO] - Training completed in 22.94 seconds
[2025-05-07 13:46:09,442][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:46:12,621][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.016282321885228157, 'rmse': 0.12760220172562917, 'r2': -0.16909587383270264}
[2025-05-07 13:46:12,622][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07109694182872772, 'rmse': 0.26664009793864035, 'r2': -0.46073591709136963}
[2025-05-07 13:46:12,622][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.07191494852304459, 'rmse': 0.26816962639912184, 'r2': -0.6199384927749634}
[2025-05-07 13:46:14,285][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer2/fi/fi/model.pt
[2025-05-07 13:46:14,286][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▄▁
wandb:     best_val_mse █▆▄▁
wandb:      best_val_r2 ▁▃▅█
wandb:    best_val_rmse █▆▅▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▃▅▅▅▄
wandb:       train_loss █▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▄▁▁▁▂▄
wandb:          val_mse █▆▄▁▁▁▂▄
wandb:           val_r2 ▁▃▅███▇▅
wandb:         val_rmse █▆▅▁▁▁▃▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07112
wandb:     best_val_mse 0.0711
wandb:      best_val_r2 -0.46074
wandb:    best_val_rmse 0.26664
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.07191
wandb:    final_test_r2 -0.61994
wandb:  final_test_rmse 0.26817
wandb:  final_train_mse 0.01628
wandb:   final_train_r2 -0.1691
wandb: final_train_rmse 0.1276
wandb:    final_val_mse 0.0711
wandb:     final_val_r2 -0.46074
wandb:   final_val_rmse 0.26664
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03568
wandb:       train_time 22.93715
wandb:         val_loss 0.08435
wandb:          val_mse 0.08435
wandb:           val_r2 -0.7331
wandb:         val_rmse 0.29044
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_134527-l5p9fg22
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_134527-l5p9fg22/logs
Experiment probe_layer2_avg_subordinate_chain_len_control2_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_avg_subordinate_chain_len_control3_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_subordinate_chain_len_control3_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control3/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=avg_subordinate_chain_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:46:50,472][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control3/layer2/fi
experiment_name: probe_layer2_avg_subordinate_chain_len_control3_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:46:50,472][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:46:50,472][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 13:46:50,472][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:46:50,472][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:46:50,477][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:46:50,477][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 13:46:50,477][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:46:53,538][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:46:55,800][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:46:55,801][src.data.datasets][INFO] - Loading 'control_avg_subordinate_chain_len_seed3' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:46:56,005][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:27:11 2025).
[2025-05-07 13:46:56,125][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:27:11 2025).
[2025-05-07 13:46:56,512][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:46:56,522][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:46:56,522][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:46:56,524][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:46:56,576][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:46:56,665][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:46:56,683][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:46:56,684][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:46:56,684][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:46:56,695][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:46:56,770][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:46:56,835][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:46:56,874][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:46:56,876][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:46:56,876][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:46:56,877][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:46:56,878][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:46:56,878][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:46:56,878][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:46:56,878][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:46:56,879][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:46:56,879][src.data.datasets][INFO] -   Mean: 0.0268, Std: 0.1180
[2025-05-07 13:46:56,879][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:46:56,879][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 13:46:56,879][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:46:56,879][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:46:56,879][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:46:56,879][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:46:56,880][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:46:56,880][src.data.datasets][INFO] -   Mean: 0.1217, Std: 0.2206
[2025-05-07 13:46:56,880][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:46:56,880][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 13:46:56,880][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:46:56,880][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:46:56,880][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:46:56,880][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:46:56,880][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:46:56,880][src.data.datasets][INFO] -   Mean: 0.1272, Std: 0.2107
[2025-05-07 13:46:56,880][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:46:56,881][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 13:46:56,881][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:46:56,881][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:46:56,881][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:46:56,881][__main__][INFO] - Using model type: lm_probe for submetric avg_subordinate_chain_len
[2025-05-07 13:46:56,881][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:47:04,144][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:47:04,145][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:47:04,145][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 13:47:04,145][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:47:04,148][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:47:04,148][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:47:04,148][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:47:04,148][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:47:04,149][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:47:04,149][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:47:04,149][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6492Epoch 1/15: [                              ] 2/75 batches, loss: 0.6494Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5575Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5107Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4961Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4774Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4609Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4915Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4975Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4774Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4662Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4622Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4434Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4442Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4353Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4521Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4392Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4350Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4328Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4238Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4316Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4332Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4269Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4145Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4069Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3997Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3924Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3895Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3889Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3866Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3792Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3743Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3695Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3660Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3604Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3602Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3549Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3507Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3515Epoch 1/15: [================              ] 40/75 batches, loss: 0.3464Epoch 1/15: [================              ] 41/75 batches, loss: 0.3416Epoch 1/15: [================              ] 42/75 batches, loss: 0.3380Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3343Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3327Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3347Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3306Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3310Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3284Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3257Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3225Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3218Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3195Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3165Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3166Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3145Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3131Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3151Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3140Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3116Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3092Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3054Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3042Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3017Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3002Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2985Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2986Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2958Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2922Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2928Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2916Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2895Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2885Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2852Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2824Epoch 1/15: [==============================] 75/75 batches, loss: 0.2811
[2025-05-07 13:47:10,847][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2811
[2025-05-07 13:47:11,150][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1078, Metrics: {'mse': 0.10760281980037689, 'rmse': 0.32802868746555824, 'r2': -1.2107744216918945}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2056Epoch 2/15: [                              ] 2/75 batches, loss: 0.1519Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1160Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1349Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1322Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1259Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1294Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1318Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1382Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1476Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1422Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1385Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1364Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1299Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1329Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1317Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1306Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1309Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1277Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1267Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1253Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1264Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1260Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1238Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1241Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1292Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1321Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1312Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1336Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1352Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1340Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1319Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1321Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1340Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1352Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1337Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1324Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1319Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1319Epoch 2/15: [================              ] 40/75 batches, loss: 0.1302Epoch 2/15: [================              ] 41/75 batches, loss: 0.1282Epoch 2/15: [================              ] 42/75 batches, loss: 0.1275Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1276Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1261Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1251Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1268Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1262Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1256Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1298Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1283Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1276Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1277Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1272Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1270Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1269Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1283Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1275Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1269Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1258Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1244Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1243Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1240Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1230Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1226Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1217Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1225Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1228Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1226Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1221Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1211Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1203Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1208Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1207Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1201Epoch 2/15: [==============================] 75/75 batches, loss: 0.1200
[2025-05-07 13:47:13,915][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1200
[2025-05-07 13:47:14,195][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1183, Metrics: {'mse': 0.11827845871448517, 'rmse': 0.3439163542410933, 'r2': -1.4301128387451172}
[2025-05-07 13:47:14,196][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1421Epoch 3/15: [                              ] 2/75 batches, loss: 0.1019Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1033Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1064Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0973Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1040Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1040Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0959Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0916Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0929Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0890Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0911Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0926Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0942Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0925Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0913Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0921Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0907Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0901Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0905Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0906Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0911Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0891Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0925Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0924Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0908Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0893Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0900Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0900Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0900Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0896Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0895Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0893Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0897Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0880Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0876Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0876Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0870Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0874Epoch 3/15: [================              ] 40/75 batches, loss: 0.0871Epoch 3/15: [================              ] 41/75 batches, loss: 0.0867Epoch 3/15: [================              ] 42/75 batches, loss: 0.0859Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0859Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0859Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0854Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0852Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0861Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0866Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0873Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0872Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0865Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0860Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0853Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0855Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0846Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0837Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0828Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0829Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0823Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0814Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0806Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0805Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0809Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0824Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0822Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0814Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0812Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0807Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0803Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0805Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0800Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0797Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0791Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0793Epoch 3/15: [==============================] 75/75 batches, loss: 0.0796
[2025-05-07 13:47:16,530][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0796
[2025-05-07 13:47:16,793][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0818, Metrics: {'mse': 0.08179895579814911, 'rmse': 0.28600516743259924, 'r2': -0.6806161403656006}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1667Epoch 4/15: [                              ] 2/75 batches, loss: 0.1123Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0925Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0900Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0817Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0794Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0762Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0738Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0775Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0723Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0765Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0794Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0761Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0762Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0797Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0783Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0762Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0741Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0741Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0732Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0723Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0739Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0738Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0736Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0748Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0755Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0765Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0773Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0765Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0753Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0755Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0746Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0737Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0732Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0733Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0731Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0722Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0744Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0750Epoch 4/15: [================              ] 40/75 batches, loss: 0.0745Epoch 4/15: [================              ] 41/75 batches, loss: 0.0760Epoch 4/15: [================              ] 42/75 batches, loss: 0.0751Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0757Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0757Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0755Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0747Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0744Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0745Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0744Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0740Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0736Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0736Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0733Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0732Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0727Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0731Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0724Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0717Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0714Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0711Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0717Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0710Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0711Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0707Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0705Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0709Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0703Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0703Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0701Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0697Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0697Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0698Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0697Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0701Epoch 4/15: [==============================] 75/75 batches, loss: 0.0699
[2025-05-07 13:47:19,489][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0699
[2025-05-07 13:47:19,713][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0637, Metrics: {'mse': 0.06368239223957062, 'rmse': 0.25235370462818774, 'r2': -0.3083988428115845}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0979Epoch 5/15: [                              ] 2/75 batches, loss: 0.0718Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0795Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0779Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0711Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0672Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0684Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0656Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0616Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0587Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0583Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0573Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0569Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0584Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0590Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0583Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0596Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0618Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0604Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0591Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0586Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0579Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0573Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0575Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0610Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0624Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0622Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0615Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0608Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0591Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0599Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0607Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0612Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0606Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0612Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0605Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0618Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0617Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0611Epoch 5/15: [================              ] 40/75 batches, loss: 0.0608Epoch 5/15: [================              ] 41/75 batches, loss: 0.0609Epoch 5/15: [================              ] 42/75 batches, loss: 0.0613Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0610Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0601Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0596Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0603Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0598Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0593Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0600Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0604Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0596Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0593Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0592Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0587Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0584Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0583Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0584Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0578Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0576Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0577Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0572Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0573Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0587Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0588Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0584Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0579Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0578Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0574Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0571Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0574Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0575Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0574Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0572Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0569Epoch 5/15: [==============================] 75/75 batches, loss: 0.0564
[2025-05-07 13:47:22,431][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0564
[2025-05-07 13:47:22,689][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0710, Metrics: {'mse': 0.07101656496524811, 'rmse': 0.2664893336800708, 'r2': -0.4590843915939331}
[2025-05-07 13:47:22,689][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0611Epoch 6/15: [                              ] 2/75 batches, loss: 0.0488Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0624Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0526Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0500Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0479Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0453Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0475Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0437Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0459Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0462Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0467Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0527Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0524Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0529Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0528Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0517Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0526Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0506Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0502Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0492Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0482Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0485Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0482Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0481Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0474Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0474Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0474Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0467Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0473Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0466Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0460Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0456Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0453Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0453Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0452Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0445Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0454Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0474Epoch 6/15: [================              ] 40/75 batches, loss: 0.0469Epoch 6/15: [================              ] 41/75 batches, loss: 0.0466Epoch 6/15: [================              ] 42/75 batches, loss: 0.0461Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0456Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0462Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0471Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0469Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0478Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0479Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0480Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0475Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0481Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0480Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0483Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0478Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0474Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0470Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0467Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0467Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0468Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0473Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0474Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0472Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0469Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0467Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0467Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0463Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0468Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0472Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0472Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0468Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0468Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0467Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0474Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0472Epoch 6/15: [==============================] 75/75 batches, loss: 0.0471
[2025-05-07 13:47:24,967][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0471
[2025-05-07 13:47:25,187][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0713, Metrics: {'mse': 0.07129547744989395, 'rmse': 0.2670121297804539, 'r2': -0.46481502056121826}
[2025-05-07 13:47:25,188][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0361Epoch 7/15: [                              ] 2/75 batches, loss: 0.0491Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0445Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0442Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0436Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0431Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0438Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0414Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0415Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0455Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0442Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0429Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0444Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0430Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0415Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0409Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0428Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0473Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0464Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0511Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0499Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0493Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0483Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0483Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0480Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0475Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0472Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0471Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0461Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0457Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0451Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0449Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0448Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0449Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0441Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0452Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0460Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0467Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0461Epoch 7/15: [================              ] 40/75 batches, loss: 0.0460Epoch 7/15: [================              ] 41/75 batches, loss: 0.0456Epoch 7/15: [================              ] 42/75 batches, loss: 0.0469Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0465Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0460Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0457Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0454Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0449Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0445Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0443Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0443Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0438Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0441Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0447Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0443Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0442Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0439Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0435Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0436Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0431Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0433Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0429Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0424Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0433Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0433Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0429Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0430Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0427Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0424Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0424Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0427Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0423Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0420Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0416Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0414Epoch 7/15: [==============================] 75/75 batches, loss: 0.0412
[2025-05-07 13:47:27,514][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0412
[2025-05-07 13:47:27,748][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0720, Metrics: {'mse': 0.07203657925128937, 'rmse': 0.2683963100552788, 'r2': -0.48004138469696045}
[2025-05-07 13:47:27,749][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0302Epoch 8/15: [                              ] 2/75 batches, loss: 0.0296Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0284Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0276Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0244Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0328Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0338Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0326Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0341Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0362Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0352Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0342Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0358Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0432Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0418Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0411Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0418Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0407Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0418Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0407Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0396Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0405Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0394Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0394Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0393Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0388Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0389Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0393Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0388Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0390Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0383Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0386Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0389Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0383Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0388Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0382Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0378Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0378Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0375Epoch 8/15: [================              ] 40/75 batches, loss: 0.0380Epoch 8/15: [================              ] 41/75 batches, loss: 0.0374Epoch 8/15: [================              ] 42/75 batches, loss: 0.0370Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0367Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0366Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0364Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0373Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0370Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0368Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0363Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0359Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0361Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0363Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0359Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0354Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0356Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0356Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0354Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0350Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0350Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0350Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0347Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0345Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0344Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0349Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0347Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0346Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0345Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0344Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0341Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0337Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0336Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0335Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0333Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0335Epoch 8/15: [==============================] 75/75 batches, loss: 0.0341
[2025-05-07 13:47:30,059][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0341
[2025-05-07 13:47:30,299][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0760, Metrics: {'mse': 0.07597827166318893, 'rmse': 0.2756415637439117, 'r2': -0.5610262155532837}
[2025-05-07 13:47:30,299][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:47:30,299][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-07 13:47:30,300][src.training.lm_trainer][INFO] - Training completed in 22.59 seconds
[2025-05-07 13:47:30,300][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:47:33,269][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.014719566330313683, 'rmse': 0.12132421988339213, 'r2': -0.05688750743865967}
[2025-05-07 13:47:33,269][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06368239223957062, 'rmse': 0.25235370462818774, 'r2': -0.3083988428115845}
[2025-05-07 13:47:33,270][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06411302834749222, 'rmse': 0.2532055061555578, 'r2': -0.44419431686401367}
[2025-05-07 13:47:34,916][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control3/layer2/fi/fi/model.pt
[2025-05-07 13:47:34,918][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▁
wandb:     best_val_mse █▄▁
wandb:      best_val_r2 ▁▅█
wandb:    best_val_rmse █▄▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▅▆▅▅▅
wandb:       train_loss █▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▃▁▂▂▂▃
wandb:          val_mse ▇█▃▁▂▂▂▃
wandb:           val_r2 ▂▁▆█▇▇▇▆
wandb:         val_rmse ▇█▄▁▂▂▂▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06372
wandb:     best_val_mse 0.06368
wandb:      best_val_r2 -0.3084
wandb:    best_val_rmse 0.25235
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.06411
wandb:    final_test_r2 -0.44419
wandb:  final_test_rmse 0.25321
wandb:  final_train_mse 0.01472
wandb:   final_train_r2 -0.05689
wandb: final_train_rmse 0.12132
wandb:    final_val_mse 0.06368
wandb:     final_val_r2 -0.3084
wandb:   final_val_rmse 0.25235
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03408
wandb:       train_time 22.59164
wandb:         val_loss 0.07598
wandb:          val_mse 0.07598
wandb:           val_r2 -0.56103
wandb:         val_rmse 0.27564
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_134650-4a85hpz5
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_134650-4a85hpz5/logs
Experiment probe_layer2_avg_subordinate_chain_len_control3_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control3/layer2/fi/fi/results.json for layer 2
=======================
PROBING LAYER 10 (SUBMETRIC CONTROL EXPERIMENTS)
=======================
Running experiment: probe_layer10_avg_max_depth_control1_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_avg_max_depth_control1_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer10/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:48:07,977][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer10/fi
experiment_name: probe_layer10_avg_max_depth_control1_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:48:07,977][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:48:07,977][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 13:48:07,977][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:48:07,977][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:48:07,982][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:48:07,982][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 13:48:07,982][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:48:12,669][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:48:15,018][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:48:15,018][src.data.datasets][INFO] - Loading 'control_avg_max_depth_seed1' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:48:15,415][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_max_depth_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:19:23 2025).
[2025-05-07 13:48:15,621][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_max_depth_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:19:23 2025).
[2025-05-07 13:48:16,011][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:48:16,026][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:48:16,029][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:48:16,036][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:48:16,112][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:48:16,310][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:48:16,368][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:48:16,369][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:48:16,370][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:48:16,371][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:48:16,673][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:48:16,816][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:48:16,839][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:48:16,840][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:48:16,840][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:48:16,841][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:48:16,842][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:48:16,842][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:48:16,843][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:48:16,843][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:48:16,843][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:48:16,843][src.data.datasets][INFO] -   Mean: 0.1934, Std: 0.1761
[2025-05-07 13:48:16,843][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:48:16,843][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 13:48:16,843][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:48:16,843][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:48:16,844][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:48:16,844][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:48:16,844][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-05-07 13:48:16,844][src.data.datasets][INFO] -   Mean: 0.2487, Std: 0.2137
[2025-05-07 13:48:16,844][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:48:16,844][src.data.datasets][INFO] - Sample label: 0.5
[2025-05-07 13:48:16,844][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:48:16,844][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:48:16,844][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:48:16,844][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:48:16,845][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-05-07 13:48:16,845][src.data.datasets][INFO] -   Mean: 0.1785, Std: 0.1746
[2025-05-07 13:48:16,845][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:48:16,845][src.data.datasets][INFO] - Sample label: 0.125
[2025-05-07 13:48:16,845][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:48:16,845][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:48:16,845][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:48:16,846][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 13:48:16,846][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:48:26,047][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:48:26,048][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:48:26,048][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-07 13:48:26,048][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:48:26,051][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:48:26,052][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:48:26,052][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:48:26,052][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:48:26,052][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:48:26,053][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:48:26,053][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6005Epoch 1/15: [                              ] 2/75 batches, loss: 0.8242Epoch 1/15: [=                             ] 3/75 batches, loss: 0.8309Epoch 1/15: [=                             ] 4/75 batches, loss: 0.8132Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7639Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6869Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6533Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6575Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6356Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6364Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6058Epoch 1/15: [====                          ] 12/75 batches, loss: 0.5793Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.5576Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.5426Epoch 1/15: [======                        ] 15/75 batches, loss: 0.5281Epoch 1/15: [======                        ] 16/75 batches, loss: 0.5118Epoch 1/15: [======                        ] 17/75 batches, loss: 0.5126Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.5036Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4919Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4879Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4958Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4974Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4892Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4799Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4729Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4692Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4763Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4735Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4678Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4712Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4689Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4669Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4597Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4574Epoch 1/15: [==============                ] 35/75 batches, loss: 0.4534Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4557Epoch 1/15: [==============                ] 37/75 batches, loss: 0.4504Epoch 1/15: [===============               ] 38/75 batches, loss: 0.4474Epoch 1/15: [===============               ] 39/75 batches, loss: 0.4422Epoch 1/15: [================              ] 40/75 batches, loss: 0.4396Epoch 1/15: [================              ] 41/75 batches, loss: 0.4383Epoch 1/15: [================              ] 42/75 batches, loss: 0.4370Epoch 1/15: [=================             ] 43/75 batches, loss: 0.4318Epoch 1/15: [=================             ] 44/75 batches, loss: 0.4297Epoch 1/15: [==================            ] 45/75 batches, loss: 0.4287Epoch 1/15: [==================            ] 46/75 batches, loss: 0.4228Epoch 1/15: [==================            ] 47/75 batches, loss: 0.4209Epoch 1/15: [===================           ] 48/75 batches, loss: 0.4158Epoch 1/15: [===================           ] 49/75 batches, loss: 0.4119Epoch 1/15: [====================          ] 50/75 batches, loss: 0.4089Epoch 1/15: [====================          ] 51/75 batches, loss: 0.4051Epoch 1/15: [====================          ] 52/75 batches, loss: 0.4023Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3989Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3980Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3969Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3941Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3906Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3890Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3854Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3816Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3778Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3760Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3731Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3711Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3688Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3661Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3636Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3603Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3598Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3575Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3530Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3510Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3484Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3449Epoch 1/15: [==============================] 75/75 batches, loss: 0.3431
[2025-05-07 13:48:32,837][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3431
[2025-05-07 13:48:33,089][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0404, Metrics: {'mse': 0.04035640507936478, 'rmse': 0.20088903673263203, 'r2': 0.11651760339736938}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2328Epoch 2/15: [                              ] 2/75 batches, loss: 0.1843Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1856Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2269Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2218Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1950Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1970Epoch 2/15: [===                           ] 8/75 batches, loss: 0.2139Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1972Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2065Epoch 2/15: [====                          ] 11/75 batches, loss: 0.2040Epoch 2/15: [====                          ] 12/75 batches, loss: 0.2047Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1995Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1960Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1953Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1894Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1871Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1886Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1933Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1885Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1869Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1852Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1898Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1845Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1846Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1860Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1929Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1899Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1921Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1913Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1881Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1897Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1868Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1903Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1888Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1887Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1872Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1857Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1873Epoch 2/15: [================              ] 40/75 batches, loss: 0.1860Epoch 2/15: [================              ] 41/75 batches, loss: 0.1850Epoch 2/15: [================              ] 42/75 batches, loss: 0.1831Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1819Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1797Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1803Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1810Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1814Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1815Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1836Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1821Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1804Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1804Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1811Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1800Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1802Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1802Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1781Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1762Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1752Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1740Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1729Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1713Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1704Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1688Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1701Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1694Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1697Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1684Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1666Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1654Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1646Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1635Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1639Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1627Epoch 2/15: [==============================] 75/75 batches, loss: 0.1612
[2025-05-07 13:48:35,807][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1612
[2025-05-07 13:48:36,062][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0635, Metrics: {'mse': 0.06359516829252243, 'rmse': 0.252180824593232, 'r2': -0.3922252655029297}
[2025-05-07 13:48:36,063][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1878Epoch 3/15: [                              ] 2/75 batches, loss: 0.1520Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1794Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1540Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1538Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1453Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1381Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1355Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1343Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1420Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1421Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1456Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1470Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1482Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1473Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1442Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1459Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1486Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1444Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1426Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1433Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1388Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1368Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1350Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1340Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1322Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1309Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1297Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1307Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1304Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1281Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1280Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1264Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1262Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1244Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1238Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1226Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1230Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1222Epoch 3/15: [================              ] 40/75 batches, loss: 0.1213Epoch 3/15: [================              ] 41/75 batches, loss: 0.1240Epoch 3/15: [================              ] 42/75 batches, loss: 0.1243Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1241Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1227Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1213Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1205Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1204Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1216Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1219Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1206Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1196Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1189Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1183Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1172Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1163Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1160Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1160Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1160Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1154Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1154Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1149Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1142Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1138Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1131Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1120Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1116Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1120Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1125Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1139Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1142Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1145Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1144Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1145Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1141Epoch 3/15: [==============================] 75/75 batches, loss: 0.1137
[2025-05-07 13:48:38,385][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1137
[2025-05-07 13:48:38,635][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0573, Metrics: {'mse': 0.05728982388973236, 'rmse': 0.23935292747265985, 'r2': -0.25418877601623535}
[2025-05-07 13:48:38,636][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1229Epoch 4/15: [                              ] 2/75 batches, loss: 0.1424Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1317Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1544Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1365Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1376Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1365Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1367Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1374Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1278Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1257Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1224Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1182Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1138Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1145Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1146Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1116Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1142Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1143Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1140Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1141Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1166Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1169Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1169Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1201Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1179Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1186Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1181Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1186Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1172Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1140Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1116Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1120Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.1107Epoch 4/15: [==============                ] 35/75 batches, loss: 0.1093Epoch 4/15: [==============                ] 36/75 batches, loss: 0.1088Epoch 4/15: [==============                ] 37/75 batches, loss: 0.1072Epoch 4/15: [===============               ] 38/75 batches, loss: 0.1063Epoch 4/15: [===============               ] 39/75 batches, loss: 0.1051Epoch 4/15: [================              ] 40/75 batches, loss: 0.1069Epoch 4/15: [================              ] 41/75 batches, loss: 0.1071Epoch 4/15: [================              ] 42/75 batches, loss: 0.1063Epoch 4/15: [=================             ] 43/75 batches, loss: 0.1072Epoch 4/15: [=================             ] 44/75 batches, loss: 0.1057Epoch 4/15: [==================            ] 45/75 batches, loss: 0.1054Epoch 4/15: [==================            ] 46/75 batches, loss: 0.1064Epoch 4/15: [==================            ] 47/75 batches, loss: 0.1060Epoch 4/15: [===================           ] 48/75 batches, loss: 0.1053Epoch 4/15: [===================           ] 49/75 batches, loss: 0.1058Epoch 4/15: [====================          ] 50/75 batches, loss: 0.1048Epoch 4/15: [====================          ] 51/75 batches, loss: 0.1041Epoch 4/15: [====================          ] 52/75 batches, loss: 0.1037Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.1040Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.1041Epoch 4/15: [======================        ] 55/75 batches, loss: 0.1033Epoch 4/15: [======================        ] 56/75 batches, loss: 0.1026Epoch 4/15: [======================        ] 57/75 batches, loss: 0.1016Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.1013Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.1013Epoch 4/15: [========================      ] 60/75 batches, loss: 0.1014Epoch 4/15: [========================      ] 61/75 batches, loss: 0.1007Epoch 4/15: [========================      ] 62/75 batches, loss: 0.1002Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.1005Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.1008Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.1003Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0995Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0995Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.1004Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.1011Epoch 4/15: [============================  ] 70/75 batches, loss: 0.1010Epoch 4/15: [============================  ] 71/75 batches, loss: 0.1015Epoch 4/15: [============================  ] 72/75 batches, loss: 0.1010Epoch 4/15: [============================= ] 73/75 batches, loss: 0.1000Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0996Epoch 4/15: [==============================] 75/75 batches, loss: 0.0993
[2025-05-07 13:48:40,942][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0993
[2025-05-07 13:48:41,206][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0526, Metrics: {'mse': 0.052637629210948944, 'rmse': 0.229428919735392, 'r2': -0.15234291553497314}
[2025-05-07 13:48:41,207][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0716Epoch 5/15: [                              ] 2/75 batches, loss: 0.0845Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0816Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0808Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0793Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0743Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0837Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0831Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0798Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0783Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0778Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0758Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0725Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0731Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0748Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0751Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0753Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0776Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0788Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0787Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0785Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0776Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0770Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0766Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0802Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0783Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0785Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0806Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0796Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0782Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0784Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0799Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0802Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0794Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0791Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0794Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0798Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0793Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0787Epoch 5/15: [================              ] 40/75 batches, loss: 0.0791Epoch 5/15: [================              ] 41/75 batches, loss: 0.0801Epoch 5/15: [================              ] 42/75 batches, loss: 0.0794Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0792Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0791Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0785Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0786Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0775Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0780Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0796Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0790Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0787Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0777Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0777Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0770Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0762Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0766Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0761Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0758Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0757Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0752Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0752Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0750Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0748Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0744Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0740Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0749Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0750Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0755Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0754Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0753Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0759Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0764Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0762Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0756Epoch 5/15: [==============================] 75/75 batches, loss: 0.0756
[2025-05-07 13:48:43,532][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0756
[2025-05-07 13:48:43,776][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0435, Metrics: {'mse': 0.04345396161079407, 'rmse': 0.2084561383380064, 'r2': 0.04870593547821045}
[2025-05-07 13:48:43,777][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:48:43,777][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-07 13:48:43,777][src.training.lm_trainer][INFO] - Training completed in 14.00 seconds
[2025-05-07 13:48:43,777][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:48:46,631][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.045165691524744034, 'rmse': 0.21252221419123232, 'r2': -0.4566071033477783}
[2025-05-07 13:48:46,632][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.04035640507936478, 'rmse': 0.20088903673263203, 'r2': 0.11651760339736938}
[2025-05-07 13:48:46,632][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03834037482738495, 'rmse': 0.19580698360218143, 'r2': -0.25734567642211914}
[2025-05-07 13:48:48,277][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer10/fi/fi/model.pt
[2025-05-07 13:48:48,279][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▁▂▃
wandb:       train_loss █▃▂▂▁
wandb:       train_time ▁
wandb:         val_loss ▁█▆▅▂
wandb:          val_mse ▁█▆▅▂
wandb:           val_r2 █▁▃▄▇
wandb:         val_rmse ▁█▆▅▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04041
wandb:     best_val_mse 0.04036
wandb:      best_val_r2 0.11652
wandb:    best_val_rmse 0.20089
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.03834
wandb:    final_test_r2 -0.25735
wandb:  final_test_rmse 0.19581
wandb:  final_train_mse 0.04517
wandb:   final_train_r2 -0.45661
wandb: final_train_rmse 0.21252
wandb:    final_val_mse 0.04036
wandb:     final_val_r2 0.11652
wandb:   final_val_rmse 0.20089
wandb:    learning_rate 0.0001
wandb:       train_loss 0.07563
wandb:       train_time 13.9997
wandb:         val_loss 0.04351
wandb:          val_mse 0.04345
wandb:           val_r2 0.04871
wandb:         val_rmse 0.20846
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_134808-6gbvzcjt
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_134808-6gbvzcjt/logs
Experiment probe_layer10_avg_max_depth_control1_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer10/fi/fi/results.json for layer 10
Running experiment: probe_layer10_avg_max_depth_control2_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_avg_max_depth_control2_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer10/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:49:14,824][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer10/fi
experiment_name: probe_layer10_avg_max_depth_control2_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:49:14,824][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:49:14,824][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 13:49:14,824][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:49:14,824][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:49:14,829][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:49:14,829][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 13:49:14,829][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:49:17,810][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:49:20,274][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:49:20,274][src.data.datasets][INFO] - Loading 'control_avg_max_depth_seed2' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:49:20,429][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_max_depth_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:21:04 2025).
[2025-05-07 13:49:20,546][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_max_depth_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:21:04 2025).
[2025-05-07 13:49:20,750][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:49:20,759][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:49:20,759][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:49:20,760][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:49:20,809][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:49:20,924][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:49:20,935][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:49:20,937][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:49:20,937][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:49:20,938][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:49:21,034][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:49:21,104][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:49:21,114][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:49:21,116][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:49:21,116][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:49:21,117][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:49:21,117][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:49:21,117][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:49:21,118][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:49:21,118][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:49:21,118][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:49:21,118][src.data.datasets][INFO] -   Mean: 0.1934, Std: 0.1761
[2025-05-07 13:49:21,118][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:49:21,118][src.data.datasets][INFO] - Sample label: 0.75
[2025-05-07 13:49:21,118][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:49:21,118][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:49:21,118][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:49:21,119][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:49:21,119][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-05-07 13:49:21,119][src.data.datasets][INFO] -   Mean: 0.2487, Std: 0.2137
[2025-05-07 13:49:21,119][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:49:21,119][src.data.datasets][INFO] - Sample label: 0.5
[2025-05-07 13:49:21,119][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:49:21,119][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:49:21,119][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:49:21,119][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:49:21,119][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-05-07 13:49:21,120][src.data.datasets][INFO] -   Mean: 0.1785, Std: 0.1746
[2025-05-07 13:49:21,120][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:49:21,120][src.data.datasets][INFO] - Sample label: 0.125
[2025-05-07 13:49:21,120][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:49:21,120][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:49:21,120][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:49:21,120][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 13:49:21,121][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:49:28,365][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:49:28,366][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:49:28,366][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-07 13:49:28,367][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:49:28,369][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:49:28,370][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:49:28,370][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:49:28,370][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:49:28,370][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:49:28,371][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:49:28,371][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4887Epoch 1/15: [                              ] 2/75 batches, loss: 0.7611Epoch 1/15: [=                             ] 3/75 batches, loss: 0.8102Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7405Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7181Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6255Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6000Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6074Epoch 1/15: [===                           ] 9/75 batches, loss: 0.5851Epoch 1/15: [====                          ] 10/75 batches, loss: 0.5982Epoch 1/15: [====                          ] 11/75 batches, loss: 0.5724Epoch 1/15: [====                          ] 12/75 batches, loss: 0.5479Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.5363Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.5171Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4994Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4900Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4997Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4928Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4830Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4810Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4857Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4836Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4728Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4633Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4546Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4516Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4568Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4589Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4537Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4512Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4448Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4441Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4371Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4360Epoch 1/15: [==============                ] 35/75 batches, loss: 0.4350Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4364Epoch 1/15: [==============                ] 37/75 batches, loss: 0.4325Epoch 1/15: [===============               ] 38/75 batches, loss: 0.4276Epoch 1/15: [===============               ] 39/75 batches, loss: 0.4224Epoch 1/15: [================              ] 40/75 batches, loss: 0.4227Epoch 1/15: [================              ] 41/75 batches, loss: 0.4208Epoch 1/15: [================              ] 42/75 batches, loss: 0.4190Epoch 1/15: [=================             ] 43/75 batches, loss: 0.4148Epoch 1/15: [=================             ] 44/75 batches, loss: 0.4121Epoch 1/15: [==================            ] 45/75 batches, loss: 0.4103Epoch 1/15: [==================            ] 46/75 batches, loss: 0.4044Epoch 1/15: [==================            ] 47/75 batches, loss: 0.4015Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3969Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3941Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3918Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3880Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3853Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3843Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3872Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3850Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3815Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3789Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3776Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3756Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3717Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3675Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3650Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3619Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3601Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3578Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3556Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3528Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3494Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3504Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3470Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3433Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3402Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3387Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3355Epoch 1/15: [==============================] 75/75 batches, loss: 0.3333
[2025-05-07 13:49:34,825][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3333
[2025-05-07 13:49:35,064][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0473, Metrics: {'mse': 0.047264229506254196, 'rmse': 0.21740337970292503, 'r2': -0.034708380699157715}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2054Epoch 2/15: [                              ] 2/75 batches, loss: 0.1999Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2205Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2226Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2075Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1866Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1928Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1957Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1886Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1996Epoch 2/15: [====                          ] 11/75 batches, loss: 0.2008Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1974Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1938Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1882Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1872Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1807Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1769Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1792Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1789Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1757Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1752Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1748Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1765Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1747Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1761Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1743Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1856Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1830Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1852Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1862Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1826Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1842Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1810Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1842Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1880Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1862Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1841Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1819Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1820Epoch 2/15: [================              ] 40/75 batches, loss: 0.1797Epoch 2/15: [================              ] 41/75 batches, loss: 0.1797Epoch 2/15: [================              ] 42/75 batches, loss: 0.1769Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1781Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1771Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1753Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1775Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1769Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1767Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1786Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1776Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1766Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1757Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1761Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1761Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1770Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1780Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1758Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1739Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1737Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1726Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1720Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1708Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1698Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1687Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1688Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1679Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1680Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1676Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1662Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1650Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1662Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1651Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1647Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1639Epoch 2/15: [==============================] 75/75 batches, loss: 0.1627
[2025-05-07 13:49:37,722][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1627
[2025-05-07 13:49:37,957][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0555, Metrics: {'mse': 0.0555664487183094, 'rmse': 0.2357253671506514, 'r2': -0.21646058559417725}
[2025-05-07 13:49:37,957][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1533Epoch 3/15: [                              ] 2/75 batches, loss: 0.1325Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1366Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1272Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1182Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1246Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1194Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1242Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1251Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1306Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1265Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1242Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1304Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1324Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1313Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1306Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1308Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1309Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1271Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1257Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1249Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1226Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1209Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1266Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1263Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1252Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1228Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1239Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1246Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1217Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1191Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1207Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1215Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1202Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1193Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1181Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1167Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1167Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1177Epoch 3/15: [================              ] 40/75 batches, loss: 0.1168Epoch 3/15: [================              ] 41/75 batches, loss: 0.1173Epoch 3/15: [================              ] 42/75 batches, loss: 0.1175Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1171Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1173Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1163Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1167Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1175Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1182Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1170Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1163Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1155Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1147Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1147Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1137Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1128Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1120Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1121Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1121Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1127Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1120Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1119Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1117Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1107Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1103Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1105Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1103Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1100Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1110Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1112Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1106Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1104Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1098Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1097Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1094Epoch 3/15: [==============================] 75/75 batches, loss: 0.1099
[2025-05-07 13:49:40,392][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1099
[2025-05-07 13:49:40,740][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0525, Metrics: {'mse': 0.052416130900382996, 'rmse': 0.22894569421673558, 'r2': -0.14749383926391602}
[2025-05-07 13:49:40,741][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0988Epoch 4/15: [                              ] 2/75 batches, loss: 0.1311Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1203Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1089Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1059Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1037Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1056Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1039Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1135Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1067Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1108Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1115Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1079Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1050Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1097Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1090Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1081Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1098Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1105Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1079Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1108Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1172Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1163Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1163Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1194Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1172Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1172Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1162Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1148Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1122Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1107Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1102Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1087Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.1075Epoch 4/15: [==============                ] 35/75 batches, loss: 0.1078Epoch 4/15: [==============                ] 36/75 batches, loss: 0.1083Epoch 4/15: [==============                ] 37/75 batches, loss: 0.1074Epoch 4/15: [===============               ] 38/75 batches, loss: 0.1067Epoch 4/15: [===============               ] 39/75 batches, loss: 0.1053Epoch 4/15: [================              ] 40/75 batches, loss: 0.1049Epoch 4/15: [================              ] 41/75 batches, loss: 0.1037Epoch 4/15: [================              ] 42/75 batches, loss: 0.1024Epoch 4/15: [=================             ] 43/75 batches, loss: 0.1021Epoch 4/15: [=================             ] 44/75 batches, loss: 0.1004Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0999Epoch 4/15: [==================            ] 46/75 batches, loss: 0.1002Epoch 4/15: [==================            ] 47/75 batches, loss: 0.1004Epoch 4/15: [===================           ] 48/75 batches, loss: 0.1007Epoch 4/15: [===================           ] 49/75 batches, loss: 0.1006Epoch 4/15: [====================          ] 50/75 batches, loss: 0.1011Epoch 4/15: [====================          ] 51/75 batches, loss: 0.1009Epoch 4/15: [====================          ] 52/75 batches, loss: 0.1012Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.1000Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0989Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0989Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0992Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0981Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0987Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0988Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0988Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0988Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0984Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0981Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0975Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0978Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0971Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0972Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0972Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0969Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0973Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0964Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0959Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0957Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0952Epoch 4/15: [==============================] 75/75 batches, loss: 0.0949
[2025-05-07 13:49:43,043][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0949
[2025-05-07 13:49:43,327][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0543, Metrics: {'mse': 0.054251860827207565, 'rmse': 0.23292028856930339, 'r2': -0.1876816749572754}
[2025-05-07 13:49:43,328][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1019Epoch 5/15: [                              ] 2/75 batches, loss: 0.1101Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0965Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0942Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0979Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0977Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0955Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0956Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0975Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0955Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0905Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0869Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0864Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0891Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0875Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0865Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0854Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0850Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0885Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0876Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0864Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0842Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0845Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0845Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0848Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0852Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0841Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0845Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0845Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0841Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0829Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0823Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0824Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0817Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0820Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0836Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0839Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0830Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0841Epoch 5/15: [================              ] 40/75 batches, loss: 0.0857Epoch 5/15: [================              ] 41/75 batches, loss: 0.0855Epoch 5/15: [================              ] 42/75 batches, loss: 0.0850Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0853Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0856Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0845Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0858Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0849Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0851Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0848Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0842Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0837Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0832Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0828Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0825Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0817Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0816Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0807Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0818Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0817Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0815Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0807Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0805Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0808Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0805Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0801Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0797Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0798Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0796Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0788Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0786Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0781Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0787Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0782Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0777Epoch 5/15: [==============================] 75/75 batches, loss: 0.0771
[2025-05-07 13:49:45,705][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0771
[2025-05-07 13:49:46,080][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0566, Metrics: {'mse': 0.05664980039000511, 'rmse': 0.23801218538134788, 'r2': -0.24017739295959473}
[2025-05-07 13:49:46,081][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:49:46,081][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-07 13:49:46,081][src.training.lm_trainer][INFO] - Training completed in 14.52 seconds
[2025-05-07 13:49:46,081][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:49:49,127][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.044795889407396317, 'rmse': 0.21165039430012011, 'r2': -0.44468069076538086}
[2025-05-07 13:49:49,128][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.047264229506254196, 'rmse': 0.21740337970292503, 'r2': -0.034708380699157715}
[2025-05-07 13:49:49,128][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04302410036325455, 'rmse': 0.20742251652907537, 'r2': -0.4109452962875366}
[2025-05-07 13:49:50,765][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer10/fi/fi/model.pt
[2025-05-07 13:49:50,767][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▁▂▁
wandb:       train_loss █▃▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▁▇▅▆█
wandb:          val_mse ▁▇▅▆█
wandb:           val_r2 █▂▄▃▁
wandb:         val_rmse ▁▇▅▆█
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04732
wandb:     best_val_mse 0.04726
wandb:      best_val_r2 -0.03471
wandb:    best_val_rmse 0.2174
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.04302
wandb:    final_test_r2 -0.41095
wandb:  final_test_rmse 0.20742
wandb:  final_train_mse 0.0448
wandb:   final_train_r2 -0.44468
wandb: final_train_rmse 0.21165
wandb:    final_val_mse 0.04726
wandb:     final_val_r2 -0.03471
wandb:   final_val_rmse 0.2174
wandb:    learning_rate 0.0001
wandb:       train_loss 0.07709
wandb:       train_time 14.52174
wandb:         val_loss 0.05664
wandb:          val_mse 0.05665
wandb:           val_r2 -0.24018
wandb:         val_rmse 0.23801
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_134914-lh423lai
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_134914-lh423lai/logs
Experiment probe_layer10_avg_max_depth_control2_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer10/fi/fi/results.json for layer 10
Running experiment: probe_layer10_avg_max_depth_control3_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_avg_max_depth_control3_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer10/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:50:24,158][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer10/fi
experiment_name: probe_layer10_avg_max_depth_control3_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:50:24,158][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:50:24,158][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 13:50:24,158][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:50:24,158][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:50:24,163][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:50:24,163][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 13:50:24,163][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:50:27,265][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:50:29,743][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:50:29,744][src.data.datasets][INFO] - Loading 'control_avg_max_depth_seed3' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:50:29,988][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_max_depth_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:22:13 2025).
[2025-05-07 13:50:30,094][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_max_depth_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:22:13 2025).
[2025-05-07 13:50:30,285][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:50:30,295][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:50:30,295][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:50:30,297][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:50:30,366][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:50:30,476][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:50:30,503][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:50:30,505][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:50:30,505][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:50:30,506][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:50:30,571][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:50:30,669][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:50:30,694][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:50:30,696][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:50:30,696][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:50:30,697][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:50:30,698][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:50:30,698][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:50:30,698][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:50:30,698][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:50:30,698][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:50:30,698][src.data.datasets][INFO] -   Mean: 0.1934, Std: 0.1761
[2025-05-07 13:50:30,699][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:50:30,699][src.data.datasets][INFO] - Sample label: 0.25
[2025-05-07 13:50:30,699][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:50:30,699][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:50:30,699][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:50:30,699][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:50:30,699][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-05-07 13:50:30,699][src.data.datasets][INFO] -   Mean: 0.2487, Std: 0.2137
[2025-05-07 13:50:30,699][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:50:30,699][src.data.datasets][INFO] - Sample label: 0.5
[2025-05-07 13:50:30,700][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:50:30,700][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:50:30,700][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:50:30,700][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:50:30,700][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-05-07 13:50:30,700][src.data.datasets][INFO] -   Mean: 0.1785, Std: 0.1746
[2025-05-07 13:50:30,700][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:50:30,700][src.data.datasets][INFO] - Sample label: 0.125
[2025-05-07 13:50:30,700][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:50:30,700][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:50:30,701][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:50:30,701][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 13:50:30,701][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:50:37,578][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:50:37,579][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:50:37,579][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-07 13:50:37,579][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:50:37,582][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:50:37,583][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:50:37,583][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:50:37,583][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:50:37,583][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:50:37,584][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:50:37,584][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6265Epoch 1/15: [                              ] 2/75 batches, loss: 0.8066Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7932Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7678Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7155Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6615Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6446Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6406Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6121Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6108Epoch 1/15: [====                          ] 11/75 batches, loss: 0.5851Epoch 1/15: [====                          ] 12/75 batches, loss: 0.5590Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.5399Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.5198Epoch 1/15: [======                        ] 15/75 batches, loss: 0.5036Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4921Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4903Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4838Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4741Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4722Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4795Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4850Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4745Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4672Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4646Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4588Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4635Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4611Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4551Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4502Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4485Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4479Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4406Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4380Epoch 1/15: [==============                ] 35/75 batches, loss: 0.4362Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4397Epoch 1/15: [==============                ] 37/75 batches, loss: 0.4339Epoch 1/15: [===============               ] 38/75 batches, loss: 0.4310Epoch 1/15: [===============               ] 39/75 batches, loss: 0.4275Epoch 1/15: [================              ] 40/75 batches, loss: 0.4281Epoch 1/15: [================              ] 41/75 batches, loss: 0.4274Epoch 1/15: [================              ] 42/75 batches, loss: 0.4241Epoch 1/15: [=================             ] 43/75 batches, loss: 0.4194Epoch 1/15: [=================             ] 44/75 batches, loss: 0.4180Epoch 1/15: [==================            ] 45/75 batches, loss: 0.4173Epoch 1/15: [==================            ] 46/75 batches, loss: 0.4109Epoch 1/15: [==================            ] 47/75 batches, loss: 0.4080Epoch 1/15: [===================           ] 48/75 batches, loss: 0.4036Epoch 1/15: [===================           ] 49/75 batches, loss: 0.4000Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3975Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3926Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3903Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3880Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3889Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3877Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3856Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3834Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3807Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3781Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3753Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3718Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3696Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3664Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3640Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3608Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3596Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3563Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3538Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3521Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3494Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3458Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3434Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3405Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3378Epoch 1/15: [==============================] 75/75 batches, loss: 0.3353
[2025-05-07 13:50:43,368][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3353
[2025-05-07 13:50:43,579][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0455, Metrics: {'mse': 0.04544423148036003, 'rmse': 0.21317652656978917, 'r2': 0.005134940147399902}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1874Epoch 2/15: [                              ] 2/75 batches, loss: 0.1783Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1846Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1947Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1888Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1674Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1802Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1924Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1884Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1967Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1993Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1927Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1923Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1893Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1879Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1813Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1779Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1840Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1885Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1816Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1804Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1815Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1824Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1796Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1818Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1823Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1887Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1877Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1877Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1921Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1895Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1897Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1893Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1906Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1909Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1885Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1862Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1839Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1840Epoch 2/15: [================              ] 40/75 batches, loss: 0.1826Epoch 2/15: [================              ] 41/75 batches, loss: 0.1819Epoch 2/15: [================              ] 42/75 batches, loss: 0.1802Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1797Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1781Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1773Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1781Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1772Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1768Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1783Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1769Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1755Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1741Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1738Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1730Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1745Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1745Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1729Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1718Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1698Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1689Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1692Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1684Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1662Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1660Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1674Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1668Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1666Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1666Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1654Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1640Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1644Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1629Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1621Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1609Epoch 2/15: [==============================] 75/75 batches, loss: 0.1595
[2025-05-07 13:50:46,243][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1595
[2025-05-07 13:50:46,504][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0607, Metrics: {'mse': 0.06076015532016754, 'rmse': 0.24649575111990782, 'r2': -0.3301612138748169}
[2025-05-07 13:50:46,504][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1464Epoch 3/15: [                              ] 2/75 batches, loss: 0.1226Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1634Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1474Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1401Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1372Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1289Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1235Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1233Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1278Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1308Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1343Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1371Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1370Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1371Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1337Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1321Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1331Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1306Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1288Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1264Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1237Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1226Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1239Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1233Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1222Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1213Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1212Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1229Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1223Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1216Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1237Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1227Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1227Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1203Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1221Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1218Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1217Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1210Epoch 3/15: [================              ] 40/75 batches, loss: 0.1202Epoch 3/15: [================              ] 41/75 batches, loss: 0.1202Epoch 3/15: [================              ] 42/75 batches, loss: 0.1196Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1188Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1196Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1181Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1176Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1173Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1164Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1162Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1151Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1139Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1132Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1131Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1119Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1117Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1112Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1123Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1123Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1124Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1113Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1111Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1104Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1096Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1100Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1098Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1099Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1096Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1097Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1100Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1097Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1089Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1096Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1093Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1090Epoch 3/15: [==============================] 75/75 batches, loss: 0.1095
[2025-05-07 13:50:48,792][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1095
[2025-05-07 13:50:49,044][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0527, Metrics: {'mse': 0.052605025470256805, 'rmse': 0.2293578546077217, 'r2': -0.15162920951843262}
[2025-05-07 13:50:49,045][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0997Epoch 4/15: [                              ] 2/75 batches, loss: 0.1043Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0873Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1088Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1060Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1049Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1091Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1086Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1100Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1033Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1072Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1182Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1145Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1089Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1107Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1116Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1104Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1098Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1120Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1091Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1083Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1103Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1102Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1115Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1136Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1132Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1155Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1142Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1135Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1132Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1114Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1104Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1095Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.1091Epoch 4/15: [==============                ] 35/75 batches, loss: 0.1088Epoch 4/15: [==============                ] 36/75 batches, loss: 0.1088Epoch 4/15: [==============                ] 37/75 batches, loss: 0.1073Epoch 4/15: [===============               ] 38/75 batches, loss: 0.1077Epoch 4/15: [===============               ] 39/75 batches, loss: 0.1064Epoch 4/15: [================              ] 40/75 batches, loss: 0.1057Epoch 4/15: [================              ] 41/75 batches, loss: 0.1057Epoch 4/15: [================              ] 42/75 batches, loss: 0.1048Epoch 4/15: [=================             ] 43/75 batches, loss: 0.1050Epoch 4/15: [=================             ] 44/75 batches, loss: 0.1050Epoch 4/15: [==================            ] 45/75 batches, loss: 0.1051Epoch 4/15: [==================            ] 46/75 batches, loss: 0.1059Epoch 4/15: [==================            ] 47/75 batches, loss: 0.1057Epoch 4/15: [===================           ] 48/75 batches, loss: 0.1062Epoch 4/15: [===================           ] 49/75 batches, loss: 0.1052Epoch 4/15: [====================          ] 50/75 batches, loss: 0.1049Epoch 4/15: [====================          ] 51/75 batches, loss: 0.1048Epoch 4/15: [====================          ] 52/75 batches, loss: 0.1052Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.1041Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.1035Epoch 4/15: [======================        ] 55/75 batches, loss: 0.1040Epoch 4/15: [======================        ] 56/75 batches, loss: 0.1038Epoch 4/15: [======================        ] 57/75 batches, loss: 0.1027Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.1019Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.1020Epoch 4/15: [========================      ] 60/75 batches, loss: 0.1014Epoch 4/15: [========================      ] 61/75 batches, loss: 0.1027Epoch 4/15: [========================      ] 62/75 batches, loss: 0.1027Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.1020Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.1027Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.1021Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.1016Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.1012Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.1015Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.1012Epoch 4/15: [============================  ] 70/75 batches, loss: 0.1012Epoch 4/15: [============================  ] 71/75 batches, loss: 0.1001Epoch 4/15: [============================  ] 72/75 batches, loss: 0.1004Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0999Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0991Epoch 4/15: [==============================] 75/75 batches, loss: 0.0984
[2025-05-07 13:50:51,310][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0984
[2025-05-07 13:50:51,588][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0666, Metrics: {'mse': 0.06666437536478043, 'rmse': 0.25819445262201207, 'r2': -0.4594165086746216}
[2025-05-07 13:50:51,589][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0750Epoch 5/15: [                              ] 2/75 batches, loss: 0.0710Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0772Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0854Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0835Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0790Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0788Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0843Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0791Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0778Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0805Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0791Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0778Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0789Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0787Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0775Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0774Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0775Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0769Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0763Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0764Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0753Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0751Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0747Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0756Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0742Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0761Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0756Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0750Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0743Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0758Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0758Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0763Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0764Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0761Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0774Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0790Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0784Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0781Epoch 5/15: [================              ] 40/75 batches, loss: 0.0791Epoch 5/15: [================              ] 41/75 batches, loss: 0.0793Epoch 5/15: [================              ] 42/75 batches, loss: 0.0785Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0779Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0779Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0767Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0770Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0768Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0766Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0767Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0760Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0754Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0750Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0752Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0751Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0742Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0744Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0750Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0746Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0752Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0757Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0747Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0747Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0752Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0748Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0749Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0746Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0744Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0740Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0734Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0739Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0740Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0743Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0741Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0742Epoch 5/15: [==============================] 75/75 batches, loss: 0.0734
[2025-05-07 13:50:53,856][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0734
[2025-05-07 13:50:54,110][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0531, Metrics: {'mse': 0.05311714485287666, 'rmse': 0.2304715705957606, 'r2': -0.16284048557281494}
[2025-05-07 13:50:54,110][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:50:54,110][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-07 13:50:54,111][src.training.lm_trainer][INFO] - Training completed in 13.81 seconds
[2025-05-07 13:50:54,111][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:50:57,005][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.0423627570271492, 'rmse': 0.20582214901984966, 'r2': -0.36621153354644775}
[2025-05-07 13:50:57,006][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.04544423148036003, 'rmse': 0.21317652656978917, 'r2': 0.005134940147399902}
[2025-05-07 13:50:57,006][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04117226228117943, 'rmse': 0.20290949283160567, 'r2': -0.3502155542373657}
[2025-05-07 13:50:58,670][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer10/fi/fi/model.pt
[2025-05-07 13:50:58,671][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▂▄▁
wandb:       train_loss █▃▂▂▁
wandb:       train_time ▁
wandb:         val_loss ▁▆▃█▄
wandb:          val_mse ▁▆▃█▄
wandb:           val_r2 █▃▆▁▅
wandb:         val_rmse ▁▆▄█▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04548
wandb:     best_val_mse 0.04544
wandb:      best_val_r2 0.00513
wandb:    best_val_rmse 0.21318
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.04117
wandb:    final_test_r2 -0.35022
wandb:  final_test_rmse 0.20291
wandb:  final_train_mse 0.04236
wandb:   final_train_r2 -0.36621
wandb: final_train_rmse 0.20582
wandb:    final_val_mse 0.04544
wandb:     final_val_r2 0.00513
wandb:   final_val_rmse 0.21318
wandb:    learning_rate 0.0001
wandb:       train_loss 0.07341
wandb:       train_time 13.80845
wandb:         val_loss 0.05312
wandb:          val_mse 0.05312
wandb:           val_r2 -0.16284
wandb:         val_rmse 0.23047
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_135024-imq970xu
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_135024-imq970xu/logs
Experiment probe_layer10_avg_max_depth_control3_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer10/fi/fi/results.json for layer 10
Running experiment: probe_layer10_avg_subordinate_chain_len_control1_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_avg_subordinate_chain_len_control1_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer10/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_subordinate_chain_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:51:29,245][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer10/fi
experiment_name: probe_layer10_avg_subordinate_chain_len_control1_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:51:29,245][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:51:29,246][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 13:51:29,246][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:51:29,246][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:51:29,250][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:51:29,250][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 13:51:29,250][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:51:33,753][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:51:36,056][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:51:36,056][src.data.datasets][INFO] - Loading 'control_avg_subordinate_chain_len_seed1' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:51:36,300][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:23:31 2025).
[2025-05-07 13:51:36,464][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:23:31 2025).
[2025-05-07 13:51:36,828][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:51:36,836][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:51:36,837][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:51:36,840][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:51:36,942][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:51:37,016][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:51:37,070][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:51:37,071][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:51:37,071][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:51:37,073][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:51:37,094][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:51:37,169][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:51:37,204][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:51:37,205][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:51:37,206][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:51:37,206][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:51:37,207][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:51:37,208][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:51:37,208][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:51:37,208][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:51:37,208][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:51:37,208][src.data.datasets][INFO] -   Mean: 0.0268, Std: 0.1180
[2025-05-07 13:51:37,208][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:51:37,208][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 13:51:37,208][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:51:37,209][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:51:37,209][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:51:37,209][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:51:37,209][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:51:37,209][src.data.datasets][INFO] -   Mean: 0.1217, Std: 0.2206
[2025-05-07 13:51:37,209][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:51:37,209][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 13:51:37,209][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:51:37,209][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:51:37,209][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:51:37,210][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:51:37,210][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:51:37,210][src.data.datasets][INFO] -   Mean: 0.1272, Std: 0.2107
[2025-05-07 13:51:37,210][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:51:37,210][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 13:51:37,210][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:51:37,210][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:51:37,210][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:51:37,211][__main__][INFO] - Using model type: lm_probe for submetric avg_subordinate_chain_len
[2025-05-07 13:51:37,211][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:51:46,947][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:51:46,948][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:51:46,948][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-07 13:51:46,948][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:51:46,951][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:51:46,952][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:51:46,952][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:51:46,952][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:51:46,952][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:51:46,953][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:51:46,953][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5989Epoch 1/15: [                              ] 2/75 batches, loss: 0.7923Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7567Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7373Epoch 1/15: [==                            ] 5/75 batches, loss: 0.6925Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6158Epoch 1/15: [==                            ] 7/75 batches, loss: 0.5906Epoch 1/15: [===                           ] 8/75 batches, loss: 0.5923Epoch 1/15: [===                           ] 9/75 batches, loss: 0.5663Epoch 1/15: [====                          ] 10/75 batches, loss: 0.5694Epoch 1/15: [====                          ] 11/75 batches, loss: 0.5511Epoch 1/15: [====                          ] 12/75 batches, loss: 0.5293Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.5127Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4968Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4775Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4643Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4656Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4593Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4484Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4462Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4576Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4592Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4486Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4390Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4323Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4292Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4362Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4356Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4283Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4284Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4226Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4221Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4141Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4124Epoch 1/15: [==============                ] 35/75 batches, loss: 0.4093Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4117Epoch 1/15: [==============                ] 37/75 batches, loss: 0.4063Epoch 1/15: [===============               ] 38/75 batches, loss: 0.4032Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3982Epoch 1/15: [================              ] 40/75 batches, loss: 0.3971Epoch 1/15: [================              ] 41/75 batches, loss: 0.3962Epoch 1/15: [================              ] 42/75 batches, loss: 0.3936Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3905Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3888Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3886Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3830Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3807Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3768Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3729Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3700Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3664Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3630Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3606Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3611Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3593Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3566Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3539Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3532Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3506Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3472Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3435Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3408Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3377Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3361Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3338Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3320Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3290Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3278Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3269Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3255Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3220Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3195Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3167Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3138Epoch 1/15: [==============================] 75/75 batches, loss: 0.3119
[2025-05-07 13:51:54,154][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3119
[2025-05-07 13:51:54,430][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0559, Metrics: {'mse': 0.05598432943224907, 'rmse': 0.23661007888982472, 'r2': -0.1502368450164795}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2263Epoch 2/15: [                              ] 2/75 batches, loss: 0.1857Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1872Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2151Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2033Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1789Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1769Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1797Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1682Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1736Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1692Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1682Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1627Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1594Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1546Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1516Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1531Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1548Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1602Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1576Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1595Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1577Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1567Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1539Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1546Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1554Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1640Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1610Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1616Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1618Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1601Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1597Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1573Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1604Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1583Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1576Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1552Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1539Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1536Epoch 2/15: [================              ] 40/75 batches, loss: 0.1512Epoch 2/15: [================              ] 41/75 batches, loss: 0.1503Epoch 2/15: [================              ] 42/75 batches, loss: 0.1489Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1485Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1472Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1461Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1462Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1460Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1455Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1468Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1452Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1441Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1432Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1431Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1424Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1435Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1443Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1426Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1421Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1410Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1402Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1394Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1384Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1381Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1373Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1377Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1368Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1368Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1357Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1342Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1330Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1329Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1319Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1316Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1313Epoch 2/15: [==============================] 75/75 batches, loss: 0.1306
[2025-05-07 13:51:57,164][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1306
[2025-05-07 13:51:57,389][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0933, Metrics: {'mse': 0.09334298968315125, 'rmse': 0.3055208498337736, 'r2': -0.9177964925765991}
[2025-05-07 13:51:57,389][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1395Epoch 3/15: [                              ] 2/75 batches, loss: 0.1206Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1373Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1205Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1127Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1134Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1059Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1025Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1013Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1037Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1022Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1001Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1005Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1063Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1067Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1038Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1051Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1070Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1036Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1014Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0990Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0967Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0987Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0989Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0974Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0978Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0961Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0960Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0982Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0981Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0969Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0975Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0961Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0958Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0944Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0964Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0948Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0954Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0963Epoch 3/15: [================              ] 40/75 batches, loss: 0.0955Epoch 3/15: [================              ] 41/75 batches, loss: 0.0972Epoch 3/15: [================              ] 42/75 batches, loss: 0.0974Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0971Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0959Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0945Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0940Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0945Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0946Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0945Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0939Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0930Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0927Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0938Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0930Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0923Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0918Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0927Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0929Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0932Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0930Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0924Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0919Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0913Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0910Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0906Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0902Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0897Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0900Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0896Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0893Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0887Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0884Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0881Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0878Epoch 3/15: [==============================] 75/75 batches, loss: 0.0876
[2025-05-07 13:51:59,721][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0876
[2025-05-07 13:52:00,009][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0797, Metrics: {'mse': 0.0797589123249054, 'rmse': 0.2824162040763692, 'r2': -0.6387020349502563}
[2025-05-07 13:52:00,009][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1073Epoch 4/15: [                              ] 2/75 batches, loss: 0.0993Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0820Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0873Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0850Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0863Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0866Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0870Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0891Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0828Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0836Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0831Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0800Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0762Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0812Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0872Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0856Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0872Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0879Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0853Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0854Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0874Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0856Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0861Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0868Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0854Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0856Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0848Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0840Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0833Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0817Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0815Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0827Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0820Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0811Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0824Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0809Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0805Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0794Epoch 4/15: [================              ] 40/75 batches, loss: 0.0793Epoch 4/15: [================              ] 41/75 batches, loss: 0.0826Epoch 4/15: [================              ] 42/75 batches, loss: 0.0816Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0818Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0807Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0802Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0805Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0800Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0798Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0801Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0797Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0795Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0792Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0796Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0796Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0794Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0794Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0787Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0781Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0786Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0792Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0791Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0794Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0791Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0796Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0796Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0788Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0785Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0786Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0790Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0789Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0790Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0787Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0781Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0776Epoch 4/15: [==============================] 75/75 batches, loss: 0.0776
[2025-05-07 13:52:02,295][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0776
[2025-05-07 13:52:02,562][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0684, Metrics: {'mse': 0.06839460879564285, 'rmse': 0.2615236295168046, 'r2': -0.4052145481109619}
[2025-05-07 13:52:02,563][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0752Epoch 5/15: [                              ] 2/75 batches, loss: 0.0651Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0761Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0766Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0752Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0742Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0783Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0795Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0752Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0711Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0694Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0656Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0617Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0614Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0610Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0604Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0615Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0625Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0628Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0619Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0615Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0594Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0595Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0615Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0617Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0605Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0615Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0627Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0618Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0607Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0602Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0624Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0617Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0620Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0631Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0641Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0638Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0631Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0629Epoch 5/15: [================              ] 40/75 batches, loss: 0.0634Epoch 5/15: [================              ] 41/75 batches, loss: 0.0647Epoch 5/15: [================              ] 42/75 batches, loss: 0.0643Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0648Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0648Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0638Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0636Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0630Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0632Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0648Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0640Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0637Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0630Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0627Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0625Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0617Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0624Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0631Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0627Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0622Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0614Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0607Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0605Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0603Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0598Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0610Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0604Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0600Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0599Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0595Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0594Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0590Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0599Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0596Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0592Epoch 5/15: [==============================] 75/75 batches, loss: 0.0586
[2025-05-07 13:52:04,905][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0586
[2025-05-07 13:52:05,187][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0642, Metrics: {'mse': 0.06428470462560654, 'rmse': 0.2535442853341533, 'r2': -0.320773720741272}
[2025-05-07 13:52:05,187][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:52:05,187][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-07 13:52:05,188][src.training.lm_trainer][INFO] - Training completed in 14.10 seconds
[2025-05-07 13:52:05,188][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:52:08,138][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.025935696437954903, 'rmse': 0.16104563464420543, 'r2': -0.8622229099273682}
[2025-05-07 13:52:08,138][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05598432943224907, 'rmse': 0.23661007888982472, 'r2': -0.1502368450164795}
[2025-05-07 13:52:08,138][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.07831907272338867, 'rmse': 0.27985544969392445, 'r2': -0.7641961574554443}
[2025-05-07 13:52:09,873][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer10/fi/fi/model.pt
[2025-05-07 13:52:09,874][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▁▃▄
wandb:       train_loss █▃▂▂▁
wandb:       train_time ▁
wandb:         val_loss ▁█▅▃▃
wandb:          val_mse ▁█▅▃▃
wandb:           val_r2 █▁▄▆▆
wandb:         val_rmse ▁█▆▄▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05595
wandb:     best_val_mse 0.05598
wandb:      best_val_r2 -0.15024
wandb:    best_val_rmse 0.23661
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.07832
wandb:    final_test_r2 -0.7642
wandb:  final_test_rmse 0.27986
wandb:  final_train_mse 0.02594
wandb:   final_train_r2 -0.86222
wandb: final_train_rmse 0.16105
wandb:    final_val_mse 0.05598
wandb:     final_val_r2 -0.15024
wandb:   final_val_rmse 0.23661
wandb:    learning_rate 0.0001
wandb:       train_loss 0.0586
wandb:       train_time 14.09591
wandb:         val_loss 0.06425
wandb:          val_mse 0.06428
wandb:           val_r2 -0.32077
wandb:         val_rmse 0.25354
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_135129-gy8srwry
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_135129-gy8srwry/logs
Experiment probe_layer10_avg_subordinate_chain_len_control1_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer10/fi/fi/results.json for layer 10
Running experiment: probe_layer10_avg_subordinate_chain_len_control2_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_avg_subordinate_chain_len_control2_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer10/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=avg_subordinate_chain_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:52:36,959][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer10/fi
experiment_name: probe_layer10_avg_subordinate_chain_len_control2_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:52:36,959][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:52:36,959][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 13:52:36,959][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:52:36,959][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:52:36,964][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:52:36,964][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 13:52:36,964][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:52:40,077][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:52:42,391][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:52:42,392][src.data.datasets][INFO] - Loading 'control_avg_subordinate_chain_len_seed2' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:52:42,515][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:25:20 2025).
[2025-05-07 13:52:42,603][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:25:20 2025).
[2025-05-07 13:52:42,854][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:52:42,862][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:52:42,863][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:52:42,865][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:52:42,962][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:52:43,050][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:52:43,083][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:52:43,085][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:52:43,085][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:52:43,086][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:52:43,188][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:52:43,272][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:52:43,287][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:52:43,288][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:52:43,289][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:52:43,289][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:52:43,290][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:52:43,290][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:52:43,290][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:52:43,290][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:52:43,290][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:52:43,290][src.data.datasets][INFO] -   Mean: 0.0268, Std: 0.1180
[2025-05-07 13:52:43,290][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:52:43,291][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 13:52:43,291][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:52:43,291][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:52:43,291][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:52:43,291][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:52:43,291][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:52:43,291][src.data.datasets][INFO] -   Mean: 0.1217, Std: 0.2206
[2025-05-07 13:52:43,291][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:52:43,291][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 13:52:43,291][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:52:43,292][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:52:43,292][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:52:43,292][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:52:43,292][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:52:43,292][src.data.datasets][INFO] -   Mean: 0.1272, Std: 0.2107
[2025-05-07 13:52:43,292][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:52:43,292][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 13:52:43,292][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:52:43,292][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:52:43,293][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:52:43,293][__main__][INFO] - Using model type: lm_probe for submetric avg_subordinate_chain_len
[2025-05-07 13:52:43,293][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:52:49,941][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:52:49,942][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:52:49,942][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-07 13:52:49,942][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:52:49,945][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:52:49,946][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:52:49,946][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:52:49,946][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:52:49,946][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:52:49,947][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:52:49,947][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6072Epoch 1/15: [                              ] 2/75 batches, loss: 0.7851Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7532Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7178Epoch 1/15: [==                            ] 5/75 batches, loss: 0.6743Epoch 1/15: [==                            ] 6/75 batches, loss: 0.5959Epoch 1/15: [==                            ] 7/75 batches, loss: 0.5721Epoch 1/15: [===                           ] 8/75 batches, loss: 0.5720Epoch 1/15: [===                           ] 9/75 batches, loss: 0.5522Epoch 1/15: [====                          ] 10/75 batches, loss: 0.5563Epoch 1/15: [====                          ] 11/75 batches, loss: 0.5370Epoch 1/15: [====                          ] 12/75 batches, loss: 0.5112Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4954Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4837Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4688Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4563Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4576Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4532Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4418Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4396Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4445Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4444Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4364Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4292Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4224Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4164Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4197Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4203Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4156Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4148Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4094Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4092Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4015Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3985Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3953Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3975Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3921Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3899Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3851Epoch 1/15: [================              ] 40/75 batches, loss: 0.3840Epoch 1/15: [================              ] 41/75 batches, loss: 0.3829Epoch 1/15: [================              ] 42/75 batches, loss: 0.3808Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3760Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3731Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3747Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3690Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3663Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3624Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3587Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3570Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3533Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3512Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3501Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3500Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3483Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3448Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3416Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3417Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3383Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3349Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3320Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3296Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3275Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3258Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3243Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3230Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3202Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3173Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3175Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3147Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3113Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3091Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3077Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3045Epoch 1/15: [==============================] 75/75 batches, loss: 0.3020
[2025-05-07 13:52:56,588][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3020
[2025-05-07 13:52:56,826][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0558, Metrics: {'mse': 0.05581622198224068, 'rmse': 0.23625457028857808, 'r2': -0.1467829942703247}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1885Epoch 2/15: [                              ] 2/75 batches, loss: 0.1627Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1791Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1898Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1788Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1602Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1634Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1658Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1550Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1598Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1607Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1597Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1583Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1543Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1510Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1482Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1465Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1510Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1512Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1472Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1456Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1441Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1477Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1447Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1481Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1481Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1562Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1550Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1562Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1563Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1545Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1546Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1518Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1536Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1536Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1520Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1503Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1486Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1490Epoch 2/15: [================              ] 40/75 batches, loss: 0.1476Epoch 2/15: [================              ] 41/75 batches, loss: 0.1479Epoch 2/15: [================              ] 42/75 batches, loss: 0.1457Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1452Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1455Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1445Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1463Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1455Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1457Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1453Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1437Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1427Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1418Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1420Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1412Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1424Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1430Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1413Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1399Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1397Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1392Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1387Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1373Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1366Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1359Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1364Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1362Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1364Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1362Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1356Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1347Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1350Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1344Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1340Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1345Epoch 2/15: [==============================] 75/75 batches, loss: 0.1331
[2025-05-07 13:52:59,496][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1331
[2025-05-07 13:52:59,743][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0818, Metrics: {'mse': 0.08188865333795547, 'rmse': 0.2861619355154621, 'r2': -0.6824591159820557}
[2025-05-07 13:52:59,743][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1319Epoch 3/15: [                              ] 2/75 batches, loss: 0.1039Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1145Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1105Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1100Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1033Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0967Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0962Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0949Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0991Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0947Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0923Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0982Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0989Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0976Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0973Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0990Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0987Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0966Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0947Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0948Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0937Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0931Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0959Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0963Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0954Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0945Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0957Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0960Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0951Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0941Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0970Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0972Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0963Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0949Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0946Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0931Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0941Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0943Epoch 3/15: [================              ] 40/75 batches, loss: 0.0936Epoch 3/15: [================              ] 41/75 batches, loss: 0.0945Epoch 3/15: [================              ] 42/75 batches, loss: 0.0941Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0938Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0928Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0921Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0939Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0941Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0943Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0937Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0932Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0923Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0916Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0919Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0912Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0907Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0903Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0905Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0907Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0904Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0900Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0899Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0894Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0885Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0883Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0884Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0880Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0875Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0878Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0875Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0873Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0874Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0872Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0871Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0870Epoch 3/15: [==============================] 75/75 batches, loss: 0.0876
[2025-05-07 13:53:02,131][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0876
[2025-05-07 13:53:02,514][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0763, Metrics: {'mse': 0.07636700570583344, 'rmse': 0.2763458081929839, 'r2': -0.5690129995346069}
[2025-05-07 13:53:02,514][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0578Epoch 4/15: [                              ] 2/75 batches, loss: 0.0758Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0743Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0785Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0759Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0772Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0786Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0773Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0808Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0759Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0797Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0815Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0783Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0749Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0805Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0825Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0848Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0853Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0859Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0859Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0858Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0873Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0859Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0857Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0894Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0880Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0886Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0877Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0865Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0849Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0845Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0838Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0847Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0840Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0842Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0858Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0847Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0842Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0831Epoch 4/15: [================              ] 40/75 batches, loss: 0.0826Epoch 4/15: [================              ] 41/75 batches, loss: 0.0829Epoch 4/15: [================              ] 42/75 batches, loss: 0.0821Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0822Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0810Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0805Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0818Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0814Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0811Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0817Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0816Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0813Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0812Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0802Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0798Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0797Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0801Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0790Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0790Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0791Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0790Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0794Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0788Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0784Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0780Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0785Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0776Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0772Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0774Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0775Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0779Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0772Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0767Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0764Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0758Epoch 4/15: [==============================] 75/75 batches, loss: 0.0756
[2025-05-07 13:53:04,841][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0756
[2025-05-07 13:53:05,118][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0750, Metrics: {'mse': 0.07503452897071838, 'rmse': 0.27392431248561777, 'r2': -0.5416363477706909}
[2025-05-07 13:53:05,118][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0958Epoch 5/15: [                              ] 2/75 batches, loss: 0.0930Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0931Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0859Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0798Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0804Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0822Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0826Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0775Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0737Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0723Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0687Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0659Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0703Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0703Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0685Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0669Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0656Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0692Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0683Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0678Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0655Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0660Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0668Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0664Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0649Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0652Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0651Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0641Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0644Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0639Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0640Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0644Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0637Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0639Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0639Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0649Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0641Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0641Epoch 5/15: [================              ] 40/75 batches, loss: 0.0663Epoch 5/15: [================              ] 41/75 batches, loss: 0.0662Epoch 5/15: [================              ] 42/75 batches, loss: 0.0657Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0656Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0650Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0644Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0644Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0637Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0632Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0644Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0636Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0628Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0621Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0616Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0610Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0602Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0604Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0600Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0597Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0599Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0591Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0585Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0583Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0582Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0581Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0579Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0580Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0578Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0576Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0572Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0570Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0574Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0580Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0575Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0572Epoch 5/15: [==============================] 75/75 batches, loss: 0.0568
[2025-05-07 13:53:07,527][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0568
[2025-05-07 13:53:07,836][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0620, Metrics: {'mse': 0.062040772289037704, 'rmse': 0.2490798512305596, 'r2': -0.2746706008911133}
[2025-05-07 13:53:07,837][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:53:07,837][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-07 13:53:07,837][src.training.lm_trainer][INFO] - Training completed in 14.47 seconds
[2025-05-07 13:53:07,837][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:53:10,839][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.025663092732429504, 'rmse': 0.16019704345720462, 'r2': -0.8426496982574463}
[2025-05-07 13:53:10,840][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05581622198224068, 'rmse': 0.23625457028857808, 'r2': -0.1467829942703247}
[2025-05-07 13:53:10,840][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.07790149003267288, 'rmse': 0.27910838402433, 'r2': -0.7547898292541504}
[2025-05-07 13:53:12,507][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer10/fi/fi/model.pt
[2025-05-07 13:53:12,508][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▁▂▂
wandb:       train_loss █▃▂▂▁
wandb:       train_time ▁
wandb:         val_loss ▁█▇▆▃
wandb:          val_mse ▁█▇▆▃
wandb:           val_r2 █▁▂▃▆
wandb:         val_rmse ▁█▇▆▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05579
wandb:     best_val_mse 0.05582
wandb:      best_val_r2 -0.14678
wandb:    best_val_rmse 0.23625
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.0779
wandb:    final_test_r2 -0.75479
wandb:  final_test_rmse 0.27911
wandb:  final_train_mse 0.02566
wandb:   final_train_r2 -0.84265
wandb: final_train_rmse 0.1602
wandb:    final_val_mse 0.05582
wandb:     final_val_r2 -0.14678
wandb:   final_val_rmse 0.23625
wandb:    learning_rate 0.0001
wandb:       train_loss 0.05677
wandb:       train_time 14.46557
wandb:         val_loss 0.06202
wandb:          val_mse 0.06204
wandb:           val_r2 -0.27467
wandb:         val_rmse 0.24908
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_135237-ecghlrqv
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_135237-ecghlrqv/logs
Experiment probe_layer10_avg_subordinate_chain_len_control2_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer10/fi/fi/results.json for layer 10
Running experiment: probe_layer10_avg_subordinate_chain_len_control3_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_avg_subordinate_chain_len_control3_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control3/layer10/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=avg_subordinate_chain_len"
slurmstepd: error: *** JOB 64464357 ON k28i22 CANCELLED AT 2025-05-07T13:53:16 ***
