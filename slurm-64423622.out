SLURM_JOB_ID: 64423622
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: finetune_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Tue Apr 29 20:32:39 CEST 2025
Walltime: 00-00:30:00
========================================================================
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 20:32:59,269][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_test_output
experiment_name: test_finetune
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: disabled
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 2
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  debug_mode: true
experiment:
  type: lm_finetune
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  finetune: true
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1
    lr: 2.0e-05
    batch_size: 8

[2025-04-29 20:32:59,270][__main__][INFO] - Normalized task: question_type
[2025-04-29 20:32:59,270][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 20:32:59,270][__main__][INFO] - Determined Task Type: classification
[2025-04-29 20:32:59,274][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 20:32:59,274][__main__][INFO] - Processing language: en
[2025-04-29 20:32:59,274][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 20:33:01,533][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 20:33:01,533][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:33:01,821][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:33:01,954][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:33:02,177][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 20:33:02,185][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:33:02,185][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 20:33:02,188][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:33:02,230][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:33:02,318][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:33:02,343][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 20:33:02,344][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:33:02,344][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 20:33:02,346][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:33:02,395][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:33:02,487][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:33:02,507][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 20:33:02,508][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:33:02,508][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 20:33:02,510][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 20:33:02,511][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:33:02,511][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:33:02,511][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:33:02,511][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:33:02,511][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 20:33:02,511][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 20:33:02,511][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 20:33:02,511][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 20:33:02,511][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:33:02,511][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:33:02,511][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:33:02,511][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:33:02,511][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 20:33:02,511][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 20:33:02,511][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 20:33:02,511][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:33:02,511][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:33:02,511][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:33:02,511][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:33:02,511][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:33:02,512][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 20:33:02,512][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 20:33:02,512][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 20:33:02,512][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:33:02,512][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 20:33:02,512][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 20:33:02,512][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 20:33:02,512][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 20:33:08,479][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 20:33:08,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,484][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,485][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,486][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,486][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,486][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,486][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,486][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,486][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,486][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:08,486][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 20:33:08,486][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 20:33:08,487][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 20:33:08,487][__main__][INFO] - Successfully created model for en
[2025-04-29 20:33:08,488][__main__][INFO] - finetuning with gradient accum steps: 1
Epoch 1/2: [Epoch 1/2: [                              ] 1/75 batches, loss: 0.7217Epoch 1/2: [                              ] 2/75 batches, loss: 0.7097Epoch 1/2: [=                             ] 3/75 batches, loss: 0.6945Epoch 1/2: [=                             ] 4/75 batches, loss: 0.6914Epoch 1/2: [==                            ] 5/75 batches, loss: 0.6888Epoch 1/2: [==                            ] 6/75 batches, loss: 0.6902Epoch 1/2: [==                            ] 7/75 batches, loss: 0.6881Epoch 1/2: [===                           ] 8/75 batches, loss: 0.6889Epoch 1/2: [===                           ] 9/75 batches, loss: 0.6907Epoch 1/2: [====                          ] 10/75 batches, loss: 0.6895Epoch 1/2: [====                          ] 11/75 batches, loss: 0.6863Epoch 1/2: [====                          ] 12/75 batches, loss: 0.6858Epoch 1/2: [=====                         ] 13/75 batches, loss: 0.6854Epoch 1/2: [=====                         ] 14/75 batches, loss: 0.6858Epoch 1/2: [======                        ] 15/75 batches, loss: 0.6863Epoch 1/2: [======                        ] 16/75 batches, loss: 0.6851Epoch 1/2: [======                        ] 17/75 batches, loss: 0.6847Epoch 1/2: [=======                       ] 18/75 batches, loss: 0.6842Epoch 1/2: [=======                       ] 19/75 batches, loss: 0.6849Epoch 1/2: [========                      ] 20/75 batches, loss: 0.6853Epoch 1/2: [========                      ] 21/75 batches, loss: 0.6850Epoch 1/2: [========                      ] 22/75 batches, loss: 0.6864Epoch 1/2: [=========                     ] 23/75 batches, loss: 0.6856Epoch 1/2: [=========                     ] 24/75 batches, loss: 0.6859Epoch 1/2: [==========                    ] 25/75 batches, loss: 0.6850Epoch 1/2: [==========                    ] 26/75 batches, loss: 0.6863Epoch 1/2: [==========                    ] 27/75 batches, loss: 0.6854Epoch 1/2: [===========                   ] 28/75 batches, loss: 0.6851Epoch 1/2: [===========                   ] 29/75 batches, loss: 0.6852Epoch 1/2: [============                  ] 30/75 batches, loss: 0.6849Epoch 1/2: [============                  ] 31/75 batches, loss: 0.6857Epoch 1/2: [============                  ] 32/75 batches, loss: 0.6843Epoch 1/2: [=============                 ] 33/75 batches, loss: 0.6842Epoch 1/2: [=============                 ] 34/75 batches, loss: 0.6844Epoch 1/2: [==============                ] 35/75 batches, loss: 0.6850Epoch 1/2: [==============                ] 36/75 batches, loss: 0.6853Epoch 1/2: [==============                ] 37/75 batches, loss: 0.6857Epoch 1/2: [===============               ] 38/75 batches, loss: 0.6856Epoch 1/2: [===============               ] 39/75 batches, loss: 0.6855Epoch 1/2: [================              ] 40/75 batches, loss: 0.6856Epoch 1/2: [================              ] 41/75 batches, loss: 0.6854Epoch 1/2: [================              ] 42/75 batches, loss: 0.6853Epoch 1/2: [=================             ] 43/75 batches, loss: 0.6857Epoch 1/2: [=================             ] 44/75 batches, loss: 0.6855Epoch 1/2: [==================            ] 45/75 batches, loss: 0.6857Epoch 1/2: [==================            ] 46/75 batches, loss: 0.6857Epoch 1/2: [==================            ] 47/75 batches, loss: 0.6855Epoch 1/2: [===================           ] 48/75 batches, loss: 0.6855Epoch 1/2: [===================           ] 49/75 batches, loss: 0.6852Epoch 1/2: [====================          ] 50/75 batches, loss: 0.6849Epoch 1/2: [====================          ] 51/75 batches, loss: 0.6840Epoch 1/2: [====================          ] 52/75 batches, loss: 0.6830Epoch 1/2: [=====================         ] 53/75 batches, loss: 0.6825Epoch 1/2: [=====================         ] 54/75 batches, loss: 0.6820Epoch 1/2: [======================        ] 55/75 batches, loss: 0.6807Epoch 1/2: [======================        ] 56/75 batches, loss: 0.6795Epoch 1/2: [======================        ] 57/75 batches, loss: 0.6787Epoch 1/2: [=======================       ] 58/75 batches, loss: 0.6779Epoch 1/2: [=======================       ] 59/75 batches, loss: 0.6776Epoch 1/2: [========================      ] 60/75 batches, loss: 0.6752Epoch 1/2: [========================      ] 61/75 batches, loss: 0.6737Epoch 1/2: [========================      ] 62/75 batches, loss: 0.6725Epoch 1/2: [=========================     ] 63/75 batches, loss: 0.6712Epoch 1/2: [=========================     ] 64/75 batches, loss: 0.6698Epoch 1/2: [==========================    ] 65/75 batches, loss: 0.6680Epoch 1/2: [==========================    ] 66/75 batches, loss: 0.6669Epoch 1/2: [==========================    ] 67/75 batches, loss: 0.6661Epoch 1/2: [===========================   ] 68/75 batches, loss: 0.6639Epoch 1/2: [===========================   ] 69/75 batches, loss: 0.6614Epoch 1/2: [============================  ] 70/75 batches, loss: 0.6597Epoch 1/2: [============================  ] 71/75 batches, loss: 0.6576Epoch 1/2: [============================  ] 72/75 batches, loss: 0.6566Epoch 1/2: [============================= ] 73/75 batches, loss: 0.6550Epoch 1/2: [============================= ] 74/75 batches, loss: 0.6538Epoch 1/2: [==============================] 75/75 batches, loss: 0.6524
[2025-04-29 20:33:19,268][src.training.lm_trainer][INFO] - Epoch 1/2, Train Loss: 0.6524
[2025-04-29 20:33:19,514][src.training.lm_trainer][INFO] - Epoch 1/2, Val Loss: 0.5793, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.935064935064935}
Epoch 2/2: [Epoch 2/2: [                              ] 1/75 batches, loss: 0.5354Epoch 2/2: [                              ] 2/75 batches, loss: 0.5665Epoch 2/2: [=                             ] 3/75 batches, loss: 0.5361Epoch 2/2: [=                             ] 4/75 batches, loss: 0.5500Epoch 2/2: [==                            ] 5/75 batches, loss: 0.5708Epoch 2/2: [==                            ] 6/75 batches, loss: 0.5843Epoch 2/2: [==                            ] 7/75 batches, loss: 0.5909Epoch 2/2: [===                           ] 8/75 batches, loss: 0.5915Epoch 2/2: [===                           ] 9/75 batches, loss: 0.5944Epoch 2/2: [====                          ] 10/75 batches, loss: 0.5845Epoch 2/2: [====                          ] 11/75 batches, loss: 0.5795Epoch 2/2: [====                          ] 12/75 batches, loss: 0.5697Epoch 2/2: [=====                         ] 13/75 batches, loss: 0.5626Epoch 2/2: [=====                         ] 14/75 batches, loss: 0.5570Epoch 2/2: [======                        ] 15/75 batches, loss: 0.5523Epoch 2/2: [======                        ] 16/75 batches, loss: 0.5530Epoch 2/2: [======                        ] 17/75 batches, loss: 0.5492Epoch 2/2: [=======                       ] 18/75 batches, loss: 0.5439Epoch 2/2: [=======                       ] 19/75 batches, loss: 0.5450Epoch 2/2: [========                      ] 20/75 batches, loss: 0.5467Epoch 2/2: [========                      ] 21/75 batches, loss: 0.5459Epoch 2/2: [========                      ] 22/75 batches, loss: 0.5460Epoch 2/2: [=========                     ] 23/75 batches, loss: 0.5477Epoch 2/2: [=========                     ] 24/75 batches, loss: 0.5466Epoch 2/2: [==========                    ] 25/75 batches, loss: 0.5482Epoch 2/2: [==========                    ] 26/75 batches, loss: 0.5456Epoch 2/2: [==========                    ] 27/75 batches, loss: 0.5449Epoch 2/2: [===========                   ] 28/75 batches, loss: 0.5439Epoch 2/2: [===========                   ] 29/75 batches, loss: 0.5420Epoch 2/2: [============                  ] 30/75 batches, loss: 0.5371Epoch 2/2: [============                  ] 31/75 batches, loss: 0.5313Epoch 2/2: [============                  ] 32/75 batches, loss: 0.5260Epoch 2/2: [=============                 ] 33/75 batches, loss: 0.5209Epoch 2/2: [=============                 ] 34/75 batches, loss: 0.5139Epoch 2/2: [==============                ] 35/75 batches, loss: 0.5097Epoch 2/2: [==============                ] 36/75 batches, loss: 0.5041Epoch 2/2: [==============                ] 37/75 batches, loss: 0.5023Epoch 2/2: [===============               ] 38/75 batches, loss: 0.4958Epoch 2/2: [===============               ] 39/75 batches, loss: 0.4879Epoch 2/2: [================              ] 40/75 batches, loss: 0.4798Epoch 2/2: [================              ] 41/75 batches, loss: 0.4772Epoch 2/2: [================              ] 42/75 batches, loss: 0.4725Epoch 2/2: [=================             ] 43/75 batches, loss: 0.4677Epoch 2/2: [=================             ] 44/75 batches, loss: 0.4642Epoch 2/2: [==================            ] 45/75 batches, loss: 0.4598Epoch 2/2: [==================            ] 46/75 batches, loss: 0.4563Epoch 2/2: [==================            ] 47/75 batches, loss: 0.4517Epoch 2/2: [===================           ] 48/75 batches, loss: 0.4481Epoch 2/2: [===================           ] 49/75 batches, loss: 0.4443Epoch 2/2: [====================          ] 50/75 batches, loss: 0.4387Epoch 2/2: [====================          ] 51/75 batches, loss: 0.4329Epoch 2/2: [====================          ] 52/75 batches, loss: 0.4272Epoch 2/2: [=====================         ] 53/75 batches, loss: 0.4219Epoch 2/2: [=====================         ] 54/75 batches, loss: 0.4164Epoch 2/2: [======================        ] 55/75 batches, loss: 0.4109Epoch 2/2: [======================        ] 56/75 batches, loss: 0.4055Epoch 2/2: [======================        ] 57/75 batches, loss: 0.3999Epoch 2/2: [=======================       ] 58/75 batches, loss: 0.3956Epoch 2/2: [=======================       ] 59/75 batches, loss: 0.3911Epoch 2/2: [========================      ] 60/75 batches, loss: 0.3859Epoch 2/2: [========================      ] 61/75 batches, loss: 0.3809Epoch 2/2: [========================      ] 62/75 batches, loss: 0.3760Epoch 2/2: [=========================     ] 63/75 batches, loss: 0.3713Epoch 2/2: [=========================     ] 64/75 batches, loss: 0.3682Epoch 2/2: [==========================    ] 65/75 batches, loss: 0.3642Epoch 2/2: [==========================    ] 66/75 batches, loss: 0.3600Epoch 2/2: [==========================    ] 67/75 batches, loss: 0.3554Epoch 2/2: [===========================   ] 68/75 batches, loss: 0.3519Epoch 2/2: [===========================   ] 69/75 batches, loss: 0.3485Epoch 2/2: [============================  ] 70/75 batches, loss: 0.3442Epoch 2/2: [============================  ] 71/75 batches, loss: 0.3431Epoch 2/2: [============================  ] 72/75 batches, loss: 0.3403Epoch 2/2: [============================= ] 73/75 batches, loss: 0.3383Epoch 2/2: [============================= ] 74/75 batches, loss: 0.3368Epoch 2/2: [==============================] 75/75 batches, loss: 0.3347
[2025-04-29 20:33:27,440][src.training.lm_trainer][INFO] - Epoch 2/2, Train Loss: 0.3347
[2025-04-29 20:33:27,699][src.training.lm_trainer][INFO] - Epoch 2/2, Val Loss: 0.2120, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-29 20:33:28,124][src.training.lm_trainer][INFO] - Training completed in 17.30 seconds
[2025-04-29 20:33:28,124][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 20:33:31,047][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9958053691275168, 'f1': 0.9958228905597326}
[2025-04-29 20:33:31,047][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-29 20:33:31,047][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9636363636363636, 'f1': 0.9642857142857143}
[2025-04-29 20:33:33,680][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_test_output/en/model.pt
Test experiment completed successfully. Proceeding with full experiments.
Running main finetuning experiments (non-control)...
Running experiment: finetune_question_type_ar
Command: python -m src.experiments.run_experiment             "hydra.job.chdir=False"             "hydra.run.dir=."             "experiment=finetune"             "experiment.tasks=question_type"             "model=glot500_finetune"             "model.lm_name=cis-lmu/glot500-base"             "model.dropout=0.1"             "model.freeze_model=false"             "model.finetune=true"             "data.languages=[ar]"             "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"             "training.task_type=classification"             "training.num_epochs=10"             "training.batch_size=16"             "training.lr=2e-5"             "+training.gradient_accumulation_steps=2"                          "experiment_name=finetune_question_type_ar"             "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/ar"             "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 20:33:46,799][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar
experiment_name: finetune_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_finetune
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  finetune: true
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1
    lr: 2.0e-05
    batch_size: 8

[2025-04-29 20:33:46,799][__main__][INFO] - Normalized task: question_type
[2025-04-29 20:33:46,799][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 20:33:46,799][__main__][INFO] - Determined Task Type: classification
[2025-04-29 20:33:46,803][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 20:33:46,803][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 20:33:49,157][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 20:33:51,439][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 20:33:51,440][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:33:51,483][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:33:51,530][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:33:51,628][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 20:33:51,635][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:33:51,635][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 20:33:51,636][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:33:51,667][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:33:51,714][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:33:51,733][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 20:33:51,734][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:33:51,734][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 20:33:51,736][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:33:51,768][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:33:51,808][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:33:51,821][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 20:33:51,822][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:33:51,823][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 20:33:51,824][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 20:33:51,825][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:33:51,825][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:33:51,825][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:33:51,825][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:33:51,825][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 20:33:51,826][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 20:33:51,826][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 20:33:51,826][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 20:33:51,826][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:33:51,826][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:33:51,826][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:33:51,826][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:33:51,826][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 20:33:51,826][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 20:33:51,826][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 20:33:51,826][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:33:51,827][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:33:51,827][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:33:51,827][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:33:51,827][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:33:51,827][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 20:33:51,827][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 20:33:51,827][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 20:33:51,827][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:33:51,827][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 20:33:51,827][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 20:33:51,828][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 20:33:51,828][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 20:33:55,919][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 20:33:55,920][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,920][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,920][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,920][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,920][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,920][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,920][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,921][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,921][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,921][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,921][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,921][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,921][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,921][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,921][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,921][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,921][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,921][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,921][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,921][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,921][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,922][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,922][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,922][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,922][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,922][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,922][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,922][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,922][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,922][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,922][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,922][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,922][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,922][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,922][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,922][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,922][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,923][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,923][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,923][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,923][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,923][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,923][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,923][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,923][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,923][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,923][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,923][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,923][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,923][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,923][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,923][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,924][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,924][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,924][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,924][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,924][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,924][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,924][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,924][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,924][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,924][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,924][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,924][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,924][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,924][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,924][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,925][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,925][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,925][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,925][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,925][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,925][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,925][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,925][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,925][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,925][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,925][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,925][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,925][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,925][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,925][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,925][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,926][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,926][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,926][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,926][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,926][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,926][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,926][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,926][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,926][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,926][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,926][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,926][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,926][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,926][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,926][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,927][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,927][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,927][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,927][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,927][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,927][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,927][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,927][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,927][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,927][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,927][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,927][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,927][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,927][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,927][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,928][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,928][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,928][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,928][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,928][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,928][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,928][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,928][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,928][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,928][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,928][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,928][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,928][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,928][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,928][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,928][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,929][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,929][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,929][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,929][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,929][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,929][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,929][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,929][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,929][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,929][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,929][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,929][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,929][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,929][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,929][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,930][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,930][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,930][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,930][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,930][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,930][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,930][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,930][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,930][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,930][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,930][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,930][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,930][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,930][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,930][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,931][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,931][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,931][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,931][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,931][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,931][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,931][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,931][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,931][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,931][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,931][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,931][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,931][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,931][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,931][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,931][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,932][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,932][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,932][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,932][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,932][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,932][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,932][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,932][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,932][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,932][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,932][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,932][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,932][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,932][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,932][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,933][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,933][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,933][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,933][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,933][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,933][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,933][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,933][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,933][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:33:55,934][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 20:33:55,934][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 20:33:55,935][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 20:33:55,935][__main__][INFO] - Successfully created model for ar
[2025-04-29 20:33:55,935][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.6787Epoch 1/10: [                              ] 2/63 batches, loss: 0.6903Epoch 1/10: [=                             ] 3/63 batches, loss: 0.6951Epoch 1/10: [=                             ] 4/63 batches, loss: 0.6898Epoch 1/10: [==                            ] 5/63 batches, loss: 0.6961Epoch 1/10: [==                            ] 6/63 batches, loss: 0.6955Epoch 1/10: [===                           ] 7/63 batches, loss: 0.6962Epoch 1/10: [===                           ] 8/63 batches, loss: 0.6966Epoch 1/10: [====                          ] 9/63 batches, loss: 0.6980Epoch 1/10: [====                          ] 10/63 batches, loss: 0.6981Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.6975Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.6953Epoch 1/10: [======                        ] 13/63 batches, loss: 0.6962Epoch 1/10: [======                        ] 14/63 batches, loss: 0.6959Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.6938Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.6931Epoch 1/10: [========                      ] 17/63 batches, loss: 0.6933Epoch 1/10: [========                      ] 18/63 batches, loss: 0.6930Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.6936Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.6933Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.6942Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.6933Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.6921Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.6925Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.6913Epoch 1/10: [============                  ] 26/63 batches, loss: 0.6901Epoch 1/10: [============                  ] 27/63 batches, loss: 0.6903Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.6912Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.6908Epoch 1/10: [==============                ] 30/63 batches, loss: 0.6923Epoch 1/10: [==============                ] 31/63 batches, loss: 0.6922Epoch 1/10: [===============               ] 32/63 batches, loss: 0.6928Epoch 1/10: [===============               ] 33/63 batches, loss: 0.6936Epoch 1/10: [================              ] 34/63 batches, loss: 0.6940Epoch 1/10: [================              ] 35/63 batches, loss: 0.6942Epoch 1/10: [=================             ] 36/63 batches, loss: 0.6946Epoch 1/10: [=================             ] 37/63 batches, loss: 0.6946Epoch 1/10: [==================            ] 38/63 batches, loss: 0.6944Epoch 1/10: [==================            ] 39/63 batches, loss: 0.6936Epoch 1/10: [===================           ] 40/63 batches, loss: 0.6934Epoch 1/10: [===================           ] 41/63 batches, loss: 0.6927Epoch 1/10: [====================          ] 42/63 batches, loss: 0.6922Epoch 1/10: [====================          ] 43/63 batches, loss: 0.6926Epoch 1/10: [====================          ] 44/63 batches, loss: 0.6922Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.6922Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.6922Epoch 1/10: [======================        ] 47/63 batches, loss: 0.6921Epoch 1/10: [======================        ] 48/63 batches, loss: 0.6915Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.6918Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.6911Epoch 1/10: [========================      ] 51/63 batches, loss: 0.6907Epoch 1/10: [========================      ] 52/63 batches, loss: 0.6911Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.6914Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.6914Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.6910Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.6918Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.6912Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.6910Epoch 1/10: [============================  ] 59/63 batches, loss: 0.6906Epoch 1/10: [============================  ] 60/63 batches, loss: 0.6902Epoch 1/10: [============================= ] 61/63 batches, loss: 0.6904Epoch 1/10: [============================= ] 62/63 batches, loss: 0.6904Epoch 1/10: [==============================] 63/63 batches, loss: 0.6907
[2025-04-29 20:34:05,209][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6907
[2025-04-29 20:34:05,419][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6962, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.6784Epoch 2/10: [                              ] 2/63 batches, loss: 0.6830Epoch 2/10: [=                             ] 3/63 batches, loss: 0.6814Epoch 2/10: [=                             ] 4/63 batches, loss: 0.6806Epoch 2/10: [==                            ] 5/63 batches, loss: 0.6803Epoch 2/10: [==                            ] 6/63 batches, loss: 0.6780Epoch 2/10: [===                           ] 7/63 batches, loss: 0.6767Epoch 2/10: [===                           ] 8/63 batches, loss: 0.6800Epoch 2/10: [====                          ] 9/63 batches, loss: 0.6806Epoch 2/10: [====                          ] 10/63 batches, loss: 0.6803Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.6836Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.6824Epoch 2/10: [======                        ] 13/63 batches, loss: 0.6827Epoch 2/10: [======                        ] 14/63 batches, loss: 0.6828Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.6835Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.6814Epoch 2/10: [========                      ] 17/63 batches, loss: 0.6806Epoch 2/10: [========                      ] 18/63 batches, loss: 0.6815Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.6809Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.6821Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.6807Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.6811Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.6796Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.6797Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.6799Epoch 2/10: [============                  ] 26/63 batches, loss: 0.6799Epoch 2/10: [============                  ] 27/63 batches, loss: 0.6798Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.6795Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.6797Epoch 2/10: [==============                ] 30/63 batches, loss: 0.6787Epoch 2/10: [==============                ] 31/63 batches, loss: 0.6780Epoch 2/10: [===============               ] 32/63 batches, loss: 0.6778Epoch 2/10: [===============               ] 33/63 batches, loss: 0.6782Epoch 2/10: [================              ] 34/63 batches, loss: 0.6783Epoch 2/10: [================              ] 35/63 batches, loss: 0.6792Epoch 2/10: [=================             ] 36/63 batches, loss: 0.6782Epoch 2/10: [=================             ] 37/63 batches, loss: 0.6774Epoch 2/10: [==================            ] 38/63 batches, loss: 0.6775Epoch 2/10: [==================            ] 39/63 batches, loss: 0.6776Epoch 2/10: [===================           ] 40/63 batches, loss: 0.6770Epoch 2/10: [===================           ] 41/63 batches, loss: 0.6767Epoch 2/10: [====================          ] 42/63 batches, loss: 0.6770Epoch 2/10: [====================          ] 43/63 batches, loss: 0.6755Epoch 2/10: [====================          ] 44/63 batches, loss: 0.6750Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.6755Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.6745Epoch 2/10: [======================        ] 47/63 batches, loss: 0.6746Epoch 2/10: [======================        ] 48/63 batches, loss: 0.6746Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.6740Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.6740Epoch 2/10: [========================      ] 51/63 batches, loss: 0.6737Epoch 2/10: [========================      ] 52/63 batches, loss: 0.6734Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.6727Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.6727Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.6720Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.6711Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.6705Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.6705Epoch 2/10: [============================  ] 59/63 batches, loss: 0.6704Epoch 2/10: [============================  ] 60/63 batches, loss: 0.6698Epoch 2/10: [============================= ] 61/63 batches, loss: 0.6696Epoch 2/10: [============================= ] 62/63 batches, loss: 0.6698Epoch 2/10: [==============================] 63/63 batches, loss: 0.6679
[2025-04-29 20:34:11,576][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6679
[2025-04-29 20:34:11,788][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6898, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.7033Epoch 3/10: [                              ] 2/63 batches, loss: 0.6858Epoch 3/10: [=                             ] 3/63 batches, loss: 0.6966Epoch 3/10: [=                             ] 4/63 batches, loss: 0.6922Epoch 3/10: [==                            ] 5/63 batches, loss: 0.6739Epoch 3/10: [==                            ] 6/63 batches, loss: 0.6749Epoch 3/10: [===                           ] 7/63 batches, loss: 0.6786Epoch 3/10: [===                           ] 8/63 batches, loss: 0.6747Epoch 3/10: [====                          ] 9/63 batches, loss: 0.6701Epoch 3/10: [====                          ] 10/63 batches, loss: 0.6705Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.6677Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.6666Epoch 3/10: [======                        ] 13/63 batches, loss: 0.6670Epoch 3/10: [======                        ] 14/63 batches, loss: 0.6670Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.6649Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.6656Epoch 3/10: [========                      ] 17/63 batches, loss: 0.6635Epoch 3/10: [========                      ] 18/63 batches, loss: 0.6654Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.6647Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.6652Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.6628Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.6618Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.6604Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.6584Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.6584Epoch 3/10: [============                  ] 26/63 batches, loss: 0.6565Epoch 3/10: [============                  ] 27/63 batches, loss: 0.6541Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.6544Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.6542Epoch 3/10: [==============                ] 30/63 batches, loss: 0.6513Epoch 3/10: [==============                ] 31/63 batches, loss: 0.6503Epoch 3/10: [===============               ] 32/63 batches, loss: 0.6491Epoch 3/10: [===============               ] 33/63 batches, loss: 0.6474Epoch 3/10: [================              ] 34/63 batches, loss: 0.6476Epoch 3/10: [================              ] 35/63 batches, loss: 0.6449Epoch 3/10: [=================             ] 36/63 batches, loss: 0.6437Epoch 3/10: [=================             ] 37/63 batches, loss: 0.6437Epoch 3/10: [==================            ] 38/63 batches, loss: 0.6428Epoch 3/10: [==================            ] 39/63 batches, loss: 0.6429Epoch 3/10: [===================           ] 40/63 batches, loss: 0.6435Epoch 3/10: [===================           ] 41/63 batches, loss: 0.6421Epoch 3/10: [====================          ] 42/63 batches, loss: 0.6424Epoch 3/10: [====================          ] 43/63 batches, loss: 0.6425Epoch 3/10: [====================          ] 44/63 batches, loss: 0.6425Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.6397Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.6399Epoch 3/10: [======================        ] 47/63 batches, loss: 0.6401Epoch 3/10: [======================        ] 48/63 batches, loss: 0.6407Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.6414Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.6420Epoch 3/10: [========================      ] 51/63 batches, loss: 0.6410Epoch 3/10: [========================      ] 52/63 batches, loss: 0.6409Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.6390Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.6387Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.6389Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.6379Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.6375Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.6371Epoch 3/10: [============================  ] 59/63 batches, loss: 0.6356Epoch 3/10: [============================  ] 60/63 batches, loss: 0.6357Epoch 3/10: [============================= ] 61/63 batches, loss: 0.6348Epoch 3/10: [============================= ] 62/63 batches, loss: 0.6324Epoch 3/10: [==============================] 63/63 batches, loss: 0.6292
[2025-04-29 20:34:17,985][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6292
[2025-04-29 20:34:18,219][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.4552, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.4844Epoch 4/10: [                              ] 2/63 batches, loss: 0.5049Epoch 4/10: [=                             ] 3/63 batches, loss: 0.5071Epoch 4/10: [=                             ] 4/63 batches, loss: 0.5030Epoch 4/10: [==                            ] 5/63 batches, loss: 0.4882Epoch 4/10: [==                            ] 6/63 batches, loss: 0.5004Epoch 4/10: [===                           ] 7/63 batches, loss: 0.4813Epoch 4/10: [===                           ] 8/63 batches, loss: 0.4690Epoch 4/10: [====                          ] 9/63 batches, loss: 0.4649Epoch 4/10: [====                          ] 10/63 batches, loss: 0.4642Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.4632Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.4637Epoch 4/10: [======                        ] 13/63 batches, loss: 0.4627Epoch 4/10: [======                        ] 14/63 batches, loss: 0.4597Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.4504Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.4508Epoch 4/10: [========                      ] 17/63 batches, loss: 0.4508Epoch 4/10: [========                      ] 18/63 batches, loss: 0.4503Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.4498Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.4491Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.4454Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.4425Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.4395Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.4337Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.4256Epoch 4/10: [============                  ] 26/63 batches, loss: 0.4197Epoch 4/10: [============                  ] 27/63 batches, loss: 0.4169Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.4119Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.4139Epoch 4/10: [==============                ] 30/63 batches, loss: 0.4146Epoch 4/10: [==============                ] 31/63 batches, loss: 0.4152Epoch 4/10: [===============               ] 32/63 batches, loss: 0.4144Epoch 4/10: [===============               ] 33/63 batches, loss: 0.4148Epoch 4/10: [================              ] 34/63 batches, loss: 0.4095Epoch 4/10: [================              ] 35/63 batches, loss: 0.4037Epoch 4/10: [=================             ] 36/63 batches, loss: 0.4007Epoch 4/10: [=================             ] 37/63 batches, loss: 0.3982Epoch 4/10: [==================            ] 38/63 batches, loss: 0.3947Epoch 4/10: [==================            ] 39/63 batches, loss: 0.3914Epoch 4/10: [===================           ] 40/63 batches, loss: 0.3880Epoch 4/10: [===================           ] 41/63 batches, loss: 0.3826Epoch 4/10: [====================          ] 42/63 batches, loss: 0.3776Epoch 4/10: [====================          ] 43/63 batches, loss: 0.3735Epoch 4/10: [====================          ] 44/63 batches, loss: 0.3705Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.3666Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.3612Epoch 4/10: [======================        ] 47/63 batches, loss: 0.3561Epoch 4/10: [======================        ] 48/63 batches, loss: 0.3516Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.3509Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.3470Epoch 4/10: [========================      ] 51/63 batches, loss: 0.3433Epoch 4/10: [========================      ] 52/63 batches, loss: 0.3394Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.3367Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.3335Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.3292Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.3256Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.3216Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.3184Epoch 4/10: [============================  ] 59/63 batches, loss: 0.3152Epoch 4/10: [============================  ] 60/63 batches, loss: 0.3135Epoch 4/10: [============================= ] 61/63 batches, loss: 0.3104Epoch 4/10: [============================= ] 62/63 batches, loss: 0.3072Epoch 4/10: [==============================] 63/63 batches, loss: 0.3064
[2025-04-29 20:34:24,346][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.3064
[2025-04-29 20:34:24,559][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.2002, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.0643Epoch 5/10: [                              ] 2/63 batches, loss: 0.0972Epoch 5/10: [=                             ] 3/63 batches, loss: 0.0863Epoch 5/10: [=                             ] 4/63 batches, loss: 0.0916Epoch 5/10: [==                            ] 5/63 batches, loss: 0.0879Epoch 5/10: [==                            ] 6/63 batches, loss: 0.0930Epoch 5/10: [===                           ] 7/63 batches, loss: 0.0931Epoch 5/10: [===                           ] 8/63 batches, loss: 0.0902Epoch 5/10: [====                          ] 9/63 batches, loss: 0.0995Epoch 5/10: [====                          ] 10/63 batches, loss: 0.1025Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.1016Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.0978Epoch 5/10: [======                        ] 13/63 batches, loss: 0.0971Epoch 5/10: [======                        ] 14/63 batches, loss: 0.0951Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.0952Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.0933Epoch 5/10: [========                      ] 17/63 batches, loss: 0.0942Epoch 5/10: [========                      ] 18/63 batches, loss: 0.0980Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.0976Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.0953Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.0949Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.0934Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.0919Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.0918Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.0898Epoch 5/10: [============                  ] 26/63 batches, loss: 0.0886Epoch 5/10: [============                  ] 27/63 batches, loss: 0.0880Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.0876Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.0861Epoch 5/10: [==============                ] 30/63 batches, loss: 0.0848Epoch 5/10: [==============                ] 31/63 batches, loss: 0.0907Epoch 5/10: [===============               ] 32/63 batches, loss: 0.0887Epoch 5/10: [===============               ] 33/63 batches, loss: 0.0871Epoch 5/10: [================              ] 34/63 batches, loss: 0.0855Epoch 5/10: [================              ] 35/63 batches, loss: 0.0844Epoch 5/10: [=================             ] 36/63 batches, loss: 0.0837Epoch 5/10: [=================             ] 37/63 batches, loss: 0.0830Epoch 5/10: [==================            ] 38/63 batches, loss: 0.0824Epoch 5/10: [==================            ] 39/63 batches, loss: 0.0826Epoch 5/10: [===================           ] 40/63 batches, loss: 0.0813Epoch 5/10: [===================           ] 41/63 batches, loss: 0.0800Epoch 5/10: [====================          ] 42/63 batches, loss: 0.0792Epoch 5/10: [====================          ] 43/63 batches, loss: 0.0786Epoch 5/10: [====================          ] 44/63 batches, loss: 0.0780Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.0771Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.0766Epoch 5/10: [======================        ] 47/63 batches, loss: 0.0759Epoch 5/10: [======================        ] 48/63 batches, loss: 0.0756Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.0751Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.0745Epoch 5/10: [========================      ] 51/63 batches, loss: 0.0735Epoch 5/10: [========================      ] 52/63 batches, loss: 0.0727Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.0730Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.0719Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.0713Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.0707Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.0698Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.0691Epoch 5/10: [============================  ] 59/63 batches, loss: 0.0690Epoch 5/10: [============================  ] 60/63 batches, loss: 0.0681Epoch 5/10: [============================= ] 61/63 batches, loss: 0.0674Epoch 5/10: [============================= ] 62/63 batches, loss: 0.0667Epoch 5/10: [==============================] 63/63 batches, loss: 0.0660
[2025-04-29 20:34:30,694][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0660
[2025-04-29 20:34:30,917][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1889, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.0349Epoch 6/10: [                              ] 2/63 batches, loss: 0.0339Epoch 6/10: [=                             ] 3/63 batches, loss: 0.0295Epoch 6/10: [=                             ] 4/63 batches, loss: 0.0278Epoch 6/10: [==                            ] 5/63 batches, loss: 0.0301Epoch 6/10: [==                            ] 6/63 batches, loss: 0.0296Epoch 6/10: [===                           ] 7/63 batches, loss: 0.0296Epoch 6/10: [===                           ] 8/63 batches, loss: 0.0283Epoch 6/10: [====                          ] 9/63 batches, loss: 0.0269Epoch 6/10: [====                          ] 10/63 batches, loss: 0.0269Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.0266Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.0256Epoch 6/10: [======                        ] 13/63 batches, loss: 0.0270Epoch 6/10: [======                        ] 14/63 batches, loss: 0.0262Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.0260Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.0251Epoch 6/10: [========                      ] 17/63 batches, loss: 0.0249Epoch 6/10: [========                      ] 18/63 batches, loss: 0.0249Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.0250Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.0259Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.0267Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.0274Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.0274Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.0274Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.0277Epoch 6/10: [============                  ] 26/63 batches, loss: 0.0277Epoch 6/10: [============                  ] 27/63 batches, loss: 0.0279Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.0279Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.0280Epoch 6/10: [==============                ] 30/63 batches, loss: 0.0276Epoch 6/10: [==============                ] 31/63 batches, loss: 0.0275Epoch 6/10: [===============               ] 32/63 batches, loss: 0.0274Epoch 6/10: [===============               ] 33/63 batches, loss: 0.0274Epoch 6/10: [================              ] 34/63 batches, loss: 0.0275Epoch 6/10: [================              ] 35/63 batches, loss: 0.0272Epoch 6/10: [=================             ] 36/63 batches, loss: 0.0268Epoch 6/10: [=================             ] 37/63 batches, loss: 0.0268Epoch 6/10: [==================            ] 38/63 batches, loss: 0.0269Epoch 6/10: [==================            ] 39/63 batches, loss: 0.0267Epoch 6/10: [===================           ] 40/63 batches, loss: 0.0265Epoch 6/10: [===================           ] 41/63 batches, loss: 0.0265Epoch 6/10: [====================          ] 42/63 batches, loss: 0.0264Epoch 6/10: [====================          ] 43/63 batches, loss: 0.0264Epoch 6/10: [====================          ] 44/63 batches, loss: 0.0262Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.0266Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.0264Epoch 6/10: [======================        ] 47/63 batches, loss: 0.0261Epoch 6/10: [======================        ] 48/63 batches, loss: 0.0260Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.0258Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.0256Epoch 6/10: [========================      ] 51/63 batches, loss: 0.0254Epoch 6/10: [========================      ] 52/63 batches, loss: 0.0251Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.0255Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.0253Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.0254Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.0254Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.0252Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.0250Epoch 6/10: [============================  ] 59/63 batches, loss: 0.0248Epoch 6/10: [============================  ] 60/63 batches, loss: 0.0246Epoch 6/10: [============================= ] 61/63 batches, loss: 0.0245Epoch 6/10: [============================= ] 62/63 batches, loss: 0.0244Epoch 6/10: [==============================] 63/63 batches, loss: 0.0244
[2025-04-29 20:34:37,162][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0244
[2025-04-29 20:34:37,409][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1307, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 7/10: [Epoch 7/10: [                              ] 1/63 batches, loss: 0.0142Epoch 7/10: [                              ] 2/63 batches, loss: 0.0203Epoch 7/10: [=                             ] 3/63 batches, loss: 0.0224Epoch 7/10: [=                             ] 4/63 batches, loss: 0.0291Epoch 7/10: [==                            ] 5/63 batches, loss: 0.0269Epoch 7/10: [==                            ] 6/63 batches, loss: 0.0241Epoch 7/10: [===                           ] 7/63 batches, loss: 0.0225Epoch 7/10: [===                           ] 8/63 batches, loss: 0.0208Epoch 7/10: [====                          ] 9/63 batches, loss: 0.0197Epoch 7/10: [====                          ] 10/63 batches, loss: 0.0189Epoch 7/10: [=====                         ] 11/63 batches, loss: 0.0185Epoch 7/10: [=====                         ] 12/63 batches, loss: 0.0182Epoch 7/10: [======                        ] 13/63 batches, loss: 0.0177Epoch 7/10: [======                        ] 14/63 batches, loss: 0.0176Epoch 7/10: [=======                       ] 15/63 batches, loss: 0.0176Epoch 7/10: [=======                       ] 16/63 batches, loss: 0.0180Epoch 7/10: [========                      ] 17/63 batches, loss: 0.0176Epoch 7/10: [========                      ] 18/63 batches, loss: 0.0175Epoch 7/10: [=========                     ] 19/63 batches, loss: 0.0171Epoch 7/10: [=========                     ] 20/63 batches, loss: 0.0170Epoch 7/10: [==========                    ] 21/63 batches, loss: 0.0169Epoch 7/10: [==========                    ] 22/63 batches, loss: 0.0166Epoch 7/10: [==========                    ] 23/63 batches, loss: 0.0163Epoch 7/10: [===========                   ] 24/63 batches, loss: 0.0164Epoch 7/10: [===========                   ] 25/63 batches, loss: 0.0161Epoch 7/10: [============                  ] 26/63 batches, loss: 0.0160Epoch 7/10: [============                  ] 27/63 batches, loss: 0.0160Epoch 7/10: [=============                 ] 28/63 batches, loss: 0.0159Epoch 7/10: [=============                 ] 29/63 batches, loss: 0.0159Epoch 7/10: [==============                ] 30/63 batches, loss: 0.0162Epoch 7/10: [==============                ] 31/63 batches, loss: 0.0220Epoch 7/10: [===============               ] 32/63 batches, loss: 0.0216Epoch 7/10: [===============               ] 33/63 batches, loss: 0.0212Epoch 7/10: [================              ] 34/63 batches, loss: 0.0210Epoch 7/10: [================              ] 35/63 batches, loss: 0.0207Epoch 7/10: [=================             ] 36/63 batches, loss: 0.0205Epoch 7/10: [=================             ] 37/63 batches, loss: 0.0206Epoch 7/10: [==================            ] 38/63 batches, loss: 0.0203Epoch 7/10: [==================            ] 39/63 batches, loss: 0.0202Epoch 7/10: [===================           ] 40/63 batches, loss: 0.0201Epoch 7/10: [===================           ] 41/63 batches, loss: 0.0200Epoch 7/10: [====================          ] 42/63 batches, loss: 0.0201Epoch 7/10: [====================          ] 43/63 batches, loss: 0.0200Epoch 7/10: [====================          ] 44/63 batches, loss: 0.0198Epoch 7/10: [=====================         ] 45/63 batches, loss: 0.0196Epoch 7/10: [=====================         ] 46/63 batches, loss: 0.0193Epoch 7/10: [======================        ] 47/63 batches, loss: 0.0192Epoch 7/10: [======================        ] 48/63 batches, loss: 0.0191Epoch 7/10: [=======================       ] 49/63 batches, loss: 0.0190Epoch 7/10: [=======================       ] 50/63 batches, loss: 0.0189Epoch 7/10: [========================      ] 51/63 batches, loss: 0.0188Epoch 7/10: [========================      ] 52/63 batches, loss: 0.0186Epoch 7/10: [=========================     ] 53/63 batches, loss: 0.0186Epoch 7/10: [=========================     ] 54/63 batches, loss: 0.0186Epoch 7/10: [==========================    ] 55/63 batches, loss: 0.0186Epoch 7/10: [==========================    ] 56/63 batches, loss: 0.0186Epoch 7/10: [===========================   ] 57/63 batches, loss: 0.0184Epoch 7/10: [===========================   ] 58/63 batches, loss: 0.0182Epoch 7/10: [============================  ] 59/63 batches, loss: 0.0181Epoch 7/10: [============================  ] 60/63 batches, loss: 0.0180Epoch 7/10: [============================= ] 61/63 batches, loss: 0.0179Epoch 7/10: [============================= ] 62/63 batches, loss: 0.0178Epoch 7/10: [==============================] 63/63 batches, loss: 0.0179
[2025-04-29 20:34:43,586][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0179
[2025-04-29 20:34:43,814][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1958, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
[2025-04-29 20:34:43,815][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/63 batches, loss: 0.0095Epoch 8/10: [                              ] 2/63 batches, loss: 0.0111Epoch 8/10: [=                             ] 3/63 batches, loss: 0.0114Epoch 8/10: [=                             ] 4/63 batches, loss: 0.0107Epoch 8/10: [==                            ] 5/63 batches, loss: 0.0109Epoch 8/10: [==                            ] 6/63 batches, loss: 0.0124Epoch 8/10: [===                           ] 7/63 batches, loss: 0.0125Epoch 8/10: [===                           ] 8/63 batches, loss: 0.0127Epoch 8/10: [====                          ] 9/63 batches, loss: 0.0135Epoch 8/10: [====                          ] 10/63 batches, loss: 0.0136Epoch 8/10: [=====                         ] 11/63 batches, loss: 0.0134Epoch 8/10: [=====                         ] 12/63 batches, loss: 0.0163Epoch 8/10: [======                        ] 13/63 batches, loss: 0.0159Epoch 8/10: [======                        ] 14/63 batches, loss: 0.0155Epoch 8/10: [=======                       ] 15/63 batches, loss: 0.0151Epoch 8/10: [=======                       ] 16/63 batches, loss: 0.0151Epoch 8/10: [========                      ] 17/63 batches, loss: 0.0148Epoch 8/10: [========                      ] 18/63 batches, loss: 0.0150Epoch 8/10: [=========                     ] 19/63 batches, loss: 0.0171Epoch 8/10: [=========                     ] 20/63 batches, loss: 0.0166Epoch 8/10: [==========                    ] 21/63 batches, loss: 0.0162Epoch 8/10: [==========                    ] 22/63 batches, loss: 0.0160Epoch 8/10: [==========                    ] 23/63 batches, loss: 0.0156Epoch 8/10: [===========                   ] 24/63 batches, loss: 0.0154Epoch 8/10: [===========                   ] 25/63 batches, loss: 0.0153Epoch 8/10: [============                  ] 26/63 batches, loss: 0.0151Epoch 8/10: [============                  ] 27/63 batches, loss: 0.0148Epoch 8/10: [=============                 ] 28/63 batches, loss: 0.0146Epoch 8/10: [=============                 ] 29/63 batches, loss: 0.0143Epoch 8/10: [==============                ] 30/63 batches, loss: 0.0148Epoch 8/10: [==============                ] 31/63 batches, loss: 0.0145Epoch 8/10: [===============               ] 32/63 batches, loss: 0.0143Epoch 8/10: [===============               ] 33/63 batches, loss: 0.0140Epoch 8/10: [================              ] 34/63 batches, loss: 0.0138Epoch 8/10: [================              ] 35/63 batches, loss: 0.0135Epoch 8/10: [=================             ] 36/63 batches, loss: 0.0134Epoch 8/10: [=================             ] 37/63 batches, loss: 0.0134Epoch 8/10: [==================            ] 38/63 batches, loss: 0.0133Epoch 8/10: [==================            ] 39/63 batches, loss: 0.0133Epoch 8/10: [===================           ] 40/63 batches, loss: 0.0133Epoch 8/10: [===================           ] 41/63 batches, loss: 0.0131Epoch 8/10: [====================          ] 42/63 batches, loss: 0.0131Epoch 8/10: [====================          ] 43/63 batches, loss: 0.0129Epoch 8/10: [====================          ] 44/63 batches, loss: 0.0128Epoch 8/10: [=====================         ] 45/63 batches, loss: 0.0126Epoch 8/10: [=====================         ] 46/63 batches, loss: 0.0125Epoch 8/10: [======================        ] 47/63 batches, loss: 0.0123Epoch 8/10: [======================        ] 48/63 batches, loss: 0.0122Epoch 8/10: [=======================       ] 49/63 batches, loss: 0.0120Epoch 8/10: [=======================       ] 50/63 batches, loss: 0.0119Epoch 8/10: [========================      ] 51/63 batches, loss: 0.0118Epoch 8/10: [========================      ] 52/63 batches, loss: 0.0116Epoch 8/10: [=========================     ] 53/63 batches, loss: 0.0116Epoch 8/10: [=========================     ] 54/63 batches, loss: 0.0115Epoch 8/10: [==========================    ] 55/63 batches, loss: 0.0114Epoch 8/10: [==========================    ] 56/63 batches, loss: 0.0113Epoch 8/10: [===========================   ] 57/63 batches, loss: 0.0112Epoch 8/10: [===========================   ] 58/63 batches, loss: 0.0112Epoch 8/10: [============================  ] 59/63 batches, loss: 0.0112Epoch 8/10: [============================  ] 60/63 batches, loss: 0.0111Epoch 8/10: [============================= ] 61/63 batches, loss: 0.0110Epoch 8/10: [============================= ] 62/63 batches, loss: 0.0110Epoch 8/10: [==============================] 63/63 batches, loss: 0.0109
[2025-04-29 20:34:49,607][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0109
[2025-04-29 20:34:49,841][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1939, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
[2025-04-29 20:34:49,842][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 9/10: [Epoch 9/10: [                              ] 1/63 batches, loss: 0.0078Epoch 9/10: [                              ] 2/63 batches, loss: 0.0065Epoch 9/10: [=                             ] 3/63 batches, loss: 0.0057Epoch 9/10: [=                             ] 4/63 batches, loss: 0.0078Epoch 9/10: [==                            ] 5/63 batches, loss: 0.0073Epoch 9/10: [==                            ] 6/63 batches, loss: 0.0069Epoch 9/10: [===                           ] 7/63 batches, loss: 0.0070Epoch 9/10: [===                           ] 8/63 batches, loss: 0.0069Epoch 9/10: [====                          ] 9/63 batches, loss: 0.0069Epoch 9/10: [====                          ] 10/63 batches, loss: 0.0072Epoch 9/10: [=====                         ] 11/63 batches, loss: 0.0071Epoch 9/10: [=====                         ] 12/63 batches, loss: 0.0069Epoch 9/10: [======                        ] 13/63 batches, loss: 0.0068Epoch 9/10: [======                        ] 14/63 batches, loss: 0.0068Epoch 9/10: [=======                       ] 15/63 batches, loss: 0.0067Epoch 9/10: [=======                       ] 16/63 batches, loss: 0.0066Epoch 9/10: [========                      ] 17/63 batches, loss: 0.0065Epoch 9/10: [========                      ] 18/63 batches, loss: 0.0066Epoch 9/10: [=========                     ] 19/63 batches, loss: 0.0068Epoch 9/10: [=========                     ] 20/63 batches, loss: 0.0068Epoch 9/10: [==========                    ] 21/63 batches, loss: 0.0067Epoch 9/10: [==========                    ] 22/63 batches, loss: 0.0068Epoch 9/10: [==========                    ] 23/63 batches, loss: 0.0069Epoch 9/10: [===========                   ] 24/63 batches, loss: 0.0069Epoch 9/10: [===========                   ] 25/63 batches, loss: 0.0071Epoch 9/10: [============                  ] 26/63 batches, loss: 0.0071Epoch 9/10: [============                  ] 27/63 batches, loss: 0.0071Epoch 9/10: [=============                 ] 28/63 batches, loss: 0.0071Epoch 9/10: [=============                 ] 29/63 batches, loss: 0.0071Epoch 9/10: [==============                ] 30/63 batches, loss: 0.0071Epoch 9/10: [==============                ] 31/63 batches, loss: 0.0070Epoch 9/10: [===============               ] 32/63 batches, loss: 0.0069Epoch 9/10: [===============               ] 33/63 batches, loss: 0.0069Epoch 9/10: [================              ] 34/63 batches, loss: 0.0068Epoch 9/10: [================              ] 35/63 batches, loss: 0.0067Epoch 9/10: [=================             ] 36/63 batches, loss: 0.0067Epoch 9/10: [=================             ] 37/63 batches, loss: 0.0081Epoch 9/10: [==================            ] 38/63 batches, loss: 0.0080Epoch 9/10: [==================            ] 39/63 batches, loss: 0.0079Epoch 9/10: [===================           ] 40/63 batches, loss: 0.0078Epoch 9/10: [===================           ] 41/63 batches, loss: 0.0077Epoch 9/10: [====================          ] 42/63 batches, loss: 0.0077Epoch 9/10: [====================          ] 43/63 batches, loss: 0.0076Epoch 9/10: [====================          ] 44/63 batches, loss: 0.0075Epoch 9/10: [=====================         ] 45/63 batches, loss: 0.0075Epoch 9/10: [=====================         ] 46/63 batches, loss: 0.0075Epoch 9/10: [======================        ] 47/63 batches, loss: 0.0075Epoch 9/10: [======================        ] 48/63 batches, loss: 0.0075Epoch 9/10: [=======================       ] 49/63 batches, loss: 0.0075Epoch 9/10: [=======================       ] 50/63 batches, loss: 0.0075Epoch 9/10: [========================      ] 51/63 batches, loss: 0.0074Epoch 9/10: [========================      ] 52/63 batches, loss: 0.0074Epoch 9/10: [=========================     ] 53/63 batches, loss: 0.0074Epoch 9/10: [=========================     ] 54/63 batches, loss: 0.0074Epoch 9/10: [==========================    ] 55/63 batches, loss: 0.0073Epoch 9/10: [==========================    ] 56/63 batches, loss: 0.0073Epoch 9/10: [===========================   ] 57/63 batches, loss: 0.0073Epoch 9/10: [===========================   ] 58/63 batches, loss: 0.0073Epoch 9/10: [============================  ] 59/63 batches, loss: 0.0072Epoch 9/10: [============================  ] 60/63 batches, loss: 0.0072Epoch 9/10: [============================= ] 61/63 batches, loss: 0.0072Epoch 9/10: [============================= ] 62/63 batches, loss: 0.0073Epoch 9/10: [==============================] 63/63 batches, loss: 0.0073
[2025-04-29 20:34:55,640][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0073
[2025-04-29 20:34:55,864][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.2054, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
[2025-04-29 20:34:55,865][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 20:34:55,865][src.training.lm_trainer][INFO] - Early stopping at epoch 9
[2025-04-29 20:34:55,865][src.training.lm_trainer][INFO] - Training completed in 57.23 seconds
[2025-04-29 20:34:55,865][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 20:34:58,374][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 1.0, 'f1': 1.0}
[2025-04-29 20:34:58,376][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
[2025-04-29 20:34:58,376][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.8311688311688312, 'f1': 0.7636363636363637}
[2025-04-29 20:35:00,022][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/ar/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▇███
wandb:          best_val_f1 ▁▁▇███
wandb:        best_val_loss ██▅▂▂▁
wandb:                epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁
wandb:           train_loss ██▇▄▂▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▇██████
wandb:               val_f1 ▁▁▇██████
wandb:             val_loss ██▅▂▂▁▂▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.95455
wandb:          best_val_f1 0.95238
wandb:        best_val_loss 0.13071
wandb:                epoch 9
wandb:  final_test_accuracy 0.83117
wandb:        final_test_f1 0.76364
wandb: final_train_accuracy 1
wandb:       final_train_f1 1
wandb:   final_val_accuracy 0.95455
wandb:         final_val_f1 0.95238
wandb:        learning_rate 2e-05
wandb:           train_loss 0.00728
wandb:           train_time 57.22783
wandb:         val_accuracy 0.95455
wandb:               val_f1 0.95238
wandb:             val_loss 0.20542
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_203346-mh7z9r3w
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_203346-mh7z9r3w/logs
Experiment finetune_question_type_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/results.json
Running experiment: finetune_complexity_ar
Command: python -m src.experiments.run_experiment             "hydra.job.chdir=False"             "hydra.run.dir=."             "experiment=finetune"             "experiment.tasks=complexity"             "model=glot500_finetune"             "model.lm_name=cis-lmu/glot500-base"             "model.dropout=0.1"             "model.freeze_model=false"             "model.finetune=true"             "data.languages=[ar]"             "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"             "training.task_type=regression"             "training.num_epochs=10"             "training.batch_size=16"             "training.lr=2e-5"             "+training.gradient_accumulation_steps=2"                          "experiment_name=finetune_complexity_ar"             "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/ar"             "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 20:35:22,884][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/ar
experiment_name: finetune_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_finetune
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  finetune: true
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1
    lr: 2.0e-05
    batch_size: 8

[2025-04-29 20:35:22,884][__main__][INFO] - Normalized task: complexity
[2025-04-29 20:35:22,884][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 20:35:22,884][__main__][INFO] - Determined Task Type: regression
[2025-04-29 20:35:22,888][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 20:35:22,889][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 20:35:25,111][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 20:35:27,371][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 20:35:27,372][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:35:27,437][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:35:27,471][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:35:27,598][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 20:35:27,605][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:35:27,605][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 20:35:27,607][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:35:27,634][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:35:27,670][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:35:27,685][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 20:35:27,686][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:35:27,686][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 20:35:27,687][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:35:27,711][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:35:27,746][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:35:27,761][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 20:35:27,762][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:35:27,762][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 20:35:27,766][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 20:35:27,766][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 20:35:27,766][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 20:35:27,766][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 20:35:27,767][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 20:35:27,767][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 20:35:27,767][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 20:35:27,767][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 20:35:27,767][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 20:35:27,767][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 20:35:27,767][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 20:35:27,767][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 20:35:27,767][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 20:35:27,768][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 20:35:27,768][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 20:35:27,768][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 20:35:27,768][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 20:35:27,768][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 20:35:27,768][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 20:35:27,768][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 20:35:27,768][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 20:35:27,768][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 20:35:27,769][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 20:35:27,769][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 20:35:27,769][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 20:35:27,769][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 20:35:27,769][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 20:35:27,769][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 20:35:27,769][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 20:35:32,228][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 20:35:32,229][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,229][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,229][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,229][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,229][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,229][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,229][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,229][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,229][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,229][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,230][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,230][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,230][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,230][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,230][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,230][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,230][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,230][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,230][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,230][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,230][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,230][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,230][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,230][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,230][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,230][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,231][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,231][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,231][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,231][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,231][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,231][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,231][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,231][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,231][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,231][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,231][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,231][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,231][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,231][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,231][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,231][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,232][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,232][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,232][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,232][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,232][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,232][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,232][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,232][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,232][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,232][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,232][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,232][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,232][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,232][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,232][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,232][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,233][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,233][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,233][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,233][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,233][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,233][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,233][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,233][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,233][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,233][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,233][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,233][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,233][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,233][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,233][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,234][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,234][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,234][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,234][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,234][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,234][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,234][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,234][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,234][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,234][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,234][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,234][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,234][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,234][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,234][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,234][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,235][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,235][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,235][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,235][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,235][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,235][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,235][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,235][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,235][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,235][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,235][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,235][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,235][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,235][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,235][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,235][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,236][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,236][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,236][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,236][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,236][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,236][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,236][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,236][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,236][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,236][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,236][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,236][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,236][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,236][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,236][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,236][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,236][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,237][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,237][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,237][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,237][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,237][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,237][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,237][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,237][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,237][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,237][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,237][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,237][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,237][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,237][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,237][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,237][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,237][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,237][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,238][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,238][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,238][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,238][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,238][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,238][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,238][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,238][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,238][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,238][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,238][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,238][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,238][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,238][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,238][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,238][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,239][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,239][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,239][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,239][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,239][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,239][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,239][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,239][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,239][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,239][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,239][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,239][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,239][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,239][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,239][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,240][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,240][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,240][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,240][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,240][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,240][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,240][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,240][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,240][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,240][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,240][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,240][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,240][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,240][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,240][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,240][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,241][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,241][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,241][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,241][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,241][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,241][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,241][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,241][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,241][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,241][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,241][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,241][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:35:32,242][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 20:35:32,242][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 20:35:32,243][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 20:35:32,243][__main__][INFO] - Successfully created model for ar
[2025-04-29 20:35:32,243][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.1842Epoch 1/10: [                              ] 2/63 batches, loss: 0.1605Epoch 1/10: [=                             ] 3/63 batches, loss: 0.1562Epoch 1/10: [=                             ] 4/63 batches, loss: 0.1463Epoch 1/10: [==                            ] 5/63 batches, loss: 0.1373Epoch 1/10: [==                            ] 6/63 batches, loss: 0.1371Epoch 1/10: [===                           ] 7/63 batches, loss: 0.1316Epoch 1/10: [===                           ] 8/63 batches, loss: 0.1245Epoch 1/10: [====                          ] 9/63 batches, loss: 0.1222Epoch 1/10: [====                          ] 10/63 batches, loss: 0.1199Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.1179Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.1222Epoch 1/10: [======                        ] 13/63 batches, loss: 0.1209Epoch 1/10: [======                        ] 14/63 batches, loss: 0.1203Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.1185Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.1234Epoch 1/10: [========                      ] 17/63 batches, loss: 0.1254Epoch 1/10: [========                      ] 18/63 batches, loss: 0.1247Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.1244Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.1235Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.1214Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.1187Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.1168Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.1171Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.1151Epoch 1/10: [============                  ] 26/63 batches, loss: 0.1132Epoch 1/10: [============                  ] 27/63 batches, loss: 0.1124Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.1097Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.1114Epoch 1/10: [==============                ] 30/63 batches, loss: 0.1117Epoch 1/10: [==============                ] 31/63 batches, loss: 0.1134Epoch 1/10: [===============               ] 32/63 batches, loss: 0.1130Epoch 1/10: [===============               ] 33/63 batches, loss: 0.1113Epoch 1/10: [================              ] 34/63 batches, loss: 0.1114Epoch 1/10: [================              ] 35/63 batches, loss: 0.1115Epoch 1/10: [=================             ] 36/63 batches, loss: 0.1102Epoch 1/10: [=================             ] 37/63 batches, loss: 0.1091Epoch 1/10: [==================            ] 38/63 batches, loss: 0.1093Epoch 1/10: [==================            ] 39/63 batches, loss: 0.1084Epoch 1/10: [===================           ] 40/63 batches, loss: 0.1095Epoch 1/10: [===================           ] 41/63 batches, loss: 0.1085Epoch 1/10: [====================          ] 42/63 batches, loss: 0.1077Epoch 1/10: [====================          ] 43/63 batches, loss: 0.1065Epoch 1/10: [====================          ] 44/63 batches, loss: 0.1054Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.1039Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.1026Epoch 1/10: [======================        ] 47/63 batches, loss: 0.1015Epoch 1/10: [======================        ] 48/63 batches, loss: 0.1004Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.0992Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.0985Epoch 1/10: [========================      ] 51/63 batches, loss: 0.0975Epoch 1/10: [========================      ] 52/63 batches, loss: 0.0967Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.0963Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.0954Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.0949Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.0941Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.0941Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.0936Epoch 1/10: [============================  ] 59/63 batches, loss: 0.0939Epoch 1/10: [============================  ] 60/63 batches, loss: 0.0938Epoch 1/10: [============================= ] 61/63 batches, loss: 0.0935Epoch 1/10: [============================= ] 62/63 batches, loss: 0.0935Epoch 1/10: [==============================] 63/63 batches, loss: 0.0924
[2025-04-29 20:35:40,589][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0924
[2025-04-29 20:35:40,788][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0830, Metrics: {'mse': 0.0826147273182869, 'rmse': 0.2874277775690563, 'r2': -0.27336764335632324}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.0493Epoch 2/10: [                              ] 2/63 batches, loss: 0.0620Epoch 2/10: [=                             ] 3/63 batches, loss: 0.0673Epoch 2/10: [=                             ] 4/63 batches, loss: 0.0631Epoch 2/10: [==                            ] 5/63 batches, loss: 0.0621Epoch 2/10: [==                            ] 6/63 batches, loss: 0.0590Epoch 2/10: [===                           ] 7/63 batches, loss: 0.0602Epoch 2/10: [===                           ] 8/63 batches, loss: 0.0565Epoch 2/10: [====                          ] 9/63 batches, loss: 0.0594Epoch 2/10: [====                          ] 10/63 batches, loss: 0.0585Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.0576Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.0564Epoch 2/10: [======                        ] 13/63 batches, loss: 0.0570Epoch 2/10: [======                        ] 14/63 batches, loss: 0.0580Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.0580Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.0594Epoch 2/10: [========                      ] 17/63 batches, loss: 0.0579Epoch 2/10: [========                      ] 18/63 batches, loss: 0.0562Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.0542Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.0555Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.0542Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.0536Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.0527Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.0538Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.0541Epoch 2/10: [============                  ] 26/63 batches, loss: 0.0549Epoch 2/10: [============                  ] 27/63 batches, loss: 0.0540Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.0531Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.0524Epoch 2/10: [==============                ] 30/63 batches, loss: 0.0528Epoch 2/10: [==============                ] 31/63 batches, loss: 0.0515Epoch 2/10: [===============               ] 32/63 batches, loss: 0.0510Epoch 2/10: [===============               ] 33/63 batches, loss: 0.0502Epoch 2/10: [================              ] 34/63 batches, loss: 0.0507Epoch 2/10: [================              ] 35/63 batches, loss: 0.0500Epoch 2/10: [=================             ] 36/63 batches, loss: 0.0500Epoch 2/10: [=================             ] 37/63 batches, loss: 0.0496Epoch 2/10: [==================            ] 38/63 batches, loss: 0.0495Epoch 2/10: [==================            ] 39/63 batches, loss: 0.0490Epoch 2/10: [===================           ] 40/63 batches, loss: 0.0486Epoch 2/10: [===================           ] 41/63 batches, loss: 0.0485Epoch 2/10: [====================          ] 42/63 batches, loss: 0.0484Epoch 2/10: [====================          ] 43/63 batches, loss: 0.0477Epoch 2/10: [====================          ] 44/63 batches, loss: 0.0470Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.0468Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.0463Epoch 2/10: [======================        ] 47/63 batches, loss: 0.0456Epoch 2/10: [======================        ] 48/63 batches, loss: 0.0455Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.0453Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.0447Epoch 2/10: [========================      ] 51/63 batches, loss: 0.0446Epoch 2/10: [========================      ] 52/63 batches, loss: 0.0442Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.0441Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.0446Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.0444Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.0442Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.0439Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.0439Epoch 2/10: [============================  ] 59/63 batches, loss: 0.0436Epoch 2/10: [============================  ] 60/63 batches, loss: 0.0434Epoch 2/10: [============================= ] 61/63 batches, loss: 0.0432Epoch 2/10: [============================= ] 62/63 batches, loss: 0.0433Epoch 2/10: [==============================] 63/63 batches, loss: 0.0436
[2025-04-29 20:35:46,966][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0436
[2025-04-29 20:35:47,216][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0635, Metrics: {'mse': 0.06325487792491913, 'rmse': 0.25150522444855716, 'r2': 0.025031983852386475}
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.0539Epoch 3/10: [                              ] 2/63 batches, loss: 0.0445Epoch 3/10: [=                             ] 3/63 batches, loss: 0.0501Epoch 3/10: [=                             ] 4/63 batches, loss: 0.0425Epoch 3/10: [==                            ] 5/63 batches, loss: 0.0397Epoch 3/10: [==                            ] 6/63 batches, loss: 0.0389Epoch 3/10: [===                           ] 7/63 batches, loss: 0.0387Epoch 3/10: [===                           ] 8/63 batches, loss: 0.0394Epoch 3/10: [====                          ] 9/63 batches, loss: 0.0403Epoch 3/10: [====                          ] 10/63 batches, loss: 0.0408Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.0401Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.0399Epoch 3/10: [======                        ] 13/63 batches, loss: 0.0395Epoch 3/10: [======                        ] 14/63 batches, loss: 0.0386Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.0382Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.0380Epoch 3/10: [========                      ] 17/63 batches, loss: 0.0374Epoch 3/10: [========                      ] 18/63 batches, loss: 0.0364Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.0354Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.0354Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.0356Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.0347Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.0339Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.0331Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.0328Epoch 3/10: [============                  ] 26/63 batches, loss: 0.0326Epoch 3/10: [============                  ] 27/63 batches, loss: 0.0328Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.0319Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.0321Epoch 3/10: [==============                ] 30/63 batches, loss: 0.0318Epoch 3/10: [==============                ] 31/63 batches, loss: 0.0319Epoch 3/10: [===============               ] 32/63 batches, loss: 0.0315Epoch 3/10: [===============               ] 33/63 batches, loss: 0.0310Epoch 3/10: [================              ] 34/63 batches, loss: 0.0314Epoch 3/10: [================              ] 35/63 batches, loss: 0.0316Epoch 3/10: [=================             ] 36/63 batches, loss: 0.0318Epoch 3/10: [=================             ] 37/63 batches, loss: 0.0316Epoch 3/10: [==================            ] 38/63 batches, loss: 0.0314Epoch 3/10: [==================            ] 39/63 batches, loss: 0.0311Epoch 3/10: [===================           ] 40/63 batches, loss: 0.0315Epoch 3/10: [===================           ] 41/63 batches, loss: 0.0314Epoch 3/10: [====================          ] 42/63 batches, loss: 0.0314Epoch 3/10: [====================          ] 43/63 batches, loss: 0.0316Epoch 3/10: [====================          ] 44/63 batches, loss: 0.0313Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.0312Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.0317Epoch 3/10: [======================        ] 47/63 batches, loss: 0.0321Epoch 3/10: [======================        ] 48/63 batches, loss: 0.0327Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.0338Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.0341Epoch 3/10: [========================      ] 51/63 batches, loss: 0.0340Epoch 3/10: [========================      ] 52/63 batches, loss: 0.0347Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.0349Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.0347Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.0349Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.0350Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.0347Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.0344Epoch 3/10: [============================  ] 59/63 batches, loss: 0.0349Epoch 3/10: [============================  ] 60/63 batches, loss: 0.0347Epoch 3/10: [============================= ] 61/63 batches, loss: 0.0345Epoch 3/10: [============================= ] 62/63 batches, loss: 0.0345Epoch 3/10: [==============================] 63/63 batches, loss: 0.0343
[2025-04-29 20:35:53,400][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0343
[2025-04-29 20:35:53,618][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0448, Metrics: {'mse': 0.04470817372202873, 'rmse': 0.21144307442436777, 'r2': 0.3108983635902405}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.0437Epoch 4/10: [                              ] 2/63 batches, loss: 0.0338Epoch 4/10: [=                             ] 3/63 batches, loss: 0.0348Epoch 4/10: [=                             ] 4/63 batches, loss: 0.0316Epoch 4/10: [==                            ] 5/63 batches, loss: 0.0316Epoch 4/10: [==                            ] 6/63 batches, loss: 0.0303Epoch 4/10: [===                           ] 7/63 batches, loss: 0.0275Epoch 4/10: [===                           ] 8/63 batches, loss: 0.0284Epoch 4/10: [====                          ] 9/63 batches, loss: 0.0273Epoch 4/10: [====                          ] 10/63 batches, loss: 0.0276Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.0280Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.0292Epoch 4/10: [======                        ] 13/63 batches, loss: 0.0293Epoch 4/10: [======                        ] 14/63 batches, loss: 0.0288Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.0290Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.0282Epoch 4/10: [========                      ] 17/63 batches, loss: 0.0283Epoch 4/10: [========                      ] 18/63 batches, loss: 0.0284Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.0278Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.0275Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.0268Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.0266Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.0264Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.0262Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.0260Epoch 4/10: [============                  ] 26/63 batches, loss: 0.0256Epoch 4/10: [============                  ] 27/63 batches, loss: 0.0253Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.0250Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.0248Epoch 4/10: [==============                ] 30/63 batches, loss: 0.0248Epoch 4/10: [==============                ] 31/63 batches, loss: 0.0244Epoch 4/10: [===============               ] 32/63 batches, loss: 0.0240Epoch 4/10: [===============               ] 33/63 batches, loss: 0.0235Epoch 4/10: [================              ] 34/63 batches, loss: 0.0232Epoch 4/10: [================              ] 35/63 batches, loss: 0.0230Epoch 4/10: [=================             ] 36/63 batches, loss: 0.0232Epoch 4/10: [=================             ] 37/63 batches, loss: 0.0232Epoch 4/10: [==================            ] 38/63 batches, loss: 0.0231Epoch 4/10: [==================            ] 39/63 batches, loss: 0.0233Epoch 4/10: [===================           ] 40/63 batches, loss: 0.0229Epoch 4/10: [===================           ] 41/63 batches, loss: 0.0229Epoch 4/10: [====================          ] 42/63 batches, loss: 0.0232Epoch 4/10: [====================          ] 43/63 batches, loss: 0.0237Epoch 4/10: [====================          ] 44/63 batches, loss: 0.0234Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.0233Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.0231Epoch 4/10: [======================        ] 47/63 batches, loss: 0.0232Epoch 4/10: [======================        ] 48/63 batches, loss: 0.0231Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.0232Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.0230Epoch 4/10: [========================      ] 51/63 batches, loss: 0.0228Epoch 4/10: [========================      ] 52/63 batches, loss: 0.0231Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.0231Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.0230Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.0228Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.0228Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.0228Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.0229Epoch 4/10: [============================  ] 59/63 batches, loss: 0.0227Epoch 4/10: [============================  ] 60/63 batches, loss: 0.0226Epoch 4/10: [============================= ] 61/63 batches, loss: 0.0228Epoch 4/10: [============================= ] 62/63 batches, loss: 0.0226Epoch 4/10: [==============================] 63/63 batches, loss: 0.0223
[2025-04-29 20:35:59,745][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0223
[2025-04-29 20:35:59,961][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0346, Metrics: {'mse': 0.034633319824934006, 'rmse': 0.18610029506944367, 'r2': 0.46618539094924927}
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.0161Epoch 5/10: [                              ] 2/63 batches, loss: 0.0224Epoch 5/10: [=                             ] 3/63 batches, loss: 0.0199Epoch 5/10: [=                             ] 4/63 batches, loss: 0.0227Epoch 5/10: [==                            ] 5/63 batches, loss: 0.0214Epoch 5/10: [==                            ] 6/63 batches, loss: 0.0207Epoch 5/10: [===                           ] 7/63 batches, loss: 0.0196Epoch 5/10: [===                           ] 8/63 batches, loss: 0.0193Epoch 5/10: [====                          ] 9/63 batches, loss: 0.0183Epoch 5/10: [====                          ] 10/63 batches, loss: 0.0180Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.0176Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.0179Epoch 5/10: [======                        ] 13/63 batches, loss: 0.0189Epoch 5/10: [======                        ] 14/63 batches, loss: 0.0195Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.0194Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.0198Epoch 5/10: [========                      ] 17/63 batches, loss: 0.0194Epoch 5/10: [========                      ] 18/63 batches, loss: 0.0194Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.0196Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.0201Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.0209Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.0214Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.0208Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.0205Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.0210Epoch 5/10: [============                  ] 26/63 batches, loss: 0.0212Epoch 5/10: [============                  ] 27/63 batches, loss: 0.0214Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.0212Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.0212Epoch 5/10: [==============                ] 30/63 batches, loss: 0.0211Epoch 5/10: [==============                ] 31/63 batches, loss: 0.0213Epoch 5/10: [===============               ] 32/63 batches, loss: 0.0213Epoch 5/10: [===============               ] 33/63 batches, loss: 0.0210Epoch 5/10: [================              ] 34/63 batches, loss: 0.0214Epoch 5/10: [================              ] 35/63 batches, loss: 0.0218Epoch 5/10: [=================             ] 36/63 batches, loss: 0.0219Epoch 5/10: [=================             ] 37/63 batches, loss: 0.0220Epoch 5/10: [==================            ] 38/63 batches, loss: 0.0219Epoch 5/10: [==================            ] 39/63 batches, loss: 0.0218Epoch 5/10: [===================           ] 40/63 batches, loss: 0.0223Epoch 5/10: [===================           ] 41/63 batches, loss: 0.0222Epoch 5/10: [====================          ] 42/63 batches, loss: 0.0222Epoch 5/10: [====================          ] 43/63 batches, loss: 0.0219Epoch 5/10: [====================          ] 44/63 batches, loss: 0.0217Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.0215Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.0213Epoch 5/10: [======================        ] 47/63 batches, loss: 0.0214Epoch 5/10: [======================        ] 48/63 batches, loss: 0.0212Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.0212Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.0215Epoch 5/10: [========================      ] 51/63 batches, loss: 0.0216Epoch 5/10: [========================      ] 52/63 batches, loss: 0.0218Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.0218Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.0216Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.0217Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.0216Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.0216Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.0217Epoch 5/10: [============================  ] 59/63 batches, loss: 0.0216Epoch 5/10: [============================  ] 60/63 batches, loss: 0.0215Epoch 5/10: [============================= ] 61/63 batches, loss: 0.0213Epoch 5/10: [============================= ] 62/63 batches, loss: 0.0212Epoch 5/10: [==============================] 63/63 batches, loss: 0.0210
[2025-04-29 20:36:06,102][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0210
[2025-04-29 20:36:06,316][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0252, Metrics: {'mse': 0.02523656375706196, 'rmse': 0.15886020192943845, 'r2': 0.6110206246376038}
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.0194Epoch 6/10: [                              ] 2/63 batches, loss: 0.0215Epoch 6/10: [=                             ] 3/63 batches, loss: 0.0187Epoch 6/10: [=                             ] 4/63 batches, loss: 0.0168Epoch 6/10: [==                            ] 5/63 batches, loss: 0.0153Epoch 6/10: [==                            ] 6/63 batches, loss: 0.0187Epoch 6/10: [===                           ] 7/63 batches, loss: 0.0201Epoch 6/10: [===                           ] 8/63 batches, loss: 0.0196Epoch 6/10: [====                          ] 9/63 batches, loss: 0.0190Epoch 6/10: [====                          ] 10/63 batches, loss: 0.0177Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.0171Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.0175Epoch 6/10: [======                        ] 13/63 batches, loss: 0.0168Epoch 6/10: [======                        ] 14/63 batches, loss: 0.0169Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.0176Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.0170Epoch 6/10: [========                      ] 17/63 batches, loss: 0.0166Epoch 6/10: [========                      ] 18/63 batches, loss: 0.0162Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.0162Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.0164Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.0161Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.0173Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.0169Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.0164Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.0162Epoch 6/10: [============                  ] 26/63 batches, loss: 0.0159Epoch 6/10: [============                  ] 27/63 batches, loss: 0.0160Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.0159Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.0162Epoch 6/10: [==============                ] 30/63 batches, loss: 0.0165Epoch 6/10: [==============                ] 31/63 batches, loss: 0.0164Epoch 6/10: [===============               ] 32/63 batches, loss: 0.0164Epoch 6/10: [===============               ] 33/63 batches, loss: 0.0169Epoch 6/10: [================              ] 34/63 batches, loss: 0.0167Epoch 6/10: [================              ] 35/63 batches, loss: 0.0166Epoch 6/10: [=================             ] 36/63 batches, loss: 0.0164Epoch 6/10: [=================             ] 37/63 batches, loss: 0.0163Epoch 6/10: [==================            ] 38/63 batches, loss: 0.0163Epoch 6/10: [==================            ] 39/63 batches, loss: 0.0165Epoch 6/10: [===================           ] 40/63 batches, loss: 0.0162Epoch 6/10: [===================           ] 41/63 batches, loss: 0.0163Epoch 6/10: [====================          ] 42/63 batches, loss: 0.0161Epoch 6/10: [====================          ] 43/63 batches, loss: 0.0159Epoch 6/10: [====================          ] 44/63 batches, loss: 0.0160Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.0158Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.0161Epoch 6/10: [======================        ] 47/63 batches, loss: 0.0159Epoch 6/10: [======================        ] 48/63 batches, loss: 0.0160Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.0161Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.0160Epoch 6/10: [========================      ] 51/63 batches, loss: 0.0160Epoch 6/10: [========================      ] 52/63 batches, loss: 0.0158Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.0156Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.0156Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.0155Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.0155Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.0156Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.0157Epoch 6/10: [============================  ] 59/63 batches, loss: 0.0158Epoch 6/10: [============================  ] 60/63 batches, loss: 0.0158Epoch 6/10: [============================= ] 61/63 batches, loss: 0.0158Epoch 6/10: [============================= ] 62/63 batches, loss: 0.0158Epoch 6/10: [==============================] 63/63 batches, loss: 0.0157
[2025-04-29 20:36:12,553][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0157
[2025-04-29 20:36:12,779][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0262, Metrics: {'mse': 0.025908786803483963, 'rmse': 0.16096206634944757, 'r2': 0.6006594300270081}
[2025-04-29 20:36:12,780][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/63 batches, loss: 0.0189Epoch 7/10: [                              ] 2/63 batches, loss: 0.0213Epoch 7/10: [=                             ] 3/63 batches, loss: 0.0168Epoch 7/10: [=                             ] 4/63 batches, loss: 0.0154Epoch 7/10: [==                            ] 5/63 batches, loss: 0.0165Epoch 7/10: [==                            ] 6/63 batches, loss: 0.0166Epoch 7/10: [===                           ] 7/63 batches, loss: 0.0162Epoch 7/10: [===                           ] 8/63 batches, loss: 0.0168Epoch 7/10: [====                          ] 9/63 batches, loss: 0.0154Epoch 7/10: [====                          ] 10/63 batches, loss: 0.0148Epoch 7/10: [=====                         ] 11/63 batches, loss: 0.0155Epoch 7/10: [=====                         ] 12/63 batches, loss: 0.0154Epoch 7/10: [======                        ] 13/63 batches, loss: 0.0150Epoch 7/10: [======                        ] 14/63 batches, loss: 0.0149Epoch 7/10: [=======                       ] 15/63 batches, loss: 0.0144Epoch 7/10: [=======                       ] 16/63 batches, loss: 0.0144Epoch 7/10: [========                      ] 17/63 batches, loss: 0.0145Epoch 7/10: [========                      ] 18/63 batches, loss: 0.0144Epoch 7/10: [=========                     ] 19/63 batches, loss: 0.0139Epoch 7/10: [=========                     ] 20/63 batches, loss: 0.0142Epoch 7/10: [==========                    ] 21/63 batches, loss: 0.0141Epoch 7/10: [==========                    ] 22/63 batches, loss: 0.0142Epoch 7/10: [==========                    ] 23/63 batches, loss: 0.0139Epoch 7/10: [===========                   ] 24/63 batches, loss: 0.0137Epoch 7/10: [===========                   ] 25/63 batches, loss: 0.0133Epoch 7/10: [============                  ] 26/63 batches, loss: 0.0134Epoch 7/10: [============                  ] 27/63 batches, loss: 0.0133Epoch 7/10: [=============                 ] 28/63 batches, loss: 0.0135Epoch 7/10: [=============                 ] 29/63 batches, loss: 0.0135Epoch 7/10: [==============                ] 30/63 batches, loss: 0.0139Epoch 7/10: [==============                ] 31/63 batches, loss: 0.0138Epoch 7/10: [===============               ] 32/63 batches, loss: 0.0138Epoch 7/10: [===============               ] 33/63 batches, loss: 0.0138Epoch 7/10: [================              ] 34/63 batches, loss: 0.0137Epoch 7/10: [================              ] 35/63 batches, loss: 0.0134Epoch 7/10: [=================             ] 36/63 batches, loss: 0.0133Epoch 7/10: [=================             ] 37/63 batches, loss: 0.0132Epoch 7/10: [==================            ] 38/63 batches, loss: 0.0133Epoch 7/10: [==================            ] 39/63 batches, loss: 0.0137Epoch 7/10: [===================           ] 40/63 batches, loss: 0.0138Epoch 7/10: [===================           ] 41/63 batches, loss: 0.0138Epoch 7/10: [====================          ] 42/63 batches, loss: 0.0137Epoch 7/10: [====================          ] 43/63 batches, loss: 0.0136Epoch 7/10: [====================          ] 44/63 batches, loss: 0.0135Epoch 7/10: [=====================         ] 45/63 batches, loss: 0.0136Epoch 7/10: [=====================         ] 46/63 batches, loss: 0.0135Epoch 7/10: [======================        ] 47/63 batches, loss: 0.0133Epoch 7/10: [======================        ] 48/63 batches, loss: 0.0132Epoch 7/10: [=======================       ] 49/63 batches, loss: 0.0136Epoch 7/10: [=======================       ] 50/63 batches, loss: 0.0136Epoch 7/10: [========================      ] 51/63 batches, loss: 0.0137Epoch 7/10: [========================      ] 52/63 batches, loss: 0.0136Epoch 7/10: [=========================     ] 53/63 batches, loss: 0.0138Epoch 7/10: [=========================     ] 54/63 batches, loss: 0.0137Epoch 7/10: [==========================    ] 55/63 batches, loss: 0.0137Epoch 7/10: [==========================    ] 56/63 batches, loss: 0.0137Epoch 7/10: [===========================   ] 57/63 batches, loss: 0.0137Epoch 7/10: [===========================   ] 58/63 batches, loss: 0.0136Epoch 7/10: [============================  ] 59/63 batches, loss: 0.0136Epoch 7/10: [============================  ] 60/63 batches, loss: 0.0138Epoch 7/10: [============================= ] 61/63 batches, loss: 0.0137Epoch 7/10: [============================= ] 62/63 batches, loss: 0.0136Epoch 7/10: [==============================] 63/63 batches, loss: 0.0138
[2025-04-29 20:36:18,569][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0138
[2025-04-29 20:36:18,796][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0286, Metrics: {'mse': 0.028236092999577522, 'rmse': 0.16803598721576732, 'r2': 0.5647878646850586}
[2025-04-29 20:36:18,797][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/63 batches, loss: 0.0249Epoch 8/10: [                              ] 2/63 batches, loss: 0.0201Epoch 8/10: [=                             ] 3/63 batches, loss: 0.0188Epoch 8/10: [=                             ] 4/63 batches, loss: 0.0197Epoch 8/10: [==                            ] 5/63 batches, loss: 0.0188Epoch 8/10: [==                            ] 6/63 batches, loss: 0.0173Epoch 8/10: [===                           ] 7/63 batches, loss: 0.0173Epoch 8/10: [===                           ] 8/63 batches, loss: 0.0174Epoch 8/10: [====                          ] 9/63 batches, loss: 0.0170Epoch 8/10: [====                          ] 10/63 batches, loss: 0.0164Epoch 8/10: [=====                         ] 11/63 batches, loss: 0.0161Epoch 8/10: [=====                         ] 12/63 batches, loss: 0.0158Epoch 8/10: [======                        ] 13/63 batches, loss: 0.0151Epoch 8/10: [======                        ] 14/63 batches, loss: 0.0148Epoch 8/10: [=======                       ] 15/63 batches, loss: 0.0145Epoch 8/10: [=======                       ] 16/63 batches, loss: 0.0144Epoch 8/10: [========                      ] 17/63 batches, loss: 0.0154Epoch 8/10: [========                      ] 18/63 batches, loss: 0.0152Epoch 8/10: [=========                     ] 19/63 batches, loss: 0.0155Epoch 8/10: [=========                     ] 20/63 batches, loss: 0.0152Epoch 8/10: [==========                    ] 21/63 batches, loss: 0.0154Epoch 8/10: [==========                    ] 22/63 batches, loss: 0.0154Epoch 8/10: [==========                    ] 23/63 batches, loss: 0.0151Epoch 8/10: [===========                   ] 24/63 batches, loss: 0.0148Epoch 8/10: [===========                   ] 25/63 batches, loss: 0.0151Epoch 8/10: [============                  ] 26/63 batches, loss: 0.0151Epoch 8/10: [============                  ] 27/63 batches, loss: 0.0152Epoch 8/10: [=============                 ] 28/63 batches, loss: 0.0154Epoch 8/10: [=============                 ] 29/63 batches, loss: 0.0150Epoch 8/10: [==============                ] 30/63 batches, loss: 0.0149Epoch 8/10: [==============                ] 31/63 batches, loss: 0.0157Epoch 8/10: [===============               ] 32/63 batches, loss: 0.0162Epoch 8/10: [===============               ] 33/63 batches, loss: 0.0163Epoch 8/10: [================              ] 34/63 batches, loss: 0.0164Epoch 8/10: [================              ] 35/63 batches, loss: 0.0168Epoch 8/10: [=================             ] 36/63 batches, loss: 0.0168Epoch 8/10: [=================             ] 37/63 batches, loss: 0.0169Epoch 8/10: [==================            ] 38/63 batches, loss: 0.0171Epoch 8/10: [==================            ] 39/63 batches, loss: 0.0173Epoch 8/10: [===================           ] 40/63 batches, loss: 0.0172Epoch 8/10: [===================           ] 41/63 batches, loss: 0.0174Epoch 8/10: [====================          ] 42/63 batches, loss: 0.0179Epoch 8/10: [====================          ] 43/63 batches, loss: 0.0176Epoch 8/10: [====================          ] 44/63 batches, loss: 0.0174Epoch 8/10: [=====================         ] 45/63 batches, loss: 0.0173Epoch 8/10: [=====================         ] 46/63 batches, loss: 0.0171Epoch 8/10: [======================        ] 47/63 batches, loss: 0.0172Epoch 8/10: [======================        ] 48/63 batches, loss: 0.0174Epoch 8/10: [=======================       ] 49/63 batches, loss: 0.0174Epoch 8/10: [=======================       ] 50/63 batches, loss: 0.0174Epoch 8/10: [========================      ] 51/63 batches, loss: 0.0174Epoch 8/10: [========================      ] 52/63 batches, loss: 0.0173Epoch 8/10: [=========================     ] 53/63 batches, loss: 0.0171Epoch 8/10: [=========================     ] 54/63 batches, loss: 0.0172Epoch 8/10: [==========================    ] 55/63 batches, loss: 0.0173Epoch 8/10: [==========================    ] 56/63 batches, loss: 0.0171Epoch 8/10: [===========================   ] 57/63 batches, loss: 0.0174Epoch 8/10: [===========================   ] 58/63 batches, loss: 0.0174Epoch 8/10: [============================  ] 59/63 batches, loss: 0.0172Epoch 8/10: [============================  ] 60/63 batches, loss: 0.0172Epoch 8/10: [============================= ] 61/63 batches, loss: 0.0171Epoch 8/10: [============================= ] 62/63 batches, loss: 0.0172Epoch 8/10: [==============================] 63/63 batches, loss: 0.0173
[2025-04-29 20:36:24,579][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0173
[2025-04-29 20:36:24,798][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0288, Metrics: {'mse': 0.02804962918162346, 'rmse': 0.16748023519694333, 'r2': 0.5676619410514832}
[2025-04-29 20:36:24,798][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 20:36:24,798][src.training.lm_trainer][INFO] - Early stopping at epoch 8
[2025-04-29 20:36:24,799][src.training.lm_trainer][INFO] - Training completed in 50.68 seconds
[2025-04-29 20:36:24,799][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 20:36:27,352][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.013290459290146828, 'rmse': 0.1152842543027747, 'r2': 0.5670489072799683}
[2025-04-29 20:36:27,353][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.02523656375706196, 'rmse': 0.15886020192943845, 'r2': 0.6110206246376038}
[2025-04-29 20:36:27,353][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.027537092566490173, 'rmse': 0.1659430401266958, 'r2': 0.525272786617279}
[2025-04-29 20:36:28,996][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/ar/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▃▂▁
wandb:     best_val_mse █▆▃▂▁
wandb:      best_val_r2 ▁▃▆▇█
wandb:    best_val_rmse █▆▄▂▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▃▂▁▁▁▁
wandb:          val_mse █▆▃▂▁▁▁▁
wandb:           val_r2 ▁▃▆▇████
wandb:         val_rmse █▆▄▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02525
wandb:     best_val_mse 0.02524
wandb:      best_val_r2 0.61102
wandb:    best_val_rmse 0.15886
wandb:            epoch 8
wandb:   final_test_mse 0.02754
wandb:    final_test_r2 0.52527
wandb:  final_test_rmse 0.16594
wandb:  final_train_mse 0.01329
wandb:   final_train_r2 0.56705
wandb: final_train_rmse 0.11528
wandb:    final_val_mse 0.02524
wandb:     final_val_r2 0.61102
wandb:   final_val_rmse 0.15886
wandb:    learning_rate 2e-05
wandb:       train_loss 0.01727
wandb:       train_time 50.67661
wandb:         val_loss 0.02883
wandb:          val_mse 0.02805
wandb:           val_r2 0.56766
wandb:         val_rmse 0.16748
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_203522-1xemytuz
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_203522-1xemytuz/logs
Experiment finetune_complexity_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/results.json
Running experiment: finetune_question_type_en
Command: python -m src.experiments.run_experiment             "hydra.job.chdir=False"             "hydra.run.dir=."             "experiment=finetune"             "experiment.tasks=question_type"             "model=glot500_finetune"             "model.lm_name=cis-lmu/glot500-base"             "model.dropout=0.1"             "model.freeze_model=false"             "model.finetune=true"             "data.languages=[en]"             "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"             "training.task_type=classification"             "training.num_epochs=10"             "training.batch_size=16"             "training.lr=2e-5"             "+training.gradient_accumulation_steps=2"             +training.debug_mode=true             "experiment_name=finetune_question_type_en"             "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/en"             "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 20:36:46,318][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/en
experiment_name: finetune_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
  debug_mode: true
experiment:
  type: lm_finetune
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  finetune: true
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1
    lr: 2.0e-05
    batch_size: 8

[2025-04-29 20:36:46,318][__main__][INFO] - Normalized task: question_type
[2025-04-29 20:36:46,318][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 20:36:46,318][__main__][INFO] - Determined Task Type: classification
[2025-04-29 20:36:46,322][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 20:36:46,323][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 20:36:48,826][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 20:36:51,130][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 20:36:51,131][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:36:51,274][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:36:51,328][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:36:51,481][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 20:36:51,490][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:36:51,491][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 20:36:51,493][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:36:51,535][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:36:51,582][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:36:51,600][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 20:36:51,601][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:36:51,601][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 20:36:51,602][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:36:51,643][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:36:51,701][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:36:51,720][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 20:36:51,722][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:36:51,722][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 20:36:51,724][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 20:36:51,724][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:36:51,724][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:36:51,724][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:36:51,724][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:36:51,724][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 20:36:51,724][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 20:36:51,725][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 20:36:51,725][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 20:36:51,725][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:36:51,725][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:36:51,725][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:36:51,725][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:36:51,725][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 20:36:51,725][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 20:36:51,725][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 20:36:51,725][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:36:51,726][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:36:51,726][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:36:51,726][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:36:51,726][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:36:51,726][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 20:36:51,726][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 20:36:51,726][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 20:36:51,726][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:36:51,726][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 20:36:51,726][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 20:36:51,727][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 20:36:51,727][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 20:36:56,772][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 20:36:56,772][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,772][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,772][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,772][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,772][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,772][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,773][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,773][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,773][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,773][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,773][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,773][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,773][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,773][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,773][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,773][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,773][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,773][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,773][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,773][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,773][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,774][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,774][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,774][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,774][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,774][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,774][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,774][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,774][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,774][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,774][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,774][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,774][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,774][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,774][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,774][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,774][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,775][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,775][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,775][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,775][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,775][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,775][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,775][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,775][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,775][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,775][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,775][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,775][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,775][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,775][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,775][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,776][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,776][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,776][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,776][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,776][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,776][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,776][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,776][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,776][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,776][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,776][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,776][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,776][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,776][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,776][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,776][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,777][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,777][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,777][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,777][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,777][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,777][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,777][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,777][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,777][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,777][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,777][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,777][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,777][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,777][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,777][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,777][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,778][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,778][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,778][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,778][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,778][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,778][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,778][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,778][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,778][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,778][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,778][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,778][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,778][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,778][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,778][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,779][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,779][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,779][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,779][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,779][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,779][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,779][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,779][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,779][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,779][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,779][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,779][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,779][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,779][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,779][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,779][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,780][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,780][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,780][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,780][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,780][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,780][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,780][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,780][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,780][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,780][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,780][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,780][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,780][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,780][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,780][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,780][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,780][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,780][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,781][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,781][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,781][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,781][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,781][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,781][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,781][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,781][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,781][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,781][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,781][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,781][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,781][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,781][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,781][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,782][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,782][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,782][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,782][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,782][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,782][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,782][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,782][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,782][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,782][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,782][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,782][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,782][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,782][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,782][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,783][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,783][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,783][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,783][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,783][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,783][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,783][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,783][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,783][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,783][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,783][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,783][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,783][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,783][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,783][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,783][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,784][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,784][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,784][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,784][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,784][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,784][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,784][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,784][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,784][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,784][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,784][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,784][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,784][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,784][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,784][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,785][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,785][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,785][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,785][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,785][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:36:56,786][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 20:36:56,786][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 20:36:56,787][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 20:36:56,787][__main__][INFO] - Successfully created model for en
[2025-04-29 20:36:56,787][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.7217Epoch 1/10: [                              ] 2/75 batches, loss: 0.7116Epoch 1/10: [=                             ] 3/75 batches, loss: 0.6962Epoch 1/10: [=                             ] 4/75 batches, loss: 0.6934Epoch 1/10: [==                            ] 5/75 batches, loss: 0.6907Epoch 1/10: [==                            ] 6/75 batches, loss: 0.6914Epoch 1/10: [==                            ] 7/75 batches, loss: 0.6890Epoch 1/10: [===                           ] 8/75 batches, loss: 0.6899Epoch 1/10: [===                           ] 9/75 batches, loss: 0.6918Epoch 1/10: [====                          ] 10/75 batches, loss: 0.6909Epoch 1/10: [====                          ] 11/75 batches, loss: 0.6875Epoch 1/10: [====                          ] 12/75 batches, loss: 0.6876Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.6870Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.6874Epoch 1/10: [======                        ] 15/75 batches, loss: 0.6884Epoch 1/10: [======                        ] 16/75 batches, loss: 0.6878Epoch 1/10: [======                        ] 17/75 batches, loss: 0.6880Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.6880Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.6887Epoch 1/10: [========                      ] 20/75 batches, loss: 0.6888Epoch 1/10: [========                      ] 21/75 batches, loss: 0.6889Epoch 1/10: [========                      ] 22/75 batches, loss: 0.6905Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.6899Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.6907Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.6897Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.6896Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.6889Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.6889Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.6885Epoch 1/10: [============                  ] 30/75 batches, loss: 0.6882Epoch 1/10: [============                  ] 31/75 batches, loss: 0.6890Epoch 1/10: [============                  ] 32/75 batches, loss: 0.6878Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.6874Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.6873Epoch 1/10: [==============                ] 35/75 batches, loss: 0.6877Epoch 1/10: [==============                ] 36/75 batches, loss: 0.6879Epoch 1/10: [==============                ] 37/75 batches, loss: 0.6881Epoch 1/10: [===============               ] 38/75 batches, loss: 0.6879Epoch 1/10: [===============               ] 39/75 batches, loss: 0.6875Epoch 1/10: [================              ] 40/75 batches, loss: 0.6874Epoch 1/10: [================              ] 41/75 batches, loss: 0.6870Epoch 1/10: [================              ] 42/75 batches, loss: 0.6867Epoch 1/10: [=================             ] 43/75 batches, loss: 0.6868Epoch 1/10: [=================             ] 44/75 batches, loss: 0.6862Epoch 1/10: [==================            ] 45/75 batches, loss: 0.6863Epoch 1/10: [==================            ] 46/75 batches, loss: 0.6855Epoch 1/10: [==================            ] 47/75 batches, loss: 0.6854Epoch 1/10: [===================           ] 48/75 batches, loss: 0.6852Epoch 1/10: [===================           ] 49/75 batches, loss: 0.6848Epoch 1/10: [====================          ] 50/75 batches, loss: 0.6842Epoch 1/10: [====================          ] 51/75 batches, loss: 0.6836Epoch 1/10: [====================          ] 52/75 batches, loss: 0.6833Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.6829Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.6827Epoch 1/10: [======================        ] 55/75 batches, loss: 0.6818Epoch 1/10: [======================        ] 56/75 batches, loss: 0.6810Epoch 1/10: [======================        ] 57/75 batches, loss: 0.6804Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.6802Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.6802Epoch 1/10: [========================      ] 60/75 batches, loss: 0.6801Epoch 1/10: [========================      ] 61/75 batches, loss: 0.6796Epoch 1/10: [========================      ] 62/75 batches, loss: 0.6798Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.6794Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.6776Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.6761Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.6761Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.6760Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.6754Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.6740Epoch 1/10: [============================  ] 70/75 batches, loss: 0.6726Epoch 1/10: [============================  ] 71/75 batches, loss: 0.6713Epoch 1/10: [============================  ] 72/75 batches, loss: 0.6708Epoch 1/10: [============================= ] 73/75 batches, loss: 0.6694Epoch 1/10: [============================= ] 74/75 batches, loss: 0.6681Epoch 1/10: [==============================] 75/75 batches, loss: 0.6662
[2025-04-29 20:37:06,512][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6662
[2025-04-29 20:37:06,764][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.5445, Metrics: {'accuracy': 0.9027777777777778, 'f1': 0.9113924050632911}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.4648Epoch 2/10: [                              ] 2/75 batches, loss: 0.4921Epoch 2/10: [=                             ] 3/75 batches, loss: 0.4773Epoch 2/10: [=                             ] 4/75 batches, loss: 0.4726Epoch 2/10: [==                            ] 5/75 batches, loss: 0.4871Epoch 2/10: [==                            ] 6/75 batches, loss: 0.4952Epoch 2/10: [==                            ] 7/75 batches, loss: 0.4953Epoch 2/10: [===                           ] 8/75 batches, loss: 0.4978Epoch 2/10: [===                           ] 9/75 batches, loss: 0.4887Epoch 2/10: [====                          ] 10/75 batches, loss: 0.4857Epoch 2/10: [====                          ] 11/75 batches, loss: 0.4943Epoch 2/10: [====                          ] 12/75 batches, loss: 0.4935Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.4943Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.4979Epoch 2/10: [======                        ] 15/75 batches, loss: 0.4938Epoch 2/10: [======                        ] 16/75 batches, loss: 0.4911Epoch 2/10: [======                        ] 17/75 batches, loss: 0.4909Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.4910Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.4911Epoch 2/10: [========                      ] 20/75 batches, loss: 0.4920Epoch 2/10: [========                      ] 21/75 batches, loss: 0.4880Epoch 2/10: [========                      ] 22/75 batches, loss: 0.4883Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.4885Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.4869Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.4852Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.4844Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.4833Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.4825Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.4853Epoch 2/10: [============                  ] 30/75 batches, loss: 0.4830Epoch 2/10: [============                  ] 31/75 batches, loss: 0.4835Epoch 2/10: [============                  ] 32/75 batches, loss: 0.4818Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.4781Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.4741Epoch 2/10: [==============                ] 35/75 batches, loss: 0.4712Epoch 2/10: [==============                ] 36/75 batches, loss: 0.4664Epoch 2/10: [==============                ] 37/75 batches, loss: 0.4654Epoch 2/10: [===============               ] 38/75 batches, loss: 0.4613Epoch 2/10: [===============               ] 39/75 batches, loss: 0.4569Epoch 2/10: [================              ] 40/75 batches, loss: 0.4541Epoch 2/10: [================              ] 41/75 batches, loss: 0.4520Epoch 2/10: [================              ] 42/75 batches, loss: 0.4482Epoch 2/10: [=================             ] 43/75 batches, loss: 0.4458Epoch 2/10: [=================             ] 44/75 batches, loss: 0.4426Epoch 2/10: [==================            ] 45/75 batches, loss: 0.4393Epoch 2/10: [==================            ] 46/75 batches, loss: 0.4352Epoch 2/10: [==================            ] 47/75 batches, loss: 0.4318Epoch 2/10: [===================           ] 48/75 batches, loss: 0.4290Epoch 2/10: [===================           ] 49/75 batches, loss: 0.4252Epoch 2/10: [====================          ] 50/75 batches, loss: 0.4224Epoch 2/10: [====================          ] 51/75 batches, loss: 0.4178Epoch 2/10: [====================          ] 52/75 batches, loss: 0.4144Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.4113Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.4086Epoch 2/10: [======================        ] 55/75 batches, loss: 0.4045Epoch 2/10: [======================        ] 56/75 batches, loss: 0.4015Epoch 2/10: [======================        ] 57/75 batches, loss: 0.3990Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.3965Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.3934Epoch 2/10: [========================      ] 60/75 batches, loss: 0.3900Epoch 2/10: [========================      ] 61/75 batches, loss: 0.3872Epoch 2/10: [========================      ] 62/75 batches, loss: 0.3838Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.3810Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.3784Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.3757Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.3730Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.3702Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.3672Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.3648Epoch 2/10: [============================  ] 70/75 batches, loss: 0.3612Epoch 2/10: [============================  ] 71/75 batches, loss: 0.3586Epoch 2/10: [============================  ] 72/75 batches, loss: 0.3562Epoch 2/10: [============================= ] 73/75 batches, loss: 0.3538Epoch 2/10: [============================= ] 74/75 batches, loss: 0.3509Epoch 2/10: [==============================] 75/75 batches, loss: 0.3480
[2025-04-29 20:37:14,076][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.3480
[2025-04-29 20:37:14,336][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2033, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.1313Epoch 3/10: [                              ] 2/75 batches, loss: 0.1363Epoch 3/10: [=                             ] 3/75 batches, loss: 0.1378Epoch 3/10: [=                             ] 4/75 batches, loss: 0.1187Epoch 3/10: [==                            ] 5/75 batches, loss: 0.1291Epoch 3/10: [==                            ] 6/75 batches, loss: 0.1327Epoch 3/10: [==                            ] 7/75 batches, loss: 0.1340Epoch 3/10: [===                           ] 8/75 batches, loss: 0.1310Epoch 3/10: [===                           ] 9/75 batches, loss: 0.1432Epoch 3/10: [====                          ] 10/75 batches, loss: 0.1453Epoch 3/10: [====                          ] 11/75 batches, loss: 0.1444Epoch 3/10: [====                          ] 12/75 batches, loss: 0.1450Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.1406Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.1396Epoch 3/10: [======                        ] 15/75 batches, loss: 0.1389Epoch 3/10: [======                        ] 16/75 batches, loss: 0.1399Epoch 3/10: [======                        ] 17/75 batches, loss: 0.1479Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.1456Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.1447Epoch 3/10: [========                      ] 20/75 batches, loss: 0.1451Epoch 3/10: [========                      ] 21/75 batches, loss: 0.1436Epoch 3/10: [========                      ] 22/75 batches, loss: 0.1424Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.1422Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.1414Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.1419Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.1438Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.1494Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.1484Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.1485Epoch 3/10: [============                  ] 30/75 batches, loss: 0.1470Epoch 3/10: [============                  ] 31/75 batches, loss: 0.1490Epoch 3/10: [============                  ] 32/75 batches, loss: 0.1477Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.1487Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.1485Epoch 3/10: [==============                ] 35/75 batches, loss: 0.1475Epoch 3/10: [==============                ] 36/75 batches, loss: 0.1472Epoch 3/10: [==============                ] 37/75 batches, loss: 0.1448Epoch 3/10: [===============               ] 38/75 batches, loss: 0.1430Epoch 3/10: [===============               ] 39/75 batches, loss: 0.1417Epoch 3/10: [================              ] 40/75 batches, loss: 0.1411Epoch 3/10: [================              ] 41/75 batches, loss: 0.1411Epoch 3/10: [================              ] 42/75 batches, loss: 0.1409Epoch 3/10: [=================             ] 43/75 batches, loss: 0.1402Epoch 3/10: [=================             ] 44/75 batches, loss: 0.1415Epoch 3/10: [==================            ] 45/75 batches, loss: 0.1413Epoch 3/10: [==================            ] 46/75 batches, loss: 0.1406Epoch 3/10: [==================            ] 47/75 batches, loss: 0.1399Epoch 3/10: [===================           ] 48/75 batches, loss: 0.1396Epoch 3/10: [===================           ] 49/75 batches, loss: 0.1396Epoch 3/10: [====================          ] 50/75 batches, loss: 0.1380Epoch 3/10: [====================          ] 51/75 batches, loss: 0.1365Epoch 3/10: [====================          ] 52/75 batches, loss: 0.1355Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.1345Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.1335Epoch 3/10: [======================        ] 55/75 batches, loss: 0.1324Epoch 3/10: [======================        ] 56/75 batches, loss: 0.1317Epoch 3/10: [======================        ] 57/75 batches, loss: 0.1310Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.1302Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.1289Epoch 3/10: [========================      ] 60/75 batches, loss: 0.1277Epoch 3/10: [========================      ] 61/75 batches, loss: 0.1268Epoch 3/10: [========================      ] 62/75 batches, loss: 0.1257Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.1246Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.1235Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.1254Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.1243Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.1237Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.1226Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.1221Epoch 3/10: [============================  ] 70/75 batches, loss: 0.1216Epoch 3/10: [============================  ] 71/75 batches, loss: 0.1207Epoch 3/10: [============================  ] 72/75 batches, loss: 0.1199Epoch 3/10: [============================= ] 73/75 batches, loss: 0.1188Epoch 3/10: [============================= ] 74/75 batches, loss: 0.1181Epoch 3/10: [==============================] 75/75 batches, loss: 0.1171
[2025-04-29 20:37:21,649][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1171
[2025-04-29 20:37:21,927][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1473, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.0498Epoch 4/10: [                              ] 2/75 batches, loss: 0.0529Epoch 4/10: [=                             ] 3/75 batches, loss: 0.0630Epoch 4/10: [=                             ] 4/75 batches, loss: 0.0563Epoch 4/10: [==                            ] 5/75 batches, loss: 0.0538Epoch 4/10: [==                            ] 6/75 batches, loss: 0.1001Epoch 4/10: [==                            ] 7/75 batches, loss: 0.0943Epoch 4/10: [===                           ] 8/75 batches, loss: 0.0915Epoch 4/10: [===                           ] 9/75 batches, loss: 0.0843Epoch 4/10: [====                          ] 10/75 batches, loss: 0.0955Epoch 4/10: [====                          ] 11/75 batches, loss: 0.1007Epoch 4/10: [====                          ] 12/75 batches, loss: 0.0968Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.0925Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.0891Epoch 4/10: [======                        ] 15/75 batches, loss: 0.0886Epoch 4/10: [======                        ] 16/75 batches, loss: 0.0875Epoch 4/10: [======                        ] 17/75 batches, loss: 0.0851Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.0835Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.0822Epoch 4/10: [========                      ] 20/75 batches, loss: 0.0795Epoch 4/10: [========                      ] 21/75 batches, loss: 0.0791Epoch 4/10: [========                      ] 22/75 batches, loss: 0.0772Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.0760Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.0749Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.0751Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.0735Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.0724Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.0711Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.0714Epoch 4/10: [============                  ] 30/75 batches, loss: 0.0701Epoch 4/10: [============                  ] 31/75 batches, loss: 0.0697Epoch 4/10: [============                  ] 32/75 batches, loss: 0.0686Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.0673Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.0673Epoch 4/10: [==============                ] 35/75 batches, loss: 0.0661Epoch 4/10: [==============                ] 36/75 batches, loss: 0.0657Epoch 4/10: [==============                ] 37/75 batches, loss: 0.0652Epoch 4/10: [===============               ] 38/75 batches, loss: 0.0647Epoch 4/10: [===============               ] 39/75 batches, loss: 0.0642Epoch 4/10: [================              ] 40/75 batches, loss: 0.0640Epoch 4/10: [================              ] 41/75 batches, loss: 0.0633Epoch 4/10: [================              ] 42/75 batches, loss: 0.0627Epoch 4/10: [=================             ] 43/75 batches, loss: 0.0621Epoch 4/10: [=================             ] 44/75 batches, loss: 0.0614Epoch 4/10: [==================            ] 45/75 batches, loss: 0.0607Epoch 4/10: [==================            ] 46/75 batches, loss: 0.0605Epoch 4/10: [==================            ] 47/75 batches, loss: 0.0604Epoch 4/10: [===================           ] 48/75 batches, loss: 0.0596Epoch 4/10: [===================           ] 49/75 batches, loss: 0.0593Epoch 4/10: [====================          ] 50/75 batches, loss: 0.0638Epoch 4/10: [====================          ] 51/75 batches, loss: 0.0640Epoch 4/10: [====================          ] 52/75 batches, loss: 0.0634Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.0629Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.0628Epoch 4/10: [======================        ] 55/75 batches, loss: 0.0621Epoch 4/10: [======================        ] 56/75 batches, loss: 0.0619Epoch 4/10: [======================        ] 57/75 batches, loss: 0.0619Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.0615Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.0612Epoch 4/10: [========================      ] 60/75 batches, loss: 0.0608Epoch 4/10: [========================      ] 61/75 batches, loss: 0.0604Epoch 4/10: [========================      ] 62/75 batches, loss: 0.0600Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.0596Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.0596Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.0592Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.0589Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.0582Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.0578Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.0575Epoch 4/10: [============================  ] 70/75 batches, loss: 0.0571Epoch 4/10: [============================  ] 71/75 batches, loss: 0.0569Epoch 4/10: [============================  ] 72/75 batches, loss: 0.0568Epoch 4/10: [============================= ] 73/75 batches, loss: 0.0568Epoch 4/10: [============================= ] 74/75 batches, loss: 0.0565Epoch 4/10: [==============================] 75/75 batches, loss: 0.0562
[2025-04-29 20:37:29,206][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0562
[2025-04-29 20:37:29,483][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1284, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.0297Epoch 5/10: [                              ] 2/75 batches, loss: 0.0286Epoch 5/10: [=                             ] 3/75 batches, loss: 0.0258Epoch 5/10: [=                             ] 4/75 batches, loss: 0.0809Epoch 5/10: [==                            ] 5/75 batches, loss: 0.0716Epoch 5/10: [==                            ] 6/75 batches, loss: 0.0650Epoch 5/10: [==                            ] 7/75 batches, loss: 0.0629Epoch 5/10: [===                           ] 8/75 batches, loss: 0.0581Epoch 5/10: [===                           ] 9/75 batches, loss: 0.0572Epoch 5/10: [====                          ] 10/75 batches, loss: 0.0581Epoch 5/10: [====                          ] 11/75 batches, loss: 0.0557Epoch 5/10: [====                          ] 12/75 batches, loss: 0.0547Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.0544Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.0535Epoch 5/10: [======                        ] 15/75 batches, loss: 0.0529Epoch 5/10: [======                        ] 16/75 batches, loss: 0.0533Epoch 5/10: [======                        ] 17/75 batches, loss: 0.0524Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.0511Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.0493Epoch 5/10: [========                      ] 20/75 batches, loss: 0.0487Epoch 5/10: [========                      ] 21/75 batches, loss: 0.0473Epoch 5/10: [========                      ] 22/75 batches, loss: 0.0460Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.0452Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.0449Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.0444Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.0543Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.0535Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.0576Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.0568Epoch 5/10: [============                  ] 30/75 batches, loss: 0.0560Epoch 5/10: [============                  ] 31/75 batches, loss: 0.0550Epoch 5/10: [============                  ] 32/75 batches, loss: 0.0537Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.0526Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.0515Epoch 5/10: [==============                ] 35/75 batches, loss: 0.0506Epoch 5/10: [==============                ] 36/75 batches, loss: 0.0498Epoch 5/10: [==============                ] 37/75 batches, loss: 0.0493Epoch 5/10: [===============               ] 38/75 batches, loss: 0.0491Epoch 5/10: [===============               ] 39/75 batches, loss: 0.0495Epoch 5/10: [================              ] 40/75 batches, loss: 0.0491Epoch 5/10: [================              ] 41/75 batches, loss: 0.0492Epoch 5/10: [================              ] 42/75 batches, loss: 0.0490Epoch 5/10: [=================             ] 43/75 batches, loss: 0.0486Epoch 5/10: [=================             ] 44/75 batches, loss: 0.0496Epoch 5/10: [==================            ] 45/75 batches, loss: 0.0493Epoch 5/10: [==================            ] 46/75 batches, loss: 0.0490Epoch 5/10: [==================            ] 47/75 batches, loss: 0.0494Epoch 5/10: [===================           ] 48/75 batches, loss: 0.0504Epoch 5/10: [===================           ] 49/75 batches, loss: 0.0555Epoch 5/10: [====================          ] 50/75 batches, loss: 0.0548Epoch 5/10: [====================          ] 51/75 batches, loss: 0.0544Epoch 5/10: [====================          ] 52/75 batches, loss: 0.0572Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.0568Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.0566Epoch 5/10: [======================        ] 55/75 batches, loss: 0.0561Epoch 5/10: [======================        ] 56/75 batches, loss: 0.0565Epoch 5/10: [======================        ] 57/75 batches, loss: 0.0560Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.0558Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.0558Epoch 5/10: [========================      ] 60/75 batches, loss: 0.0554Epoch 5/10: [========================      ] 61/75 batches, loss: 0.0550Epoch 5/10: [========================      ] 62/75 batches, loss: 0.0550Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.0560Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.0558Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.0552Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.0548Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.0545Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.0544Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.0542Epoch 5/10: [============================  ] 70/75 batches, loss: 0.0536Epoch 5/10: [============================  ] 71/75 batches, loss: 0.0533Epoch 5/10: [============================  ] 72/75 batches, loss: 0.0529Epoch 5/10: [============================= ] 73/75 batches, loss: 0.0526Epoch 5/10: [============================= ] 74/75 batches, loss: 0.0522Epoch 5/10: [==============================] 75/75 batches, loss: 0.0520
[2025-04-29 20:37:36,800][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0520
[2025-04-29 20:37:37,078][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1490, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-29 20:37:37,078][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.0254Epoch 6/10: [                              ] 2/75 batches, loss: 0.0272Epoch 6/10: [=                             ] 3/75 batches, loss: 0.0276Epoch 6/10: [=                             ] 4/75 batches, loss: 0.0294Epoch 6/10: [==                            ] 5/75 batches, loss: 0.0305Epoch 6/10: [==                            ] 6/75 batches, loss: 0.0283Epoch 6/10: [==                            ] 7/75 batches, loss: 0.0277Epoch 6/10: [===                           ] 8/75 batches, loss: 0.0267Epoch 6/10: [===                           ] 9/75 batches, loss: 0.0246Epoch 6/10: [====                          ] 10/75 batches, loss: 0.0239Epoch 6/10: [====                          ] 11/75 batches, loss: 0.0247Epoch 6/10: [====                          ] 12/75 batches, loss: 0.0257Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.0247Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.0240Epoch 6/10: [======                        ] 15/75 batches, loss: 0.0250Epoch 6/10: [======                        ] 16/75 batches, loss: 0.0272Epoch 6/10: [======                        ] 17/75 batches, loss: 0.0277Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.0347Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.0349Epoch 6/10: [========                      ] 20/75 batches, loss: 0.0343Epoch 6/10: [========                      ] 21/75 batches, loss: 0.0340Epoch 6/10: [========                      ] 22/75 batches, loss: 0.0336Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.0325Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.0325Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.0317Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.0310Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.0303Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.0299Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.0293Epoch 6/10: [============                  ] 30/75 batches, loss: 0.0289Epoch 6/10: [============                  ] 31/75 batches, loss: 0.0285Epoch 6/10: [============                  ] 32/75 batches, loss: 0.0279Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.0274Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.0269Epoch 6/10: [==============                ] 35/75 batches, loss: 0.0265Epoch 6/10: [==============                ] 36/75 batches, loss: 0.0262Epoch 6/10: [==============                ] 37/75 batches, loss: 0.0260Epoch 6/10: [===============               ] 38/75 batches, loss: 0.0256Epoch 6/10: [===============               ] 39/75 batches, loss: 0.0252Epoch 6/10: [================              ] 40/75 batches, loss: 0.0249Epoch 6/10: [================              ] 41/75 batches, loss: 0.0246Epoch 6/10: [================              ] 42/75 batches, loss: 0.0245Epoch 6/10: [=================             ] 43/75 batches, loss: 0.0240Epoch 6/10: [=================             ] 44/75 batches, loss: 0.0238Epoch 6/10: [==================            ] 45/75 batches, loss: 0.0236Epoch 6/10: [==================            ] 46/75 batches, loss: 0.0233Epoch 6/10: [==================            ] 47/75 batches, loss: 0.0230Epoch 6/10: [===================           ] 48/75 batches, loss: 0.0228Epoch 6/10: [===================           ] 49/75 batches, loss: 0.0226Epoch 6/10: [====================          ] 50/75 batches, loss: 0.0245Epoch 6/10: [====================          ] 51/75 batches, loss: 0.0242Epoch 6/10: [====================          ] 52/75 batches, loss: 0.0239Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.0236Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.0233Epoch 6/10: [======================        ] 55/75 batches, loss: 0.0232Epoch 6/10: [======================        ] 56/75 batches, loss: 0.0230Epoch 6/10: [======================        ] 57/75 batches, loss: 0.0229Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.0227Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.0258Epoch 6/10: [========================      ] 60/75 batches, loss: 0.0257Epoch 6/10: [========================      ] 61/75 batches, loss: 0.0254Epoch 6/10: [========================      ] 62/75 batches, loss: 0.0252Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.0250Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.0248Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.0267Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.0264Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.0271Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.0269Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.0268Epoch 6/10: [============================  ] 70/75 batches, loss: 0.0267Epoch 6/10: [============================  ] 71/75 batches, loss: 0.0265Epoch 6/10: [============================  ] 72/75 batches, loss: 0.0264Epoch 6/10: [============================= ] 73/75 batches, loss: 0.0265Epoch 6/10: [============================= ] 74/75 batches, loss: 0.0264Epoch 6/10: [==============================] 75/75 batches, loss: 0.0264
[2025-04-29 20:37:43,965][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0264
[2025-04-29 20:37:44,243][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1359, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-29 20:37:44,244][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.0088Epoch 7/10: [                              ] 2/75 batches, loss: 0.0084Epoch 7/10: [=                             ] 3/75 batches, loss: 0.0075Epoch 7/10: [=                             ] 4/75 batches, loss: 0.0072Epoch 7/10: [==                            ] 5/75 batches, loss: 0.0089Epoch 7/10: [==                            ] 6/75 batches, loss: 0.0085Epoch 7/10: [==                            ] 7/75 batches, loss: 0.0081Epoch 7/10: [===                           ] 8/75 batches, loss: 0.0087Epoch 7/10: [===                           ] 9/75 batches, loss: 0.0098Epoch 7/10: [====                          ] 10/75 batches, loss: 0.0097Epoch 7/10: [====                          ] 11/75 batches, loss: 0.0098Epoch 7/10: [====                          ] 12/75 batches, loss: 0.0102Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.0098Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.0101Epoch 7/10: [======                        ] 15/75 batches, loss: 0.0104Epoch 7/10: [======                        ] 16/75 batches, loss: 0.0108Epoch 7/10: [======                        ] 17/75 batches, loss: 0.0115Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.0115Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.0120Epoch 7/10: [========                      ] 20/75 batches, loss: 0.0121Epoch 7/10: [========                      ] 21/75 batches, loss: 0.0121Epoch 7/10: [========                      ] 22/75 batches, loss: 0.0120Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.0117Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.0117Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.0117Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.0118Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.0120Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.0119Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.0120Epoch 7/10: [============                  ] 30/75 batches, loss: 0.0248Epoch 7/10: [============                  ] 31/75 batches, loss: 0.0249Epoch 7/10: [============                  ] 32/75 batches, loss: 0.0245Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.0244Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.0243Epoch 7/10: [==============                ] 35/75 batches, loss: 0.0242Epoch 7/10: [==============                ] 36/75 batches, loss: 0.0241Epoch 7/10: [==============                ] 37/75 batches, loss: 0.0245Epoch 7/10: [===============               ] 38/75 batches, loss: 0.0244Epoch 7/10: [===============               ] 39/75 batches, loss: 0.0268Epoch 7/10: [================              ] 40/75 batches, loss: 0.0265Epoch 7/10: [================              ] 41/75 batches, loss: 0.0260Epoch 7/10: [================              ] 42/75 batches, loss: 0.0257Epoch 7/10: [=================             ] 43/75 batches, loss: 0.0302Epoch 7/10: [=================             ] 44/75 batches, loss: 0.0297Epoch 7/10: [==================            ] 45/75 batches, loss: 0.0293Epoch 7/10: [==================            ] 46/75 batches, loss: 0.0290Epoch 7/10: [==================            ] 47/75 batches, loss: 0.0286Epoch 7/10: [===================           ] 48/75 batches, loss: 0.0282Epoch 7/10: [===================           ] 49/75 batches, loss: 0.0287Epoch 7/10: [====================          ] 50/75 batches, loss: 0.0282Epoch 7/10: [====================          ] 51/75 batches, loss: 0.0278Epoch 7/10: [====================          ] 52/75 batches, loss: 0.0275Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.0273Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.0271Epoch 7/10: [======================        ] 55/75 batches, loss: 0.0268Epoch 7/10: [======================        ] 56/75 batches, loss: 0.0264Epoch 7/10: [======================        ] 57/75 batches, loss: 0.0261Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.0257Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.0254Epoch 7/10: [========================      ] 60/75 batches, loss: 0.0251Epoch 7/10: [========================      ] 61/75 batches, loss: 0.0249Epoch 7/10: [========================      ] 62/75 batches, loss: 0.0247Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.0245Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.0242Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.0239Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.0237Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.0236Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.0234Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.0232Epoch 7/10: [============================  ] 70/75 batches, loss: 0.0229Epoch 7/10: [============================  ] 71/75 batches, loss: 0.0227Epoch 7/10: [============================  ] 72/75 batches, loss: 0.0225Epoch 7/10: [============================= ] 73/75 batches, loss: 0.0224Epoch 7/10: [============================= ] 74/75 batches, loss: 0.0223Epoch 7/10: [==============================] 75/75 batches, loss: 0.0223
[2025-04-29 20:37:51,166][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0223
[2025-04-29 20:37:51,449][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1358, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-29 20:37:51,450][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 20:37:51,450][src.training.lm_trainer][INFO] - Early stopping at epoch 7
[2025-04-29 20:37:51,450][src.training.lm_trainer][INFO] - Training completed in 52.63 seconds
[2025-04-29 20:37:51,450][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 20:37:54,478][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.99748322147651, 'f1': 0.9974895397489539}
[2025-04-29 20:37:54,480][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-29 20:37:54,480][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9454545454545454, 'f1': 0.9482758620689655}
[2025-04-29 20:37:56,117][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/en/en/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁███
wandb:          best_val_f1 ▁███
wandb:        best_val_loss █▂▁▁
wandb:                epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁
wandb:           train_loss █▅▂▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁██████
wandb:               val_f1 ▁██████
wandb:             val_loss █▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.94444
wandb:          best_val_f1 0.94737
wandb:        best_val_loss 0.12843
wandb:                epoch 7
wandb:  final_test_accuracy 0.94545
wandb:        final_test_f1 0.94828
wandb: final_train_accuracy 0.99748
wandb:       final_train_f1 0.99749
wandb:   final_val_accuracy 0.94444
wandb:         final_val_f1 0.94737
wandb:        learning_rate 2e-05
wandb:           train_loss 0.02235
wandb:           train_time 52.63397
wandb:         val_accuracy 0.94444
wandb:               val_f1 0.94737
wandb:             val_loss 0.1358
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_203646-k3vchjb3
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_203646-k3vchjb3/logs
Experiment finetune_question_type_en completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/en/results.json
Running experiment: finetune_complexity_en
Command: python -m src.experiments.run_experiment             "hydra.job.chdir=False"             "hydra.run.dir=."             "experiment=finetune"             "experiment.tasks=complexity"             "model=glot500_finetune"             "model.lm_name=cis-lmu/glot500-base"             "model.dropout=0.1"             "model.freeze_model=false"             "model.finetune=true"             "data.languages=[en]"             "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"             "training.task_type=regression"             "training.num_epochs=10"             "training.batch_size=16"             "training.lr=2e-5"             "+training.gradient_accumulation_steps=2"             +training.debug_mode=true             "experiment_name=finetune_complexity_en"             "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/en"             "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 20:38:14,950][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/en
experiment_name: finetune_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
  debug_mode: true
experiment:
  type: lm_finetune
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  finetune: true
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1
    lr: 2.0e-05
    batch_size: 8

[2025-04-29 20:38:14,950][__main__][INFO] - Normalized task: complexity
[2025-04-29 20:38:14,950][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 20:38:14,950][__main__][INFO] - Determined Task Type: regression
[2025-04-29 20:38:14,955][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-29 20:38:14,955][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 20:38:17,086][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 20:38:19,345][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 20:38:19,346][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:38:19,417][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:38:19,453][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:38:19,574][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 20:38:19,582][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:38:19,583][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 20:38:19,585][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:38:19,620][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:38:19,657][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:38:19,672][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 20:38:19,673][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:38:19,673][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 20:38:19,677][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:38:19,708][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:38:19,751][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:38:19,767][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 20:38:19,768][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:38:19,768][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 20:38:19,769][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 20:38:19,770][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 20:38:19,770][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 20:38:19,770][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 20:38:19,770][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 20:38:19,771][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 20:38:19,771][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-29 20:38:19,771][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 20:38:19,771][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-29 20:38:19,771][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 20:38:19,771][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 20:38:19,771][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 20:38:19,771][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 20:38:19,771][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 20:38:19,772][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-29 20:38:19,772][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 20:38:19,772][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-29 20:38:19,772][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 20:38:19,772][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 20:38:19,772][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 20:38:19,772][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 20:38:19,772][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 20:38:19,772][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-29 20:38:19,773][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 20:38:19,773][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-29 20:38:19,773][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 20:38:19,773][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 20:38:19,773][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 20:38:19,773][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 20:38:24,510][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 20:38:24,510][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,510][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,510][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,510][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,510][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,510][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,511][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,511][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,511][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,511][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,511][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,511][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,511][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,511][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,511][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,511][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,511][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,511][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,511][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,511][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,512][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,512][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,512][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,512][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,512][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,512][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,512][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,512][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,512][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,512][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,512][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,512][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,512][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,512][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,512][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,513][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,513][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,513][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,513][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,513][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,513][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,513][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,513][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,513][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,513][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,513][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,513][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,513][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,513][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,513][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,514][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,514][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,514][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,514][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,514][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,514][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,514][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,514][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,514][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,514][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,514][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,514][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,514][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,514][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,514][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,514][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,515][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,515][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,515][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,515][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,515][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,515][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,515][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,515][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,515][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,515][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,515][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,515][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,515][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,515][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,515][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:38:24,524][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 20:38:24,524][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 20:38:24,525][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 20:38:24,525][__main__][INFO] - Successfully created model for en
[2025-04-29 20:38:24,525][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.1088Epoch 1/10: [                              ] 2/75 batches, loss: 0.1242Epoch 1/10: [=                             ] 3/75 batches, loss: 0.1364Epoch 1/10: [=                             ] 4/75 batches, loss: 0.1426Epoch 1/10: [==                            ] 5/75 batches, loss: 0.1302Epoch 1/10: [==                            ] 6/75 batches, loss: 0.1235Epoch 1/10: [==                            ] 7/75 batches, loss: 0.1224Epoch 1/10: [===                           ] 8/75 batches, loss: 0.1136Epoch 1/10: [===                           ] 9/75 batches, loss: 0.1114Epoch 1/10: [====                          ] 10/75 batches, loss: 0.1081Epoch 1/10: [====                          ] 11/75 batches, loss: 0.1051Epoch 1/10: [====                          ] 12/75 batches, loss: 0.1047Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.1038Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.1025Epoch 1/10: [======                        ] 15/75 batches, loss: 0.1034Epoch 1/10: [======                        ] 16/75 batches, loss: 0.1045Epoch 1/10: [======                        ] 17/75 batches, loss: 0.1030Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.1043Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.1019Epoch 1/10: [========                      ] 20/75 batches, loss: 0.1032Epoch 1/10: [========                      ] 21/75 batches, loss: 0.1025Epoch 1/10: [========                      ] 22/75 batches, loss: 0.1013Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.1005Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.0998Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.0984Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.0979Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.0960Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.0958Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.0940Epoch 1/10: [============                  ] 30/75 batches, loss: 0.0933Epoch 1/10: [============                  ] 31/75 batches, loss: 0.0937Epoch 1/10: [============                  ] 32/75 batches, loss: 0.0936Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.0934Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.0930Epoch 1/10: [==============                ] 35/75 batches, loss: 0.0917Epoch 1/10: [==============                ] 36/75 batches, loss: 0.0901Epoch 1/10: [==============                ] 37/75 batches, loss: 0.0905Epoch 1/10: [===============               ] 38/75 batches, loss: 0.0897Epoch 1/10: [===============               ] 39/75 batches, loss: 0.0896Epoch 1/10: [================              ] 40/75 batches, loss: 0.0898Epoch 1/10: [================              ] 41/75 batches, loss: 0.0892Epoch 1/10: [================              ] 42/75 batches, loss: 0.0884Epoch 1/10: [=================             ] 43/75 batches, loss: 0.0881Epoch 1/10: [=================             ] 44/75 batches, loss: 0.0876Epoch 1/10: [==================            ] 45/75 batches, loss: 0.0865Epoch 1/10: [==================            ] 46/75 batches, loss: 0.0855Epoch 1/10: [==================            ] 47/75 batches, loss: 0.0847Epoch 1/10: [===================           ] 48/75 batches, loss: 0.0840Epoch 1/10: [===================           ] 49/75 batches, loss: 0.0834Epoch 1/10: [====================          ] 50/75 batches, loss: 0.0826Epoch 1/10: [====================          ] 51/75 batches, loss: 0.0821Epoch 1/10: [====================          ] 52/75 batches, loss: 0.0831Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.0831Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.0826Epoch 1/10: [======================        ] 55/75 batches, loss: 0.0820Epoch 1/10: [======================        ] 56/75 batches, loss: 0.0816Epoch 1/10: [======================        ] 57/75 batches, loss: 0.0811Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.0806Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.0800Epoch 1/10: [========================      ] 60/75 batches, loss: 0.0800Epoch 1/10: [========================      ] 61/75 batches, loss: 0.0797Epoch 1/10: [========================      ] 62/75 batches, loss: 0.0794Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.0788Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.0779Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.0773Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.0765Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.0762Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.0755Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.0757Epoch 1/10: [============================  ] 70/75 batches, loss: 0.0754Epoch 1/10: [============================  ] 71/75 batches, loss: 0.0754Epoch 1/10: [============================  ] 72/75 batches, loss: 0.0750Epoch 1/10: [============================= ] 73/75 batches, loss: 0.0747Epoch 1/10: [============================= ] 74/75 batches, loss: 0.0743Epoch 1/10: [==============================] 75/75 batches, loss: 0.0747
[2025-04-29 20:38:34,332][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0747
[2025-04-29 20:38:34,582][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0480, Metrics: {'mse': 0.051581818610429764, 'rmse': 0.22711631075382888, 'r2': -0.23250484466552734}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.0522Epoch 2/10: [                              ] 2/75 batches, loss: 0.0535Epoch 2/10: [=                             ] 3/75 batches, loss: 0.0476Epoch 2/10: [=                             ] 4/75 batches, loss: 0.0511Epoch 2/10: [==                            ] 5/75 batches, loss: 0.0623Epoch 2/10: [==                            ] 6/75 batches, loss: 0.0625Epoch 2/10: [==                            ] 7/75 batches, loss: 0.0635Epoch 2/10: [===                           ] 8/75 batches, loss: 0.0672Epoch 2/10: [===                           ] 9/75 batches, loss: 0.0690Epoch 2/10: [====                          ] 10/75 batches, loss: 0.0686Epoch 2/10: [====                          ] 11/75 batches, loss: 0.0707Epoch 2/10: [====                          ] 12/75 batches, loss: 0.0718Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.0692Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.0712Epoch 2/10: [======                        ] 15/75 batches, loss: 0.0712Epoch 2/10: [======                        ] 16/75 batches, loss: 0.0696Epoch 2/10: [======                        ] 17/75 batches, loss: 0.0713Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.0711Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.0700Epoch 2/10: [========                      ] 20/75 batches, loss: 0.0674Epoch 2/10: [========                      ] 21/75 batches, loss: 0.0681Epoch 2/10: [========                      ] 22/75 batches, loss: 0.0669Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.0670Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.0654Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.0661Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.0649Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.0638Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.0630Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.0622Epoch 2/10: [============                  ] 30/75 batches, loss: 0.0614Epoch 2/10: [============                  ] 31/75 batches, loss: 0.0609Epoch 2/10: [============                  ] 32/75 batches, loss: 0.0609Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.0610Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.0609Epoch 2/10: [==============                ] 35/75 batches, loss: 0.0608Epoch 2/10: [==============                ] 36/75 batches, loss: 0.0602Epoch 2/10: [==============                ] 37/75 batches, loss: 0.0595Epoch 2/10: [===============               ] 38/75 batches, loss: 0.0599Epoch 2/10: [===============               ] 39/75 batches, loss: 0.0598Epoch 2/10: [================              ] 40/75 batches, loss: 0.0593Epoch 2/10: [================              ] 41/75 batches, loss: 0.0592Epoch 2/10: [================              ] 42/75 batches, loss: 0.0592Epoch 2/10: [=================             ] 43/75 batches, loss: 0.0586Epoch 2/10: [=================             ] 44/75 batches, loss: 0.0584Epoch 2/10: [==================            ] 45/75 batches, loss: 0.0588Epoch 2/10: [==================            ] 46/75 batches, loss: 0.0592Epoch 2/10: [==================            ] 47/75 batches, loss: 0.0595Epoch 2/10: [===================           ] 48/75 batches, loss: 0.0597Epoch 2/10: [===================           ] 49/75 batches, loss: 0.0604Epoch 2/10: [====================          ] 50/75 batches, loss: 0.0602Epoch 2/10: [====================          ] 51/75 batches, loss: 0.0599Epoch 2/10: [====================          ] 52/75 batches, loss: 0.0594Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.0591Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.0590Epoch 2/10: [======================        ] 55/75 batches, loss: 0.0585Epoch 2/10: [======================        ] 56/75 batches, loss: 0.0579Epoch 2/10: [======================        ] 57/75 batches, loss: 0.0576Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.0574Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.0579Epoch 2/10: [========================      ] 60/75 batches, loss: 0.0577Epoch 2/10: [========================      ] 61/75 batches, loss: 0.0576Epoch 2/10: [========================      ] 62/75 batches, loss: 0.0571Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.0569Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.0564Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.0560Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.0554Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.0549Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.0548Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.0547Epoch 2/10: [============================  ] 70/75 batches, loss: 0.0545Epoch 2/10: [============================  ] 71/75 batches, loss: 0.0544Epoch 2/10: [============================  ] 72/75 batches, loss: 0.0540Epoch 2/10: [============================= ] 73/75 batches, loss: 0.0535Epoch 2/10: [============================= ] 74/75 batches, loss: 0.0533Epoch 2/10: [==============================] 75/75 batches, loss: 0.0533
[2025-04-29 20:38:41,837][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0533
[2025-04-29 20:38:42,106][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0370, Metrics: {'mse': 0.039023444056510925, 'rmse': 0.19754352446109422, 'r2': 0.06756705045700073}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.0554Epoch 3/10: [                              ] 2/75 batches, loss: 0.0441Epoch 3/10: [=                             ] 3/75 batches, loss: 0.0522Epoch 3/10: [=                             ] 4/75 batches, loss: 0.0492Epoch 3/10: [==                            ] 5/75 batches, loss: 0.0479Epoch 3/10: [==                            ] 6/75 batches, loss: 0.0499Epoch 3/10: [==                            ] 7/75 batches, loss: 0.0559Epoch 3/10: [===                           ] 8/75 batches, loss: 0.0597Epoch 3/10: [===                           ] 9/75 batches, loss: 0.0601Epoch 3/10: [====                          ] 10/75 batches, loss: 0.0603Epoch 3/10: [====                          ] 11/75 batches, loss: 0.0588Epoch 3/10: [====                          ] 12/75 batches, loss: 0.0581Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.0582Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.0564Epoch 3/10: [======                        ] 15/75 batches, loss: 0.0553Epoch 3/10: [======                        ] 16/75 batches, loss: 0.0570Epoch 3/10: [======                        ] 17/75 batches, loss: 0.0563Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.0559Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.0544Epoch 3/10: [========                      ] 20/75 batches, loss: 0.0548Epoch 3/10: [========                      ] 21/75 batches, loss: 0.0542Epoch 3/10: [========                      ] 22/75 batches, loss: 0.0524Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.0513Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.0516Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.0518Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.0527Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.0529Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.0524Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.0523Epoch 3/10: [============                  ] 30/75 batches, loss: 0.0520Epoch 3/10: [============                  ] 31/75 batches, loss: 0.0511Epoch 3/10: [============                  ] 32/75 batches, loss: 0.0500Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.0494Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.0491Epoch 3/10: [==============                ] 35/75 batches, loss: 0.0486Epoch 3/10: [==============                ] 36/75 batches, loss: 0.0483Epoch 3/10: [==============                ] 37/75 batches, loss: 0.0475Epoch 3/10: [===============               ] 38/75 batches, loss: 0.0468Epoch 3/10: [===============               ] 39/75 batches, loss: 0.0464Epoch 3/10: [================              ] 40/75 batches, loss: 0.0460Epoch 3/10: [================              ] 41/75 batches, loss: 0.0453Epoch 3/10: [================              ] 42/75 batches, loss: 0.0448Epoch 3/10: [=================             ] 43/75 batches, loss: 0.0444Epoch 3/10: [=================             ] 44/75 batches, loss: 0.0443Epoch 3/10: [==================            ] 45/75 batches, loss: 0.0441Epoch 3/10: [==================            ] 46/75 batches, loss: 0.0442Epoch 3/10: [==================            ] 47/75 batches, loss: 0.0440Epoch 3/10: [===================           ] 48/75 batches, loss: 0.0440Epoch 3/10: [===================           ] 49/75 batches, loss: 0.0439Epoch 3/10: [====================          ] 50/75 batches, loss: 0.0436Epoch 3/10: [====================          ] 51/75 batches, loss: 0.0434Epoch 3/10: [====================          ] 52/75 batches, loss: 0.0432Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.0426Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.0424Epoch 3/10: [======================        ] 55/75 batches, loss: 0.0424Epoch 3/10: [======================        ] 56/75 batches, loss: 0.0418Epoch 3/10: [======================        ] 57/75 batches, loss: 0.0414Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.0412Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.0408Epoch 3/10: [========================      ] 60/75 batches, loss: 0.0407Epoch 3/10: [========================      ] 61/75 batches, loss: 0.0410Epoch 3/10: [========================      ] 62/75 batches, loss: 0.0410Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.0407Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.0407Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.0409Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.0411Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.0409Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.0405Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.0407Epoch 3/10: [============================  ] 70/75 batches, loss: 0.0405Epoch 3/10: [============================  ] 71/75 batches, loss: 0.0405Epoch 3/10: [============================  ] 72/75 batches, loss: 0.0407Epoch 3/10: [============================= ] 73/75 batches, loss: 0.0404Epoch 3/10: [============================= ] 74/75 batches, loss: 0.0404Epoch 3/10: [==============================] 75/75 batches, loss: 0.0401
[2025-04-29 20:38:49,416][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0401
[2025-04-29 20:38:49,787][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0385, Metrics: {'mse': 0.03829311951994896, 'rmse': 0.19568627831288773, 'r2': 0.0850176215171814}
[2025-04-29 20:38:49,788][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.0221Epoch 4/10: [                              ] 2/75 batches, loss: 0.0288Epoch 4/10: [=                             ] 3/75 batches, loss: 0.0294Epoch 4/10: [=                             ] 4/75 batches, loss: 0.0289Epoch 4/10: [==                            ] 5/75 batches, loss: 0.0282Epoch 4/10: [==                            ] 6/75 batches, loss: 0.0303Epoch 4/10: [==                            ] 7/75 batches, loss: 0.0312Epoch 4/10: [===                           ] 8/75 batches, loss: 0.0300Epoch 4/10: [===                           ] 9/75 batches, loss: 0.0284Epoch 4/10: [====                          ] 10/75 batches, loss: 0.0279Epoch 4/10: [====                          ] 11/75 batches, loss: 0.0269Epoch 4/10: [====                          ] 12/75 batches, loss: 0.0277Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.0275Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.0280Epoch 4/10: [======                        ] 15/75 batches, loss: 0.0286Epoch 4/10: [======                        ] 16/75 batches, loss: 0.0284Epoch 4/10: [======                        ] 17/75 batches, loss: 0.0282Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.0284Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.0278Epoch 4/10: [========                      ] 20/75 batches, loss: 0.0274Epoch 4/10: [========                      ] 21/75 batches, loss: 0.0276Epoch 4/10: [========                      ] 22/75 batches, loss: 0.0271Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.0275Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.0282Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.0286Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.0282Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.0288Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.0289Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.0301Epoch 4/10: [============                  ] 30/75 batches, loss: 0.0302Epoch 4/10: [============                  ] 31/75 batches, loss: 0.0295Epoch 4/10: [============                  ] 32/75 batches, loss: 0.0295Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.0298Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.0305Epoch 4/10: [==============                ] 35/75 batches, loss: 0.0302Epoch 4/10: [==============                ] 36/75 batches, loss: 0.0305Epoch 4/10: [==============                ] 37/75 batches, loss: 0.0307Epoch 4/10: [===============               ] 38/75 batches, loss: 0.0306Epoch 4/10: [===============               ] 39/75 batches, loss: 0.0308Epoch 4/10: [================              ] 40/75 batches, loss: 0.0313Epoch 4/10: [================              ] 41/75 batches, loss: 0.0315Epoch 4/10: [================              ] 42/75 batches, loss: 0.0317Epoch 4/10: [=================             ] 43/75 batches, loss: 0.0315Epoch 4/10: [=================             ] 44/75 batches, loss: 0.0319Epoch 4/10: [==================            ] 45/75 batches, loss: 0.0316Epoch 4/10: [==================            ] 46/75 batches, loss: 0.0312Epoch 4/10: [==================            ] 47/75 batches, loss: 0.0310Epoch 4/10: [===================           ] 48/75 batches, loss: 0.0309Epoch 4/10: [===================           ] 49/75 batches, loss: 0.0306Epoch 4/10: [====================          ] 50/75 batches, loss: 0.0307Epoch 4/10: [====================          ] 51/75 batches, loss: 0.0310Epoch 4/10: [====================          ] 52/75 batches, loss: 0.0312Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.0309Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.0306Epoch 4/10: [======================        ] 55/75 batches, loss: 0.0305Epoch 4/10: [======================        ] 56/75 batches, loss: 0.0305Epoch 4/10: [======================        ] 57/75 batches, loss: 0.0306Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.0304Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.0304Epoch 4/10: [========================      ] 60/75 batches, loss: 0.0302Epoch 4/10: [========================      ] 61/75 batches, loss: 0.0305Epoch 4/10: [========================      ] 62/75 batches, loss: 0.0307Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.0306Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.0306Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.0305Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.0307Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.0305Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.0306Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.0306Epoch 4/10: [============================  ] 70/75 batches, loss: 0.0304Epoch 4/10: [============================  ] 71/75 batches, loss: 0.0302Epoch 4/10: [============================  ] 72/75 batches, loss: 0.0301Epoch 4/10: [============================= ] 73/75 batches, loss: 0.0301Epoch 4/10: [============================= ] 74/75 batches, loss: 0.0297Epoch 4/10: [==============================] 75/75 batches, loss: 0.0294
[2025-04-29 20:38:56,775][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0294
[2025-04-29 20:38:57,036][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0713, Metrics: {'mse': 0.06994320452213287, 'rmse': 0.26446777596170934, 'r2': -0.6712349653244019}
[2025-04-29 20:38:57,037][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.0126Epoch 5/10: [                              ] 2/75 batches, loss: 0.0149Epoch 5/10: [=                             ] 3/75 batches, loss: 0.0166Epoch 5/10: [=                             ] 4/75 batches, loss: 0.0215Epoch 5/10: [==                            ] 5/75 batches, loss: 0.0244Epoch 5/10: [==                            ] 6/75 batches, loss: 0.0255Epoch 5/10: [==                            ] 7/75 batches, loss: 0.0254Epoch 5/10: [===                           ] 8/75 batches, loss: 0.0251Epoch 5/10: [===                           ] 9/75 batches, loss: 0.0250Epoch 5/10: [====                          ] 10/75 batches, loss: 0.0256Epoch 5/10: [====                          ] 11/75 batches, loss: 0.0273Epoch 5/10: [====                          ] 12/75 batches, loss: 0.0265Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.0266Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.0256Epoch 5/10: [======                        ] 15/75 batches, loss: 0.0249Epoch 5/10: [======                        ] 16/75 batches, loss: 0.0252Epoch 5/10: [======                        ] 17/75 batches, loss: 0.0243Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.0237Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.0243Epoch 5/10: [========                      ] 20/75 batches, loss: 0.0242Epoch 5/10: [========                      ] 21/75 batches, loss: 0.0240Epoch 5/10: [========                      ] 22/75 batches, loss: 0.0242Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.0238Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.0237Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.0242Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.0246Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.0244Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.0247Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.0246Epoch 5/10: [============                  ] 30/75 batches, loss: 0.0247Epoch 5/10: [============                  ] 31/75 batches, loss: 0.0245Epoch 5/10: [============                  ] 32/75 batches, loss: 0.0242Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.0244Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.0244Epoch 5/10: [==============                ] 35/75 batches, loss: 0.0243Epoch 5/10: [==============                ] 36/75 batches, loss: 0.0241Epoch 5/10: [==============                ] 37/75 batches, loss: 0.0239Epoch 5/10: [===============               ] 38/75 batches, loss: 0.0239Epoch 5/10: [===============               ] 39/75 batches, loss: 0.0240Epoch 5/10: [================              ] 40/75 batches, loss: 0.0240Epoch 5/10: [================              ] 41/75 batches, loss: 0.0244Epoch 5/10: [================              ] 42/75 batches, loss: 0.0244Epoch 5/10: [=================             ] 43/75 batches, loss: 0.0243Epoch 5/10: [=================             ] 44/75 batches, loss: 0.0239Epoch 5/10: [==================            ] 45/75 batches, loss: 0.0237Epoch 5/10: [==================            ] 46/75 batches, loss: 0.0236Epoch 5/10: [==================            ] 47/75 batches, loss: 0.0235Epoch 5/10: [===================           ] 48/75 batches, loss: 0.0236Epoch 5/10: [===================           ] 49/75 batches, loss: 0.0238Epoch 5/10: [====================          ] 50/75 batches, loss: 0.0236Epoch 5/10: [====================          ] 51/75 batches, loss: 0.0240Epoch 5/10: [====================          ] 52/75 batches, loss: 0.0242Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.0241Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.0240Epoch 5/10: [======================        ] 55/75 batches, loss: 0.0239Epoch 5/10: [======================        ] 56/75 batches, loss: 0.0238Epoch 5/10: [======================        ] 57/75 batches, loss: 0.0237Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.0236Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.0236Epoch 5/10: [========================      ] 60/75 batches, loss: 0.0235Epoch 5/10: [========================      ] 61/75 batches, loss: 0.0237Epoch 5/10: [========================      ] 62/75 batches, loss: 0.0237Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.0237Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.0237Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.0240Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.0238Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.0237Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.0237Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.0237Epoch 5/10: [============================  ] 70/75 batches, loss: 0.0235Epoch 5/10: [============================  ] 71/75 batches, loss: 0.0233Epoch 5/10: [============================  ] 72/75 batches, loss: 0.0236Epoch 5/10: [============================= ] 73/75 batches, loss: 0.0234Epoch 5/10: [============================= ] 74/75 batches, loss: 0.0233Epoch 5/10: [==============================] 75/75 batches, loss: 0.0232
[2025-04-29 20:39:03,921][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0232
[2025-04-29 20:39:04,170][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0322, Metrics: {'mse': 0.03287101536989212, 'rmse': 0.1813036551476338, 'r2': 0.21457433700561523}
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.0088Epoch 6/10: [                              ] 2/75 batches, loss: 0.0096Epoch 6/10: [=                             ] 3/75 batches, loss: 0.0137Epoch 6/10: [=                             ] 4/75 batches, loss: 0.0158Epoch 6/10: [==                            ] 5/75 batches, loss: 0.0175Epoch 6/10: [==                            ] 6/75 batches, loss: 0.0171Epoch 6/10: [==                            ] 7/75 batches, loss: 0.0168Epoch 6/10: [===                           ] 8/75 batches, loss: 0.0185Epoch 6/10: [===                           ] 9/75 batches, loss: 0.0172Epoch 6/10: [====                          ] 10/75 batches, loss: 0.0177Epoch 6/10: [====                          ] 11/75 batches, loss: 0.0182Epoch 6/10: [====                          ] 12/75 batches, loss: 0.0174Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.0172Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.0168Epoch 6/10: [======                        ] 15/75 batches, loss: 0.0171Epoch 6/10: [======                        ] 16/75 batches, loss: 0.0168Epoch 6/10: [======                        ] 17/75 batches, loss: 0.0177Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.0192Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.0193Epoch 6/10: [========                      ] 20/75 batches, loss: 0.0193Epoch 6/10: [========                      ] 21/75 batches, loss: 0.0195Epoch 6/10: [========                      ] 22/75 batches, loss: 0.0201Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.0220Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.0223Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.0222Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.0222Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.0226Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.0223Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.0230Epoch 6/10: [============                  ] 30/75 batches, loss: 0.0235Epoch 6/10: [============                  ] 31/75 batches, loss: 0.0238Epoch 6/10: [============                  ] 32/75 batches, loss: 0.0242Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.0242Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.0246Epoch 6/10: [==============                ] 35/75 batches, loss: 0.0244Epoch 6/10: [==============                ] 36/75 batches, loss: 0.0244Epoch 6/10: [==============                ] 37/75 batches, loss: 0.0243Epoch 6/10: [===============               ] 38/75 batches, loss: 0.0244Epoch 6/10: [===============               ] 39/75 batches, loss: 0.0246Epoch 6/10: [================              ] 40/75 batches, loss: 0.0246Epoch 6/10: [================              ] 41/75 batches, loss: 0.0243Epoch 6/10: [================              ] 42/75 batches, loss: 0.0246Epoch 6/10: [=================             ] 43/75 batches, loss: 0.0248Epoch 6/10: [=================             ] 44/75 batches, loss: 0.0250Epoch 6/10: [==================            ] 45/75 batches, loss: 0.0251Epoch 6/10: [==================            ] 46/75 batches, loss: 0.0248Epoch 6/10: [==================            ] 47/75 batches, loss: 0.0253Epoch 6/10: [===================           ] 48/75 batches, loss: 0.0252Epoch 6/10: [===================           ] 49/75 batches, loss: 0.0251Epoch 6/10: [====================          ] 50/75 batches, loss: 0.0250Epoch 6/10: [====================          ] 51/75 batches, loss: 0.0248Epoch 6/10: [====================          ] 52/75 batches, loss: 0.0246Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.0243Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.0244Epoch 6/10: [======================        ] 55/75 batches, loss: 0.0242Epoch 6/10: [======================        ] 56/75 batches, loss: 0.0241Epoch 6/10: [======================        ] 57/75 batches, loss: 0.0240Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.0238Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.0235Epoch 6/10: [========================      ] 60/75 batches, loss: 0.0233Epoch 6/10: [========================      ] 61/75 batches, loss: 0.0231Epoch 6/10: [========================      ] 62/75 batches, loss: 0.0228Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.0227Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.0225Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.0225Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.0224Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.0223Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.0223Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.0223Epoch 6/10: [============================  ] 70/75 batches, loss: 0.0223Epoch 6/10: [============================  ] 71/75 batches, loss: 0.0223Epoch 6/10: [============================  ] 72/75 batches, loss: 0.0223Epoch 6/10: [============================= ] 73/75 batches, loss: 0.0226Epoch 6/10: [============================= ] 74/75 batches, loss: 0.0226Epoch 6/10: [==============================] 75/75 batches, loss: 0.0224
[2025-04-29 20:39:11,459][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0224
[2025-04-29 20:39:11,731][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0526, Metrics: {'mse': 0.051275674253702164, 'rmse': 0.2264413262938154, 'r2': -0.22518980503082275}
[2025-04-29 20:39:11,732][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.0090Epoch 7/10: [                              ] 2/75 batches, loss: 0.0087Epoch 7/10: [=                             ] 3/75 batches, loss: 0.0097Epoch 7/10: [=                             ] 4/75 batches, loss: 0.0099Epoch 7/10: [==                            ] 5/75 batches, loss: 0.0114Epoch 7/10: [==                            ] 6/75 batches, loss: 0.0141Epoch 7/10: [==                            ] 7/75 batches, loss: 0.0139Epoch 7/10: [===                           ] 8/75 batches, loss: 0.0144Epoch 7/10: [===                           ] 9/75 batches, loss: 0.0146Epoch 7/10: [====                          ] 10/75 batches, loss: 0.0163Epoch 7/10: [====                          ] 11/75 batches, loss: 0.0159Epoch 7/10: [====                          ] 12/75 batches, loss: 0.0172Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.0174Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.0177Epoch 7/10: [======                        ] 15/75 batches, loss: 0.0173Epoch 7/10: [======                        ] 16/75 batches, loss: 0.0178Epoch 7/10: [======                        ] 17/75 batches, loss: 0.0177Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.0187Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.0184Epoch 7/10: [========                      ] 20/75 batches, loss: 0.0183Epoch 7/10: [========                      ] 21/75 batches, loss: 0.0184Epoch 7/10: [========                      ] 22/75 batches, loss: 0.0195Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.0195Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.0190Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.0188Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.0185Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.0182Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.0182Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.0181Epoch 7/10: [============                  ] 30/75 batches, loss: 0.0182Epoch 7/10: [============                  ] 31/75 batches, loss: 0.0178Epoch 7/10: [============                  ] 32/75 batches, loss: 0.0177Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.0177Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.0183Epoch 7/10: [==============                ] 35/75 batches, loss: 0.0184Epoch 7/10: [==============                ] 36/75 batches, loss: 0.0182Epoch 7/10: [==============                ] 37/75 batches, loss: 0.0184Epoch 7/10: [===============               ] 38/75 batches, loss: 0.0186Epoch 7/10: [===============               ] 39/75 batches, loss: 0.0187Epoch 7/10: [================              ] 40/75 batches, loss: 0.0185Epoch 7/10: [================              ] 41/75 batches, loss: 0.0190Epoch 7/10: [================              ] 42/75 batches, loss: 0.0188Epoch 7/10: [=================             ] 43/75 batches, loss: 0.0185Epoch 7/10: [=================             ] 44/75 batches, loss: 0.0184Epoch 7/10: [==================            ] 45/75 batches, loss: 0.0183Epoch 7/10: [==================            ] 46/75 batches, loss: 0.0183Epoch 7/10: [==================            ] 47/75 batches, loss: 0.0186Epoch 7/10: [===================           ] 48/75 batches, loss: 0.0187Epoch 7/10: [===================           ] 49/75 batches, loss: 0.0187Epoch 7/10: [====================          ] 50/75 batches, loss: 0.0187Epoch 7/10: [====================          ] 51/75 batches, loss: 0.0188Epoch 7/10: [====================          ] 52/75 batches, loss: 0.0191Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.0191Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.0192Epoch 7/10: [======================        ] 55/75 batches, loss: 0.0194Epoch 7/10: [======================        ] 56/75 batches, loss: 0.0193Epoch 7/10: [======================        ] 57/75 batches, loss: 0.0191Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.0190Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.0190Epoch 7/10: [========================      ] 60/75 batches, loss: 0.0188Epoch 7/10: [========================      ] 61/75 batches, loss: 0.0188Epoch 7/10: [========================      ] 62/75 batches, loss: 0.0189Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.0190Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.0188Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.0187Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.0186Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.0185Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.0184Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.0184Epoch 7/10: [============================  ] 70/75 batches, loss: 0.0183Epoch 7/10: [============================  ] 71/75 batches, loss: 0.0182Epoch 7/10: [============================  ] 72/75 batches, loss: 0.0182Epoch 7/10: [============================= ] 73/75 batches, loss: 0.0180Epoch 7/10: [============================= ] 74/75 batches, loss: 0.0179Epoch 7/10: [==============================] 75/75 batches, loss: 0.0179
[2025-04-29 20:39:18,645][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0179
[2025-04-29 20:39:18,939][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0334, Metrics: {'mse': 0.0328684076666832, 'rmse': 0.18129646346987355, 'r2': 0.21463656425476074}
[2025-04-29 20:39:18,940][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/75 batches, loss: 0.0073Epoch 8/10: [                              ] 2/75 batches, loss: 0.0111Epoch 8/10: [=                             ] 3/75 batches, loss: 0.0108Epoch 8/10: [=                             ] 4/75 batches, loss: 0.0114Epoch 8/10: [==                            ] 5/75 batches, loss: 0.0115Epoch 8/10: [==                            ] 6/75 batches, loss: 0.0119Epoch 8/10: [==                            ] 7/75 batches, loss: 0.0118Epoch 8/10: [===                           ] 8/75 batches, loss: 0.0131Epoch 8/10: [===                           ] 9/75 batches, loss: 0.0129Epoch 8/10: [====                          ] 10/75 batches, loss: 0.0124Epoch 8/10: [====                          ] 11/75 batches, loss: 0.0132Epoch 8/10: [====                          ] 12/75 batches, loss: 0.0126Epoch 8/10: [=====                         ] 13/75 batches, loss: 0.0126Epoch 8/10: [=====                         ] 14/75 batches, loss: 0.0128Epoch 8/10: [======                        ] 15/75 batches, loss: 0.0135Epoch 8/10: [======                        ] 16/75 batches, loss: 0.0132Epoch 8/10: [======                        ] 17/75 batches, loss: 0.0134Epoch 8/10: [=======                       ] 18/75 batches, loss: 0.0131Epoch 8/10: [=======                       ] 19/75 batches, loss: 0.0133Epoch 8/10: [========                      ] 20/75 batches, loss: 0.0134Epoch 8/10: [========                      ] 21/75 batches, loss: 0.0135Epoch 8/10: [========                      ] 22/75 batches, loss: 0.0132Epoch 8/10: [=========                     ] 23/75 batches, loss: 0.0131Epoch 8/10: [=========                     ] 24/75 batches, loss: 0.0134Epoch 8/10: [==========                    ] 25/75 batches, loss: 0.0138Epoch 8/10: [==========                    ] 26/75 batches, loss: 0.0140Epoch 8/10: [==========                    ] 27/75 batches, loss: 0.0145Epoch 8/10: [===========                   ] 28/75 batches, loss: 0.0145Epoch 8/10: [===========                   ] 29/75 batches, loss: 0.0147Epoch 8/10: [============                  ] 30/75 batches, loss: 0.0149Epoch 8/10: [============                  ] 31/75 batches, loss: 0.0150Epoch 8/10: [============                  ] 32/75 batches, loss: 0.0150Epoch 8/10: [=============                 ] 33/75 batches, loss: 0.0149Epoch 8/10: [=============                 ] 34/75 batches, loss: 0.0152Epoch 8/10: [==============                ] 35/75 batches, loss: 0.0162Epoch 8/10: [==============                ] 36/75 batches, loss: 0.0163Epoch 8/10: [==============                ] 37/75 batches, loss: 0.0162Epoch 8/10: [===============               ] 38/75 batches, loss: 0.0169Epoch 8/10: [===============               ] 39/75 batches, loss: 0.0166Epoch 8/10: [================              ] 40/75 batches, loss: 0.0167Epoch 8/10: [================              ] 41/75 batches, loss: 0.0168Epoch 8/10: [================              ] 42/75 batches, loss: 0.0168Epoch 8/10: [=================             ] 43/75 batches, loss: 0.0168Epoch 8/10: [=================             ] 44/75 batches, loss: 0.0167Epoch 8/10: [==================            ] 45/75 batches, loss: 0.0164Epoch 8/10: [==================            ] 46/75 batches, loss: 0.0163Epoch 8/10: [==================            ] 47/75 batches, loss: 0.0162Epoch 8/10: [===================           ] 48/75 batches, loss: 0.0162Epoch 8/10: [===================           ] 49/75 batches, loss: 0.0164Epoch 8/10: [====================          ] 50/75 batches, loss: 0.0164Epoch 8/10: [====================          ] 51/75 batches, loss: 0.0162Epoch 8/10: [====================          ] 52/75 batches, loss: 0.0163Epoch 8/10: [=====================         ] 53/75 batches, loss: 0.0163Epoch 8/10: [=====================         ] 54/75 batches, loss: 0.0161Epoch 8/10: [======================        ] 55/75 batches, loss: 0.0160Epoch 8/10: [======================        ] 56/75 batches, loss: 0.0160Epoch 8/10: [======================        ] 57/75 batches, loss: 0.0160Epoch 8/10: [=======================       ] 58/75 batches, loss: 0.0158Epoch 8/10: [=======================       ] 59/75 batches, loss: 0.0158Epoch 8/10: [========================      ] 60/75 batches, loss: 0.0158Epoch 8/10: [========================      ] 61/75 batches, loss: 0.0156Epoch 8/10: [========================      ] 62/75 batches, loss: 0.0155Epoch 8/10: [=========================     ] 63/75 batches, loss: 0.0154Epoch 8/10: [=========================     ] 64/75 batches, loss: 0.0153Epoch 8/10: [==========================    ] 65/75 batches, loss: 0.0154Epoch 8/10: [==========================    ] 66/75 batches, loss: 0.0156Epoch 8/10: [==========================    ] 67/75 batches, loss: 0.0156Epoch 8/10: [===========================   ] 68/75 batches, loss: 0.0158Epoch 8/10: [===========================   ] 69/75 batches, loss: 0.0160Epoch 8/10: [============================  ] 70/75 batches, loss: 0.0159Epoch 8/10: [============================  ] 71/75 batches, loss: 0.0158Epoch 8/10: [============================  ] 72/75 batches, loss: 0.0158Epoch 8/10: [============================= ] 73/75 batches, loss: 0.0159Epoch 8/10: [============================= ] 74/75 batches, loss: 0.0161Epoch 8/10: [==============================] 75/75 batches, loss: 0.0161
[2025-04-29 20:39:25,851][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0161
[2025-04-29 20:39:26,133][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0606, Metrics: {'mse': 0.06072864308953285, 'rmse': 0.24643182239624178, 'r2': -0.4510606527328491}
[2025-04-29 20:39:26,133][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 20:39:26,134][src.training.lm_trainer][INFO] - Early stopping at epoch 8
[2025-04-29 20:39:26,134][src.training.lm_trainer][INFO] - Training completed in 59.59 seconds
[2025-04-29 20:39:26,134][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 20:39:29,092][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.015678614377975464, 'rmse': 0.12521427385875566, 'r2': 0.41560226678848267}
[2025-04-29 20:39:29,092][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03287101536989212, 'rmse': 0.1813036551476338, 'r2': 0.21457433700561523}
[2025-04-29 20:39:29,092][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.027355827391147614, 'rmse': 0.16539597150821908, 'r2': 0.29017090797424316}
[2025-04-29 20:39:30,727][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/en/en/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▁
wandb:     best_val_mse █▃▁
wandb:      best_val_r2 ▁▆█
wandb:    best_val_rmse █▃▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▅▄▃▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▄▂▂█▁▅▁▆
wandb:          val_mse ▅▂▂█▁▄▁▆
wandb:           val_r2 ▄▇▇▁█▅█▃
wandb:         val_rmse ▅▂▂█▁▅▁▆
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03219
wandb:     best_val_mse 0.03287
wandb:      best_val_r2 0.21457
wandb:    best_val_rmse 0.1813
wandb:            epoch 8
wandb:   final_test_mse 0.02736
wandb:    final_test_r2 0.29017
wandb:  final_test_rmse 0.1654
wandb:  final_train_mse 0.01568
wandb:   final_train_r2 0.4156
wandb: final_train_rmse 0.12521
wandb:    final_val_mse 0.03287
wandb:     final_val_r2 0.21457
wandb:   final_val_rmse 0.1813
wandb:    learning_rate 2e-05
wandb:       train_loss 0.01608
wandb:       train_time 59.58661
wandb:         val_loss 0.06061
wandb:          val_mse 0.06073
wandb:           val_r2 -0.45106
wandb:         val_rmse 0.24643
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_203815-l0bd97hz
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_203815-l0bd97hz/logs
Experiment finetune_complexity_en completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/en/results.json
Running experiment: finetune_question_type_fi
Command: python -m src.experiments.run_experiment             "hydra.job.chdir=False"             "hydra.run.dir=."             "experiment=finetune"             "experiment.tasks=question_type"             "model=glot500_finetune"             "model.lm_name=cis-lmu/glot500-base"             "model.dropout=0.1"             "model.freeze_model=false"             "model.finetune=true"             "data.languages=[fi]"             "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"             "training.task_type=classification"             "training.num_epochs=10"             "training.batch_size=16"             "training.lr=2e-5"             "+training.gradient_accumulation_steps=2"                          "experiment_name=finetune_question_type_fi"             "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/fi"             "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 20:39:51,229][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/fi
experiment_name: finetune_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_finetune
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  finetune: true
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1
    lr: 2.0e-05
    batch_size: 8

[2025-04-29 20:39:51,229][__main__][INFO] - Normalized task: question_type
[2025-04-29 20:39:51,229][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 20:39:51,229][__main__][INFO] - Determined Task Type: classification
[2025-04-29 20:39:51,233][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-04-29 20:39:51,234][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 20:39:53,029][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 20:39:55,386][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 20:39:55,387][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:39:55,451][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:39:55,483][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:39:55,581][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 20:39:55,589][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:39:55,590][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 20:39:55,591][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:39:55,617][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:39:55,651][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:39:55,665][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 20:39:55,666][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:39:55,666][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 20:39:55,667][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:39:55,690][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:39:55,723][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:39:55,736][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 20:39:55,738][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:39:55,738][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 20:39:55,739][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 20:39:55,739][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:39:55,739][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:39:55,739][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:39:55,740][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:39:55,740][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-29 20:39:55,740][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-29 20:39:55,740][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 20:39:55,740][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 20:39:55,740][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:39:55,740][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:39:55,740][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:39:55,740][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:39:55,741][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-29 20:39:55,741][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-29 20:39:55,741][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 20:39:55,741][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 20:39:55,741][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:39:55,741][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:39:55,741][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:39:55,741][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:39:55,741][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 20:39:55,741][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 20:39:55,741][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 20:39:55,742][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:39:55,742][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 20:39:55,742][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 20:39:55,742][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 20:39:55,742][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 20:40:00,031][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 20:40:00,031][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,031][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,031][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,031][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,031][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,031][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,032][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,032][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,032][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,032][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,032][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,032][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,032][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,032][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,032][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,032][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,032][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,032][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,032][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,032][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,033][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,033][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,033][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,033][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,033][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,033][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,033][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,033][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,033][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,033][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,033][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,033][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,033][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,033][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,033][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,034][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,034][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,034][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,034][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,034][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,034][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,034][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,034][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,034][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,034][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,034][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,034][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,034][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,034][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,034][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,035][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,035][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,035][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,035][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,035][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,035][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,035][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,035][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,035][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,035][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,035][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,035][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,035][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,035][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,035][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,036][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,036][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,036][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,036][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,036][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,036][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,036][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,036][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,036][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,036][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,036][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,036][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,036][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,036][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,036][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,037][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,037][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,037][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,037][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,037][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,037][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,037][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,037][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,037][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,037][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,037][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,037][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,037][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,037][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,037][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,038][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,038][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,038][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,038][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,038][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,038][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,038][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,038][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,038][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,038][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,038][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,038][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,038][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,038][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,038][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,039][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,039][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,039][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,039][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,039][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,039][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,039][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,039][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,039][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,039][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,039][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,039][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,039][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,039][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,039][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,040][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,040][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,040][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,040][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,040][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,040][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,040][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,040][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,040][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,040][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,040][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,040][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,040][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,040][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,040][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,041][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,041][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,041][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,041][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,041][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,041][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,041][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,041][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,041][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,041][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,041][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,041][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,041][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,041][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,041][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,042][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,042][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,042][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,042][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,042][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,042][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,042][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,042][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,042][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,042][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,042][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,042][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,042][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,042][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,042][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,043][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,043][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,043][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,043][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,043][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,043][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,043][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,043][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,043][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,043][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,043][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,043][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,043][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,043][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,043][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,044][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,044][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,044][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,044][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,044][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,044][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,044][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,044][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,044][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,044][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,044][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,044][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,044][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,044][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:40:00,045][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 20:40:00,045][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 20:40:00,046][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 20:40:00,046][__main__][INFO] - Successfully created model for fi
[2025-04-29 20:40:00,047][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.7035Epoch 1/10: [                              ] 2/75 batches, loss: 0.7095Epoch 1/10: [=                             ] 3/75 batches, loss: 0.7066Epoch 1/10: [=                             ] 4/75 batches, loss: 0.7054Epoch 1/10: [==                            ] 5/75 batches, loss: 0.7015Epoch 1/10: [==                            ] 6/75 batches, loss: 0.7008Epoch 1/10: [==                            ] 7/75 batches, loss: 0.6974Epoch 1/10: [===                           ] 8/75 batches, loss: 0.6982Epoch 1/10: [===                           ] 9/75 batches, loss: 0.6935Epoch 1/10: [====                          ] 10/75 batches, loss: 0.6926Epoch 1/10: [====                          ] 11/75 batches, loss: 0.6905Epoch 1/10: [====                          ] 12/75 batches, loss: 0.6902Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.6918Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.6914Epoch 1/10: [======                        ] 15/75 batches, loss: 0.6910Epoch 1/10: [======                        ] 16/75 batches, loss: 0.6907Epoch 1/10: [======                        ] 17/75 batches, loss: 0.6918Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.6913Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.6917Epoch 1/10: [========                      ] 20/75 batches, loss: 0.6905Epoch 1/10: [========                      ] 21/75 batches, loss: 0.6917Epoch 1/10: [========                      ] 22/75 batches, loss: 0.6916Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.6919Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.6925Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.6917Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.6921Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.6912Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.6917Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.6922Epoch 1/10: [============                  ] 30/75 batches, loss: 0.6925Epoch 1/10: [============                  ] 31/75 batches, loss: 0.6928Epoch 1/10: [============                  ] 32/75 batches, loss: 0.6920Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.6918Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.6923Epoch 1/10: [==============                ] 35/75 batches, loss: 0.6920Epoch 1/10: [==============                ] 36/75 batches, loss: 0.6920Epoch 1/10: [==============                ] 37/75 batches, loss: 0.6914Epoch 1/10: [===============               ] 38/75 batches, loss: 0.6913Epoch 1/10: [===============               ] 39/75 batches, loss: 0.6907Epoch 1/10: [================              ] 40/75 batches, loss: 0.6909Epoch 1/10: [================              ] 41/75 batches, loss: 0.6905Epoch 1/10: [================              ] 42/75 batches, loss: 0.6910Epoch 1/10: [=================             ] 43/75 batches, loss: 0.6909Epoch 1/10: [=================             ] 44/75 batches, loss: 0.6908Epoch 1/10: [==================            ] 45/75 batches, loss: 0.6907Epoch 1/10: [==================            ] 46/75 batches, loss: 0.6900Epoch 1/10: [==================            ] 47/75 batches, loss: 0.6904Epoch 1/10: [===================           ] 48/75 batches, loss: 0.6902Epoch 1/10: [===================           ] 49/75 batches, loss: 0.6905Epoch 1/10: [====================          ] 50/75 batches, loss: 0.6896Epoch 1/10: [====================          ] 51/75 batches, loss: 0.6897Epoch 1/10: [====================          ] 52/75 batches, loss: 0.6901Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.6901Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.6899Epoch 1/10: [======================        ] 55/75 batches, loss: 0.6898Epoch 1/10: [======================        ] 56/75 batches, loss: 0.6903Epoch 1/10: [======================        ] 57/75 batches, loss: 0.6901Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.6902Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.6903Epoch 1/10: [========================      ] 60/75 batches, loss: 0.6903Epoch 1/10: [========================      ] 61/75 batches, loss: 0.6898Epoch 1/10: [========================      ] 62/75 batches, loss: 0.6896Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.6893Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.6886Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.6879Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.6880Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.6883Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.6877Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.6874Epoch 1/10: [============================  ] 70/75 batches, loss: 0.6871Epoch 1/10: [============================  ] 71/75 batches, loss: 0.6870Epoch 1/10: [============================  ] 72/75 batches, loss: 0.6871Epoch 1/10: [============================= ] 73/75 batches, loss: 0.6868Epoch 1/10: [============================= ] 74/75 batches, loss: 0.6870Epoch 1/10: [==============================] 75/75 batches, loss: 0.6868
[2025-04-29 20:40:09,758][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6868
[2025-04-29 20:40:09,972][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6873, Metrics: {'accuracy': 0.47619047619047616, 'f1': 0.6451612903225806}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.6131Epoch 2/10: [                              ] 2/75 batches, loss: 0.6370Epoch 2/10: [=                             ] 3/75 batches, loss: 0.6408Epoch 2/10: [=                             ] 4/75 batches, loss: 0.6515Epoch 2/10: [==                            ] 5/75 batches, loss: 0.6582Epoch 2/10: [==                            ] 6/75 batches, loss: 0.6486Epoch 2/10: [==                            ] 7/75 batches, loss: 0.6514Epoch 2/10: [===                           ] 8/75 batches, loss: 0.6598Epoch 2/10: [===                           ] 9/75 batches, loss: 0.6628Epoch 2/10: [====                          ] 10/75 batches, loss: 0.6640Epoch 2/10: [====                          ] 11/75 batches, loss: 0.6699Epoch 2/10: [====                          ] 12/75 batches, loss: 0.6687Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.6696Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.6686Epoch 2/10: [======                        ] 15/75 batches, loss: 0.6688Epoch 2/10: [======                        ] 16/75 batches, loss: 0.6672Epoch 2/10: [======                        ] 17/75 batches, loss: 0.6684Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.6677Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.6690Epoch 2/10: [========                      ] 20/75 batches, loss: 0.6700Epoch 2/10: [========                      ] 21/75 batches, loss: 0.6709Epoch 2/10: [========                      ] 22/75 batches, loss: 0.6708Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.6716Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.6722Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.6716Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.6705Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.6698Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.6677Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.6643Epoch 2/10: [============                  ] 30/75 batches, loss: 0.6635Epoch 2/10: [============                  ] 31/75 batches, loss: 0.6641Epoch 2/10: [============                  ] 32/75 batches, loss: 0.6638Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.6625Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.6615Epoch 2/10: [==============                ] 35/75 batches, loss: 0.6601Epoch 2/10: [==============                ] 36/75 batches, loss: 0.6592Epoch 2/10: [==============                ] 37/75 batches, loss: 0.6589Epoch 2/10: [===============               ] 38/75 batches, loss: 0.6570Epoch 2/10: [===============               ] 39/75 batches, loss: 0.6542Epoch 2/10: [================              ] 40/75 batches, loss: 0.6534Epoch 2/10: [================              ] 41/75 batches, loss: 0.6523Epoch 2/10: [================              ] 42/75 batches, loss: 0.6481Epoch 2/10: [=================             ] 43/75 batches, loss: 0.6468Epoch 2/10: [=================             ] 44/75 batches, loss: 0.6462Epoch 2/10: [==================            ] 45/75 batches, loss: 0.6440Epoch 2/10: [==================            ] 46/75 batches, loss: 0.6439Epoch 2/10: [==================            ] 47/75 batches, loss: 0.6420Epoch 2/10: [===================           ] 48/75 batches, loss: 0.6410Epoch 2/10: [===================           ] 49/75 batches, loss: 0.6399Epoch 2/10: [====================          ] 50/75 batches, loss: 0.6396Epoch 2/10: [====================          ] 51/75 batches, loss: 0.6387Epoch 2/10: [====================          ] 52/75 batches, loss: 0.6375Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.6353Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.6350Epoch 2/10: [======================        ] 55/75 batches, loss: 0.6343Epoch 2/10: [======================        ] 56/75 batches, loss: 0.6338Epoch 2/10: [======================        ] 57/75 batches, loss: 0.6315Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.6293Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.6278Epoch 2/10: [========================      ] 60/75 batches, loss: 0.6266Epoch 2/10: [========================      ] 61/75 batches, loss: 0.6270Epoch 2/10: [========================      ] 62/75 batches, loss: 0.6254Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.6242Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.6241Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.6231Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.6224Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.6215Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.6200Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.6205Epoch 2/10: [============================  ] 70/75 batches, loss: 0.6204Epoch 2/10: [============================  ] 71/75 batches, loss: 0.6203Epoch 2/10: [============================  ] 72/75 batches, loss: 0.6197Epoch 2/10: [============================= ] 73/75 batches, loss: 0.6180Epoch 2/10: [============================= ] 74/75 batches, loss: 0.6183Epoch 2/10: [==============================] 75/75 batches, loss: 0.6185
[2025-04-29 20:40:17,260][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6185
[2025-04-29 20:40:17,498][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6777, Metrics: {'accuracy': 0.47619047619047616, 'f1': 0.6451612903225806}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.5871Epoch 3/10: [                              ] 2/75 batches, loss: 0.6000Epoch 3/10: [=                             ] 3/75 batches, loss: 0.5963Epoch 3/10: [=                             ] 4/75 batches, loss: 0.5796Epoch 3/10: [==                            ] 5/75 batches, loss: 0.5674Epoch 3/10: [==                            ] 6/75 batches, loss: 0.5754Epoch 3/10: [==                            ] 7/75 batches, loss: 0.5694Epoch 3/10: [===                           ] 8/75 batches, loss: 0.5671Epoch 3/10: [===                           ] 9/75 batches, loss: 0.5689Epoch 3/10: [====                          ] 10/75 batches, loss: 0.5565Epoch 3/10: [====                          ] 11/75 batches, loss: 0.5477Epoch 3/10: [====                          ] 12/75 batches, loss: 0.5497Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.5455Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.5443Epoch 3/10: [======                        ] 15/75 batches, loss: 0.5395Epoch 3/10: [======                        ] 16/75 batches, loss: 0.5365Epoch 3/10: [======                        ] 17/75 batches, loss: 0.5302Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.5282Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.5235Epoch 3/10: [========                      ] 20/75 batches, loss: 0.5213Epoch 3/10: [========                      ] 21/75 batches, loss: 0.5229Epoch 3/10: [========                      ] 22/75 batches, loss: 0.5250Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.5241Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.5250Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.5245Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.5255Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.5274Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.5246Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.5246Epoch 3/10: [============                  ] 30/75 batches, loss: 0.5217Epoch 3/10: [============                  ] 31/75 batches, loss: 0.5233Epoch 3/10: [============                  ] 32/75 batches, loss: 0.5200Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.5182Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.5150Epoch 3/10: [==============                ] 35/75 batches, loss: 0.5121Epoch 3/10: [==============                ] 36/75 batches, loss: 0.5091Epoch 3/10: [==============                ] 37/75 batches, loss: 0.5064Epoch 3/10: [===============               ] 38/75 batches, loss: 0.5038Epoch 3/10: [===============               ] 39/75 batches, loss: 0.4999Epoch 3/10: [================              ] 40/75 batches, loss: 0.4958Epoch 3/10: [================              ] 41/75 batches, loss: 0.4925Epoch 3/10: [================              ] 42/75 batches, loss: 0.4890Epoch 3/10: [=================             ] 43/75 batches, loss: 0.4858Epoch 3/10: [=================             ] 44/75 batches, loss: 0.4846Epoch 3/10: [==================            ] 45/75 batches, loss: 0.4832Epoch 3/10: [==================            ] 46/75 batches, loss: 0.4804Epoch 3/10: [==================            ] 47/75 batches, loss: 0.4779Epoch 3/10: [===================           ] 48/75 batches, loss: 0.4752Epoch 3/10: [===================           ] 49/75 batches, loss: 0.4721Epoch 3/10: [====================          ] 50/75 batches, loss: 0.4703Epoch 3/10: [====================          ] 51/75 batches, loss: 0.4678Epoch 3/10: [====================          ] 52/75 batches, loss: 0.4656Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.4620Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.4572Epoch 3/10: [======================        ] 55/75 batches, loss: 0.4557Epoch 3/10: [======================        ] 56/75 batches, loss: 0.4523Epoch 3/10: [======================        ] 57/75 batches, loss: 0.4489Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.4482Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.4450Epoch 3/10: [========================      ] 60/75 batches, loss: 0.4430Epoch 3/10: [========================      ] 61/75 batches, loss: 0.4412Epoch 3/10: [========================      ] 62/75 batches, loss: 0.4403Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.4377Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.4343Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.4311Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.4278Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.4249Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.4207Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.4181Epoch 3/10: [============================  ] 70/75 batches, loss: 0.4151Epoch 3/10: [============================  ] 71/75 batches, loss: 0.4128Epoch 3/10: [============================  ] 72/75 batches, loss: 0.4100Epoch 3/10: [============================= ] 73/75 batches, loss: 0.4079Epoch 3/10: [============================= ] 74/75 batches, loss: 0.4082Epoch 3/10: [==============================] 75/75 batches, loss: 0.4059
[2025-04-29 20:40:24,822][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.4059
[2025-04-29 20:40:25,078][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.3404, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.3011Epoch 4/10: [                              ] 2/75 batches, loss: 0.2878Epoch 4/10: [=                             ] 3/75 batches, loss: 0.2765Epoch 4/10: [=                             ] 4/75 batches, loss: 0.2927Epoch 4/10: [==                            ] 5/75 batches, loss: 0.2644Epoch 4/10: [==                            ] 6/75 batches, loss: 0.2934Epoch 4/10: [==                            ] 7/75 batches, loss: 0.2683Epoch 4/10: [===                           ] 8/75 batches, loss: 0.2564Epoch 4/10: [===                           ] 9/75 batches, loss: 0.2472Epoch 4/10: [====                          ] 10/75 batches, loss: 0.2361Epoch 4/10: [====                          ] 11/75 batches, loss: 0.2258Epoch 4/10: [====                          ] 12/75 batches, loss: 0.2173Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.2145Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.2078Epoch 4/10: [======                        ] 15/75 batches, loss: 0.2088Epoch 4/10: [======                        ] 16/75 batches, loss: 0.2039Epoch 4/10: [======                        ] 17/75 batches, loss: 0.2041Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.2031Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.2008Epoch 4/10: [========                      ] 20/75 batches, loss: 0.1985Epoch 4/10: [========                      ] 21/75 batches, loss: 0.2030Epoch 4/10: [========                      ] 22/75 batches, loss: 0.1972Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.1929Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.1977Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.1927Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.1912Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.1867Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.2015Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.2015Epoch 4/10: [============                  ] 30/75 batches, loss: 0.1985Epoch 4/10: [============                  ] 31/75 batches, loss: 0.1952Epoch 4/10: [============                  ] 32/75 batches, loss: 0.1993Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.1969Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.1936Epoch 4/10: [==============                ] 35/75 batches, loss: 0.1916Epoch 4/10: [==============                ] 36/75 batches, loss: 0.1934Epoch 4/10: [==============                ] 37/75 batches, loss: 0.1900Epoch 4/10: [===============               ] 38/75 batches, loss: 0.1958Epoch 4/10: [===============               ] 39/75 batches, loss: 0.1927Epoch 4/10: [================              ] 40/75 batches, loss: 0.1910Epoch 4/10: [================              ] 41/75 batches, loss: 0.1886Epoch 4/10: [================              ] 42/75 batches, loss: 0.1857Epoch 4/10: [=================             ] 43/75 batches, loss: 0.1846Epoch 4/10: [=================             ] 44/75 batches, loss: 0.1866Epoch 4/10: [==================            ] 45/75 batches, loss: 0.1865Epoch 4/10: [==================            ] 46/75 batches, loss: 0.1847Epoch 4/10: [==================            ] 47/75 batches, loss: 0.1838Epoch 4/10: [===================           ] 48/75 batches, loss: 0.1845Epoch 4/10: [===================           ] 49/75 batches, loss: 0.1822Epoch 4/10: [====================          ] 50/75 batches, loss: 0.1816Epoch 4/10: [====================          ] 51/75 batches, loss: 0.1793Epoch 4/10: [====================          ] 52/75 batches, loss: 0.1776Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.1751Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.1731Epoch 4/10: [======================        ] 55/75 batches, loss: 0.1765Epoch 4/10: [======================        ] 56/75 batches, loss: 0.1747Epoch 4/10: [======================        ] 57/75 batches, loss: 0.1738Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.1717Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.1700Epoch 4/10: [========================      ] 60/75 batches, loss: 0.1681Epoch 4/10: [========================      ] 61/75 batches, loss: 0.1666Epoch 4/10: [========================      ] 62/75 batches, loss: 0.1655Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.1651Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.1640Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.1622Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.1607Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.1596Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.1582Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.1588Epoch 4/10: [============================  ] 70/75 batches, loss: 0.1581Epoch 4/10: [============================  ] 71/75 batches, loss: 0.1567Epoch 4/10: [============================  ] 72/75 batches, loss: 0.1572Epoch 4/10: [============================= ] 73/75 batches, loss: 0.1634Epoch 4/10: [============================= ] 74/75 batches, loss: 0.1626Epoch 4/10: [==============================] 75/75 batches, loss: 0.1611
[2025-04-29 20:40:32,360][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1611
[2025-04-29 20:40:32,613][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1912, Metrics: {'accuracy': 0.9682539682539683, 'f1': 0.9666666666666667}
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.0788Epoch 5/10: [                              ] 2/75 batches, loss: 0.0703Epoch 5/10: [=                             ] 3/75 batches, loss: 0.0663Epoch 5/10: [=                             ] 4/75 batches, loss: 0.0772Epoch 5/10: [==                            ] 5/75 batches, loss: 0.0744Epoch 5/10: [==                            ] 6/75 batches, loss: 0.0886Epoch 5/10: [==                            ] 7/75 batches, loss: 0.0879Epoch 5/10: [===                           ] 8/75 batches, loss: 0.0834Epoch 5/10: [===                           ] 9/75 batches, loss: 0.0813Epoch 5/10: [====                          ] 10/75 batches, loss: 0.1079Epoch 5/10: [====                          ] 11/75 batches, loss: 0.1019Epoch 5/10: [====                          ] 12/75 batches, loss: 0.0994Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.0994Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.0936Epoch 5/10: [======                        ] 15/75 batches, loss: 0.0912Epoch 5/10: [======                        ] 16/75 batches, loss: 0.0882Epoch 5/10: [======                        ] 17/75 batches, loss: 0.0879Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.0860Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.0833Epoch 5/10: [========                      ] 20/75 batches, loss: 0.0826Epoch 5/10: [========                      ] 21/75 batches, loss: 0.0817Epoch 5/10: [========                      ] 22/75 batches, loss: 0.0798Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.0783Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.0767Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.0774Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.0769Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.0775Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.0877Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.0862Epoch 5/10: [============                  ] 30/75 batches, loss: 0.0952Epoch 5/10: [============                  ] 31/75 batches, loss: 0.0937Epoch 5/10: [============                  ] 32/75 batches, loss: 0.0921Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.0904Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.0894Epoch 5/10: [==============                ] 35/75 batches, loss: 0.0886Epoch 5/10: [==============                ] 36/75 batches, loss: 0.0949Epoch 5/10: [==============                ] 37/75 batches, loss: 0.0949Epoch 5/10: [===============               ] 38/75 batches, loss: 0.1011Epoch 5/10: [===============               ] 39/75 batches, loss: 0.0997Epoch 5/10: [================              ] 40/75 batches, loss: 0.1008Epoch 5/10: [================              ] 41/75 batches, loss: 0.1019Epoch 5/10: [================              ] 42/75 batches, loss: 0.1043Epoch 5/10: [=================             ] 43/75 batches, loss: 0.1042Epoch 5/10: [=================             ] 44/75 batches, loss: 0.1059Epoch 5/10: [==================            ] 45/75 batches, loss: 0.1051Epoch 5/10: [==================            ] 46/75 batches, loss: 0.1054Epoch 5/10: [==================            ] 47/75 batches, loss: 0.1049Epoch 5/10: [===================           ] 48/75 batches, loss: 0.1062Epoch 5/10: [===================           ] 49/75 batches, loss: 0.1052Epoch 5/10: [====================          ] 50/75 batches, loss: 0.1041Epoch 5/10: [====================          ] 51/75 batches, loss: 0.1025Epoch 5/10: [====================          ] 52/75 batches, loss: 0.1013Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.0999Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.0988Epoch 5/10: [======================        ] 55/75 batches, loss: 0.0976Epoch 5/10: [======================        ] 56/75 batches, loss: 0.0974Epoch 5/10: [======================        ] 57/75 batches, loss: 0.0967Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.0965Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.0960Epoch 5/10: [========================      ] 60/75 batches, loss: 0.0960Epoch 5/10: [========================      ] 61/75 batches, loss: 0.0951Epoch 5/10: [========================      ] 62/75 batches, loss: 0.0954Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.0946Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.0935Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.0929Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.0920Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.0919Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.0910Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.0902Epoch 5/10: [============================  ] 70/75 batches, loss: 0.0921Epoch 5/10: [============================  ] 71/75 batches, loss: 0.0914Epoch 5/10: [============================  ] 72/75 batches, loss: 0.0920Epoch 5/10: [============================= ] 73/75 batches, loss: 0.0917Epoch 5/10: [============================= ] 74/75 batches, loss: 0.0955Epoch 5/10: [==============================] 75/75 batches, loss: 0.0946
[2025-04-29 20:40:39,944][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0946
[2025-04-29 20:40:40,218][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2269, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
[2025-04-29 20:40:40,218][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.1691Epoch 6/10: [                              ] 2/75 batches, loss: 0.1318Epoch 6/10: [=                             ] 3/75 batches, loss: 0.1124Epoch 6/10: [=                             ] 4/75 batches, loss: 0.1152Epoch 6/10: [==                            ] 5/75 batches, loss: 0.1126Epoch 6/10: [==                            ] 6/75 batches, loss: 0.1401Epoch 6/10: [==                            ] 7/75 batches, loss: 0.1292Epoch 6/10: [===                           ] 8/75 batches, loss: 0.1234Epoch 6/10: [===                           ] 9/75 batches, loss: 0.1159Epoch 6/10: [====                          ] 10/75 batches, loss: 0.1193Epoch 6/10: [====                          ] 11/75 batches, loss: 0.1180Epoch 6/10: [====                          ] 12/75 batches, loss: 0.1116Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.1046Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.0992Epoch 6/10: [======                        ] 15/75 batches, loss: 0.0941Epoch 6/10: [======                        ] 16/75 batches, loss: 0.0906Epoch 6/10: [======                        ] 17/75 batches, loss: 0.0905Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.1002Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.0985Epoch 6/10: [========                      ] 20/75 batches, loss: 0.0958Epoch 6/10: [========                      ] 21/75 batches, loss: 0.0926Epoch 6/10: [========                      ] 22/75 batches, loss: 0.0913Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.0894Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.0906Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.0889Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.0875Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.0859Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.0838Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.0829Epoch 6/10: [============                  ] 30/75 batches, loss: 0.0919Epoch 6/10: [============                  ] 31/75 batches, loss: 0.0907Epoch 6/10: [============                  ] 32/75 batches, loss: 0.0899Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.0896Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.0894Epoch 6/10: [==============                ] 35/75 batches, loss: 0.0907Epoch 6/10: [==============                ] 36/75 batches, loss: 0.0898Epoch 6/10: [==============                ] 37/75 batches, loss: 0.0902Epoch 6/10: [===============               ] 38/75 batches, loss: 0.0895Epoch 6/10: [===============               ] 39/75 batches, loss: 0.0938Epoch 6/10: [================              ] 40/75 batches, loss: 0.0930Epoch 6/10: [================              ] 41/75 batches, loss: 0.0918Epoch 6/10: [================              ] 42/75 batches, loss: 0.0904Epoch 6/10: [=================             ] 43/75 batches, loss: 0.0891Epoch 6/10: [=================             ] 44/75 batches, loss: 0.0886Epoch 6/10: [==================            ] 45/75 batches, loss: 0.0875Epoch 6/10: [==================            ] 46/75 batches, loss: 0.0860Epoch 6/10: [==================            ] 47/75 batches, loss: 0.0852Epoch 6/10: [===================           ] 48/75 batches, loss: 0.0842Epoch 6/10: [===================           ] 49/75 batches, loss: 0.0829Epoch 6/10: [====================          ] 50/75 batches, loss: 0.0822Epoch 6/10: [====================          ] 51/75 batches, loss: 0.0812Epoch 6/10: [====================          ] 52/75 batches, loss: 0.0802Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.0793Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.0785Epoch 6/10: [======================        ] 55/75 batches, loss: 0.0832Epoch 6/10: [======================        ] 56/75 batches, loss: 0.0826Epoch 6/10: [======================        ] 57/75 batches, loss: 0.0821Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.0811Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.0802Epoch 6/10: [========================      ] 60/75 batches, loss: 0.0796Epoch 6/10: [========================      ] 61/75 batches, loss: 0.0787Epoch 6/10: [========================      ] 62/75 batches, loss: 0.0780Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.0771Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.0772Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.0764Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.0795Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.0787Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.0779Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.0774Epoch 6/10: [============================  ] 70/75 batches, loss: 0.0816Epoch 6/10: [============================  ] 71/75 batches, loss: 0.0808Epoch 6/10: [============================  ] 72/75 batches, loss: 0.0829Epoch 6/10: [============================= ] 73/75 batches, loss: 0.0820Epoch 6/10: [============================= ] 74/75 batches, loss: 0.0810Epoch 6/10: [==============================] 75/75 batches, loss: 0.0803
[2025-04-29 20:40:47,125][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0803
[2025-04-29 20:40:47,395][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.2501, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
[2025-04-29 20:40:47,396][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.2453Epoch 7/10: [                              ] 2/75 batches, loss: 0.1261Epoch 7/10: [=                             ] 3/75 batches, loss: 0.0908Epoch 7/10: [=                             ] 4/75 batches, loss: 0.0773Epoch 7/10: [==                            ] 5/75 batches, loss: 0.0685Epoch 7/10: [==                            ] 6/75 batches, loss: 0.0631Epoch 7/10: [==                            ] 7/75 batches, loss: 0.0581Epoch 7/10: [===                           ] 8/75 batches, loss: 0.0570Epoch 7/10: [===                           ] 9/75 batches, loss: 0.0539Epoch 7/10: [====                          ] 10/75 batches, loss: 0.0508Epoch 7/10: [====                          ] 11/75 batches, loss: 0.0492Epoch 7/10: [====                          ] 12/75 batches, loss: 0.0469Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.0444Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.0437Epoch 7/10: [======                        ] 15/75 batches, loss: 0.0430Epoch 7/10: [======                        ] 16/75 batches, loss: 0.0413Epoch 7/10: [======                        ] 17/75 batches, loss: 0.0395Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.0386Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.0372Epoch 7/10: [========                      ] 20/75 batches, loss: 0.0361Epoch 7/10: [========                      ] 21/75 batches, loss: 0.0364Epoch 7/10: [========                      ] 22/75 batches, loss: 0.0353Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.0348Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.0340Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.0333Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.0454Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.0444Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.0440Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.0440Epoch 7/10: [============                  ] 30/75 batches, loss: 0.0571Epoch 7/10: [============                  ] 31/75 batches, loss: 0.0567Epoch 7/10: [============                  ] 32/75 batches, loss: 0.0562Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.0555Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.0548Epoch 7/10: [==============                ] 35/75 batches, loss: 0.0546Epoch 7/10: [==============                ] 36/75 batches, loss: 0.0600Epoch 7/10: [==============                ] 37/75 batches, loss: 0.0598Epoch 7/10: [===============               ] 38/75 batches, loss: 0.0625Epoch 7/10: [===============               ] 39/75 batches, loss: 0.0629Epoch 7/10: [================              ] 40/75 batches, loss: 0.0623Epoch 7/10: [================              ] 41/75 batches, loss: 0.0764Epoch 7/10: [================              ] 42/75 batches, loss: 0.0821Epoch 7/10: [=================             ] 43/75 batches, loss: 0.0810Epoch 7/10: [=================             ] 44/75 batches, loss: 0.0796Epoch 7/10: [==================            ] 45/75 batches, loss: 0.0810Epoch 7/10: [==================            ] 46/75 batches, loss: 0.0796Epoch 7/10: [==================            ] 47/75 batches, loss: 0.0793Epoch 7/10: [===================           ] 48/75 batches, loss: 0.0786Epoch 7/10: [===================           ] 49/75 batches, loss: 0.0776Epoch 7/10: [====================          ] 50/75 batches, loss: 0.0771Epoch 7/10: [====================          ] 51/75 batches, loss: 0.0761Epoch 7/10: [====================          ] 52/75 batches, loss: 0.0751Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.0740Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.0780Epoch 7/10: [======================        ] 55/75 batches, loss: 0.0772Epoch 7/10: [======================        ] 56/75 batches, loss: 0.0760Epoch 7/10: [======================        ] 57/75 batches, loss: 0.0748Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.0737Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.0727Epoch 7/10: [========================      ] 60/75 batches, loss: 0.0717Epoch 7/10: [========================      ] 61/75 batches, loss: 0.0710Epoch 7/10: [========================      ] 62/75 batches, loss: 0.0701Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.0695Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.0686Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.0678Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.0670Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.0662Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.0653Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.0646Epoch 7/10: [============================  ] 70/75 batches, loss: 0.0641Epoch 7/10: [============================  ] 71/75 batches, loss: 0.0642Epoch 7/10: [============================  ] 72/75 batches, loss: 0.0634Epoch 7/10: [============================= ] 73/75 batches, loss: 0.0643Epoch 7/10: [============================= ] 74/75 batches, loss: 0.0635Epoch 7/10: [==============================] 75/75 batches, loss: 0.0663
[2025-04-29 20:40:54,295][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0663
[2025-04-29 20:40:54,572][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1332, Metrics: {'accuracy': 0.9682539682539683, 'f1': 0.967741935483871}
Epoch 8/10: [Epoch 8/10: [                              ] 1/75 batches, loss: 0.0107Epoch 8/10: [                              ] 2/75 batches, loss: 0.2013Epoch 8/10: [=                             ] 3/75 batches, loss: 0.1387Epoch 8/10: [=                             ] 4/75 batches, loss: 0.1864Epoch 8/10: [==                            ] 5/75 batches, loss: 0.1515Epoch 8/10: [==                            ] 6/75 batches, loss: 0.1924Epoch 8/10: [==                            ] 7/75 batches, loss: 0.1661Epoch 8/10: [===                           ] 8/75 batches, loss: 0.1780Epoch 8/10: [===                           ] 9/75 batches, loss: 0.1878Epoch 8/10: [====                          ] 10/75 batches, loss: 0.1912Epoch 8/10: [====                          ] 11/75 batches, loss: 0.1788Epoch 8/10: [====                          ] 12/75 batches, loss: 0.1702Epoch 8/10: [=====                         ] 13/75 batches, loss: 0.1699Epoch 8/10: [=====                         ] 14/75 batches, loss: 0.1633Epoch 8/10: [======                        ] 15/75 batches, loss: 0.1608Epoch 8/10: [======                        ] 16/75 batches, loss: 0.1572Epoch 8/10: [======                        ] 17/75 batches, loss: 0.1535Epoch 8/10: [=======                       ] 18/75 batches, loss: 0.1483Epoch 8/10: [=======                       ] 19/75 batches, loss: 0.1445Epoch 8/10: [========                      ] 20/75 batches, loss: 0.1423Epoch 8/10: [========                      ] 21/75 batches, loss: 0.1488Epoch 8/10: [========                      ] 22/75 batches, loss: 0.1469Epoch 8/10: [=========                     ] 23/75 batches, loss: 0.1444Epoch 8/10: [=========                     ] 24/75 batches, loss: 0.1394Epoch 8/10: [==========                    ] 25/75 batches, loss: 0.1435Epoch 8/10: [==========                    ] 26/75 batches, loss: 0.1391Epoch 8/10: [==========                    ] 27/75 batches, loss: 0.1370Epoch 8/10: [===========                   ] 28/75 batches, loss: 0.1346Epoch 8/10: [===========                   ] 29/75 batches, loss: 0.1338Epoch 8/10: [============                  ] 30/75 batches, loss: 0.1303Epoch 8/10: [============                  ] 31/75 batches, loss: 0.1279Epoch 8/10: [============                  ] 32/75 batches, loss: 0.1292Epoch 8/10: [=============                 ] 33/75 batches, loss: 0.1269Epoch 8/10: [=============                 ] 34/75 batches, loss: 0.1240Epoch 8/10: [==============                ] 35/75 batches, loss: 0.1211Epoch 8/10: [==============                ] 36/75 batches, loss: 0.1256Epoch 8/10: [==============                ] 37/75 batches, loss: 0.1343Epoch 8/10: [===============               ] 38/75 batches, loss: 0.1318Epoch 8/10: [===============               ] 39/75 batches, loss: 0.1291Epoch 8/10: [================              ] 40/75 batches, loss: 0.1296Epoch 8/10: [================              ] 41/75 batches, loss: 0.1297Epoch 8/10: [================              ] 42/75 batches, loss: 0.1289Epoch 8/10: [=================             ] 43/75 batches, loss: 0.1282Epoch 8/10: [=================             ] 44/75 batches, loss: 0.1270Epoch 8/10: [==================            ] 45/75 batches, loss: 0.1264Epoch 8/10: [==================            ] 46/75 batches, loss: 0.1270Epoch 8/10: [==================            ] 47/75 batches, loss: 0.1255Epoch 8/10: [===================           ] 48/75 batches, loss: 0.1240Epoch 8/10: [===================           ] 49/75 batches, loss: 0.1225Epoch 8/10: [====================          ] 50/75 batches, loss: 0.1212Epoch 8/10: [====================          ] 51/75 batches, loss: 0.1201Epoch 8/10: [====================          ] 52/75 batches, loss: 0.1199Epoch 8/10: [=====================         ] 53/75 batches, loss: 0.1184Epoch 8/10: [=====================         ] 54/75 batches, loss: 0.1171Epoch 8/10: [======================        ] 55/75 batches, loss: 0.1158Epoch 8/10: [======================        ] 56/75 batches, loss: 0.1143Epoch 8/10: [======================        ] 57/75 batches, loss: 0.1127Epoch 8/10: [=======================       ] 58/75 batches, loss: 0.1113Epoch 8/10: [=======================       ] 59/75 batches, loss: 0.1097Epoch 8/10: [========================      ] 60/75 batches, loss: 0.1131Epoch 8/10: [========================      ] 61/75 batches, loss: 0.1119Epoch 8/10: [========================      ] 62/75 batches, loss: 0.1107Epoch 8/10: [=========================     ] 63/75 batches, loss: 0.1102Epoch 8/10: [=========================     ] 64/75 batches, loss: 0.1088Epoch 8/10: [==========================    ] 65/75 batches, loss: 0.1087Epoch 8/10: [==========================    ] 66/75 batches, loss: 0.1076Epoch 8/10: [==========================    ] 67/75 batches, loss: 0.1071Epoch 8/10: [===========================   ] 68/75 batches, loss: 0.1066Epoch 8/10: [===========================   ] 69/75 batches, loss: 0.1056Epoch 8/10: [============================  ] 70/75 batches, loss: 0.1049Epoch 8/10: [============================  ] 71/75 batches, loss: 0.1043Epoch 8/10: [============================  ] 72/75 batches, loss: 0.1034Epoch 8/10: [============================= ] 73/75 batches, loss: 0.1023Epoch 8/10: [============================= ] 74/75 batches, loss: 0.1012Epoch 8/10: [==============================] 75/75 batches, loss: 0.0999
[2025-04-29 20:41:01,885][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0999
[2025-04-29 20:41:02,148][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1421, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
[2025-04-29 20:41:02,149][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 9/10: [Epoch 9/10: [                              ] 1/75 batches, loss: 0.0086Epoch 9/10: [                              ] 2/75 batches, loss: 0.0116Epoch 9/10: [=                             ] 3/75 batches, loss: 0.0134Epoch 9/10: [=                             ] 4/75 batches, loss: 0.0460Epoch 9/10: [==                            ] 5/75 batches, loss: 0.1152Epoch 9/10: [==                            ] 6/75 batches, loss: 0.0975Epoch 9/10: [==                            ] 7/75 batches, loss: 0.0851Epoch 9/10: [===                           ] 8/75 batches, loss: 0.0763Epoch 9/10: [===                           ] 9/75 batches, loss: 0.0745Epoch 9/10: [====                          ] 10/75 batches, loss: 0.0691Epoch 9/10: [====                          ] 11/75 batches, loss: 0.0652Epoch 9/10: [====                          ] 12/75 batches, loss: 0.0750Epoch 9/10: [=====                         ] 13/75 batches, loss: 0.0732Epoch 9/10: [=====                         ] 14/75 batches, loss: 0.0714Epoch 9/10: [======                        ] 15/75 batches, loss: 0.0686Epoch 9/10: [======                        ] 16/75 batches, loss: 0.0873Epoch 9/10: [======                        ] 17/75 batches, loss: 0.0838Epoch 9/10: [=======                       ] 18/75 batches, loss: 0.0960Epoch 9/10: [=======                       ] 19/75 batches, loss: 0.0934Epoch 9/10: [========                      ] 20/75 batches, loss: 0.0935Epoch 9/10: [========                      ] 21/75 batches, loss: 0.0907Epoch 9/10: [========                      ] 22/75 batches, loss: 0.0876Epoch 9/10: [=========                     ] 23/75 batches, loss: 0.0877Epoch 9/10: [=========                     ] 24/75 batches, loss: 0.0864Epoch 9/10: [==========                    ] 25/75 batches, loss: 0.0846Epoch 9/10: [==========                    ] 26/75 batches, loss: 0.0861Epoch 9/10: [==========                    ] 27/75 batches, loss: 0.0841Epoch 9/10: [===========                   ] 28/75 batches, loss: 0.0821Epoch 9/10: [===========                   ] 29/75 batches, loss: 0.0800Epoch 9/10: [============                  ] 30/75 batches, loss: 0.0778Epoch 9/10: [============                  ] 31/75 batches, loss: 0.0762Epoch 9/10: [============                  ] 32/75 batches, loss: 0.0770Epoch 9/10: [=============                 ] 33/75 batches, loss: 0.0749Epoch 9/10: [=============                 ] 34/75 batches, loss: 0.0733Epoch 9/10: [==============                ] 35/75 batches, loss: 0.0715Epoch 9/10: [==============                ] 36/75 batches, loss: 0.0698Epoch 9/10: [==============                ] 37/75 batches, loss: 0.0684Epoch 9/10: [===============               ] 38/75 batches, loss: 0.0674Epoch 9/10: [===============               ] 39/75 batches, loss: 0.0661Epoch 9/10: [================              ] 40/75 batches, loss: 0.0648Epoch 9/10: [================              ] 41/75 batches, loss: 0.0643Epoch 9/10: [================              ] 42/75 batches, loss: 0.0639Epoch 9/10: [=================             ] 43/75 batches, loss: 0.0625Epoch 9/10: [=================             ] 44/75 batches, loss: 0.0615Epoch 9/10: [==================            ] 45/75 batches, loss: 0.0607Epoch 9/10: [==================            ] 46/75 batches, loss: 0.0599Epoch 9/10: [==================            ] 47/75 batches, loss: 0.0588Epoch 9/10: [===================           ] 48/75 batches, loss: 0.0584Epoch 9/10: [===================           ] 49/75 batches, loss: 0.0659Epoch 9/10: [====================          ] 50/75 batches, loss: 0.0798Epoch 9/10: [====================          ] 51/75 batches, loss: 0.0787Epoch 9/10: [====================          ] 52/75 batches, loss: 0.0778Epoch 9/10: [=====================         ] 53/75 batches, loss: 0.0765Epoch 9/10: [=====================         ] 54/75 batches, loss: 0.0753Epoch 9/10: [======================        ] 55/75 batches, loss: 0.0741Epoch 9/10: [======================        ] 56/75 batches, loss: 0.0729Epoch 9/10: [======================        ] 57/75 batches, loss: 0.0719Epoch 9/10: [=======================       ] 58/75 batches, loss: 0.0710Epoch 9/10: [=======================       ] 59/75 batches, loss: 0.0703Epoch 9/10: [========================      ] 60/75 batches, loss: 0.0699Epoch 9/10: [========================      ] 61/75 batches, loss: 0.0690Epoch 9/10: [========================      ] 62/75 batches, loss: 0.0696Epoch 9/10: [=========================     ] 63/75 batches, loss: 0.0688Epoch 9/10: [=========================     ] 64/75 batches, loss: 0.0679Epoch 9/10: [==========================    ] 65/75 batches, loss: 0.0672Epoch 9/10: [==========================    ] 66/75 batches, loss: 0.0663Epoch 9/10: [==========================    ] 67/75 batches, loss: 0.0655Epoch 9/10: [===========================   ] 68/75 batches, loss: 0.0646Epoch 9/10: [===========================   ] 69/75 batches, loss: 0.0639Epoch 9/10: [============================  ] 70/75 batches, loss: 0.0631Epoch 9/10: [============================  ] 71/75 batches, loss: 0.0625Epoch 9/10: [============================  ] 72/75 batches, loss: 0.0618Epoch 9/10: [============================= ] 73/75 batches, loss: 0.0610Epoch 9/10: [============================= ] 74/75 batches, loss: 0.0603Epoch 9/10: [==============================] 75/75 batches, loss: 0.0599
[2025-04-29 20:41:09,067][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0599
[2025-04-29 20:41:09,330][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1271, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
Epoch 10/10: [Epoch 10/10: [                              ] 1/75 batches, loss: 0.0111Epoch 10/10: [                              ] 2/75 batches, loss: 0.0082Epoch 10/10: [=                             ] 3/75 batches, loss: 0.0069Epoch 10/10: [=                             ] 4/75 batches, loss: 0.0062Epoch 10/10: [==                            ] 5/75 batches, loss: 0.0068Epoch 10/10: [==                            ] 6/75 batches, loss: 0.0492Epoch 10/10: [==                            ] 7/75 batches, loss: 0.0428Epoch 10/10: [===                           ] 8/75 batches, loss: 0.0778Epoch 10/10: [===                           ] 9/75 batches, loss: 0.0698Epoch 10/10: [====                          ] 10/75 batches, loss: 0.0635Epoch 10/10: [====                          ] 11/75 batches, loss: 0.0588Epoch 10/10: [====                          ] 12/75 batches, loss: 0.0544Epoch 10/10: [=====                         ] 13/75 batches, loss: 0.0507Epoch 10/10: [=====                         ] 14/75 batches, loss: 0.0479Epoch 10/10: [======                        ] 15/75 batches, loss: 0.0458Epoch 10/10: [======                        ] 16/75 batches, loss: 0.0668Epoch 10/10: [======                        ] 17/75 batches, loss: 0.0648Epoch 10/10: [=======                       ] 18/75 batches, loss: 0.0623Epoch 10/10: [=======                       ] 19/75 batches, loss: 0.0597Epoch 10/10: [========                      ] 20/75 batches, loss: 0.0574Epoch 10/10: [========                      ] 21/75 batches, loss: 0.0553Epoch 10/10: [========                      ] 22/75 batches, loss: 0.0575Epoch 10/10: [=========                     ] 23/75 batches, loss: 0.0596Epoch 10/10: [=========                     ] 24/75 batches, loss: 0.0576Epoch 10/10: [==========                    ] 25/75 batches, loss: 0.0556Epoch 10/10: [==========                    ] 26/75 batches, loss: 0.0575Epoch 10/10: [==========                    ] 27/75 batches, loss: 0.0562Epoch 10/10: [===========                   ] 28/75 batches, loss: 0.0559Epoch 10/10: [===========                   ] 29/75 batches, loss: 0.0543Epoch 10/10: [============                  ] 30/75 batches, loss: 0.0570Epoch 10/10: [============                  ] 31/75 batches, loss: 0.0553Epoch 10/10: [============                  ] 32/75 batches, loss: 0.0539Epoch 10/10: [=============                 ] 33/75 batches, loss: 0.0525Epoch 10/10: [=============                 ] 34/75 batches, loss: 0.0605Epoch 10/10: [==============                ] 35/75 batches, loss: 0.0590Epoch 10/10: [==============                ] 36/75 batches, loss: 0.0678Epoch 10/10: [==============                ] 37/75 batches, loss: 0.0663Epoch 10/10: [===============               ] 38/75 batches, loss: 0.0691Epoch 10/10: [===============               ] 39/75 batches, loss: 0.0677Epoch 10/10: [================              ] 40/75 batches, loss: 0.0664Epoch 10/10: [================              ] 41/75 batches, loss: 0.0653Epoch 10/10: [================              ] 42/75 batches, loss: 0.0642Epoch 10/10: [=================             ] 43/75 batches, loss: 0.0631Epoch 10/10: [=================             ] 44/75 batches, loss: 0.0622Epoch 10/10: [==================            ] 45/75 batches, loss: 0.0616Epoch 10/10: [==================            ] 46/75 batches, loss: 0.0607Epoch 10/10: [==================            ] 47/75 batches, loss: 0.0598Epoch 10/10: [===================           ] 48/75 batches, loss: 0.0589Epoch 10/10: [===================           ] 49/75 batches, loss: 0.0581Epoch 10/10: [====================          ] 50/75 batches, loss: 0.0573Epoch 10/10: [====================          ] 51/75 batches, loss: 0.0586Epoch 10/10: [====================          ] 52/75 batches, loss: 0.0579Epoch 10/10: [=====================         ] 53/75 batches, loss: 0.0625Epoch 10/10: [=====================         ] 54/75 batches, loss: 0.0616Epoch 10/10: [======================        ] 55/75 batches, loss: 0.0606Epoch 10/10: [======================        ] 56/75 batches, loss: 0.0599Epoch 10/10: [======================        ] 57/75 batches, loss: 0.0591Epoch 10/10: [=======================       ] 58/75 batches, loss: 0.0584Epoch 10/10: [=======================       ] 59/75 batches, loss: 0.0584Epoch 10/10: [========================      ] 60/75 batches, loss: 0.0578Epoch 10/10: [========================      ] 61/75 batches, loss: 0.0573Epoch 10/10: [========================      ] 62/75 batches, loss: 0.0568Epoch 10/10: [=========================     ] 63/75 batches, loss: 0.0565Epoch 10/10: [=========================     ] 64/75 batches, loss: 0.0558Epoch 10/10: [==========================    ] 65/75 batches, loss: 0.0551Epoch 10/10: [==========================    ] 66/75 batches, loss: 0.0551Epoch 10/10: [==========================    ] 67/75 batches, loss: 0.0547Epoch 10/10: [===========================   ] 68/75 batches, loss: 0.0544Epoch 10/10: [===========================   ] 69/75 batches, loss: 0.0537Epoch 10/10: [============================  ] 70/75 batches, loss: 0.0530Epoch 10/10: [============================  ] 71/75 batches, loss: 0.0537Epoch 10/10: [============================  ] 72/75 batches, loss: 0.0545Epoch 10/10: [============================= ] 73/75 batches, loss: 0.0539Epoch 10/10: [============================= ] 74/75 batches, loss: 0.0533Epoch 10/10: [==============================] 75/75 batches, loss: 0.0527
[2025-04-29 20:41:16,687][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0527
[2025-04-29 20:41:16,951][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0792, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
[2025-04-29 20:41:17,352][src.training.lm_trainer][INFO] - Training completed in 75.25 seconds
[2025-04-29 20:41:17,352][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 20:41:20,320][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9933054393305439, 'f1': 0.9932659932659933}
[2025-04-29 20:41:20,321][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
[2025-04-29 20:41:20,321][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9272727272727272, 'f1': 0.9298245614035088}
[2025-04-29 20:41:21,946][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/fi/fi/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁█████
wandb:          best_val_f1 ▁▁█████
wandb:        best_val_loss ██▄▂▂▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▅▂▁▁▁▂▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁████████
wandb:               val_f1 ▁▁████████
wandb:             val_loss ██▄▂▃▃▂▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.95238
wandb:          best_val_f1 0.95082
wandb:        best_val_loss 0.07921
wandb:                epoch 10
wandb:  final_test_accuracy 0.92727
wandb:        final_test_f1 0.92982
wandb: final_train_accuracy 0.99331
wandb:       final_train_f1 0.99327
wandb:   final_val_accuracy 0.95238
wandb:         final_val_f1 0.95082
wandb:        learning_rate 2e-05
wandb:           train_loss 0.05274
wandb:           train_time 75.24663
wandb:         val_accuracy 0.95238
wandb:               val_f1 0.95082
wandb:             val_loss 0.07921
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_203951-um6r1ptz
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_203951-um6r1ptz/logs
Experiment finetune_question_type_fi completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/fi/results.json
Running experiment: finetune_complexity_fi
Command: python -m src.experiments.run_experiment             "hydra.job.chdir=False"             "hydra.run.dir=."             "experiment=finetune"             "experiment.tasks=complexity"             "model=glot500_finetune"             "model.lm_name=cis-lmu/glot500-base"             "model.dropout=0.1"             "model.freeze_model=false"             "model.finetune=true"             "data.languages=[fi]"             "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"             "training.task_type=regression"             "training.num_epochs=10"             "training.batch_size=16"             "training.lr=2e-5"             "+training.gradient_accumulation_steps=2"                          "experiment_name=finetune_complexity_fi"             "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/fi"             "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 20:41:37,917][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/fi
experiment_name: finetune_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_finetune
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  finetune: true
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1
    lr: 2.0e-05
    batch_size: 8

[2025-04-29 20:41:37,917][__main__][INFO] - Normalized task: complexity
[2025-04-29 20:41:37,917][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 20:41:37,917][__main__][INFO] - Determined Task Type: regression
[2025-04-29 20:41:37,921][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-04-29 20:41:37,922][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 20:41:39,785][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 20:41:42,062][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 20:41:42,062][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:41:42,149][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:41:42,190][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:41:42,303][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 20:41:42,311][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:41:42,312][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 20:41:42,314][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:41:42,344][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:41:42,386][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:41:42,403][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 20:41:42,404][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:41:42,405][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 20:41:42,406][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:41:42,446][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:41:42,499][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:41:42,519][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 20:41:42,521][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:41:42,521][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 20:41:42,523][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 20:41:42,524][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 20:41:42,524][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 20:41:42,524][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 20:41:42,524][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 20:41:42,524][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 20:41:42,525][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-29 20:41:42,525][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 20:41:42,525][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-29 20:41:42,525][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 20:41:42,525][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 20:41:42,525][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 20:41:42,525][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 20:41:42,525][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 20:41:42,525][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-29 20:41:42,526][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 20:41:42,526][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-29 20:41:42,526][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 20:41:42,526][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 20:41:42,526][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 20:41:42,526][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 20:41:42,526][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 20:41:42,526][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-29 20:41:42,526][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 20:41:42,526][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-29 20:41:42,526][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 20:41:42,527][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 20:41:42,527][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 20:41:42,527][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 20:41:47,799][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 20:41:47,799][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,800][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,800][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,800][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,800][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,800][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,800][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,800][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,800][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,800][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,800][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,800][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,800][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,800][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,801][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,801][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,801][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,801][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,801][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,801][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,801][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,801][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,801][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,801][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,801][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,801][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,801][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,801][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,802][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,802][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,802][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,802][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,802][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,802][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,802][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,802][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,802][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,802][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,802][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,802][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,802][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,802][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,802][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,803][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,803][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,803][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,803][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,803][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,803][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,803][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,803][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,803][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,803][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,803][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,803][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,803][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,803][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,803][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,803][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,804][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,804][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,804][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,804][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,804][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,804][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,804][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,804][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,804][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,804][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,804][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,804][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,804][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,804][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,805][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,805][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,805][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,805][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,805][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,805][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,805][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,805][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,805][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,805][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,805][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,805][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,805][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,805][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,805][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,806][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,806][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,806][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,806][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,806][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,806][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,806][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,806][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,806][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,806][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,806][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,806][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,806][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,806][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,806][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,807][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,807][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,807][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,807][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,807][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,807][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,807][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,807][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,807][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,807][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,807][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,807][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,807][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,807][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,807][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,808][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,808][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,808][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,808][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,808][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,808][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,808][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,808][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,808][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,808][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,808][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,808][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,808][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,808][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,808][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,808][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,809][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,809][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,809][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,809][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,809][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,809][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,809][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,809][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,809][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,809][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,809][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,809][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,809][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,809][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,809][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,810][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,810][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,810][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,810][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,810][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,810][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,810][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,810][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,810][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,810][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,810][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,810][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,810][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,810][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,811][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,811][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,811][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,811][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,811][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,811][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,811][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,811][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,811][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,811][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,811][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,811][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,811][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,811][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,811][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,811][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,812][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,812][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,812][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,812][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,812][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,812][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,812][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,812][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,812][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,812][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,812][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,812][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,812][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,812][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,812][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,813][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,813][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,813][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,813][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,813][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:41:47,814][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 20:41:47,814][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 20:41:47,815][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 20:41:47,815][__main__][INFO] - Successfully created model for fi
[2025-04-29 20:41:47,815][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.1239Epoch 1/10: [                              ] 2/75 batches, loss: 0.1253Epoch 1/10: [=                             ] 3/75 batches, loss: 0.1005Epoch 1/10: [=                             ] 4/75 batches, loss: 0.0911Epoch 1/10: [==                            ] 5/75 batches, loss: 0.0859Epoch 1/10: [==                            ] 6/75 batches, loss: 0.0850Epoch 1/10: [==                            ] 7/75 batches, loss: 0.0859Epoch 1/10: [===                           ] 8/75 batches, loss: 0.0861Epoch 1/10: [===                           ] 9/75 batches, loss: 0.0844Epoch 1/10: [====                          ] 10/75 batches, loss: 0.0835Epoch 1/10: [====                          ] 11/75 batches, loss: 0.0828Epoch 1/10: [====                          ] 12/75 batches, loss: 0.0825Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.0807Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.0801Epoch 1/10: [======                        ] 15/75 batches, loss: 0.0805Epoch 1/10: [======                        ] 16/75 batches, loss: 0.0773Epoch 1/10: [======                        ] 17/75 batches, loss: 0.0759Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.0758Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.0747Epoch 1/10: [========                      ] 20/75 batches, loss: 0.0743Epoch 1/10: [========                      ] 21/75 batches, loss: 0.0738Epoch 1/10: [========                      ] 22/75 batches, loss: 0.0725Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.0719Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.0702Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.0698Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.0689Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.0682Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.0676Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.0676Epoch 1/10: [============                  ] 30/75 batches, loss: 0.0668Epoch 1/10: [============                  ] 31/75 batches, loss: 0.0672Epoch 1/10: [============                  ] 32/75 batches, loss: 0.0664Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.0653Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.0653Epoch 1/10: [==============                ] 35/75 batches, loss: 0.0652Epoch 1/10: [==============                ] 36/75 batches, loss: 0.0643Epoch 1/10: [==============                ] 37/75 batches, loss: 0.0643Epoch 1/10: [===============               ] 38/75 batches, loss: 0.0633Epoch 1/10: [===============               ] 39/75 batches, loss: 0.0626Epoch 1/10: [================              ] 40/75 batches, loss: 0.0618Epoch 1/10: [================              ] 41/75 batches, loss: 0.0617Epoch 1/10: [================              ] 42/75 batches, loss: 0.0608Epoch 1/10: [=================             ] 43/75 batches, loss: 0.0608Epoch 1/10: [=================             ] 44/75 batches, loss: 0.0601Epoch 1/10: [==================            ] 45/75 batches, loss: 0.0591Epoch 1/10: [==================            ] 46/75 batches, loss: 0.0586Epoch 1/10: [==================            ] 47/75 batches, loss: 0.0580Epoch 1/10: [===================           ] 48/75 batches, loss: 0.0571Epoch 1/10: [===================           ] 49/75 batches, loss: 0.0568Epoch 1/10: [====================          ] 50/75 batches, loss: 0.0562Epoch 1/10: [====================          ] 51/75 batches, loss: 0.0561Epoch 1/10: [====================          ] 52/75 batches, loss: 0.0564Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.0559Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.0556Epoch 1/10: [======================        ] 55/75 batches, loss: 0.0553Epoch 1/10: [======================        ] 56/75 batches, loss: 0.0549Epoch 1/10: [======================        ] 57/75 batches, loss: 0.0546Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.0540Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.0539Epoch 1/10: [========================      ] 60/75 batches, loss: 0.0535Epoch 1/10: [========================      ] 61/75 batches, loss: 0.0530Epoch 1/10: [========================      ] 62/75 batches, loss: 0.0527Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.0524Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.0519Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.0515Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.0512Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.0508Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.0503Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.0504Epoch 1/10: [============================  ] 70/75 batches, loss: 0.0500Epoch 1/10: [============================  ] 71/75 batches, loss: 0.0496Epoch 1/10: [============================  ] 72/75 batches, loss: 0.0493Epoch 1/10: [============================= ] 73/75 batches, loss: 0.0490Epoch 1/10: [============================= ] 74/75 batches, loss: 0.0488Epoch 1/10: [==============================] 75/75 batches, loss: 0.0485
[2025-04-29 20:41:58,452][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0485
[2025-04-29 20:41:58,674][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1066, Metrics: {'mse': 0.10664906352758408, 'rmse': 0.3265716820662564, 'r2': -0.6267263889312744}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.0436Epoch 2/10: [                              ] 2/75 batches, loss: 0.0458Epoch 2/10: [=                             ] 3/75 batches, loss: 0.0377Epoch 2/10: [=                             ] 4/75 batches, loss: 0.0354Epoch 2/10: [==                            ] 5/75 batches, loss: 0.0309Epoch 2/10: [==                            ] 6/75 batches, loss: 0.0332Epoch 2/10: [==                            ] 7/75 batches, loss: 0.0317Epoch 2/10: [===                           ] 8/75 batches, loss: 0.0311Epoch 2/10: [===                           ] 9/75 batches, loss: 0.0313Epoch 2/10: [====                          ] 10/75 batches, loss: 0.0311Epoch 2/10: [====                          ] 11/75 batches, loss: 0.0315Epoch 2/10: [====                          ] 12/75 batches, loss: 0.0318Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.0331Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.0319Epoch 2/10: [======                        ] 15/75 batches, loss: 0.0310Epoch 2/10: [======                        ] 16/75 batches, loss: 0.0314Epoch 2/10: [======                        ] 17/75 batches, loss: 0.0321Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.0321Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.0315Epoch 2/10: [========                      ] 20/75 batches, loss: 0.0308Epoch 2/10: [========                      ] 21/75 batches, loss: 0.0310Epoch 2/10: [========                      ] 22/75 batches, loss: 0.0310Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.0311Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.0311Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.0314Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.0318Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.0325Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.0321Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.0327Epoch 2/10: [============                  ] 30/75 batches, loss: 0.0328Epoch 2/10: [============                  ] 31/75 batches, loss: 0.0327Epoch 2/10: [============                  ] 32/75 batches, loss: 0.0329Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.0331Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.0327Epoch 2/10: [==============                ] 35/75 batches, loss: 0.0326Epoch 2/10: [==============                ] 36/75 batches, loss: 0.0325Epoch 2/10: [==============                ] 37/75 batches, loss: 0.0324Epoch 2/10: [===============               ] 38/75 batches, loss: 0.0321Epoch 2/10: [===============               ] 39/75 batches, loss: 0.0319Epoch 2/10: [================              ] 40/75 batches, loss: 0.0323Epoch 2/10: [================              ] 41/75 batches, loss: 0.0320Epoch 2/10: [================              ] 42/75 batches, loss: 0.0321Epoch 2/10: [=================             ] 43/75 batches, loss: 0.0323Epoch 2/10: [=================             ] 44/75 batches, loss: 0.0325Epoch 2/10: [==================            ] 45/75 batches, loss: 0.0323Epoch 2/10: [==================            ] 46/75 batches, loss: 0.0323Epoch 2/10: [==================            ] 47/75 batches, loss: 0.0321Epoch 2/10: [===================           ] 48/75 batches, loss: 0.0317Epoch 2/10: [===================           ] 49/75 batches, loss: 0.0319Epoch 2/10: [====================          ] 50/75 batches, loss: 0.0317Epoch 2/10: [====================          ] 51/75 batches, loss: 0.0316Epoch 2/10: [====================          ] 52/75 batches, loss: 0.0314Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.0313Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.0312Epoch 2/10: [======================        ] 55/75 batches, loss: 0.0317Epoch 2/10: [======================        ] 56/75 batches, loss: 0.0317Epoch 2/10: [======================        ] 57/75 batches, loss: 0.0320Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.0317Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.0316Epoch 2/10: [========================      ] 60/75 batches, loss: 0.0314Epoch 2/10: [========================      ] 61/75 batches, loss: 0.0313Epoch 2/10: [========================      ] 62/75 batches, loss: 0.0313Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.0315Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.0312Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.0310Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.0316Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.0317Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.0318Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.0316Epoch 2/10: [============================  ] 70/75 batches, loss: 0.0319Epoch 2/10: [============================  ] 71/75 batches, loss: 0.0319Epoch 2/10: [============================  ] 72/75 batches, loss: 0.0318Epoch 2/10: [============================= ] 73/75 batches, loss: 0.0319Epoch 2/10: [============================= ] 74/75 batches, loss: 0.0318Epoch 2/10: [==============================] 75/75 batches, loss: 0.0319
[2025-04-29 20:42:05,992][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0319
[2025-04-29 20:42:06,234][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1437, Metrics: {'mse': 0.14382046461105347, 'rmse': 0.3792366867947423, 'r2': -1.19370436668396}
[2025-04-29 20:42:06,234][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.0543Epoch 3/10: [                              ] 2/75 batches, loss: 0.0683Epoch 3/10: [=                             ] 3/75 batches, loss: 0.0562Epoch 3/10: [=                             ] 4/75 batches, loss: 0.0510Epoch 3/10: [==                            ] 5/75 batches, loss: 0.0472Epoch 3/10: [==                            ] 6/75 batches, loss: 0.0483Epoch 3/10: [==                            ] 7/75 batches, loss: 0.0437Epoch 3/10: [===                           ] 8/75 batches, loss: 0.0434Epoch 3/10: [===                           ] 9/75 batches, loss: 0.0436Epoch 3/10: [====                          ] 10/75 batches, loss: 0.0407Epoch 3/10: [====                          ] 11/75 batches, loss: 0.0397Epoch 3/10: [====                          ] 12/75 batches, loss: 0.0395Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.0408Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.0396Epoch 3/10: [======                        ] 15/75 batches, loss: 0.0386Epoch 3/10: [======                        ] 16/75 batches, loss: 0.0380Epoch 3/10: [======                        ] 17/75 batches, loss: 0.0385Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.0388Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.0388Epoch 3/10: [========                      ] 20/75 batches, loss: 0.0392Epoch 3/10: [========                      ] 21/75 batches, loss: 0.0389Epoch 3/10: [========                      ] 22/75 batches, loss: 0.0380Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.0373Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.0369Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.0374Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.0386Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.0385Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.0395Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.0390Epoch 3/10: [============                  ] 30/75 batches, loss: 0.0384Epoch 3/10: [============                  ] 31/75 batches, loss: 0.0382Epoch 3/10: [============                  ] 32/75 batches, loss: 0.0379Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.0380Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.0383Epoch 3/10: [==============                ] 35/75 batches, loss: 0.0380Epoch 3/10: [==============                ] 36/75 batches, loss: 0.0374Epoch 3/10: [==============                ] 37/75 batches, loss: 0.0377Epoch 3/10: [===============               ] 38/75 batches, loss: 0.0373Epoch 3/10: [===============               ] 39/75 batches, loss: 0.0369Epoch 3/10: [================              ] 40/75 batches, loss: 0.0366Epoch 3/10: [================              ] 41/75 batches, loss: 0.0361Epoch 3/10: [================              ] 42/75 batches, loss: 0.0361Epoch 3/10: [=================             ] 43/75 batches, loss: 0.0358Epoch 3/10: [=================             ] 44/75 batches, loss: 0.0356Epoch 3/10: [==================            ] 45/75 batches, loss: 0.0355Epoch 3/10: [==================            ] 46/75 batches, loss: 0.0351Epoch 3/10: [==================            ] 47/75 batches, loss: 0.0346Epoch 3/10: [===================           ] 48/75 batches, loss: 0.0346Epoch 3/10: [===================           ] 49/75 batches, loss: 0.0344Epoch 3/10: [====================          ] 50/75 batches, loss: 0.0341Epoch 3/10: [====================          ] 51/75 batches, loss: 0.0338Epoch 3/10: [====================          ] 52/75 batches, loss: 0.0334Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.0332Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.0332Epoch 3/10: [======================        ] 55/75 batches, loss: 0.0335Epoch 3/10: [======================        ] 56/75 batches, loss: 0.0334Epoch 3/10: [======================        ] 57/75 batches, loss: 0.0336Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.0332Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.0333Epoch 3/10: [========================      ] 60/75 batches, loss: 0.0329Epoch 3/10: [========================      ] 61/75 batches, loss: 0.0325Epoch 3/10: [========================      ] 62/75 batches, loss: 0.0322Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.0321Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.0318Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.0314Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.0312Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.0309Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.0307Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.0307Epoch 3/10: [============================  ] 70/75 batches, loss: 0.0305Epoch 3/10: [============================  ] 71/75 batches, loss: 0.0303Epoch 3/10: [============================  ] 72/75 batches, loss: 0.0301Epoch 3/10: [============================= ] 73/75 batches, loss: 0.0299Epoch 3/10: [============================= ] 74/75 batches, loss: 0.0299Epoch 3/10: [==============================] 75/75 batches, loss: 0.0299
[2025-04-29 20:42:13,132][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0299
[2025-04-29 20:42:13,384][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0610, Metrics: {'mse': 0.06082364544272423, 'rmse': 0.24662450292443414, 'r2': 0.07225227355957031}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.0220Epoch 4/10: [                              ] 2/75 batches, loss: 0.0191Epoch 4/10: [=                             ] 3/75 batches, loss: 0.0197Epoch 4/10: [=                             ] 4/75 batches, loss: 0.0191Epoch 4/10: [==                            ] 5/75 batches, loss: 0.0188Epoch 4/10: [==                            ] 6/75 batches, loss: 0.0192Epoch 4/10: [==                            ] 7/75 batches, loss: 0.0203Epoch 4/10: [===                           ] 8/75 batches, loss: 0.0201Epoch 4/10: [===                           ] 9/75 batches, loss: 0.0192Epoch 4/10: [====                          ] 10/75 batches, loss: 0.0190Epoch 4/10: [====                          ] 11/75 batches, loss: 0.0184Epoch 4/10: [====                          ] 12/75 batches, loss: 0.0182Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.0183Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.0177Epoch 4/10: [======                        ] 15/75 batches, loss: 0.0179Epoch 4/10: [======                        ] 16/75 batches, loss: 0.0176Epoch 4/10: [======                        ] 17/75 batches, loss: 0.0170Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.0176Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.0172Epoch 4/10: [========                      ] 20/75 batches, loss: 0.0172Epoch 4/10: [========                      ] 21/75 batches, loss: 0.0178Epoch 4/10: [========                      ] 22/75 batches, loss: 0.0189Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.0188Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.0187Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.0185Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.0185Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.0188Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.0188Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.0187Epoch 4/10: [============                  ] 30/75 batches, loss: 0.0193Epoch 4/10: [============                  ] 31/75 batches, loss: 0.0190Epoch 4/10: [============                  ] 32/75 batches, loss: 0.0189Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.0196Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.0202Epoch 4/10: [==============                ] 35/75 batches, loss: 0.0203Epoch 4/10: [==============                ] 36/75 batches, loss: 0.0205Epoch 4/10: [==============                ] 37/75 batches, loss: 0.0203Epoch 4/10: [===============               ] 38/75 batches, loss: 0.0202Epoch 4/10: [===============               ] 39/75 batches, loss: 0.0201Epoch 4/10: [================              ] 40/75 batches, loss: 0.0197Epoch 4/10: [================              ] 41/75 batches, loss: 0.0197Epoch 4/10: [================              ] 42/75 batches, loss: 0.0197Epoch 4/10: [=================             ] 43/75 batches, loss: 0.0196Epoch 4/10: [=================             ] 44/75 batches, loss: 0.0194Epoch 4/10: [==================            ] 45/75 batches, loss: 0.0192Epoch 4/10: [==================            ] 46/75 batches, loss: 0.0194Epoch 4/10: [==================            ] 47/75 batches, loss: 0.0193Epoch 4/10: [===================           ] 48/75 batches, loss: 0.0193Epoch 4/10: [===================           ] 49/75 batches, loss: 0.0193Epoch 4/10: [====================          ] 50/75 batches, loss: 0.0193Epoch 4/10: [====================          ] 51/75 batches, loss: 0.0192Epoch 4/10: [====================          ] 52/75 batches, loss: 0.0190Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.0189Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.0188Epoch 4/10: [======================        ] 55/75 batches, loss: 0.0188Epoch 4/10: [======================        ] 56/75 batches, loss: 0.0189Epoch 4/10: [======================        ] 57/75 batches, loss: 0.0187Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.0188Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.0189Epoch 4/10: [========================      ] 60/75 batches, loss: 0.0190Epoch 4/10: [========================      ] 61/75 batches, loss: 0.0190Epoch 4/10: [========================      ] 62/75 batches, loss: 0.0189Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.0189Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.0187Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.0188Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.0188Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.0188Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.0187Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.0187Epoch 4/10: [============================  ] 70/75 batches, loss: 0.0186Epoch 4/10: [============================  ] 71/75 batches, loss: 0.0186Epoch 4/10: [============================  ] 72/75 batches, loss: 0.0186Epoch 4/10: [============================= ] 73/75 batches, loss: 0.0187Epoch 4/10: [============================= ] 74/75 batches, loss: 0.0186Epoch 4/10: [==============================] 75/75 batches, loss: 0.0185
[2025-04-29 20:42:20,703][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0185
[2025-04-29 20:42:20,942][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0399, Metrics: {'mse': 0.03984874114394188, 'rmse': 0.19962149469418838, 'r2': 0.39218413829803467}
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.0190Epoch 5/10: [                              ] 2/75 batches, loss: 0.0147Epoch 5/10: [=                             ] 3/75 batches, loss: 0.0140Epoch 5/10: [=                             ] 4/75 batches, loss: 0.0141Epoch 5/10: [==                            ] 5/75 batches, loss: 0.0132Epoch 5/10: [==                            ] 6/75 batches, loss: 0.0121Epoch 5/10: [==                            ] 7/75 batches, loss: 0.0117Epoch 5/10: [===                           ] 8/75 batches, loss: 0.0112Epoch 5/10: [===                           ] 9/75 batches, loss: 0.0106Epoch 5/10: [====                          ] 10/75 batches, loss: 0.0107Epoch 5/10: [====                          ] 11/75 batches, loss: 0.0108Epoch 5/10: [====                          ] 12/75 batches, loss: 0.0111Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.0122Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.0131Epoch 5/10: [======                        ] 15/75 batches, loss: 0.0128Epoch 5/10: [======                        ] 16/75 batches, loss: 0.0132Epoch 5/10: [======                        ] 17/75 batches, loss: 0.0131Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.0134Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.0133Epoch 5/10: [========                      ] 20/75 batches, loss: 0.0131Epoch 5/10: [========                      ] 21/75 batches, loss: 0.0129Epoch 5/10: [========                      ] 22/75 batches, loss: 0.0127Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.0126Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.0130Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.0129Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.0137Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.0136Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.0136Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.0133Epoch 5/10: [============                  ] 30/75 batches, loss: 0.0132Epoch 5/10: [============                  ] 31/75 batches, loss: 0.0133Epoch 5/10: [============                  ] 32/75 batches, loss: 0.0131Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.0131Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.0130Epoch 5/10: [==============                ] 35/75 batches, loss: 0.0128Epoch 5/10: [==============                ] 36/75 batches, loss: 0.0135Epoch 5/10: [==============                ] 37/75 batches, loss: 0.0133Epoch 5/10: [===============               ] 38/75 batches, loss: 0.0137Epoch 5/10: [===============               ] 39/75 batches, loss: 0.0136Epoch 5/10: [================              ] 40/75 batches, loss: 0.0137Epoch 5/10: [================              ] 41/75 batches, loss: 0.0139Epoch 5/10: [================              ] 42/75 batches, loss: 0.0140Epoch 5/10: [=================             ] 43/75 batches, loss: 0.0147Epoch 5/10: [=================             ] 44/75 batches, loss: 0.0147Epoch 5/10: [==================            ] 45/75 batches, loss: 0.0147Epoch 5/10: [==================            ] 46/75 batches, loss: 0.0147Epoch 5/10: [==================            ] 47/75 batches, loss: 0.0146Epoch 5/10: [===================           ] 48/75 batches, loss: 0.0147Epoch 5/10: [===================           ] 49/75 batches, loss: 0.0146Epoch 5/10: [====================          ] 50/75 batches, loss: 0.0146Epoch 5/10: [====================          ] 51/75 batches, loss: 0.0148Epoch 5/10: [====================          ] 52/75 batches, loss: 0.0148Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.0148Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.0146Epoch 5/10: [======================        ] 55/75 batches, loss: 0.0146Epoch 5/10: [======================        ] 56/75 batches, loss: 0.0145Epoch 5/10: [======================        ] 57/75 batches, loss: 0.0144Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.0143Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.0143Epoch 5/10: [========================      ] 60/75 batches, loss: 0.0142Epoch 5/10: [========================      ] 61/75 batches, loss: 0.0142Epoch 5/10: [========================      ] 62/75 batches, loss: 0.0143Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.0142Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.0141Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.0141Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.0141Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.0141Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.0140Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.0140Epoch 5/10: [============================  ] 70/75 batches, loss: 0.0139Epoch 5/10: [============================  ] 71/75 batches, loss: 0.0139Epoch 5/10: [============================  ] 72/75 batches, loss: 0.0138Epoch 5/10: [============================= ] 73/75 batches, loss: 0.0137Epoch 5/10: [============================= ] 74/75 batches, loss: 0.0139Epoch 5/10: [==============================] 75/75 batches, loss: 0.0140
[2025-04-29 20:42:28,267][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0140
[2025-04-29 20:42:28,535][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0309, Metrics: {'mse': 0.030817484483122826, 'rmse': 0.17554909422472914, 'r2': 0.5299385786056519}
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.0096Epoch 6/10: [                              ] 2/75 batches, loss: 0.0139Epoch 6/10: [=                             ] 3/75 batches, loss: 0.0151Epoch 6/10: [=                             ] 4/75 batches, loss: 0.0182Epoch 6/10: [==                            ] 5/75 batches, loss: 0.0199Epoch 6/10: [==                            ] 6/75 batches, loss: 0.0199Epoch 6/10: [==                            ] 7/75 batches, loss: 0.0191Epoch 6/10: [===                           ] 8/75 batches, loss: 0.0199Epoch 6/10: [===                           ] 9/75 batches, loss: 0.0187Epoch 6/10: [====                          ] 10/75 batches, loss: 0.0177Epoch 6/10: [====                          ] 11/75 batches, loss: 0.0171Epoch 6/10: [====                          ] 12/75 batches, loss: 0.0168Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.0165Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.0162Epoch 6/10: [======                        ] 15/75 batches, loss: 0.0163Epoch 6/10: [======                        ] 16/75 batches, loss: 0.0165Epoch 6/10: [======                        ] 17/75 batches, loss: 0.0159Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.0152Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.0148Epoch 6/10: [========                      ] 20/75 batches, loss: 0.0143Epoch 6/10: [========                      ] 21/75 batches, loss: 0.0143Epoch 6/10: [========                      ] 22/75 batches, loss: 0.0141Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.0138Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.0140Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.0139Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.0142Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.0139Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.0138Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.0138Epoch 6/10: [============                  ] 30/75 batches, loss: 0.0137Epoch 6/10: [============                  ] 31/75 batches, loss: 0.0134Epoch 6/10: [============                  ] 32/75 batches, loss: 0.0133Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.0133Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.0133Epoch 6/10: [==============                ] 35/75 batches, loss: 0.0137Epoch 6/10: [==============                ] 36/75 batches, loss: 0.0137Epoch 6/10: [==============                ] 37/75 batches, loss: 0.0141Epoch 6/10: [===============               ] 38/75 batches, loss: 0.0139Epoch 6/10: [===============               ] 39/75 batches, loss: 0.0138Epoch 6/10: [================              ] 40/75 batches, loss: 0.0138Epoch 6/10: [================              ] 41/75 batches, loss: 0.0137Epoch 6/10: [================              ] 42/75 batches, loss: 0.0137Epoch 6/10: [=================             ] 43/75 batches, loss: 0.0136Epoch 6/10: [=================             ] 44/75 batches, loss: 0.0136Epoch 6/10: [==================            ] 45/75 batches, loss: 0.0136Epoch 6/10: [==================            ] 46/75 batches, loss: 0.0136Epoch 6/10: [==================            ] 47/75 batches, loss: 0.0134Epoch 6/10: [===================           ] 48/75 batches, loss: 0.0133Epoch 6/10: [===================           ] 49/75 batches, loss: 0.0132Epoch 6/10: [====================          ] 50/75 batches, loss: 0.0135Epoch 6/10: [====================          ] 51/75 batches, loss: 0.0133Epoch 6/10: [====================          ] 52/75 batches, loss: 0.0133Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.0132Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.0132Epoch 6/10: [======================        ] 55/75 batches, loss: 0.0131Epoch 6/10: [======================        ] 56/75 batches, loss: 0.0132Epoch 6/10: [======================        ] 57/75 batches, loss: 0.0130Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.0129Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.0128Epoch 6/10: [========================      ] 60/75 batches, loss: 0.0128Epoch 6/10: [========================      ] 61/75 batches, loss: 0.0127Epoch 6/10: [========================      ] 62/75 batches, loss: 0.0128Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.0128Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.0128Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.0130Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.0130Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.0132Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.0133Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.0133Epoch 6/10: [============================  ] 70/75 batches, loss: 0.0133Epoch 6/10: [============================  ] 71/75 batches, loss: 0.0132Epoch 6/10: [============================  ] 72/75 batches, loss: 0.0131Epoch 6/10: [============================= ] 73/75 batches, loss: 0.0131Epoch 6/10: [============================= ] 74/75 batches, loss: 0.0130Epoch 6/10: [==============================] 75/75 batches, loss: 0.0129
[2025-04-29 20:42:35,794][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0129
[2025-04-29 20:42:36,300][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0617, Metrics: {'mse': 0.06192290410399437, 'rmse': 0.24884313151862233, 'r2': 0.05548518896102905}
[2025-04-29 20:42:36,301][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.0050Epoch 7/10: [                              ] 2/75 batches, loss: 0.0072Epoch 7/10: [=                             ] 3/75 batches, loss: 0.0079Epoch 7/10: [=                             ] 4/75 batches, loss: 0.0080Epoch 7/10: [==                            ] 5/75 batches, loss: 0.0084Epoch 7/10: [==                            ] 6/75 batches, loss: 0.0101Epoch 7/10: [==                            ] 7/75 batches, loss: 0.0148Epoch 7/10: [===                           ] 8/75 batches, loss: 0.0139Epoch 7/10: [===                           ] 9/75 batches, loss: 0.0146Epoch 7/10: [====                          ] 10/75 batches, loss: 0.0139Epoch 7/10: [====                          ] 11/75 batches, loss: 0.0134Epoch 7/10: [====                          ] 12/75 batches, loss: 0.0128Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.0127Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.0123Epoch 7/10: [======                        ] 15/75 batches, loss: 0.0123Epoch 7/10: [======                        ] 16/75 batches, loss: 0.0119Epoch 7/10: [======                        ] 17/75 batches, loss: 0.0117Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.0117Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.0118Epoch 7/10: [========                      ] 20/75 batches, loss: 0.0116Epoch 7/10: [========                      ] 21/75 batches, loss: 0.0112Epoch 7/10: [========                      ] 22/75 batches, loss: 0.0112Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.0111Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.0108Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.0110Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.0113Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.0112Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.0111Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.0109Epoch 7/10: [============                  ] 30/75 batches, loss: 0.0108Epoch 7/10: [============                  ] 31/75 batches, loss: 0.0106Epoch 7/10: [============                  ] 32/75 batches, loss: 0.0105Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.0104Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.0103Epoch 7/10: [==============                ] 35/75 batches, loss: 0.0103Epoch 7/10: [==============                ] 36/75 batches, loss: 0.0103Epoch 7/10: [==============                ] 37/75 batches, loss: 0.0102Epoch 7/10: [===============               ] 38/75 batches, loss: 0.0102Epoch 7/10: [===============               ] 39/75 batches, loss: 0.0102Epoch 7/10: [================              ] 40/75 batches, loss: 0.0101Epoch 7/10: [================              ] 41/75 batches, loss: 0.0103Epoch 7/10: [================              ] 42/75 batches, loss: 0.0101Epoch 7/10: [=================             ] 43/75 batches, loss: 0.0102Epoch 7/10: [=================             ] 44/75 batches, loss: 0.0104Epoch 7/10: [==================            ] 45/75 batches, loss: 0.0105Epoch 7/10: [==================            ] 46/75 batches, loss: 0.0104Epoch 7/10: [==================            ] 47/75 batches, loss: 0.0104Epoch 7/10: [===================           ] 48/75 batches, loss: 0.0104Epoch 7/10: [===================           ] 49/75 batches, loss: 0.0103Epoch 7/10: [====================          ] 50/75 batches, loss: 0.0105Epoch 7/10: [====================          ] 51/75 batches, loss: 0.0104Epoch 7/10: [====================          ] 52/75 batches, loss: 0.0104Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.0103Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.0102Epoch 7/10: [======================        ] 55/75 batches, loss: 0.0101Epoch 7/10: [======================        ] 56/75 batches, loss: 0.0101Epoch 7/10: [======================        ] 57/75 batches, loss: 0.0101Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.0101Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.0102Epoch 7/10: [========================      ] 60/75 batches, loss: 0.0102Epoch 7/10: [========================      ] 61/75 batches, loss: 0.0101Epoch 7/10: [========================      ] 62/75 batches, loss: 0.0100Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.0100Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.0101Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.0102Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.0102Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.0102Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.0102Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.0102Epoch 7/10: [============================  ] 70/75 batches, loss: 0.0102Epoch 7/10: [============================  ] 71/75 batches, loss: 0.0105Epoch 7/10: [============================  ] 72/75 batches, loss: 0.0104Epoch 7/10: [============================= ] 73/75 batches, loss: 0.0106Epoch 7/10: [============================= ] 74/75 batches, loss: 0.0106Epoch 7/10: [==============================] 75/75 batches, loss: 0.0105
[2025-04-29 20:42:43,454][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0105
[2025-04-29 20:42:43,716][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0323, Metrics: {'mse': 0.03219188377261162, 'rmse': 0.1794209680405599, 'r2': 0.5089747905731201}
[2025-04-29 20:42:43,717][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/75 batches, loss: 0.0100Epoch 8/10: [                              ] 2/75 batches, loss: 0.0097Epoch 8/10: [=                             ] 3/75 batches, loss: 0.0104Epoch 8/10: [=                             ] 4/75 batches, loss: 0.0094Epoch 8/10: [==                            ] 5/75 batches, loss: 0.0105Epoch 8/10: [==                            ] 6/75 batches, loss: 0.0093Epoch 8/10: [==                            ] 7/75 batches, loss: 0.0097Epoch 8/10: [===                           ] 8/75 batches, loss: 0.0097Epoch 8/10: [===                           ] 9/75 batches, loss: 0.0095Epoch 8/10: [====                          ] 10/75 batches, loss: 0.0094Epoch 8/10: [====                          ] 11/75 batches, loss: 0.0093Epoch 8/10: [====                          ] 12/75 batches, loss: 0.0091Epoch 8/10: [=====                         ] 13/75 batches, loss: 0.0099Epoch 8/10: [=====                         ] 14/75 batches, loss: 0.0097Epoch 8/10: [======                        ] 15/75 batches, loss: 0.0101Epoch 8/10: [======                        ] 16/75 batches, loss: 0.0100Epoch 8/10: [======                        ] 17/75 batches, loss: 0.0099Epoch 8/10: [=======                       ] 18/75 batches, loss: 0.0100Epoch 8/10: [=======                       ] 19/75 batches, loss: 0.0097Epoch 8/10: [========                      ] 20/75 batches, loss: 0.0098Epoch 8/10: [========                      ] 21/75 batches, loss: 0.0098Epoch 8/10: [========                      ] 22/75 batches, loss: 0.0097Epoch 8/10: [=========                     ] 23/75 batches, loss: 0.0096Epoch 8/10: [=========                     ] 24/75 batches, loss: 0.0094Epoch 8/10: [==========                    ] 25/75 batches, loss: 0.0095Epoch 8/10: [==========                    ] 26/75 batches, loss: 0.0093Epoch 8/10: [==========                    ] 27/75 batches, loss: 0.0096Epoch 8/10: [===========                   ] 28/75 batches, loss: 0.0096Epoch 8/10: [===========                   ] 29/75 batches, loss: 0.0095Epoch 8/10: [============                  ] 30/75 batches, loss: 0.0094Epoch 8/10: [============                  ] 31/75 batches, loss: 0.0093Epoch 8/10: [============                  ] 32/75 batches, loss: 0.0092Epoch 8/10: [=============                 ] 33/75 batches, loss: 0.0091Epoch 8/10: [=============                 ] 34/75 batches, loss: 0.0091Epoch 8/10: [==============                ] 35/75 batches, loss: 0.0091Epoch 8/10: [==============                ] 36/75 batches, loss: 0.0090Epoch 8/10: [==============                ] 37/75 batches, loss: 0.0093Epoch 8/10: [===============               ] 38/75 batches, loss: 0.0092Epoch 8/10: [===============               ] 39/75 batches, loss: 0.0091Epoch 8/10: [================              ] 40/75 batches, loss: 0.0092Epoch 8/10: [================              ] 41/75 batches, loss: 0.0093Epoch 8/10: [================              ] 42/75 batches, loss: 0.0093Epoch 8/10: [=================             ] 43/75 batches, loss: 0.0093Epoch 8/10: [=================             ] 44/75 batches, loss: 0.0093Epoch 8/10: [==================            ] 45/75 batches, loss: 0.0092Epoch 8/10: [==================            ] 46/75 batches, loss: 0.0092Epoch 8/10: [==================            ] 47/75 batches, loss: 0.0091Epoch 8/10: [===================           ] 48/75 batches, loss: 0.0091Epoch 8/10: [===================           ] 49/75 batches, loss: 0.0092Epoch 8/10: [====================          ] 50/75 batches, loss: 0.0094Epoch 8/10: [====================          ] 51/75 batches, loss: 0.0093Epoch 8/10: [====================          ] 52/75 batches, loss: 0.0093Epoch 8/10: [=====================         ] 53/75 batches, loss: 0.0093Epoch 8/10: [=====================         ] 54/75 batches, loss: 0.0093Epoch 8/10: [======================        ] 55/75 batches, loss: 0.0093Epoch 8/10: [======================        ] 56/75 batches, loss: 0.0093Epoch 8/10: [======================        ] 57/75 batches, loss: 0.0094Epoch 8/10: [=======================       ] 58/75 batches, loss: 0.0093Epoch 8/10: [=======================       ] 59/75 batches, loss: 0.0093Epoch 8/10: [========================      ] 60/75 batches, loss: 0.0094Epoch 8/10: [========================      ] 61/75 batches, loss: 0.0094Epoch 8/10: [========================      ] 62/75 batches, loss: 0.0095Epoch 8/10: [=========================     ] 63/75 batches, loss: 0.0095Epoch 8/10: [=========================     ] 64/75 batches, loss: 0.0095Epoch 8/10: [==========================    ] 65/75 batches, loss: 0.0096Epoch 8/10: [==========================    ] 66/75 batches, loss: 0.0096Epoch 8/10: [==========================    ] 67/75 batches, loss: 0.0095Epoch 8/10: [===========================   ] 68/75 batches, loss: 0.0095Epoch 8/10: [===========================   ] 69/75 batches, loss: 0.0096Epoch 8/10: [============================  ] 70/75 batches, loss: 0.0097Epoch 8/10: [============================  ] 71/75 batches, loss: 0.0096Epoch 8/10: [============================  ] 72/75 batches, loss: 0.0097Epoch 8/10: [============================= ] 73/75 batches, loss: 0.0097Epoch 8/10: [============================= ] 74/75 batches, loss: 0.0097Epoch 8/10: [==============================] 75/75 batches, loss: 0.0096
[2025-04-29 20:42:50,658][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0096
[2025-04-29 20:42:50,914][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0619, Metrics: {'mse': 0.06191669777035713, 'rmse': 0.24883066083253713, 'r2': 0.05557984113693237}
[2025-04-29 20:42:50,915][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 20:42:50,915][src.training.lm_trainer][INFO] - Early stopping at epoch 8
[2025-04-29 20:42:50,915][src.training.lm_trainer][INFO] - Training completed in 60.06 seconds
[2025-04-29 20:42:50,915][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 20:42:53,908][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.007385095115751028, 'rmse': 0.08593657612303988, 'r2': 0.6345468163490295}
[2025-04-29 20:42:53,908][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.030817484483122826, 'rmse': 0.17554909422472914, 'r2': 0.5299385786056519}
[2025-04-29 20:42:53,908][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.019965143874287605, 'rmse': 0.14129806748249463, 'r2': 0.49438703060150146}
[2025-04-29 20:42:55,538][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/fi/fi/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁
wandb:     best_val_mse █▄▂▁
wandb:      best_val_r2 ▁▅▇█
wandb:    best_val_rmse █▄▂▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▅▅▃▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▆█▃▂▁▃▁▃
wandb:          val_mse ▆█▃▂▁▃▁▃
wandb:           val_r2 ▃▁▆▇█▆█▆
wandb:         val_rmse ▆█▃▂▁▄▁▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03087
wandb:     best_val_mse 0.03082
wandb:      best_val_r2 0.52994
wandb:    best_val_rmse 0.17555
wandb:            epoch 8
wandb:   final_test_mse 0.01997
wandb:    final_test_r2 0.49439
wandb:  final_test_rmse 0.1413
wandb:  final_train_mse 0.00739
wandb:   final_train_r2 0.63455
wandb: final_train_rmse 0.08594
wandb:    final_val_mse 0.03082
wandb:     final_val_r2 0.52994
wandb:   final_val_rmse 0.17555
wandb:    learning_rate 2e-05
wandb:       train_loss 0.00957
wandb:       train_time 60.05575
wandb:         val_loss 0.06193
wandb:          val_mse 0.06192
wandb:           val_r2 0.05558
wandb:         val_rmse 0.24883
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_204137-xqhjvd5b
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_204137-xqhjvd5b/logs
Experiment finetune_complexity_fi completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/fi/results.json
Running control finetuning experiments...
Running experiment: finetune_question_type_control1_ar
Command: python -m src.experiments.run_experiment             "hydra.job.chdir=False"             "hydra.run.dir=."             "experiment=finetune"             "experiment.tasks=question_type"             "experiment.use_controls=true"             "experiment.control_index=1"             "model=glot500_finetune"             "model.lm_name=cis-lmu/glot500-base"             "model.dropout=0.1"             "model.freeze_model=false"             "model.finetune=true"             "data.languages=[ar]"             "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"             "training.task_type=classification"             "training.num_epochs=10"             "training.batch_size=16"             "training.lr=2e-5"             "+training.gradient_accumulation_steps=2"                          "experiment_name=finetune_question_type_control1_ar"             "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control1"             "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 20:43:12,016][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control1
experiment_name: finetune_question_type_control1_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_finetune
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  finetune: true
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1
    lr: 2.0e-05
    batch_size: 8

[2025-04-29 20:43:12,016][__main__][INFO] - Normalized task: question_type
[2025-04-29 20:43:12,016][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 20:43:12,016][__main__][INFO] - Determined Task Type: classification
[2025-04-29 20:43:12,020][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 20:43:12,021][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 20:43:14,207][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 20:43:16,487][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 20:43:16,488][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:43:16,568][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-04-29 20:43:16,610][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-04-29 20:43:16,745][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 20:43:16,752][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:43:16,752][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 20:43:16,754][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:43:16,788][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:43:16,838][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:43:16,857][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 20:43:16,858][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:43:16,858][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 20:43:16,859][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:43:16,895][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:43:16,939][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:43:16,956][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 20:43:16,957][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:43:16,958][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 20:43:16,960][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 20:43:16,960][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:43:16,960][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:43:16,960][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:43:16,961][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:43:16,961][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 20:43:16,961][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 20:43:16,961][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 20:43:16,961][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:43:16,961][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:43:16,961][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:43:16,961][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:43:16,961][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:43:16,962][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 20:43:16,962][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 20:43:16,962][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 20:43:16,962][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:43:16,962][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:43:16,962][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:43:16,962][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:43:16,962][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:43:16,962][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 20:43:16,962][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 20:43:16,963][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 20:43:16,963][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:43:16,963][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 20:43:16,963][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 20:43:16,963][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 20:43:16,963][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 20:43:22,045][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 20:43:22,046][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,046][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,046][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,046][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,046][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,046][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,046][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,047][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,047][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,047][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,047][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,047][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,047][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,047][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,047][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,047][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,047][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,047][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,047][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,047][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,047][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,047][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,048][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,048][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,048][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,048][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,048][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,048][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,048][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,048][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,048][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,048][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,048][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,048][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,048][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,048][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,048][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,049][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,049][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,049][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,049][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,049][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,049][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,049][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,049][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,049][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,049][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,049][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,049][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,049][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,049][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:43:22,060][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 20:43:22,060][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 20:43:22,061][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 20:43:22,061][__main__][INFO] - Successfully created model for ar
[2025-04-29 20:43:22,061][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.6958Epoch 1/10: [                              ] 2/63 batches, loss: 0.6843Epoch 1/10: [=                             ] 3/63 batches, loss: 0.6826Epoch 1/10: [=                             ] 4/63 batches, loss: 0.6863Epoch 1/10: [==                            ] 5/63 batches, loss: 0.6900Epoch 1/10: [==                            ] 6/63 batches, loss: 0.6941Epoch 1/10: [===                           ] 7/63 batches, loss: 0.6971Epoch 1/10: [===                           ] 8/63 batches, loss: 0.6987Epoch 1/10: [====                          ] 9/63 batches, loss: 0.6999Epoch 1/10: [====                          ] 10/63 batches, loss: 0.7021Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.7037Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.7023Epoch 1/10: [======                        ] 13/63 batches, loss: 0.7014Epoch 1/10: [======                        ] 14/63 batches, loss: 0.7022Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.7013Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.7001Epoch 1/10: [========                      ] 17/63 batches, loss: 0.6988Epoch 1/10: [========                      ] 18/63 batches, loss: 0.6992Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.6983Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.7007Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.7000Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.7009Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.6997Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.7004Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.7019Epoch 1/10: [============                  ] 26/63 batches, loss: 0.7021Epoch 1/10: [============                  ] 27/63 batches, loss: 0.7028Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.7026Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.7023Epoch 1/10: [==============                ] 30/63 batches, loss: 0.7028Epoch 1/10: [==============                ] 31/63 batches, loss: 0.7029Epoch 1/10: [===============               ] 32/63 batches, loss: 0.7037Epoch 1/10: [===============               ] 33/63 batches, loss: 0.7033Epoch 1/10: [================              ] 34/63 batches, loss: 0.7028Epoch 1/10: [================              ] 35/63 batches, loss: 0.7037Epoch 1/10: [=================             ] 36/63 batches, loss: 0.7027Epoch 1/10: [=================             ] 37/63 batches, loss: 0.7026Epoch 1/10: [==================            ] 38/63 batches, loss: 0.7026Epoch 1/10: [==================            ] 39/63 batches, loss: 0.7024Epoch 1/10: [===================           ] 40/63 batches, loss: 0.7021Epoch 1/10: [===================           ] 41/63 batches, loss: 0.7017Epoch 1/10: [====================          ] 42/63 batches, loss: 0.7013Epoch 1/10: [====================          ] 43/63 batches, loss: 0.7009Epoch 1/10: [====================          ] 44/63 batches, loss: 0.7006Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.7000Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.6999Epoch 1/10: [======================        ] 47/63 batches, loss: 0.7001Epoch 1/10: [======================        ] 48/63 batches, loss: 0.7003Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.7002Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.7002Epoch 1/10: [========================      ] 51/63 batches, loss: 0.7007Epoch 1/10: [========================      ] 52/63 batches, loss: 0.7005Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.6999Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.6995Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.6998Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.7000Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.6997Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.6994Epoch 1/10: [============================  ] 59/63 batches, loss: 0.6993Epoch 1/10: [============================  ] 60/63 batches, loss: 0.6990Epoch 1/10: [============================= ] 61/63 batches, loss: 0.6987Epoch 1/10: [============================= ] 62/63 batches, loss: 0.6987Epoch 1/10: [==============================] 63/63 batches, loss: 0.6995
[2025-04-29 20:43:31,439][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6995
[2025-04-29 20:43:31,634][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6999, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.7305Epoch 2/10: [                              ] 2/63 batches, loss: 0.7058Epoch 2/10: [=                             ] 3/63 batches, loss: 0.6996Epoch 2/10: [=                             ] 4/63 batches, loss: 0.7002Epoch 2/10: [==                            ] 5/63 batches, loss: 0.6978Epoch 2/10: [==                            ] 6/63 batches, loss: 0.6995Epoch 2/10: [===                           ] 7/63 batches, loss: 0.7006Epoch 2/10: [===                           ] 8/63 batches, loss: 0.6991Epoch 2/10: [====                          ] 9/63 batches, loss: 0.7008Epoch 2/10: [====                          ] 10/63 batches, loss: 0.7010Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.6998Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.6987Epoch 2/10: [======                        ] 13/63 batches, loss: 0.6976Epoch 2/10: [======                        ] 14/63 batches, loss: 0.6982Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.6995Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.6990Epoch 2/10: [========                      ] 17/63 batches, loss: 0.7008Epoch 2/10: [========                      ] 18/63 batches, loss: 0.7008Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.7005Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.6992Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.6989Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.6987Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.6989Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.6990Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.6987Epoch 2/10: [============                  ] 26/63 batches, loss: 0.6986Epoch 2/10: [============                  ] 27/63 batches, loss: 0.6984Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.6979Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.6977Epoch 2/10: [==============                ] 30/63 batches, loss: 0.6969Epoch 2/10: [==============                ] 31/63 batches, loss: 0.6968Epoch 2/10: [===============               ] 32/63 batches, loss: 0.6964Epoch 2/10: [===============               ] 33/63 batches, loss: 0.6969Epoch 2/10: [================              ] 34/63 batches, loss: 0.6966Epoch 2/10: [================              ] 35/63 batches, loss: 0.6963Epoch 2/10: [=================             ] 36/63 batches, loss: 0.6961Epoch 2/10: [=================             ] 37/63 batches, loss: 0.6960Epoch 2/10: [==================            ] 38/63 batches, loss: 0.6958Epoch 2/10: [==================            ] 39/63 batches, loss: 0.6959Epoch 2/10: [===================           ] 40/63 batches, loss: 0.6963Epoch 2/10: [===================           ] 41/63 batches, loss: 0.6965Epoch 2/10: [====================          ] 42/63 batches, loss: 0.6962Epoch 2/10: [====================          ] 43/63 batches, loss: 0.6949Epoch 2/10: [====================          ] 44/63 batches, loss: 0.6950Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.6952Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.6949Epoch 2/10: [======================        ] 47/63 batches, loss: 0.6950Epoch 2/10: [======================        ] 48/63 batches, loss: 0.6949Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.6948Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.6942Epoch 2/10: [========================      ] 51/63 batches, loss: 0.6942Epoch 2/10: [========================      ] 52/63 batches, loss: 0.6943Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.6946Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.6943Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.6939Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.6939Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.6938Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.6936Epoch 2/10: [============================  ] 59/63 batches, loss: 0.6939Epoch 2/10: [============================  ] 60/63 batches, loss: 0.6942Epoch 2/10: [============================= ] 61/63 batches, loss: 0.6945Epoch 2/10: [============================= ] 62/63 batches, loss: 0.6947Epoch 2/10: [==============================] 63/63 batches, loss: 0.6944
[2025-04-29 20:43:37,822][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6944
[2025-04-29 20:43:38,027][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6989, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.6939Epoch 3/10: [                              ] 2/63 batches, loss: 0.7006Epoch 3/10: [=                             ] 3/63 batches, loss: 0.6965Epoch 3/10: [=                             ] 4/63 batches, loss: 0.6923Epoch 3/10: [==                            ] 5/63 batches, loss: 0.6903Epoch 3/10: [==                            ] 6/63 batches, loss: 0.6928Epoch 3/10: [===                           ] 7/63 batches, loss: 0.6964Epoch 3/10: [===                           ] 8/63 batches, loss: 0.6970Epoch 3/10: [====                          ] 9/63 batches, loss: 0.6986Epoch 3/10: [====                          ] 10/63 batches, loss: 0.6986Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.6975Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.6968Epoch 3/10: [======                        ] 13/63 batches, loss: 0.6971Epoch 3/10: [======                        ] 14/63 batches, loss: 0.6972Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.6973Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.6964Epoch 3/10: [========                      ] 17/63 batches, loss: 0.6966Epoch 3/10: [========                      ] 18/63 batches, loss: 0.6960Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.6961Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.6954Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.6960Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.6955Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.6955Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.6950Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.6950Epoch 3/10: [============                  ] 26/63 batches, loss: 0.6955Epoch 3/10: [============                  ] 27/63 batches, loss: 0.6952Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.6952Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.6948Epoch 3/10: [==============                ] 30/63 batches, loss: 0.6945Epoch 3/10: [==============                ] 31/63 batches, loss: 0.6949Epoch 3/10: [===============               ] 32/63 batches, loss: 0.6950Epoch 3/10: [===============               ] 33/63 batches, loss: 0.6951Epoch 3/10: [================              ] 34/63 batches, loss: 0.6953Epoch 3/10: [================              ] 35/63 batches, loss: 0.6951Epoch 3/10: [=================             ] 36/63 batches, loss: 0.6947Epoch 3/10: [=================             ] 37/63 batches, loss: 0.6954Epoch 3/10: [==================            ] 38/63 batches, loss: 0.6964Epoch 3/10: [==================            ] 39/63 batches, loss: 0.6964Epoch 3/10: [===================           ] 40/63 batches, loss: 0.6967Epoch 3/10: [===================           ] 41/63 batches, loss: 0.6968Epoch 3/10: [====================          ] 42/63 batches, loss: 0.6966Epoch 3/10: [====================          ] 43/63 batches, loss: 0.6967Epoch 3/10: [====================          ] 44/63 batches, loss: 0.6969Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.6980Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.6975Epoch 3/10: [======================        ] 47/63 batches, loss: 0.6979Epoch 3/10: [======================        ] 48/63 batches, loss: 0.6978Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.6978Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.6973Epoch 3/10: [========================      ] 51/63 batches, loss: 0.6975Epoch 3/10: [========================      ] 52/63 batches, loss: 0.6971Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.6974Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.6975Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.6974Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.6972Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.6972Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.6965Epoch 3/10: [============================  ] 59/63 batches, loss: 0.6968Epoch 3/10: [============================  ] 60/63 batches, loss: 0.6967Epoch 3/10: [============================= ] 61/63 batches, loss: 0.6968Epoch 3/10: [============================= ] 62/63 batches, loss: 0.6967Epoch 3/10: [==============================] 63/63 batches, loss: 0.6965
[2025-04-29 20:43:44,251][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6965
[2025-04-29 20:43:44,465][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6987, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.7001Epoch 4/10: [                              ] 2/63 batches, loss: 0.7044Epoch 4/10: [=                             ] 3/63 batches, loss: 0.7042Epoch 4/10: [=                             ] 4/63 batches, loss: 0.7005Epoch 4/10: [==                            ] 5/63 batches, loss: 0.6997Epoch 4/10: [==                            ] 6/63 batches, loss: 0.6981Epoch 4/10: [===                           ] 7/63 batches, loss: 0.6990Epoch 4/10: [===                           ] 8/63 batches, loss: 0.7006Epoch 4/10: [====                          ] 9/63 batches, loss: 0.7017Epoch 4/10: [====                          ] 10/63 batches, loss: 0.7006Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.6966Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.6965Epoch 4/10: [======                        ] 13/63 batches, loss: 0.6980Epoch 4/10: [======                        ] 14/63 batches, loss: 0.6976Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.6965Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.6994Epoch 4/10: [========                      ] 17/63 batches, loss: 0.6995Epoch 4/10: [========                      ] 18/63 batches, loss: 0.7005Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.7009Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.6994Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.7006Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.7000Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.7000Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.6997Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.6982Epoch 4/10: [============                  ] 26/63 batches, loss: 0.6990Epoch 4/10: [============                  ] 27/63 batches, loss: 0.6982Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.6988Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.6985Epoch 4/10: [==============                ] 30/63 batches, loss: 0.6995Epoch 4/10: [==============                ] 31/63 batches, loss: 0.6995Epoch 4/10: [===============               ] 32/63 batches, loss: 0.6997Epoch 4/10: [===============               ] 33/63 batches, loss: 0.6996Epoch 4/10: [================              ] 34/63 batches, loss: 0.6996Epoch 4/10: [================              ] 35/63 batches, loss: 0.6997Epoch 4/10: [=================             ] 36/63 batches, loss: 0.6986Epoch 4/10: [=================             ] 37/63 batches, loss: 0.6982Epoch 4/10: [==================            ] 38/63 batches, loss: 0.6982Epoch 4/10: [==================            ] 39/63 batches, loss: 0.6985Epoch 4/10: [===================           ] 40/63 batches, loss: 0.6979Epoch 4/10: [===================           ] 41/63 batches, loss: 0.6981Epoch 4/10: [====================          ] 42/63 batches, loss: 0.6981Epoch 4/10: [====================          ] 43/63 batches, loss: 0.6985Epoch 4/10: [====================          ] 44/63 batches, loss: 0.6984Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.6982Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.6980Epoch 4/10: [======================        ] 47/63 batches, loss: 0.6982Epoch 4/10: [======================        ] 48/63 batches, loss: 0.6981Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.6978Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.6977Epoch 4/10: [========================      ] 51/63 batches, loss: 0.6974Epoch 4/10: [========================      ] 52/63 batches, loss: 0.6975Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.6976Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.6976Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.6975Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.6975Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.6979Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.6977Epoch 4/10: [============================  ] 59/63 batches, loss: 0.6977Epoch 4/10: [============================  ] 60/63 batches, loss: 0.6973Epoch 4/10: [============================= ] 61/63 batches, loss: 0.6968Epoch 4/10: [============================= ] 62/63 batches, loss: 0.6966Epoch 4/10: [==============================] 63/63 batches, loss: 0.6964
[2025-04-29 20:43:50,628][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6964
[2025-04-29 20:43:50,855][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6985, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.6970Epoch 5/10: [                              ] 2/63 batches, loss: 0.6925Epoch 5/10: [=                             ] 3/63 batches, loss: 0.6941Epoch 5/10: [=                             ] 4/63 batches, loss: 0.6929Epoch 5/10: [==                            ] 5/63 batches, loss: 0.6946Epoch 5/10: [==                            ] 6/63 batches, loss: 0.6926Epoch 5/10: [===                           ] 7/63 batches, loss: 0.6942Epoch 5/10: [===                           ] 8/63 batches, loss: 0.6946Epoch 5/10: [====                          ] 9/63 batches, loss: 0.6936Epoch 5/10: [====                          ] 10/63 batches, loss: 0.6942Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.6952Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.6956Epoch 5/10: [======                        ] 13/63 batches, loss: 0.6962Epoch 5/10: [======                        ] 14/63 batches, loss: 0.6962Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.6972Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.6967Epoch 5/10: [========                      ] 17/63 batches, loss: 0.6958Epoch 5/10: [========                      ] 18/63 batches, loss: 0.6952Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.6965Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.6978Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.6975Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.6979Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.6969Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.6971Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.6966Epoch 5/10: [============                  ] 26/63 batches, loss: 0.6967Epoch 5/10: [============                  ] 27/63 batches, loss: 0.6961Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.6958Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.6958Epoch 5/10: [==============                ] 30/63 batches, loss: 0.6951Epoch 5/10: [==============                ] 31/63 batches, loss: 0.6950Epoch 5/10: [===============               ] 32/63 batches, loss: 0.6952Epoch 5/10: [===============               ] 33/63 batches, loss: 0.6951Epoch 5/10: [================              ] 34/63 batches, loss: 0.6951Epoch 5/10: [================              ] 35/63 batches, loss: 0.6945Epoch 5/10: [=================             ] 36/63 batches, loss: 0.6946Epoch 5/10: [=================             ] 37/63 batches, loss: 0.6940Epoch 5/10: [==================            ] 38/63 batches, loss: 0.6937Epoch 5/10: [==================            ] 39/63 batches, loss: 0.6938Epoch 5/10: [===================           ] 40/63 batches, loss: 0.6940Epoch 5/10: [===================           ] 41/63 batches, loss: 0.6939Epoch 5/10: [====================          ] 42/63 batches, loss: 0.6941Epoch 5/10: [====================          ] 43/63 batches, loss: 0.6940Epoch 5/10: [====================          ] 44/63 batches, loss: 0.6944Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.6946Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.6947Epoch 5/10: [======================        ] 47/63 batches, loss: 0.6945Epoch 5/10: [======================        ] 48/63 batches, loss: 0.6943Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.6945Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.6946Epoch 5/10: [========================      ] 51/63 batches, loss: 0.6945Epoch 5/10: [========================      ] 52/63 batches, loss: 0.6950Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.6951Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.6952Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.6952Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.6951Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.6956Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.6956Epoch 5/10: [============================  ] 59/63 batches, loss: 0.6955Epoch 5/10: [============================  ] 60/63 batches, loss: 0.6955Epoch 5/10: [============================= ] 61/63 batches, loss: 0.6953Epoch 5/10: [============================= ] 62/63 batches, loss: 0.6951Epoch 5/10: [==============================] 63/63 batches, loss: 0.6948
[2025-04-29 20:43:57,006][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6948
[2025-04-29 20:43:57,216][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6984, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.6977Epoch 6/10: [                              ] 2/63 batches, loss: 0.7046Epoch 6/10: [=                             ] 3/63 batches, loss: 0.6976Epoch 6/10: [=                             ] 4/63 batches, loss: 0.7026Epoch 6/10: [==                            ] 5/63 batches, loss: 0.7014Epoch 6/10: [==                            ] 6/63 batches, loss: 0.7035Epoch 6/10: [===                           ] 7/63 batches, loss: 0.7011Epoch 6/10: [===                           ] 8/63 batches, loss: 0.6975Epoch 6/10: [====                          ] 9/63 batches, loss: 0.6961Epoch 6/10: [====                          ] 10/63 batches, loss: 0.6945Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.6952Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.6929Epoch 6/10: [======                        ] 13/63 batches, loss: 0.6919Epoch 6/10: [======                        ] 14/63 batches, loss: 0.6937Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.6929Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.6931Epoch 6/10: [========                      ] 17/63 batches, loss: 0.6926Epoch 6/10: [========                      ] 18/63 batches, loss: 0.6921Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.6918Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.6917Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.6926Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.6926Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.6923Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.6915Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.6918Epoch 6/10: [============                  ] 26/63 batches, loss: 0.6925Epoch 6/10: [============                  ] 27/63 batches, loss: 0.6918Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.6917Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.6913Epoch 6/10: [==============                ] 30/63 batches, loss: 0.6908Epoch 6/10: [==============                ] 31/63 batches, loss: 0.6906Epoch 6/10: [===============               ] 32/63 batches, loss: 0.6907Epoch 6/10: [===============               ] 33/63 batches, loss: 0.6913Epoch 6/10: [================              ] 34/63 batches, loss: 0.6915Epoch 6/10: [================              ] 35/63 batches, loss: 0.6910Epoch 6/10: [=================             ] 36/63 batches, loss: 0.6915Epoch 6/10: [=================             ] 37/63 batches, loss: 0.6922Epoch 6/10: [==================            ] 38/63 batches, loss: 0.6926Epoch 6/10: [==================            ] 39/63 batches, loss: 0.6929Epoch 6/10: [===================           ] 40/63 batches, loss: 0.6927Epoch 6/10: [===================           ] 41/63 batches, loss: 0.6928Epoch 6/10: [====================          ] 42/63 batches, loss: 0.6930Epoch 6/10: [====================          ] 43/63 batches, loss: 0.6931Epoch 6/10: [====================          ] 44/63 batches, loss: 0.6933Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.6929Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.6924Epoch 6/10: [======================        ] 47/63 batches, loss: 0.6924Epoch 6/10: [======================        ] 48/63 batches, loss: 0.6928Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.6929Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.6933Epoch 6/10: [========================      ] 51/63 batches, loss: 0.6933Epoch 6/10: [========================      ] 52/63 batches, loss: 0.6937Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.6942Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.6948Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.6951Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.6948Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.6949Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.6946Epoch 6/10: [============================  ] 59/63 batches, loss: 0.6949Epoch 6/10: [============================  ] 60/63 batches, loss: 0.6942Epoch 6/10: [============================= ] 61/63 batches, loss: 0.6941Epoch 6/10: [============================= ] 62/63 batches, loss: 0.6936Epoch 6/10: [==============================] 63/63 batches, loss: 0.6943
[2025-04-29 20:44:03,429][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6943
[2025-04-29 20:44:03,666][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6987, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
[2025-04-29 20:44:03,666][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/63 batches, loss: 0.6648Epoch 7/10: [                              ] 2/63 batches, loss: 0.6791Epoch 7/10: [=                             ] 3/63 batches, loss: 0.6919Epoch 7/10: [=                             ] 4/63 batches, loss: 0.6958Epoch 7/10: [==                            ] 5/63 batches, loss: 0.6941Epoch 7/10: [==                            ] 6/63 batches, loss: 0.6968Epoch 7/10: [===                           ] 7/63 batches, loss: 0.6957Epoch 7/10: [===                           ] 8/63 batches, loss: 0.6956Epoch 7/10: [====                          ] 9/63 batches, loss: 0.6932Epoch 7/10: [====                          ] 10/63 batches, loss: 0.6942Epoch 7/10: [=====                         ] 11/63 batches, loss: 0.6928Epoch 7/10: [=====                         ] 12/63 batches, loss: 0.6929Epoch 7/10: [======                        ] 13/63 batches, loss: 0.6933Epoch 7/10: [======                        ] 14/63 batches, loss: 0.6946Epoch 7/10: [=======                       ] 15/63 batches, loss: 0.6945Epoch 7/10: [=======                       ] 16/63 batches, loss: 0.6941Epoch 7/10: [========                      ] 17/63 batches, loss: 0.6950Epoch 7/10: [========                      ] 18/63 batches, loss: 0.6936Epoch 7/10: [=========                     ] 19/63 batches, loss: 0.6941Epoch 7/10: [=========                     ] 20/63 batches, loss: 0.6944Epoch 7/10: [==========                    ] 21/63 batches, loss: 0.6931Epoch 7/10: [==========                    ] 22/63 batches, loss: 0.6936Epoch 7/10: [==========                    ] 23/63 batches, loss: 0.6940Epoch 7/10: [===========                   ] 24/63 batches, loss: 0.6932Epoch 7/10: [===========                   ] 25/63 batches, loss: 0.6935Epoch 7/10: [============                  ] 26/63 batches, loss: 0.6928Epoch 7/10: [============                  ] 27/63 batches, loss: 0.6932Epoch 7/10: [=============                 ] 28/63 batches, loss: 0.6937Epoch 7/10: [=============                 ] 29/63 batches, loss: 0.6941Epoch 7/10: [==============                ] 30/63 batches, loss: 0.6942Epoch 7/10: [==============                ] 31/63 batches, loss: 0.6944Epoch 7/10: [===============               ] 32/63 batches, loss: 0.6947Epoch 7/10: [===============               ] 33/63 batches, loss: 0.6951Epoch 7/10: [================              ] 34/63 batches, loss: 0.6944Epoch 7/10: [================              ] 35/63 batches, loss: 0.6945Epoch 7/10: [=================             ] 36/63 batches, loss: 0.6942Epoch 7/10: [=================             ] 37/63 batches, loss: 0.6944Epoch 7/10: [==================            ] 38/63 batches, loss: 0.6950Epoch 7/10: [==================            ] 39/63 batches, loss: 0.6952Epoch 7/10: [===================           ] 40/63 batches, loss: 0.6951Epoch 7/10: [===================           ] 41/63 batches, loss: 0.6951Epoch 7/10: [====================          ] 42/63 batches, loss: 0.6953Epoch 7/10: [====================          ] 43/63 batches, loss: 0.6955Epoch 7/10: [====================          ] 44/63 batches, loss: 0.6952Epoch 7/10: [=====================         ] 45/63 batches, loss: 0.6950Epoch 7/10: [=====================         ] 46/63 batches, loss: 0.6951Epoch 7/10: [======================        ] 47/63 batches, loss: 0.6949Epoch 7/10: [======================        ] 48/63 batches, loss: 0.6949Epoch 7/10: [=======================       ] 49/63 batches, loss: 0.6947Epoch 7/10: [=======================       ] 50/63 batches, loss: 0.6947Epoch 7/10: [========================      ] 51/63 batches, loss: 0.6946Epoch 7/10: [========================      ] 52/63 batches, loss: 0.6945Epoch 7/10: [=========================     ] 53/63 batches, loss: 0.6946Epoch 7/10: [=========================     ] 54/63 batches, loss: 0.6941Epoch 7/10: [==========================    ] 55/63 batches, loss: 0.6942Epoch 7/10: [==========================    ] 56/63 batches, loss: 0.6937Epoch 7/10: [===========================   ] 57/63 batches, loss: 0.6938Epoch 7/10: [===========================   ] 58/63 batches, loss: 0.6938Epoch 7/10: [============================  ] 59/63 batches, loss: 0.6938Epoch 7/10: [============================  ] 60/63 batches, loss: 0.6938Epoch 7/10: [============================= ] 61/63 batches, loss: 0.6937Epoch 7/10: [============================= ] 62/63 batches, loss: 0.6940Epoch 7/10: [==============================] 63/63 batches, loss: 0.6942
[2025-04-29 20:44:09,495][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6942
[2025-04-29 20:44:09,722][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6976, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
Epoch 8/10: [Epoch 8/10: [                              ] 1/63 batches, loss: 0.6828Epoch 8/10: [                              ] 2/63 batches, loss: 0.6891Epoch 8/10: [=                             ] 3/63 batches, loss: 0.6901Epoch 8/10: [=                             ] 4/63 batches, loss: 0.6927Epoch 8/10: [==                            ] 5/63 batches, loss: 0.6937Epoch 8/10: [==                            ] 6/63 batches, loss: 0.6948Epoch 8/10: [===                           ] 7/63 batches, loss: 0.6938Epoch 8/10: [===                           ] 8/63 batches, loss: 0.6983Epoch 8/10: [====                          ] 9/63 batches, loss: 0.6952Epoch 8/10: [====                          ] 10/63 batches, loss: 0.6942Epoch 8/10: [=====                         ] 11/63 batches, loss: 0.6935Epoch 8/10: [=====                         ] 12/63 batches, loss: 0.6927Epoch 8/10: [======                        ] 13/63 batches, loss: 0.6924Epoch 8/10: [======                        ] 14/63 batches, loss: 0.6940Epoch 8/10: [=======                       ] 15/63 batches, loss: 0.6932Epoch 8/10: [=======                       ] 16/63 batches, loss: 0.6942Epoch 8/10: [========                      ] 17/63 batches, loss: 0.6940Epoch 8/10: [========                      ] 18/63 batches, loss: 0.6927Epoch 8/10: [=========                     ] 19/63 batches, loss: 0.6940Epoch 8/10: [=========                     ] 20/63 batches, loss: 0.6938Epoch 8/10: [==========                    ] 21/63 batches, loss: 0.6933Epoch 8/10: [==========                    ] 22/63 batches, loss: 0.6925Epoch 8/10: [==========                    ] 23/63 batches, loss: 0.6929Epoch 8/10: [===========                   ] 24/63 batches, loss: 0.6927Epoch 8/10: [===========                   ] 25/63 batches, loss: 0.6932Epoch 8/10: [============                  ] 26/63 batches, loss: 0.6932Epoch 8/10: [============                  ] 27/63 batches, loss: 0.6932Epoch 8/10: [=============                 ] 28/63 batches, loss: 0.6941Epoch 8/10: [=============                 ] 29/63 batches, loss: 0.6946Epoch 8/10: [==============                ] 30/63 batches, loss: 0.6946Epoch 8/10: [==============                ] 31/63 batches, loss: 0.6939Epoch 8/10: [===============               ] 32/63 batches, loss: 0.6942Epoch 8/10: [===============               ] 33/63 batches, loss: 0.6929Epoch 8/10: [================              ] 34/63 batches, loss: 0.6925Epoch 8/10: [================              ] 35/63 batches, loss: 0.6925Epoch 8/10: [=================             ] 36/63 batches, loss: 0.6926Epoch 8/10: [=================             ] 37/63 batches, loss: 0.6929Epoch 8/10: [==================            ] 38/63 batches, loss: 0.6931Epoch 8/10: [==================            ] 39/63 batches, loss: 0.6938Epoch 8/10: [===================           ] 40/63 batches, loss: 0.6936Epoch 8/10: [===================           ] 41/63 batches, loss: 0.6935Epoch 8/10: [====================          ] 42/63 batches, loss: 0.6939Epoch 8/10: [====================          ] 43/63 batches, loss: 0.6937Epoch 8/10: [====================          ] 44/63 batches, loss: 0.6935Epoch 8/10: [=====================         ] 45/63 batches, loss: 0.6938Epoch 8/10: [=====================         ] 46/63 batches, loss: 0.6938Epoch 8/10: [======================        ] 47/63 batches, loss: 0.6937Epoch 8/10: [======================        ] 48/63 batches, loss: 0.6933Epoch 8/10: [=======================       ] 49/63 batches, loss: 0.6938Epoch 8/10: [=======================       ] 50/63 batches, loss: 0.6933Epoch 8/10: [========================      ] 51/63 batches, loss: 0.6933Epoch 8/10: [========================      ] 52/63 batches, loss: 0.6932Epoch 8/10: [=========================     ] 53/63 batches, loss: 0.6934Epoch 8/10: [=========================     ] 54/63 batches, loss: 0.6935Epoch 8/10: [==========================    ] 55/63 batches, loss: 0.6934Epoch 8/10: [==========================    ] 56/63 batches, loss: 0.6933Epoch 8/10: [===========================   ] 57/63 batches, loss: 0.6934Epoch 8/10: [===========================   ] 58/63 batches, loss: 0.6939Epoch 8/10: [============================  ] 59/63 batches, loss: 0.6938Epoch 8/10: [============================  ] 60/63 batches, loss: 0.6938Epoch 8/10: [============================= ] 61/63 batches, loss: 0.6941Epoch 8/10: [============================= ] 62/63 batches, loss: 0.6940Epoch 8/10: [==============================] 63/63 batches, loss: 0.6942
[2025-04-29 20:44:15,909][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6942
[2025-04-29 20:44:16,149][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6977, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
[2025-04-29 20:44:16,150][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 9/10: [Epoch 9/10: [                              ] 1/63 batches, loss: 0.7166Epoch 9/10: [                              ] 2/63 batches, loss: 0.7142Epoch 9/10: [=                             ] 3/63 batches, loss: 0.7125Epoch 9/10: [=                             ] 4/63 batches, loss: 0.7071Epoch 9/10: [==                            ] 5/63 batches, loss: 0.7025Epoch 9/10: [==                            ] 6/63 batches, loss: 0.7053Epoch 9/10: [===                           ] 7/63 batches, loss: 0.7010Epoch 9/10: [===                           ] 8/63 batches, loss: 0.6979Epoch 9/10: [====                          ] 9/63 batches, loss: 0.6975Epoch 9/10: [====                          ] 10/63 batches, loss: 0.6976Epoch 9/10: [=====                         ] 11/63 batches, loss: 0.6991Epoch 9/10: [=====                         ] 12/63 batches, loss: 0.6994Epoch 9/10: [======                        ] 13/63 batches, loss: 0.6996Epoch 9/10: [======                        ] 14/63 batches, loss: 0.6986Epoch 9/10: [=======                       ] 15/63 batches, loss: 0.6988Epoch 9/10: [=======                       ] 16/63 batches, loss: 0.6982Epoch 9/10: [========                      ] 17/63 batches, loss: 0.6971Epoch 9/10: [========                      ] 18/63 batches, loss: 0.6972Epoch 9/10: [=========                     ] 19/63 batches, loss: 0.6965Epoch 9/10: [=========                     ] 20/63 batches, loss: 0.6964Epoch 9/10: [==========                    ] 21/63 batches, loss: 0.6961Epoch 9/10: [==========                    ] 22/63 batches, loss: 0.6968Epoch 9/10: [==========                    ] 23/63 batches, loss: 0.6973Epoch 9/10: [===========                   ] 24/63 batches, loss: 0.6968Epoch 9/10: [===========                   ] 25/63 batches, loss: 0.6965Epoch 9/10: [============                  ] 26/63 batches, loss: 0.6970Epoch 9/10: [============                  ] 27/63 batches, loss: 0.6970Epoch 9/10: [=============                 ] 28/63 batches, loss: 0.6970Epoch 9/10: [=============                 ] 29/63 batches, loss: 0.6968Epoch 9/10: [==============                ] 30/63 batches, loss: 0.6970Epoch 9/10: [==============                ] 31/63 batches, loss: 0.6968Epoch 9/10: [===============               ] 32/63 batches, loss: 0.6970Epoch 9/10: [===============               ] 33/63 batches, loss: 0.6971Epoch 9/10: [================              ] 34/63 batches, loss: 0.6967Epoch 9/10: [================              ] 35/63 batches, loss: 0.6969Epoch 9/10: [=================             ] 36/63 batches, loss: 0.6966Epoch 9/10: [=================             ] 37/63 batches, loss: 0.6965Epoch 9/10: [==================            ] 38/63 batches, loss: 0.6961Epoch 9/10: [==================            ] 39/63 batches, loss: 0.6957Epoch 9/10: [===================           ] 40/63 batches, loss: 0.6955Epoch 9/10: [===================           ] 41/63 batches, loss: 0.6953Epoch 9/10: [====================          ] 42/63 batches, loss: 0.6949Epoch 9/10: [====================          ] 43/63 batches, loss: 0.6951Epoch 9/10: [====================          ] 44/63 batches, loss: 0.6955Epoch 9/10: [=====================         ] 45/63 batches, loss: 0.6953Epoch 9/10: [=====================         ] 46/63 batches, loss: 0.6951Epoch 9/10: [======================        ] 47/63 batches, loss: 0.6951Epoch 9/10: [======================        ] 48/63 batches, loss: 0.6957Epoch 9/10: [=======================       ] 49/63 batches, loss: 0.6958Epoch 9/10: [=======================       ] 50/63 batches, loss: 0.6959Epoch 9/10: [========================      ] 51/63 batches, loss: 0.6959Epoch 9/10: [========================      ] 52/63 batches, loss: 0.6957Epoch 9/10: [=========================     ] 53/63 batches, loss: 0.6954Epoch 9/10: [=========================     ] 54/63 batches, loss: 0.6949Epoch 9/10: [==========================    ] 55/63 batches, loss: 0.6949Epoch 9/10: [==========================    ] 56/63 batches, loss: 0.6951Epoch 9/10: [===========================   ] 57/63 batches, loss: 0.6953Epoch 9/10: [===========================   ] 58/63 batches, loss: 0.6949Epoch 9/10: [============================  ] 59/63 batches, loss: 0.6949Epoch 9/10: [============================  ] 60/63 batches, loss: 0.6952Epoch 9/10: [============================= ] 61/63 batches, loss: 0.6949Epoch 9/10: [============================= ] 62/63 batches, loss: 0.6954Epoch 9/10: [==============================] 63/63 batches, loss: 0.6960
[2025-04-29 20:44:21,973][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.6960
[2025-04-29 20:44:22,195][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.6979, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
[2025-04-29 20:44:22,196][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 10/10: [Epoch 10/10: [                              ] 1/63 batches, loss: 0.6932Epoch 10/10: [                              ] 2/63 batches, loss: 0.7008Epoch 10/10: [=                             ] 3/63 batches, loss: 0.7014Epoch 10/10: [=                             ] 4/63 batches, loss: 0.7017Epoch 10/10: [==                            ] 5/63 batches, loss: 0.6993Epoch 10/10: [==                            ] 6/63 batches, loss: 0.6960Epoch 10/10: [===                           ] 7/63 batches, loss: 0.6979Epoch 10/10: [===                           ] 8/63 batches, loss: 0.6942Epoch 10/10: [====                          ] 9/63 batches, loss: 0.6946Epoch 10/10: [====                          ] 10/63 batches, loss: 0.6946Epoch 10/10: [=====                         ] 11/63 batches, loss: 0.6949Epoch 10/10: [=====                         ] 12/63 batches, loss: 0.6942Epoch 10/10: [======                        ] 13/63 batches, loss: 0.6939Epoch 10/10: [======                        ] 14/63 batches, loss: 0.6940Epoch 10/10: [=======                       ] 15/63 batches, loss: 0.6933Epoch 10/10: [=======                       ] 16/63 batches, loss: 0.6940Epoch 10/10: [========                      ] 17/63 batches, loss: 0.6938Epoch 10/10: [========                      ] 18/63 batches, loss: 0.6935Epoch 10/10: [=========                     ] 19/63 batches, loss: 0.6940Epoch 10/10: [=========                     ] 20/63 batches, loss: 0.6932Epoch 10/10: [==========                    ] 21/63 batches, loss: 0.6935Epoch 10/10: [==========                    ] 22/63 batches, loss: 0.6938Epoch 10/10: [==========                    ] 23/63 batches, loss: 0.6943Epoch 10/10: [===========                   ] 24/63 batches, loss: 0.6947Epoch 10/10: [===========                   ] 25/63 batches, loss: 0.6954Epoch 10/10: [============                  ] 26/63 batches, loss: 0.6949Epoch 10/10: [============                  ] 27/63 batches, loss: 0.6947Epoch 10/10: [=============                 ] 28/63 batches, loss: 0.6950Epoch 10/10: [=============                 ] 29/63 batches, loss: 0.6947Epoch 10/10: [==============                ] 30/63 batches, loss: 0.6947Epoch 10/10: [==============                ] 31/63 batches, loss: 0.6944Epoch 10/10: [===============               ] 32/63 batches, loss: 0.6948Epoch 10/10: [===============               ] 33/63 batches, loss: 0.6945Epoch 10/10: [================              ] 34/63 batches, loss: 0.6947Epoch 10/10: [================              ] 35/63 batches, loss: 0.6943Epoch 10/10: [=================             ] 36/63 batches, loss: 0.6943Epoch 10/10: [=================             ] 37/63 batches, loss: 0.6939Epoch 10/10: [==================            ] 38/63 batches, loss: 0.6937Epoch 10/10: [==================            ] 39/63 batches, loss: 0.6934Epoch 10/10: [===================           ] 40/63 batches, loss: 0.6928Epoch 10/10: [===================           ] 41/63 batches, loss: 0.6933Epoch 10/10: [====================          ] 42/63 batches, loss: 0.6933Epoch 10/10: [====================          ] 43/63 batches, loss: 0.6934Epoch 10/10: [====================          ] 44/63 batches, loss: 0.6933Epoch 10/10: [=====================         ] 45/63 batches, loss: 0.6934Epoch 10/10: [=====================         ] 46/63 batches, loss: 0.6935Epoch 10/10: [======================        ] 47/63 batches, loss: 0.6939Epoch 10/10: [======================        ] 48/63 batches, loss: 0.6941Epoch 10/10: [=======================       ] 49/63 batches, loss: 0.6941Epoch 10/10: [=======================       ] 50/63 batches, loss: 0.6942Epoch 10/10: [========================      ] 51/63 batches, loss: 0.6942Epoch 10/10: [========================      ] 52/63 batches, loss: 0.6944Epoch 10/10: [=========================     ] 53/63 batches, loss: 0.6946Epoch 10/10: [=========================     ] 54/63 batches, loss: 0.6950Epoch 10/10: [==========================    ] 55/63 batches, loss: 0.6951Epoch 10/10: [==========================    ] 56/63 batches, loss: 0.6952Epoch 10/10: [===========================   ] 57/63 batches, loss: 0.6951Epoch 10/10: [===========================   ] 58/63 batches, loss: 0.6948Epoch 10/10: [============================  ] 59/63 batches, loss: 0.6948Epoch 10/10: [============================  ] 60/63 batches, loss: 0.6950Epoch 10/10: [============================= ] 61/63 batches, loss: 0.6949Epoch 10/10: [============================= ] 62/63 batches, loss: 0.6947Epoch 10/10: [==============================] 63/63 batches, loss: 0.6949
[2025-04-29 20:44:27,970][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.6949
[2025-04-29 20:44:28,197][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.6976, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
[2025-04-29 20:44:28,607][src.training.lm_trainer][INFO] - Training completed in 64.00 seconds
[2025-04-29 20:44:28,607][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 20:44:31,149][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.4994974874371859, 'f1': 0.6662198391420912}
[2025-04-29 20:44:31,149][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
[2025-04-29 20:44:31,149][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.2857142857142857, 'f1': 0.4444444444444444}
[2025-04-29 20:44:32,783][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control1/ar/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▁▁▁▁
wandb:          best_val_f1 ▁▁▁▁▁▁▁
wandb:        best_val_loss █▅▅▄▄▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▁▄▄▂▁▁▁▃▂
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁▁▁▁▁
wandb:             val_loss █▅▅▄▄▄▁▁▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.45455
wandb:          best_val_f1 0.625
wandb:        best_val_loss 0.69755
wandb:                epoch 10
wandb:  final_test_accuracy 0.28571
wandb:        final_test_f1 0.44444
wandb: final_train_accuracy 0.4995
wandb:       final_train_f1 0.66622
wandb:   final_val_accuracy 0.45455
wandb:         final_val_f1 0.625
wandb:        learning_rate 2e-05
wandb:           train_loss 0.69486
wandb:           train_time 64.00473
wandb:         val_accuracy 0.45455
wandb:               val_f1 0.625
wandb:             val_loss 0.69755
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_204312-pqut6h54
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_204312-pqut6h54/logs
Experiment finetune_question_type_control1_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control1/results.json
Running experiment: finetune_question_type_control2_ar
Command: python -m src.experiments.run_experiment             "hydra.job.chdir=False"             "hydra.run.dir=."             "experiment=finetune"             "experiment.tasks=question_type"             "experiment.use_controls=true"             "experiment.control_index=2"             "model=glot500_finetune"             "model.lm_name=cis-lmu/glot500-base"             "model.dropout=0.1"             "model.freeze_model=false"             "model.finetune=true"             "data.languages=[ar]"             "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"             "training.task_type=classification"             "training.num_epochs=10"             "training.batch_size=16"             "training.lr=2e-5"             "+training.gradient_accumulation_steps=2"                          "experiment_name=finetune_question_type_control2_ar"             "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control2"             "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 20:44:48,016][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control2
experiment_name: finetune_question_type_control2_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_finetune
  tasks: question_type
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  finetune: true
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1
    lr: 2.0e-05
    batch_size: 8

[2025-04-29 20:44:48,017][__main__][INFO] - Normalized task: question_type
[2025-04-29 20:44:48,017][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 20:44:48,017][__main__][INFO] - Determined Task Type: classification
[2025-04-29 20:44:48,021][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 20:44:48,021][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 20:44:50,131][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 20:44:52,395][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 20:44:52,396][src.data.datasets][INFO] - Loading 'control_question_type_seed2' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:44:52,520][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-04-29 20:44:52,578][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-04-29 20:44:52,834][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 20:44:52,841][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:44:52,841][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 20:44:52,845][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:44:52,902][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:44:52,963][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:44:52,986][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 20:44:52,987][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:44:52,988][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 20:44:52,990][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:44:53,052][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:44:53,113][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:44:53,134][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 20:44:53,135][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:44:53,135][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 20:44:53,137][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 20:44:53,138][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:44:53,138][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:44:53,138][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:44:53,138][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:44:53,138][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 20:44:53,138][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 20:44:53,139][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 20:44:53,139][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 20:44:53,139][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:44:53,139][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:44:53,139][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:44:53,139][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:44:53,139][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 20:44:53,139][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 20:44:53,139][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 20:44:53,139][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:44:53,140][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:44:53,140][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:44:53,140][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:44:53,140][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:44:53,140][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 20:44:53,140][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 20:44:53,140][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 20:44:53,140][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:44:53,140][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 20:44:53,140][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 20:44:53,141][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 20:44:53,141][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 20:44:58,469][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 20:44:58,469][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,469][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,483][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:44:58,483][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 20:44:58,484][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 20:44:58,484][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 20:44:58,485][__main__][INFO] - Successfully created model for ar
[2025-04-29 20:44:58,485][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.6880Epoch 1/10: [                              ] 2/63 batches, loss: 0.6987Epoch 1/10: [=                             ] 3/63 batches, loss: 0.6934Epoch 1/10: [=                             ] 4/63 batches, loss: 0.6918Epoch 1/10: [==                            ] 5/63 batches, loss: 0.6936Epoch 1/10: [==                            ] 6/63 batches, loss: 0.6930Epoch 1/10: [===                           ] 7/63 batches, loss: 0.6917Epoch 1/10: [===                           ] 8/63 batches, loss: 0.6934Epoch 1/10: [====                          ] 9/63 batches, loss: 0.6961Epoch 1/10: [====                          ] 10/63 batches, loss: 0.6977Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.6980Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.6987Epoch 1/10: [======                        ] 13/63 batches, loss: 0.6987Epoch 1/10: [======                        ] 14/63 batches, loss: 0.6963Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.6984Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.6985Epoch 1/10: [========                      ] 17/63 batches, loss: 0.6985Epoch 1/10: [========                      ] 18/63 batches, loss: 0.6979Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.6974Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.6982Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.6982Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.6993Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.6992Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.7001Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.6987Epoch 1/10: [============                  ] 26/63 batches, loss: 0.6973Epoch 1/10: [============                  ] 27/63 batches, loss: 0.6979Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.6974Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.6965Epoch 1/10: [==============                ] 30/63 batches, loss: 0.6962Epoch 1/10: [==============                ] 31/63 batches, loss: 0.6962Epoch 1/10: [===============               ] 32/63 batches, loss: 0.6963Epoch 1/10: [===============               ] 33/63 batches, loss: 0.6959Epoch 1/10: [================              ] 34/63 batches, loss: 0.6956Epoch 1/10: [================              ] 35/63 batches, loss: 0.6949Epoch 1/10: [=================             ] 36/63 batches, loss: 0.6952Epoch 1/10: [=================             ] 37/63 batches, loss: 0.6949Epoch 1/10: [==================            ] 38/63 batches, loss: 0.6944Epoch 1/10: [==================            ] 39/63 batches, loss: 0.6941Epoch 1/10: [===================           ] 40/63 batches, loss: 0.6947Epoch 1/10: [===================           ] 41/63 batches, loss: 0.6948Epoch 1/10: [====================          ] 42/63 batches, loss: 0.6959Epoch 1/10: [====================          ] 43/63 batches, loss: 0.6956Epoch 1/10: [====================          ] 44/63 batches, loss: 0.6954Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.6957Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.6962Epoch 1/10: [======================        ] 47/63 batches, loss: 0.6963Epoch 1/10: [======================        ] 48/63 batches, loss: 0.6959Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.6955Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.6956Epoch 1/10: [========================      ] 51/63 batches, loss: 0.6962Epoch 1/10: [========================      ] 52/63 batches, loss: 0.6961Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.6957Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.6960Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.6963Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.6955Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.6959Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.6960Epoch 1/10: [============================  ] 59/63 batches, loss: 0.6959Epoch 1/10: [============================  ] 60/63 batches, loss: 0.6958Epoch 1/10: [============================= ] 61/63 batches, loss: 0.6960Epoch 1/10: [============================= ] 62/63 batches, loss: 0.6958Epoch 1/10: [==============================] 63/63 batches, loss: 0.6955
[2025-04-29 20:45:07,297][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6955
[2025-04-29 20:45:07,499][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6986, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.6803Epoch 2/10: [                              ] 2/63 batches, loss: 0.6929Epoch 2/10: [=                             ] 3/63 batches, loss: 0.6862Epoch 2/10: [=                             ] 4/63 batches, loss: 0.6885Epoch 2/10: [==                            ] 5/63 batches, loss: 0.6865Epoch 2/10: [==                            ] 6/63 batches, loss: 0.6888Epoch 2/10: [===                           ] 7/63 batches, loss: 0.6905Epoch 2/10: [===                           ] 8/63 batches, loss: 0.6929Epoch 2/10: [====                          ] 9/63 batches, loss: 0.6944Epoch 2/10: [====                          ] 10/63 batches, loss: 0.6976Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.6973Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.6975Epoch 2/10: [======                        ] 13/63 batches, loss: 0.6970Epoch 2/10: [======                        ] 14/63 batches, loss: 0.6962Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.6956Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.6955Epoch 2/10: [========                      ] 17/63 batches, loss: 0.6954Epoch 2/10: [========                      ] 18/63 batches, loss: 0.6949Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.6950Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.6955Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.6965Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.6962Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.6961Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.6957Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.6957Epoch 2/10: [============                  ] 26/63 batches, loss: 0.6955Epoch 2/10: [============                  ] 27/63 batches, loss: 0.6957Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.6961Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.6961Epoch 2/10: [==============                ] 30/63 batches, loss: 0.6963Epoch 2/10: [==============                ] 31/63 batches, loss: 0.6954Epoch 2/10: [===============               ] 32/63 batches, loss: 0.6959Epoch 2/10: [===============               ] 33/63 batches, loss: 0.6962Epoch 2/10: [================              ] 34/63 batches, loss: 0.6968Epoch 2/10: [================              ] 35/63 batches, loss: 0.6976Epoch 2/10: [=================             ] 36/63 batches, loss: 0.6973Epoch 2/10: [=================             ] 37/63 batches, loss: 0.6966Epoch 2/10: [==================            ] 38/63 batches, loss: 0.6966Epoch 2/10: [==================            ] 39/63 batches, loss: 0.6965Epoch 2/10: [===================           ] 40/63 batches, loss: 0.6962Epoch 2/10: [===================           ] 41/63 batches, loss: 0.6955Epoch 2/10: [====================          ] 42/63 batches, loss: 0.6958Epoch 2/10: [====================          ] 43/63 batches, loss: 0.6958Epoch 2/10: [====================          ] 44/63 batches, loss: 0.6957Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.6954Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.6950Epoch 2/10: [======================        ] 47/63 batches, loss: 0.6948Epoch 2/10: [======================        ] 48/63 batches, loss: 0.6947Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.6947Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.6946Epoch 2/10: [========================      ] 51/63 batches, loss: 0.6947Epoch 2/10: [========================      ] 52/63 batches, loss: 0.6951Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.6951Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.6951Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.6953Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.6953Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.6954Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.6950Epoch 2/10: [============================  ] 59/63 batches, loss: 0.6946Epoch 2/10: [============================  ] 60/63 batches, loss: 0.6949Epoch 2/10: [============================= ] 61/63 batches, loss: 0.6949Epoch 2/10: [============================= ] 62/63 batches, loss: 0.6946Epoch 2/10: [==============================] 63/63 batches, loss: 0.6945
[2025-04-29 20:45:13,680][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6945
[2025-04-29 20:45:13,899][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6991, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
[2025-04-29 20:45:13,900][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.7092Epoch 3/10: [                              ] 2/63 batches, loss: 0.7071Epoch 3/10: [=                             ] 3/63 batches, loss: 0.7007Epoch 3/10: [=                             ] 4/63 batches, loss: 0.7025Epoch 3/10: [==                            ] 5/63 batches, loss: 0.7072Epoch 3/10: [==                            ] 6/63 batches, loss: 0.7092Epoch 3/10: [===                           ] 7/63 batches, loss: 0.7053Epoch 3/10: [===                           ] 8/63 batches, loss: 0.7046Epoch 3/10: [====                          ] 9/63 batches, loss: 0.7021Epoch 3/10: [====                          ] 10/63 batches, loss: 0.7029Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.7033Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.7031Epoch 3/10: [======                        ] 13/63 batches, loss: 0.7019Epoch 3/10: [======                        ] 14/63 batches, loss: 0.7018Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.7017Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.7015Epoch 3/10: [========                      ] 17/63 batches, loss: 0.7019Epoch 3/10: [========                      ] 18/63 batches, loss: 0.7008Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.7003Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.6993Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.6997Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.6990Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.6987Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.6978Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.6982Epoch 3/10: [============                  ] 26/63 batches, loss: 0.6976Epoch 3/10: [============                  ] 27/63 batches, loss: 0.6976Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.6968Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.6967Epoch 3/10: [==============                ] 30/63 batches, loss: 0.6967Epoch 3/10: [==============                ] 31/63 batches, loss: 0.6965Epoch 3/10: [===============               ] 32/63 batches, loss: 0.6963Epoch 3/10: [===============               ] 33/63 batches, loss: 0.6962Epoch 3/10: [================              ] 34/63 batches, loss: 0.6963Epoch 3/10: [================              ] 35/63 batches, loss: 0.6954Epoch 3/10: [=================             ] 36/63 batches, loss: 0.6953Epoch 3/10: [=================             ] 37/63 batches, loss: 0.6953Epoch 3/10: [==================            ] 38/63 batches, loss: 0.6955Epoch 3/10: [==================            ] 39/63 batches, loss: 0.6952Epoch 3/10: [===================           ] 40/63 batches, loss: 0.6957Epoch 3/10: [===================           ] 41/63 batches, loss: 0.6960Epoch 3/10: [====================          ] 42/63 batches, loss: 0.6957Epoch 3/10: [====================          ] 43/63 batches, loss: 0.6955Epoch 3/10: [====================          ] 44/63 batches, loss: 0.6959Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.6954Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.6953Epoch 3/10: [======================        ] 47/63 batches, loss: 0.6953Epoch 3/10: [======================        ] 48/63 batches, loss: 0.6951Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.6950Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.6950Epoch 3/10: [========================      ] 51/63 batches, loss: 0.6950Epoch 3/10: [========================      ] 52/63 batches, loss: 0.6952Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.6949Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.6951Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.6950Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.6950Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.6948Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.6950Epoch 3/10: [============================  ] 59/63 batches, loss: 0.6948Epoch 3/10: [============================  ] 60/63 batches, loss: 0.6951Epoch 3/10: [============================= ] 61/63 batches, loss: 0.6953Epoch 3/10: [============================= ] 62/63 batches, loss: 0.6954Epoch 3/10: [==============================] 63/63 batches, loss: 0.6950
[2025-04-29 20:45:19,667][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6950
[2025-04-29 20:45:19,878][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6984, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.7017Epoch 4/10: [                              ] 2/63 batches, loss: 0.6934Epoch 4/10: [=                             ] 3/63 batches, loss: 0.6911Epoch 4/10: [=                             ] 4/63 batches, loss: 0.6932Epoch 4/10: [==                            ] 5/63 batches, loss: 0.6956Epoch 4/10: [==                            ] 6/63 batches, loss: 0.6922Epoch 4/10: [===                           ] 7/63 batches, loss: 0.6918Epoch 4/10: [===                           ] 8/63 batches, loss: 0.6935Epoch 4/10: [====                          ] 9/63 batches, loss: 0.6950Epoch 4/10: [====                          ] 10/63 batches, loss: 0.6958Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.6962Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.6969Epoch 4/10: [======                        ] 13/63 batches, loss: 0.6973Epoch 4/10: [======                        ] 14/63 batches, loss: 0.6989Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.6976Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.6958Epoch 4/10: [========                      ] 17/63 batches, loss: 0.6950Epoch 4/10: [========                      ] 18/63 batches, loss: 0.6948Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.6946Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.6947Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.6942Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.6937Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.6925Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.6929Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.6932Epoch 4/10: [============                  ] 26/63 batches, loss: 0.6945Epoch 4/10: [============                  ] 27/63 batches, loss: 0.6948Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.6945Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.6945Epoch 4/10: [==============                ] 30/63 batches, loss: 0.6942Epoch 4/10: [==============                ] 31/63 batches, loss: 0.6944Epoch 4/10: [===============               ] 32/63 batches, loss: 0.6946Epoch 4/10: [===============               ] 33/63 batches, loss: 0.6944Epoch 4/10: [================              ] 34/63 batches, loss: 0.6947Epoch 4/10: [================              ] 35/63 batches, loss: 0.6942Epoch 4/10: [=================             ] 36/63 batches, loss: 0.6937Epoch 4/10: [=================             ] 37/63 batches, loss: 0.6937Epoch 4/10: [==================            ] 38/63 batches, loss: 0.6935Epoch 4/10: [==================            ] 39/63 batches, loss: 0.6937Epoch 4/10: [===================           ] 40/63 batches, loss: 0.6935Epoch 4/10: [===================           ] 41/63 batches, loss: 0.6936Epoch 4/10: [====================          ] 42/63 batches, loss: 0.6937Epoch 4/10: [====================          ] 43/63 batches, loss: 0.6938Epoch 4/10: [====================          ] 44/63 batches, loss: 0.6940Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.6936Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.6935Epoch 4/10: [======================        ] 47/63 batches, loss: 0.6934Epoch 4/10: [======================        ] 48/63 batches, loss: 0.6938Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.6935Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.6937Epoch 4/10: [========================      ] 51/63 batches, loss: 0.6940Epoch 4/10: [========================      ] 52/63 batches, loss: 0.6939Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.6940Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.6941Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.6941Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.6942Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.6941Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.6941Epoch 4/10: [============================  ] 59/63 batches, loss: 0.6941Epoch 4/10: [============================  ] 60/63 batches, loss: 0.6944Epoch 4/10: [============================= ] 61/63 batches, loss: 0.6946Epoch 4/10: [============================= ] 62/63 batches, loss: 0.6948Epoch 4/10: [==============================] 63/63 batches, loss: 0.6954
[2025-04-29 20:45:26,095][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6954
[2025-04-29 20:45:26,329][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6985, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
[2025-04-29 20:45:26,330][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.6863Epoch 5/10: [                              ] 2/63 batches, loss: 0.6806Epoch 5/10: [=                             ] 3/63 batches, loss: 0.6819Epoch 5/10: [=                             ] 4/63 batches, loss: 0.6838Epoch 5/10: [==                            ] 5/63 batches, loss: 0.6870Epoch 5/10: [==                            ] 6/63 batches, loss: 0.6880Epoch 5/10: [===                           ] 7/63 batches, loss: 0.6868Epoch 5/10: [===                           ] 8/63 batches, loss: 0.6865Epoch 5/10: [====                          ] 9/63 batches, loss: 0.6890Epoch 5/10: [====                          ] 10/63 batches, loss: 0.6899Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.6912Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.6920Epoch 5/10: [======                        ] 13/63 batches, loss: 0.6900Epoch 5/10: [======                        ] 14/63 batches, loss: 0.6908Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.6902Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.6915Epoch 5/10: [========                      ] 17/63 batches, loss: 0.6920Epoch 5/10: [========                      ] 18/63 batches, loss: 0.6938Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.6954Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.6950Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.6948Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.6942Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.6935Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.6937Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.6939Epoch 5/10: [============                  ] 26/63 batches, loss: 0.6943Epoch 5/10: [============                  ] 27/63 batches, loss: 0.6939Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.6935Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.6935Epoch 5/10: [==============                ] 30/63 batches, loss: 0.6938Epoch 5/10: [==============                ] 31/63 batches, loss: 0.6937Epoch 5/10: [===============               ] 32/63 batches, loss: 0.6935Epoch 5/10: [===============               ] 33/63 batches, loss: 0.6932Epoch 5/10: [================              ] 34/63 batches, loss: 0.6931Epoch 5/10: [================              ] 35/63 batches, loss: 0.6933Epoch 5/10: [=================             ] 36/63 batches, loss: 0.6931Epoch 5/10: [=================             ] 37/63 batches, loss: 0.6929Epoch 5/10: [==================            ] 38/63 batches, loss: 0.6926Epoch 5/10: [==================            ] 39/63 batches, loss: 0.6926Epoch 5/10: [===================           ] 40/63 batches, loss: 0.6921Epoch 5/10: [===================           ] 41/63 batches, loss: 0.6926Epoch 5/10: [====================          ] 42/63 batches, loss: 0.6928Epoch 5/10: [====================          ] 43/63 batches, loss: 0.6932Epoch 5/10: [====================          ] 44/63 batches, loss: 0.6932Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.6930Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.6930Epoch 5/10: [======================        ] 47/63 batches, loss: 0.6928Epoch 5/10: [======================        ] 48/63 batches, loss: 0.6928Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.6928Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.6929Epoch 5/10: [========================      ] 51/63 batches, loss: 0.6930Epoch 5/10: [========================      ] 52/63 batches, loss: 0.6932Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.6932Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.6935Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.6937Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.6939Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.6942Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.6942Epoch 5/10: [============================  ] 59/63 batches, loss: 0.6945Epoch 5/10: [============================  ] 60/63 batches, loss: 0.6948Epoch 5/10: [============================= ] 61/63 batches, loss: 0.6948Epoch 5/10: [============================= ] 62/63 batches, loss: 0.6947Epoch 5/10: [==============================] 63/63 batches, loss: 0.6947
[2025-04-29 20:45:32,118][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6947
[2025-04-29 20:45:32,327][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6975, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.7082Epoch 6/10: [                              ] 2/63 batches, loss: 0.6968Epoch 6/10: [=                             ] 3/63 batches, loss: 0.6966Epoch 6/10: [=                             ] 4/63 batches, loss: 0.6949Epoch 6/10: [==                            ] 5/63 batches, loss: 0.6960Epoch 6/10: [==                            ] 6/63 batches, loss: 0.6940Epoch 6/10: [===                           ] 7/63 batches, loss: 0.6928Epoch 6/10: [===                           ] 8/63 batches, loss: 0.6924Epoch 6/10: [====                          ] 9/63 batches, loss: 0.6917Epoch 6/10: [====                          ] 10/63 batches, loss: 0.6910Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.6910Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.6915Epoch 6/10: [======                        ] 13/63 batches, loss: 0.6907Epoch 6/10: [======                        ] 14/63 batches, loss: 0.6914Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.6912Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.6904Epoch 6/10: [========                      ] 17/63 batches, loss: 0.6904Epoch 6/10: [========                      ] 18/63 batches, loss: 0.6911Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.6914Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.6914Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.6920Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.6920Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.6915Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.6918Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.6914Epoch 6/10: [============                  ] 26/63 batches, loss: 0.6917Epoch 6/10: [============                  ] 27/63 batches, loss: 0.6921Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.6916Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.6914Epoch 6/10: [==============                ] 30/63 batches, loss: 0.6908Epoch 6/10: [==============                ] 31/63 batches, loss: 0.6910Epoch 6/10: [===============               ] 32/63 batches, loss: 0.6907Epoch 6/10: [===============               ] 33/63 batches, loss: 0.6914Epoch 6/10: [================              ] 34/63 batches, loss: 0.6910Epoch 6/10: [================              ] 35/63 batches, loss: 0.6913Epoch 6/10: [=================             ] 36/63 batches, loss: 0.6911Epoch 6/10: [=================             ] 37/63 batches, loss: 0.6920Epoch 6/10: [==================            ] 38/63 batches, loss: 0.6922Epoch 6/10: [==================            ] 39/63 batches, loss: 0.6922Epoch 6/10: [===================           ] 40/63 batches, loss: 0.6927Epoch 6/10: [===================           ] 41/63 batches, loss: 0.6924Epoch 6/10: [====================          ] 42/63 batches, loss: 0.6931Epoch 6/10: [====================          ] 43/63 batches, loss: 0.6930Epoch 6/10: [====================          ] 44/63 batches, loss: 0.6931Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.6936Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.6938Epoch 6/10: [======================        ] 47/63 batches, loss: 0.6939Epoch 6/10: [======================        ] 48/63 batches, loss: 0.6939Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.6942Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.6941Epoch 6/10: [========================      ] 51/63 batches, loss: 0.6942Epoch 6/10: [========================      ] 52/63 batches, loss: 0.6941Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.6945Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.6945Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.6942Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.6945Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.6943Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.6943Epoch 6/10: [============================  ] 59/63 batches, loss: 0.6942Epoch 6/10: [============================  ] 60/63 batches, loss: 0.6942Epoch 6/10: [============================= ] 61/63 batches, loss: 0.6940Epoch 6/10: [============================= ] 62/63 batches, loss: 0.6938Epoch 6/10: [==============================] 63/63 batches, loss: 0.6940
[2025-04-29 20:45:38,562][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6940
[2025-04-29 20:45:38,781][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6978, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
[2025-04-29 20:45:38,782][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/63 batches, loss: 0.7110Epoch 7/10: [                              ] 2/63 batches, loss: 0.7038Epoch 7/10: [=                             ] 3/63 batches, loss: 0.7007Epoch 7/10: [=                             ] 4/63 batches, loss: 0.7001Epoch 7/10: [==                            ] 5/63 batches, loss: 0.6934Epoch 7/10: [==                            ] 6/63 batches, loss: 0.6954Epoch 7/10: [===                           ] 7/63 batches, loss: 0.6940Epoch 7/10: [===                           ] 8/63 batches, loss: 0.6915Epoch 7/10: [====                          ] 9/63 batches, loss: 0.6944Epoch 7/10: [====                          ] 10/63 batches, loss: 0.6940Epoch 7/10: [=====                         ] 11/63 batches, loss: 0.6921Epoch 7/10: [=====                         ] 12/63 batches, loss: 0.6930Epoch 7/10: [======                        ] 13/63 batches, loss: 0.6940Epoch 7/10: [======                        ] 14/63 batches, loss: 0.6938Epoch 7/10: [=======                       ] 15/63 batches, loss: 0.6948Epoch 7/10: [=======                       ] 16/63 batches, loss: 0.6954Epoch 7/10: [========                      ] 17/63 batches, loss: 0.6965Epoch 7/10: [========                      ] 18/63 batches, loss: 0.6953Epoch 7/10: [=========                     ] 19/63 batches, loss: 0.6953Epoch 7/10: [=========                     ] 20/63 batches, loss: 0.6956Epoch 7/10: [==========                    ] 21/63 batches, loss: 0.6951Epoch 7/10: [==========                    ] 22/63 batches, loss: 0.6947Epoch 7/10: [==========                    ] 23/63 batches, loss: 0.6944Epoch 7/10: [===========                   ] 24/63 batches, loss: 0.6948Epoch 7/10: [===========                   ] 25/63 batches, loss: 0.6949Epoch 7/10: [============                  ] 26/63 batches, loss: 0.6953Epoch 7/10: [============                  ] 27/63 batches, loss: 0.6949Epoch 7/10: [=============                 ] 28/63 batches, loss: 0.6948Epoch 7/10: [=============                 ] 29/63 batches, loss: 0.6950Epoch 7/10: [==============                ] 30/63 batches, loss: 0.6950Epoch 7/10: [==============                ] 31/63 batches, loss: 0.6947Epoch 7/10: [===============               ] 32/63 batches, loss: 0.6949Epoch 7/10: [===============               ] 33/63 batches, loss: 0.6946Epoch 7/10: [================              ] 34/63 batches, loss: 0.6941Epoch 7/10: [================              ] 35/63 batches, loss: 0.6940Epoch 7/10: [=================             ] 36/63 batches, loss: 0.6941Epoch 7/10: [=================             ] 37/63 batches, loss: 0.6942Epoch 7/10: [==================            ] 38/63 batches, loss: 0.6943Epoch 7/10: [==================            ] 39/63 batches, loss: 0.6940Epoch 7/10: [===================           ] 40/63 batches, loss: 0.6940Epoch 7/10: [===================           ] 41/63 batches, loss: 0.6936Epoch 7/10: [====================          ] 42/63 batches, loss: 0.6934Epoch 7/10: [====================          ] 43/63 batches, loss: 0.6935Epoch 7/10: [====================          ] 44/63 batches, loss: 0.6936Epoch 7/10: [=====================         ] 45/63 batches, loss: 0.6940Epoch 7/10: [=====================         ] 46/63 batches, loss: 0.6940Epoch 7/10: [======================        ] 47/63 batches, loss: 0.6940Epoch 7/10: [======================        ] 48/63 batches, loss: 0.6938Epoch 7/10: [=======================       ] 49/63 batches, loss: 0.6939Epoch 7/10: [=======================       ] 50/63 batches, loss: 0.6938Epoch 7/10: [========================      ] 51/63 batches, loss: 0.6935Epoch 7/10: [========================      ] 52/63 batches, loss: 0.6935Epoch 7/10: [=========================     ] 53/63 batches, loss: 0.6935Epoch 7/10: [=========================     ] 54/63 batches, loss: 0.6936Epoch 7/10: [==========================    ] 55/63 batches, loss: 0.6934Epoch 7/10: [==========================    ] 56/63 batches, loss: 0.6938Epoch 7/10: [===========================   ] 57/63 batches, loss: 0.6936Epoch 7/10: [===========================   ] 58/63 batches, loss: 0.6935Epoch 7/10: [============================  ] 59/63 batches, loss: 0.6933Epoch 7/10: [============================  ] 60/63 batches, loss: 0.6933Epoch 7/10: [============================= ] 61/63 batches, loss: 0.6930Epoch 7/10: [============================= ] 62/63 batches, loss: 0.6929Epoch 7/10: [==============================] 63/63 batches, loss: 0.6931
[2025-04-29 20:45:44,591][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6931
[2025-04-29 20:45:44,818][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6976, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
[2025-04-29 20:45:44,819][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/63 batches, loss: 0.6890Epoch 8/10: [                              ] 2/63 batches, loss: 0.6814Epoch 8/10: [=                             ] 3/63 batches, loss: 0.6780Epoch 8/10: [=                             ] 4/63 batches, loss: 0.6837Epoch 8/10: [==                            ] 5/63 batches, loss: 0.6798Epoch 8/10: [==                            ] 6/63 batches, loss: 0.6819Epoch 8/10: [===                           ] 7/63 batches, loss: 0.6831Epoch 8/10: [===                           ] 8/63 batches, loss: 0.6896Epoch 8/10: [====                          ] 9/63 batches, loss: 0.6902Epoch 8/10: [====                          ] 10/63 batches, loss: 0.6910Epoch 8/10: [=====                         ] 11/63 batches, loss: 0.6896Epoch 8/10: [=====                         ] 12/63 batches, loss: 0.6909Epoch 8/10: [======                        ] 13/63 batches, loss: 0.6897Epoch 8/10: [======                        ] 14/63 batches, loss: 0.6916Epoch 8/10: [=======                       ] 15/63 batches, loss: 0.6920Epoch 8/10: [=======                       ] 16/63 batches, loss: 0.6937Epoch 8/10: [========                      ] 17/63 batches, loss: 0.6940Epoch 8/10: [========                      ] 18/63 batches, loss: 0.6944Epoch 8/10: [=========                     ] 19/63 batches, loss: 0.6935Epoch 8/10: [=========                     ] 20/63 batches, loss: 0.6926Epoch 8/10: [==========                    ] 21/63 batches, loss: 0.6918Epoch 8/10: [==========                    ] 22/63 batches, loss: 0.6918Epoch 8/10: [==========                    ] 23/63 batches, loss: 0.6917Epoch 8/10: [===========                   ] 24/63 batches, loss: 0.6916Epoch 8/10: [===========                   ] 25/63 batches, loss: 0.6923Epoch 8/10: [============                  ] 26/63 batches, loss: 0.6922Epoch 8/10: [============                  ] 27/63 batches, loss: 0.6929Epoch 8/10: [=============                 ] 28/63 batches, loss: 0.6936Epoch 8/10: [=============                 ] 29/63 batches, loss: 0.6941Epoch 8/10: [==============                ] 30/63 batches, loss: 0.6952Epoch 8/10: [==============                ] 31/63 batches, loss: 0.6948Epoch 8/10: [===============               ] 32/63 batches, loss: 0.6945Epoch 8/10: [===============               ] 33/63 batches, loss: 0.6947Epoch 8/10: [================              ] 34/63 batches, loss: 0.6949Epoch 8/10: [================              ] 35/63 batches, loss: 0.6945Epoch 8/10: [=================             ] 36/63 batches, loss: 0.6944Epoch 8/10: [=================             ] 37/63 batches, loss: 0.6945Epoch 8/10: [==================            ] 38/63 batches, loss: 0.6947Epoch 8/10: [==================            ] 39/63 batches, loss: 0.6949Epoch 8/10: [===================           ] 40/63 batches, loss: 0.6949Epoch 8/10: [===================           ] 41/63 batches, loss: 0.6950Epoch 8/10: [====================          ] 42/63 batches, loss: 0.6947Epoch 8/10: [====================          ] 43/63 batches, loss: 0.6947Epoch 8/10: [====================          ] 44/63 batches, loss: 0.6947Epoch 8/10: [=====================         ] 45/63 batches, loss: 0.6950Epoch 8/10: [=====================         ] 46/63 batches, loss: 0.6955Epoch 8/10: [======================        ] 47/63 batches, loss: 0.6956Epoch 8/10: [======================        ] 48/63 batches, loss: 0.6954Epoch 8/10: [=======================       ] 49/63 batches, loss: 0.6953Epoch 8/10: [=======================       ] 50/63 batches, loss: 0.6952Epoch 8/10: [========================      ] 51/63 batches, loss: 0.6951Epoch 8/10: [========================      ] 52/63 batches, loss: 0.6946Epoch 8/10: [=========================     ] 53/63 batches, loss: 0.6944Epoch 8/10: [=========================     ] 54/63 batches, loss: 0.6943Epoch 8/10: [==========================    ] 55/63 batches, loss: 0.6939Epoch 8/10: [==========================    ] 56/63 batches, loss: 0.6938Epoch 8/10: [===========================   ] 57/63 batches, loss: 0.6938Epoch 8/10: [===========================   ] 58/63 batches, loss: 0.6938Epoch 8/10: [============================  ] 59/63 batches, loss: 0.6937Epoch 8/10: [============================  ] 60/63 batches, loss: 0.6935Epoch 8/10: [============================= ] 61/63 batches, loss: 0.6936Epoch 8/10: [============================= ] 62/63 batches, loss: 0.6936Epoch 8/10: [==============================] 63/63 batches, loss: 0.6937
[2025-04-29 20:45:50,628][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6937
[2025-04-29 20:45:50,862][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6978, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
[2025-04-29 20:45:50,862][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 20:45:50,862][src.training.lm_trainer][INFO] - Early stopping at epoch 8
[2025-04-29 20:45:50,862][src.training.lm_trainer][INFO] - Training completed in 50.18 seconds
[2025-04-29 20:45:50,862][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 20:45:53,393][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.4994974874371859, 'f1': 0.6662198391420912}
[2025-04-29 20:45:53,394][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
[2025-04-29 20:45:53,394][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.2857142857142857, 'f1': 0.4444444444444444}
[2025-04-29 20:45:55,037][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control2/ar/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁
wandb:          best_val_f1 ▁▁▁
wandb:        best_val_loss █▇▁
wandb:                epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁
wandb:           train_loss █▅▇█▆▄▁▃
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁▁▁
wandb:             val_loss ▆█▅▅▁▂▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.45455
wandb:          best_val_f1 0.625
wandb:        best_val_loss 0.6975
wandb:                epoch 8
wandb:  final_test_accuracy 0.28571
wandb:        final_test_f1 0.44444
wandb: final_train_accuracy 0.4995
wandb:       final_train_f1 0.66622
wandb:   final_val_accuracy 0.45455
wandb:         final_val_f1 0.625
wandb:        learning_rate 2e-05
wandb:           train_loss 0.69372
wandb:           train_time 50.18044
wandb:         val_accuracy 0.45455
wandb:               val_f1 0.625
wandb:             val_loss 0.69776
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_204448-yx5w5qfq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_204448-yx5w5qfq/logs
Experiment finetune_question_type_control2_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control2/results.json
Running experiment: finetune_complexity_control1_ar
Command: python -m src.experiments.run_experiment             "hydra.job.chdir=False"             "hydra.run.dir=."             "experiment=finetune"             "experiment.tasks=complexity"             "experiment.use_controls=true"             "experiment.control_index=1"             "model=glot500_finetune"             "model.lm_name=cis-lmu/glot500-base"             "model.dropout=0.1"             "model.freeze_model=false"             "model.finetune=true"             "data.languages=[ar]"             "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"             "training.task_type=regression"             "training.num_epochs=10"             "training.batch_size=16"             "training.lr=2e-5"             "+training.gradient_accumulation_steps=2"                          "experiment_name=finetune_complexity_control1_ar"             "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/ar/control1"             "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 20:46:11,971][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/control1
experiment_name: finetune_complexity_control1_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_finetune
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  finetune: true
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1
    lr: 2.0e-05
    batch_size: 8

[2025-04-29 20:46:11,971][__main__][INFO] - Normalized task: complexity
[2025-04-29 20:46:11,971][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 20:46:11,971][__main__][INFO] - Determined Task Type: regression
[2025-04-29 20:46:11,975][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 20:46:11,976][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 20:46:13,952][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 20:46:16,295][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 20:46:16,295][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:46:16,370][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-04-29 20:46:16,411][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-04-29 20:46:16,550][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 20:46:16,557][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:46:16,558][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 20:46:16,560][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:46:16,589][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:46:16,631][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:46:16,649][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 20:46:16,650][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:46:16,650][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 20:46:16,651][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:46:16,682][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:46:16,724][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:46:16,740][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 20:46:16,741][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:46:16,741][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 20:46:16,742][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 20:46:16,743][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 20:46:16,743][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 20:46:16,743][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 20:46:16,743][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 20:46:16,744][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 20:46:16,744][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 20:46:16,744][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 20:46:16,744][src.data.datasets][INFO] - Sample label: 0.20462249219417572
[2025-04-29 20:46:16,744][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 20:46:16,744][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 20:46:16,744][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 20:46:16,744][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 20:46:16,744][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 20:46:16,745][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 20:46:16,745][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 20:46:16,745][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 20:46:16,745][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 20:46:16,745][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 20:46:16,745][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 20:46:16,745][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 20:46:16,745][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 20:46:16,745][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 20:46:16,745][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 20:46:16,746][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 20:46:16,746][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 20:46:16,746][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 20:46:16,746][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 20:46:16,746][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 20:46:21,388][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 20:46:21,388][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,388][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,388][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,389][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,389][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,389][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,389][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,389][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,389][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,389][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,389][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,389][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,389][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,389][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,389][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,389][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,390][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,390][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,390][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,390][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,390][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,390][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,390][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,390][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,390][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,390][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,390][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,390][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,390][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,390][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,390][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,391][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,391][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,391][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,391][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,391][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,391][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,391][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,391][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,391][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,391][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,391][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,391][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,391][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,391][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,391][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,391][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,392][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,392][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,392][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,392][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,392][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,392][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,392][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,392][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,392][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,392][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,392][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,392][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,392][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,392][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,392][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,393][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,393][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,393][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,393][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,393][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,393][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,393][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,393][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,393][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,393][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,393][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,393][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,393][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,393][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,393][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,394][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,394][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,394][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,394][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,394][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,394][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,394][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,394][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,394][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,394][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,394][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,394][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,394][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,394][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,394][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,395][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,395][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,395][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,395][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,395][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,395][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,395][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,395][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,395][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,395][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,395][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,395][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,395][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,395][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,395][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,396][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,396][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,396][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,396][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,396][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,396][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,396][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,396][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,396][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,396][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,396][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,396][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,396][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,396][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,396][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,396][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,396][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,397][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,397][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,397][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,397][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,397][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,397][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,397][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,397][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,397][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,397][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,397][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,397][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,397][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,397][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,397][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,397][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,398][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,398][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,398][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,398][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,398][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,398][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,398][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,398][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,398][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,398][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,398][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,398][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,398][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,398][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,398][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,399][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,399][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,399][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,399][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,399][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,399][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,399][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,399][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,399][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,399][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,399][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,399][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,399][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,399][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,399][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,399][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,400][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,400][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,400][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,400][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,400][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,400][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,400][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,400][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,400][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,400][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,400][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,400][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,400][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,400][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,400][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,401][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,401][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,401][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,401][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,401][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,401][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,401][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,401][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,401][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,401][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,401][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,401][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,401][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 20:46:21,402][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 20:46:21,402][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 20:46:21,403][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 20:46:21,403][__main__][INFO] - Successfully created model for ar
[2025-04-29 20:46:21,404][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.1305Epoch 1/10: [                              ] 2/63 batches, loss: 0.1621Epoch 1/10: [=                             ] 3/63 batches, loss: 0.1735Epoch 1/10: [=                             ] 4/63 batches, loss: 0.1489Epoch 1/10: [==                            ] 5/63 batches, loss: 0.1347Epoch 1/10: [==                            ] 6/63 batches, loss: 0.1262Epoch 1/10: [===                           ] 7/63 batches, loss: 0.1156Epoch 1/10: [===                           ] 8/63 batches, loss: 0.1136Epoch 1/10: [====                          ] 9/63 batches, loss: 0.1095Epoch 1/10: [====                          ] 10/63 batches, loss: 0.1077Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.1078Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.1097Epoch 1/10: [======                        ] 13/63 batches, loss: 0.1078Epoch 1/10: [======                        ] 14/63 batches, loss: 0.1096Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.1108Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.1117Epoch 1/10: [========                      ] 17/63 batches, loss: 0.1095Epoch 1/10: [========                      ] 18/63 batches, loss: 0.1108Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.1118Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.1099Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.1095Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.1091Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.1102Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.1095Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.1077Epoch 1/10: [============                  ] 26/63 batches, loss: 0.1103Epoch 1/10: [============                  ] 27/63 batches, loss: 0.1092Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.1081Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.1074Epoch 1/10: [==============                ] 30/63 batches, loss: 0.1075Epoch 1/10: [==============                ] 31/63 batches, loss: 0.1085Epoch 1/10: [===============               ] 32/63 batches, loss: 0.1071Epoch 1/10: [===============               ] 33/63 batches, loss: 0.1073Epoch 1/10: [================              ] 34/63 batches, loss: 0.1066Epoch 1/10: [================              ] 35/63 batches, loss: 0.1054Epoch 1/10: [=================             ] 36/63 batches, loss: 0.1054Epoch 1/10: [=================             ] 37/63 batches, loss: 0.1049Epoch 1/10: [==================            ] 38/63 batches, loss: 0.1043Epoch 1/10: [==================            ] 39/63 batches, loss: 0.1039Epoch 1/10: [===================           ] 40/63 batches, loss: 0.1034Epoch 1/10: [===================           ] 41/63 batches, loss: 0.1030Epoch 1/10: [====================          ] 42/63 batches, loss: 0.1028Epoch 1/10: [====================          ] 43/63 batches, loss: 0.1024Epoch 1/10: [====================          ] 44/63 batches, loss: 0.1015Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.1010Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.1001Epoch 1/10: [======================        ] 47/63 batches, loss: 0.1005Epoch 1/10: [======================        ] 48/63 batches, loss: 0.1011Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.1001Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.0997Epoch 1/10: [========================      ] 51/63 batches, loss: 0.0981Epoch 1/10: [========================      ] 52/63 batches, loss: 0.0973Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.0963Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.0953Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.0947Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.0947Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.0936Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.0934Epoch 1/10: [============================  ] 59/63 batches, loss: 0.0925Epoch 1/10: [============================  ] 60/63 batches, loss: 0.0927Epoch 1/10: [============================= ] 61/63 batches, loss: 0.0921Epoch 1/10: [============================= ] 62/63 batches, loss: 0.0917Epoch 1/10: [==============================] 63/63 batches, loss: 0.0906
[2025-04-29 20:46:30,677][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0906
[2025-04-29 20:46:30,869][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0842, Metrics: {'mse': 0.0843929797410965, 'rmse': 0.29050469831157033, 'r2': -0.30077648162841797}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.1122Epoch 2/10: [                              ] 2/63 batches, loss: 0.0998Epoch 2/10: [=                             ] 3/63 batches, loss: 0.0986Epoch 2/10: [=                             ] 4/63 batches, loss: 0.0962Epoch 2/10: [==                            ] 5/63 batches, loss: 0.0885Epoch 2/10: [==                            ] 6/63 batches, loss: 0.0888Epoch 2/10: [===                           ] 7/63 batches, loss: 0.0856Epoch 2/10: [===                           ] 8/63 batches, loss: 0.0946Epoch 2/10: [====                          ] 9/63 batches, loss: 0.0935Epoch 2/10: [====                          ] 10/63 batches, loss: 0.0874Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.0854Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.0802Epoch 2/10: [======                        ] 13/63 batches, loss: 0.0773Epoch 2/10: [======                        ] 14/63 batches, loss: 0.0751Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.0740Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.0731Epoch 2/10: [========                      ] 17/63 batches, loss: 0.0718Epoch 2/10: [========                      ] 18/63 batches, loss: 0.0712Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.0702Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.0692Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.0690Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.0683Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.0677Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.0659Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.0665Epoch 2/10: [============                  ] 26/63 batches, loss: 0.0655Epoch 2/10: [============                  ] 27/63 batches, loss: 0.0638Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.0633Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.0628Epoch 2/10: [==============                ] 30/63 batches, loss: 0.0627Epoch 2/10: [==============                ] 31/63 batches, loss: 0.0631Epoch 2/10: [===============               ] 32/63 batches, loss: 0.0633Epoch 2/10: [===============               ] 33/63 batches, loss: 0.0619Epoch 2/10: [================              ] 34/63 batches, loss: 0.0612Epoch 2/10: [================              ] 35/63 batches, loss: 0.0608Epoch 2/10: [=================             ] 36/63 batches, loss: 0.0612Epoch 2/10: [=================             ] 37/63 batches, loss: 0.0608Epoch 2/10: [==================            ] 38/63 batches, loss: 0.0611Epoch 2/10: [==================            ] 39/63 batches, loss: 0.0617Epoch 2/10: [===================           ] 40/63 batches, loss: 0.0615Epoch 2/10: [===================           ] 41/63 batches, loss: 0.0613Epoch 2/10: [====================          ] 42/63 batches, loss: 0.0621Epoch 2/10: [====================          ] 43/63 batches, loss: 0.0618Epoch 2/10: [====================          ] 44/63 batches, loss: 0.0614Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.0613Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.0610Epoch 2/10: [======================        ] 47/63 batches, loss: 0.0606Epoch 2/10: [======================        ] 48/63 batches, loss: 0.0599Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.0594Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.0592Epoch 2/10: [========================      ] 51/63 batches, loss: 0.0589Epoch 2/10: [========================      ] 52/63 batches, loss: 0.0588Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.0583Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.0579Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.0575Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.0577Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.0573Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.0568Epoch 2/10: [============================  ] 59/63 batches, loss: 0.0564Epoch 2/10: [============================  ] 60/63 batches, loss: 0.0562Epoch 2/10: [============================= ] 61/63 batches, loss: 0.0561Epoch 2/10: [============================= ] 62/63 batches, loss: 0.0560Epoch 2/10: [==============================] 63/63 batches, loss: 0.0560
[2025-04-29 20:46:37,024][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0560
[2025-04-29 20:46:37,241][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0780, Metrics: {'mse': 0.07836001366376877, 'rmse': 0.27992858672127213, 'r2': -0.20778846740722656}
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.0966Epoch 3/10: [                              ] 2/63 batches, loss: 0.0817Epoch 3/10: [=                             ] 3/63 batches, loss: 0.0738Epoch 3/10: [=                             ] 4/63 batches, loss: 0.0673Epoch 3/10: [==                            ] 5/63 batches, loss: 0.0721Epoch 3/10: [==                            ] 6/63 batches, loss: 0.0706Epoch 3/10: [===                           ] 7/63 batches, loss: 0.0665Epoch 3/10: [===                           ] 8/63 batches, loss: 0.0632Epoch 3/10: [====                          ] 9/63 batches, loss: 0.0599Epoch 3/10: [====                          ] 10/63 batches, loss: 0.0611Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.0612Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.0612Epoch 3/10: [======                        ] 13/63 batches, loss: 0.0599Epoch 3/10: [======                        ] 14/63 batches, loss: 0.0600Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.0578Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.0585Epoch 3/10: [========                      ] 17/63 batches, loss: 0.0584Epoch 3/10: [========                      ] 18/63 batches, loss: 0.0564Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.0562Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.0560Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.0546Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.0530Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.0530Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.0526Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.0527Epoch 3/10: [============                  ] 26/63 batches, loss: 0.0523Epoch 3/10: [============                  ] 27/63 batches, loss: 0.0521Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.0520Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.0513Epoch 3/10: [==============                ] 30/63 batches, loss: 0.0507Epoch 3/10: [==============                ] 31/63 batches, loss: 0.0506Epoch 3/10: [===============               ] 32/63 batches, loss: 0.0512Epoch 3/10: [===============               ] 33/63 batches, loss: 0.0509Epoch 3/10: [================              ] 34/63 batches, loss: 0.0507Epoch 3/10: [================              ] 35/63 batches, loss: 0.0506Epoch 3/10: [=================             ] 36/63 batches, loss: 0.0500Epoch 3/10: [=================             ] 37/63 batches, loss: 0.0502Epoch 3/10: [==================            ] 38/63 batches, loss: 0.0509Epoch 3/10: [==================            ] 39/63 batches, loss: 0.0517Epoch 3/10: [===================           ] 40/63 batches, loss: 0.0515Epoch 3/10: [===================           ] 41/63 batches, loss: 0.0518Epoch 3/10: [====================          ] 42/63 batches, loss: 0.0517Epoch 3/10: [====================          ] 43/63 batches, loss: 0.0512Epoch 3/10: [====================          ] 44/63 batches, loss: 0.0508Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.0505Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.0502Epoch 3/10: [======================        ] 47/63 batches, loss: 0.0504Epoch 3/10: [======================        ] 48/63 batches, loss: 0.0504Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.0504Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.0506Epoch 3/10: [========================      ] 51/63 batches, loss: 0.0509Epoch 3/10: [========================      ] 52/63 batches, loss: 0.0511Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.0515Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.0515Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.0508Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.0508Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.0508Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.0503Epoch 3/10: [============================  ] 59/63 batches, loss: 0.0507Epoch 3/10: [============================  ] 60/63 batches, loss: 0.0504Epoch 3/10: [============================= ] 61/63 batches, loss: 0.0503Epoch 3/10: [============================= ] 62/63 batches, loss: 0.0498Epoch 3/10: [==============================] 63/63 batches, loss: 0.0498
[2025-04-29 20:46:43,437][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0498
[2025-04-29 20:46:43,668][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0752, Metrics: {'mse': 0.07556869089603424, 'rmse': 0.27489760074623104, 'r2': -0.1647648811340332}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.0457Epoch 4/10: [                              ] 2/63 batches, loss: 0.0534Epoch 4/10: [=                             ] 3/63 batches, loss: 0.0623Epoch 4/10: [=                             ] 4/63 batches, loss: 0.0605Epoch 4/10: [==                            ] 5/63 batches, loss: 0.0637Epoch 4/10: [==                            ] 6/63 batches, loss: 0.0596Epoch 4/10: [===                           ] 7/63 batches, loss: 0.0585Epoch 4/10: [===                           ] 8/63 batches, loss: 0.0545Epoch 4/10: [====                          ] 9/63 batches, loss: 0.0570Epoch 4/10: [====                          ] 10/63 batches, loss: 0.0616Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.0650Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.0645Epoch 4/10: [======                        ] 13/63 batches, loss: 0.0645Epoch 4/10: [======                        ] 14/63 batches, loss: 0.0662Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.0661Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.0670Epoch 4/10: [========                      ] 17/63 batches, loss: 0.0663Epoch 4/10: [========                      ] 18/63 batches, loss: 0.0640Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.0639Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.0641Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.0639Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.0623Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.0633Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.0635Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.0634Epoch 4/10: [============                  ] 26/63 batches, loss: 0.0636Epoch 4/10: [============                  ] 27/63 batches, loss: 0.0634Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.0623Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.0629Epoch 4/10: [==============                ] 30/63 batches, loss: 0.0623Epoch 4/10: [==============                ] 31/63 batches, loss: 0.0633Epoch 4/10: [===============               ] 32/63 batches, loss: 0.0629Epoch 4/10: [===============               ] 33/63 batches, loss: 0.0625Epoch 4/10: [================              ] 34/63 batches, loss: 0.0621Epoch 4/10: [================              ] 35/63 batches, loss: 0.0617Epoch 4/10: [=================             ] 36/63 batches, loss: 0.0625Epoch 4/10: [=================             ] 37/63 batches, loss: 0.0628Epoch 4/10: [==================            ] 38/63 batches, loss: 0.0630Epoch 4/10: [==================            ] 39/63 batches, loss: 0.0635Epoch 4/10: [===================           ] 40/63 batches, loss: 0.0627Epoch 4/10: [===================           ] 41/63 batches, loss: 0.0626Epoch 4/10: [====================          ] 42/63 batches, loss: 0.0627Epoch 4/10: [====================          ] 43/63 batches, loss: 0.0619Epoch 4/10: [====================          ] 44/63 batches, loss: 0.0619Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.0614Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.0608Epoch 4/10: [======================        ] 47/63 batches, loss: 0.0602Epoch 4/10: [======================        ] 48/63 batches, loss: 0.0605Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.0608Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.0607Epoch 4/10: [========================      ] 51/63 batches, loss: 0.0606Epoch 4/10: [========================      ] 52/63 batches, loss: 0.0600Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.0599Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.0596Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.0589Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.0583Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.0576Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.0571Epoch 4/10: [============================  ] 59/63 batches, loss: 0.0565Epoch 4/10: [============================  ] 60/63 batches, loss: 0.0569Epoch 4/10: [============================= ] 61/63 batches, loss: 0.0563Epoch 4/10: [============================= ] 62/63 batches, loss: 0.0562Epoch 4/10: [==============================] 63/63 batches, loss: 0.0560
[2025-04-29 20:46:49,800][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0560
[2025-04-29 20:46:50,025][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1077, Metrics: {'mse': 0.10639529675245285, 'rmse': 0.3261829191610941, 'r2': -0.6399054527282715}
[2025-04-29 20:46:50,025][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.0496Epoch 5/10: [                              ] 2/63 batches, loss: 0.0349Epoch 5/10: [=                             ] 3/63 batches, loss: 0.0365Epoch 5/10: [=                             ] 4/63 batches, loss: 0.0392Epoch 5/10: [==                            ] 5/63 batches, loss: 0.0418Epoch 5/10: [==                            ] 6/63 batches, loss: 0.0395Epoch 5/10: [===                           ] 7/63 batches, loss: 0.0437Epoch 5/10: [===                           ] 8/63 batches, loss: 0.0429Epoch 5/10: [====                          ] 9/63 batches, loss: 0.0432Epoch 5/10: [====                          ] 10/63 batches, loss: 0.0435Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.0421Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.0425Epoch 5/10: [======                        ] 13/63 batches, loss: 0.0431Epoch 5/10: [======                        ] 14/63 batches, loss: 0.0425Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.0407Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.0409Epoch 5/10: [========                      ] 17/63 batches, loss: 0.0404Epoch 5/10: [========                      ] 18/63 batches, loss: 0.0406Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.0414Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.0416Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.0416Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.0427Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.0427Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.0446Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.0444Epoch 5/10: [============                  ] 26/63 batches, loss: 0.0443Epoch 5/10: [============                  ] 27/63 batches, loss: 0.0443Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.0437Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.0434Epoch 5/10: [==============                ] 30/63 batches, loss: 0.0431Epoch 5/10: [==============                ] 31/63 batches, loss: 0.0434Epoch 5/10: [===============               ] 32/63 batches, loss: 0.0432Epoch 5/10: [===============               ] 33/63 batches, loss: 0.0431Epoch 5/10: [================              ] 34/63 batches, loss: 0.0429Epoch 5/10: [================              ] 35/63 batches, loss: 0.0428Epoch 5/10: [=================             ] 36/63 batches, loss: 0.0421Epoch 5/10: [=================             ] 37/63 batches, loss: 0.0417Epoch 5/10: [==================            ] 38/63 batches, loss: 0.0414Epoch 5/10: [==================            ] 39/63 batches, loss: 0.0411Epoch 5/10: [===================           ] 40/63 batches, loss: 0.0415Epoch 5/10: [===================           ] 41/63 batches, loss: 0.0413Epoch 5/10: [====================          ] 42/63 batches, loss: 0.0414Epoch 5/10: [====================          ] 43/63 batches, loss: 0.0411Epoch 5/10: [====================          ] 44/63 batches, loss: 0.0416Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.0412Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.0415Epoch 5/10: [======================        ] 47/63 batches, loss: 0.0415Epoch 5/10: [======================        ] 48/63 batches, loss: 0.0412Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.0409Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.0410Epoch 5/10: [========================      ] 51/63 batches, loss: 0.0411Epoch 5/10: [========================      ] 52/63 batches, loss: 0.0409Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.0408Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.0407Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.0409Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.0411Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.0408Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.0405Epoch 5/10: [============================  ] 59/63 batches, loss: 0.0409Epoch 5/10: [============================  ] 60/63 batches, loss: 0.0410Epoch 5/10: [============================= ] 61/63 batches, loss: 0.0409Epoch 5/10: [============================= ] 62/63 batches, loss: 0.0410Epoch 5/10: [==============================] 63/63 batches, loss: 0.0414
[2025-04-29 20:46:55,823][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0414
[2025-04-29 20:46:56,040][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0563, Metrics: {'mse': 0.05673797056078911, 'rmse': 0.2381973353351987, 'r2': 0.12547916173934937}
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.0493Epoch 6/10: [                              ] 2/63 batches, loss: 0.0634Epoch 6/10: [=                             ] 3/63 batches, loss: 0.0546Epoch 6/10: [=                             ] 4/63 batches, loss: 0.0567Epoch 6/10: [==                            ] 5/63 batches, loss: 0.0522Epoch 6/10: [==                            ] 6/63 batches, loss: 0.0488Epoch 6/10: [===                           ] 7/63 batches, loss: 0.0473Epoch 6/10: [===                           ] 8/63 batches, loss: 0.0478Epoch 6/10: [====                          ] 9/63 batches, loss: 0.0490Epoch 6/10: [====                          ] 10/63 batches, loss: 0.0476Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.0462Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.0466Epoch 6/10: [======                        ] 13/63 batches, loss: 0.0456Epoch 6/10: [======                        ] 14/63 batches, loss: 0.0437Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.0473Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.0464Epoch 6/10: [========                      ] 17/63 batches, loss: 0.0454Epoch 6/10: [========                      ] 18/63 batches, loss: 0.0459Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.0460Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.0458Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.0453Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.0465Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.0456Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.0451Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.0446Epoch 6/10: [============                  ] 26/63 batches, loss: 0.0434Epoch 6/10: [============                  ] 27/63 batches, loss: 0.0441Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.0440Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.0455Epoch 6/10: [==============                ] 30/63 batches, loss: 0.0460Epoch 6/10: [==============                ] 31/63 batches, loss: 0.0460Epoch 6/10: [===============               ] 32/63 batches, loss: 0.0469Epoch 6/10: [===============               ] 33/63 batches, loss: 0.0468Epoch 6/10: [================              ] 34/63 batches, loss: 0.0462Epoch 6/10: [================              ] 35/63 batches, loss: 0.0461Epoch 6/10: [=================             ] 36/63 batches, loss: 0.0459Epoch 6/10: [=================             ] 37/63 batches, loss: 0.0464Epoch 6/10: [==================            ] 38/63 batches, loss: 0.0459Epoch 6/10: [==================            ] 39/63 batches, loss: 0.0461Epoch 6/10: [===================           ] 40/63 batches, loss: 0.0462Epoch 6/10: [===================           ] 41/63 batches, loss: 0.0466Epoch 6/10: [====================          ] 42/63 batches, loss: 0.0462Epoch 6/10: [====================          ] 43/63 batches, loss: 0.0467Epoch 6/10: [====================          ] 44/63 batches, loss: 0.0471Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.0473Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.0477Epoch 6/10: [======================        ] 47/63 batches, loss: 0.0476Epoch 6/10: [======================        ] 48/63 batches, loss: 0.0477Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.0475Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.0476Epoch 6/10: [========================      ] 51/63 batches, loss: 0.0471Epoch 6/10: [========================      ] 52/63 batches, loss: 0.0469Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.0471Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.0473Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.0471Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.0473Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.0473Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.0471Epoch 6/10: [============================  ] 59/63 batches, loss: 0.0468Epoch 6/10: [============================  ] 60/63 batches, loss: 0.0466Epoch 6/10: [============================= ] 61/63 batches, loss: 0.0466Epoch 6/10: [============================= ] 62/63 batches, loss: 0.0469Epoch 6/10: [==============================] 63/63 batches, loss: 0.0463
[2025-04-29 20:47:02,263][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0463
[2025-04-29 20:47:02,487][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0700, Metrics: {'mse': 0.06995337456464767, 'rmse': 0.264487002638405, 'r2': -0.07821404933929443}
[2025-04-29 20:47:02,487][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/63 batches, loss: 0.0789Epoch 7/10: [                              ] 2/63 batches, loss: 0.0680Epoch 7/10: [=                             ] 3/63 batches, loss: 0.0638Epoch 7/10: [=                             ] 4/63 batches, loss: 0.0590Epoch 7/10: [==                            ] 5/63 batches, loss: 0.0546Epoch 7/10: [==                            ] 6/63 batches, loss: 0.0524Epoch 7/10: [===                           ] 7/63 batches, loss: 0.0489Epoch 7/10: [===                           ] 8/63 batches, loss: 0.0464Epoch 7/10: [====                          ] 9/63 batches, loss: 0.0437Epoch 7/10: [====                          ] 10/63 batches, loss: 0.0411Epoch 7/10: [=====                         ] 11/63 batches, loss: 0.0407Epoch 7/10: [=====                         ] 12/63 batches, loss: 0.0427Epoch 7/10: [======                        ] 13/63 batches, loss: 0.0421Epoch 7/10: [======                        ] 14/63 batches, loss: 0.0431Epoch 7/10: [=======                       ] 15/63 batches, loss: 0.0417Epoch 7/10: [=======                       ] 16/63 batches, loss: 0.0413Epoch 7/10: [========                      ] 17/63 batches, loss: 0.0413Epoch 7/10: [========                      ] 18/63 batches, loss: 0.0414Epoch 7/10: [=========                     ] 19/63 batches, loss: 0.0408Epoch 7/10: [=========                     ] 20/63 batches, loss: 0.0401Epoch 7/10: [==========                    ] 21/63 batches, loss: 0.0394Epoch 7/10: [==========                    ] 22/63 batches, loss: 0.0393Epoch 7/10: [==========                    ] 23/63 batches, loss: 0.0391Epoch 7/10: [===========                   ] 24/63 batches, loss: 0.0396Epoch 7/10: [===========                   ] 25/63 batches, loss: 0.0395Epoch 7/10: [============                  ] 26/63 batches, loss: 0.0393Epoch 7/10: [============                  ] 27/63 batches, loss: 0.0390Epoch 7/10: [=============                 ] 28/63 batches, loss: 0.0385Epoch 7/10: [=============                 ] 29/63 batches, loss: 0.0378Epoch 7/10: [==============                ] 30/63 batches, loss: 0.0375Epoch 7/10: [==============                ] 31/63 batches, loss: 0.0372Epoch 7/10: [===============               ] 32/63 batches, loss: 0.0368Epoch 7/10: [===============               ] 33/63 batches, loss: 0.0367Epoch 7/10: [================              ] 34/63 batches, loss: 0.0374Epoch 7/10: [================              ] 35/63 batches, loss: 0.0375Epoch 7/10: [=================             ] 36/63 batches, loss: 0.0379Epoch 7/10: [=================             ] 37/63 batches, loss: 0.0378Epoch 7/10: [==================            ] 38/63 batches, loss: 0.0383Epoch 7/10: [==================            ] 39/63 batches, loss: 0.0378Epoch 7/10: [===================           ] 40/63 batches, loss: 0.0379Epoch 7/10: [===================           ] 41/63 batches, loss: 0.0378Epoch 7/10: [====================          ] 42/63 batches, loss: 0.0379Epoch 7/10: [====================          ] 43/63 batches, loss: 0.0382Epoch 7/10: [====================          ] 44/63 batches, loss: 0.0379Epoch 7/10: [=====================         ] 45/63 batches, loss: 0.0376Epoch 7/10: [=====================         ] 46/63 batches, loss: 0.0375Epoch 7/10: [======================        ] 47/63 batches, loss: 0.0378Epoch 7/10: [======================        ] 48/63 batches, loss: 0.0380Epoch 7/10: [=======================       ] 49/63 batches, loss: 0.0377Epoch 7/10: [=======================       ] 50/63 batches, loss: 0.0376Epoch 7/10: [========================      ] 51/63 batches, loss: 0.0375Epoch 7/10: [========================      ] 52/63 batches, loss: 0.0378Epoch 7/10: [=========================     ] 53/63 batches, loss: 0.0378Epoch 7/10: [=========================     ] 54/63 batches, loss: 0.0379Epoch 7/10: [==========================    ] 55/63 batches, loss: 0.0383Epoch 7/10: [==========================    ] 56/63 batches, loss: 0.0384Epoch 7/10: [===========================   ] 57/63 batches, loss: 0.0386Epoch 7/10: [===========================   ] 58/63 batches, loss: 0.0384Epoch 7/10: [============================  ] 59/63 batches, loss: 0.0382Epoch 7/10: [============================  ] 60/63 batches, loss: 0.0382Epoch 7/10: [============================= ] 61/63 batches, loss: 0.0380Epoch 7/10: [============================= ] 62/63 batches, loss: 0.0379Epoch 7/10: [==============================] 63/63 batches, loss: 0.0389
[2025-04-29 20:47:08,257][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0389
[2025-04-29 20:47:08,491][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0706, Metrics: {'mse': 0.07049758732318878, 'rmse': 0.265513817574884, 'r2': -0.08660233020782471}
[2025-04-29 20:47:08,492][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/63 batches, loss: 0.0305Epoch 8/10: [                              ] 2/63 batches, loss: 0.0348Epoch 8/10: [=                             ] 3/63 batches, loss: 0.0360Epoch 8/10: [=                             ] 4/63 batches, loss: 0.0368Epoch 8/10: [==                            ] 5/63 batches, loss: 0.0394Epoch 8/10: [==                            ] 6/63 batches, loss: 0.0421Epoch 8/10: [===                           ] 7/63 batches, loss: 0.0418Epoch 8/10: [===                           ] 8/63 batches, loss: 0.0431Epoch 8/10: [====                          ] 9/63 batches, loss: 0.0455Epoch 8/10: [====                          ] 10/63 batches, loss: 0.0455Epoch 8/10: [=====                         ] 11/63 batches, loss: 0.0444Epoch 8/10: [=====                         ] 12/63 batches, loss: 0.0433Epoch 8/10: [======                        ] 13/63 batches, loss: 0.0452Epoch 8/10: [======                        ] 14/63 batches, loss: 0.0457Epoch 8/10: [=======                       ] 15/63 batches, loss: 0.0463Epoch 8/10: [=======                       ] 16/63 batches, loss: 0.0455Epoch 8/10: [========                      ] 17/63 batches, loss: 0.0462Epoch 8/10: [========                      ] 18/63 batches, loss: 0.0469Epoch 8/10: [=========                     ] 19/63 batches, loss: 0.0459Epoch 8/10: [=========                     ] 20/63 batches, loss: 0.0462Epoch 8/10: [==========                    ] 21/63 batches, loss: 0.0458Epoch 8/10: [==========                    ] 22/63 batches, loss: 0.0452Epoch 8/10: [==========                    ] 23/63 batches, loss: 0.0449Epoch 8/10: [===========                   ] 24/63 batches, loss: 0.0445Epoch 8/10: [===========                   ] 25/63 batches, loss: 0.0441Epoch 8/10: [============                  ] 26/63 batches, loss: 0.0432Epoch 8/10: [============                  ] 27/63 batches, loss: 0.0428Epoch 8/10: [=============                 ] 28/63 batches, loss: 0.0432Epoch 8/10: [=============                 ] 29/63 batches, loss: 0.0433Epoch 8/10: [==============                ] 30/63 batches, loss: 0.0432Epoch 8/10: [==============                ] 31/63 batches, loss: 0.0431Epoch 8/10: [===============               ] 32/63 batches, loss: 0.0428Epoch 8/10: [===============               ] 33/63 batches, loss: 0.0428Epoch 8/10: [================              ] 34/63 batches, loss: 0.0427Epoch 8/10: [================              ] 35/63 batches, loss: 0.0420Epoch 8/10: [=================             ] 36/63 batches, loss: 0.0416Epoch 8/10: [=================             ] 37/63 batches, loss: 0.0426Epoch 8/10: [==================            ] 38/63 batches, loss: 0.0428Epoch 8/10: [==================            ] 39/63 batches, loss: 0.0432Epoch 8/10: [===================           ] 40/63 batches, loss: 0.0427Epoch 8/10: [===================           ] 41/63 batches, loss: 0.0423Epoch 8/10: [====================          ] 42/63 batches, loss: 0.0418Epoch 8/10: [====================          ] 43/63 batches, loss: 0.0418Epoch 8/10: [====================          ] 44/63 batches, loss: 0.0416Epoch 8/10: [=====================         ] 45/63 batches, loss: 0.0413Epoch 8/10: [=====================         ] 46/63 batches, loss: 0.0415Epoch 8/10: [======================        ] 47/63 batches, loss: 0.0414Epoch 8/10: [======================        ] 48/63 batches, loss: 0.0413Epoch 8/10: [=======================       ] 49/63 batches, loss: 0.0409Epoch 8/10: [=======================       ] 50/63 batches, loss: 0.0408Epoch 8/10: [========================      ] 51/63 batches, loss: 0.0406Epoch 8/10: [========================      ] 52/63 batches, loss: 0.0411Epoch 8/10: [=========================     ] 53/63 batches, loss: 0.0411Epoch 8/10: [=========================     ] 54/63 batches, loss: 0.0410Epoch 8/10: [==========================    ] 55/63 batches, loss: 0.0414Epoch 8/10: [==========================    ] 56/63 batches, loss: 0.0416Epoch 8/10: [===========================   ] 57/63 batches, loss: 0.0415Epoch 8/10: [===========================   ] 58/63 batches, loss: 0.0413Epoch 8/10: [============================  ] 59/63 batches, loss: 0.0412Epoch 8/10: [============================  ] 60/63 batches, loss: 0.0411Epoch 8/10: [============================= ] 61/63 batches, loss: 0.0408Epoch 8/10: [============================= ] 62/63 batches, loss: 0.0407Epoch 8/10: [==============================] 63/63 batches, loss: 0.0402
[2025-04-29 20:47:14,297][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0402
[2025-04-29 20:47:14,530][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0803, Metrics: {'mse': 0.07993925362825394, 'rmse': 0.2827353066531556, 'r2': -0.23212969303131104}
[2025-04-29 20:47:14,531][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 20:47:14,531][src.training.lm_trainer][INFO] - Early stopping at epoch 8
[2025-04-29 20:47:14,531][src.training.lm_trainer][INFO] - Training completed in 50.39 seconds
[2025-04-29 20:47:14,531][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 20:47:17,069][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.04532364755868912, 'rmse': 0.21289351225128753, 'r2': -0.4764671325683594}
[2025-04-29 20:47:17,070][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05673797056078911, 'rmse': 0.2381973353351987, 'r2': 0.12547916173934937}
[2025-04-29 20:47:17,070][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.053900137543678284, 'rmse': 0.2321640315459703, 'r2': 0.07078564167022705}
[2025-04-29 20:47:18,742][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/control1/ar/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▆▁
wandb:     best_val_mse █▆▆▁
wandb:      best_val_r2 ▁▃▃█
wandb:    best_val_rmse █▇▆▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▃▁▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▅▄▄█▁▃▃▄
wandb:          val_mse ▅▄▄█▁▃▃▄
wandb:           val_r2 ▄▅▅▁█▆▆▅
wandb:         val_rmse ▅▄▄█▁▃▃▅
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05631
wandb:     best_val_mse 0.05674
wandb:      best_val_r2 0.12548
wandb:    best_val_rmse 0.2382
wandb:            epoch 8
wandb:   final_test_mse 0.0539
wandb:    final_test_r2 0.07079
wandb:  final_test_rmse 0.23216
wandb:  final_train_mse 0.04532
wandb:   final_train_r2 -0.47647
wandb: final_train_rmse 0.21289
wandb:    final_val_mse 0.05674
wandb:     final_val_r2 0.12548
wandb:   final_val_rmse 0.2382
wandb:    learning_rate 2e-05
wandb:       train_loss 0.0402
wandb:       train_time 50.39328
wandb:         val_loss 0.08032
wandb:          val_mse 0.07994
wandb:           val_r2 -0.23213
wandb:         val_rmse 0.28274
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_204612-dgubojk6
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_204612-dgubojk6/logs
Experiment finetune_complexity_control1_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/control1/results.json
Running experiment: finetune_complexity_control2_ar
Command: python -m src.experiments.run_experiment             "hydra.job.chdir=False"             "hydra.run.dir=."             "experiment=finetune"             "experiment.tasks=complexity"             "experiment.use_controls=true"             "experiment.control_index=2"             "model=glot500_finetune"             "model.lm_name=cis-lmu/glot500-base"             "model.dropout=0.1"             "model.freeze_model=false"             "model.finetune=true"             "data.languages=[ar]"             "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"             "training.task_type=regression"             "training.num_epochs=10"             "training.batch_size=16"             "training.lr=2e-5"             "+training.gradient_accumulation_steps=2"                          "experiment_name=finetune_complexity_control2_ar"             "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/ar/control2"             "wandb.mode=offline"
slurmstepd: error: *** JOB 64423622 ON k28i22 CANCELLED AT 2025-04-29T20:47:31 ***
