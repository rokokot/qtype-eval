SLURM_JOB_ID: 64467094
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: qtype_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Wed May  7 21:56:48 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main probing experiments (non-control)...
=======================
PROBING LAYER 2
=======================
Running experiment: probe_layer2_complexity_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/ko"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 21:57:31,522][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/ko
experiment_name: probe_layer2_complexity_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 21:57:31,522][__main__][INFO] - Normalized task: complexity
[2025-05-07 21:57:31,522][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 21:57:31,522][__main__][INFO] - Determined Task Type: regression
[2025-05-07 21:57:31,527][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ko']
[2025-05-07 21:57:31,527][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 21:57:36,532][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 21:57:38,927][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 21:57:38,928][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:57:39,446][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:57:39,763][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:57:40,265][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 21:57:40,271][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:57:40,271][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 21:57:40,275][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:57:40,443][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:57:40,635][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:57:40,686][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 21:57:40,687][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:57:40,687][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 21:57:40,691][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:57:40,827][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:57:40,990][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:57:41,034][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 21:57:41,035][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:57:41,035][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 21:57:41,042][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 21:57:41,042][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 21:57:41,042][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 21:57:41,042][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 21:57:41,042][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 21:57:41,042][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:57:41,043][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-05-07 21:57:41,043][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 21:57:41,043][src.data.datasets][INFO] - Sample label: 0.5104557871818542
[2025-05-07 21:57:41,043][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 21:57:41,043][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 21:57:41,043][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 21:57:41,043][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 21:57:41,043][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:57:41,043][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-05-07 21:57:41,044][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 21:57:41,044][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-05-07 21:57:41,044][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 21:57:41,044][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 21:57:41,044][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 21:57:41,044][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 21:57:41,044][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:57:41,044][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-05-07 21:57:41,044][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 21:57:41,044][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-05-07 21:57:41,044][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 21:57:41,045][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 21:57:41,045][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 21:57:41,045][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 21:57:41,045][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 21:57:50,960][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 21:57:50,962][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 21:57:50,962][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 21:57:50,962][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 21:57:50,965][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 21:57:50,965][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 21:57:50,965][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 21:57:50,965][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 21:57:50,966][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 21:57:50,966][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 21:57:50,967][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.3921Epoch 1/15: [=                             ] 2/47 batches, loss: 0.5526Epoch 1/15: [=                             ] 3/47 batches, loss: 0.4921Epoch 1/15: [==                            ] 4/47 batches, loss: 0.4927Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4735Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4248Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4057Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4141Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4176Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4098Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4005Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.3985Epoch 1/15: [========                      ] 13/47 batches, loss: 0.3824Epoch 1/15: [========                      ] 14/47 batches, loss: 0.3886Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.3775Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.3810Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.3721Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.3874Epoch 1/15: [============                  ] 19/47 batches, loss: 0.3793Epoch 1/15: [============                  ] 20/47 batches, loss: 0.3749Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.3816Epoch 1/15: [==============                ] 22/47 batches, loss: 0.3813Epoch 1/15: [==============                ] 23/47 batches, loss: 0.3695Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3613Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3584Epoch 1/15: [================              ] 26/47 batches, loss: 0.3534Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3530Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3463Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3515Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3481Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3414Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3360Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3339Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3359Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3309Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3286Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3233Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3213Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3181Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3188Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3184Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3162Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3137Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3109Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3120Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3073Epoch 1/15: [==============================] 47/47 batches, loss: 0.3097
[2025-05-07 21:57:57,548][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3097
[2025-05-07 21:57:57,831][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0916, Metrics: {'mse': 0.09674559533596039, 'rmse': 0.31103953982727084, 'r2': -1.0535712242126465}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2696Epoch 2/15: [=                             ] 2/47 batches, loss: 0.1937Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2166Epoch 2/15: [==                            ] 4/47 batches, loss: 0.1809Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2032Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2240Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2136Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2111Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2123Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2038Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2091Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2093Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2094Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2102Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2079Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2012Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.1968Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2029Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2016Epoch 2/15: [============                  ] 20/47 batches, loss: 0.1967Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.1943Epoch 2/15: [==============                ] 22/47 batches, loss: 0.1936Epoch 2/15: [==============                ] 23/47 batches, loss: 0.1940Epoch 2/15: [===============               ] 24/47 batches, loss: 0.1945Epoch 2/15: [===============               ] 25/47 batches, loss: 0.1933Epoch 2/15: [================              ] 26/47 batches, loss: 0.1898Epoch 2/15: [=================             ] 27/47 batches, loss: 0.1877Epoch 2/15: [=================             ] 28/47 batches, loss: 0.1866Epoch 2/15: [==================            ] 29/47 batches, loss: 0.1833Epoch 2/15: [===================           ] 30/47 batches, loss: 0.1842Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1834Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1830Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1854Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1855Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1822Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1796Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1800Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1770Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1753Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1741Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1721Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1704Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1685Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1667Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1662Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1651Epoch 2/15: [==============================] 47/47 batches, loss: 0.1651
[2025-05-07 21:57:59,735][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1651
[2025-05-07 21:58:00,027][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0548, Metrics: {'mse': 0.05791819095611572, 'rmse': 0.2406619848586721, 'r2': -0.22940099239349365}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1579Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1167Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1119Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1009Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1067Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1313Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1287Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1211Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1282Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1261Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1257Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1277Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1308Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1265Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1256Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1250Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1295Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1307Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1289Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1255Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1250Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1225Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1238Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1238Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1213Epoch 3/15: [================              ] 26/47 batches, loss: 0.1214Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1228Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1216Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1199Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1205Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1223Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1210Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1211Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1209Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1220Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1203Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1202Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1189Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1171Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1168Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1157Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1151Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1150Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1173Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1181Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1175Epoch 3/15: [==============================] 47/47 batches, loss: 0.1189
[2025-05-07 21:58:01,912][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1189
[2025-05-07 21:58:02,225][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0667, Metrics: {'mse': 0.07061457633972168, 'rmse': 0.26573403308519156, 'r2': -0.4989008903503418}
[2025-05-07 21:58:02,225][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1444Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1117Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1057Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1144Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1126Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1110Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1124Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1138Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1102Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1114Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1183Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1167Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1135Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1099Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1085Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1051Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1049Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1032Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1022Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1071Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1072Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1062Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1049Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1056Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1054Epoch 4/15: [================              ] 26/47 batches, loss: 0.1040Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1045Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1047Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1058Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1040Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1052Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1073Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1051Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1051Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1054Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1039Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1047Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1040Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1030Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1026Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1020Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1017Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1002Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1001Epoch 4/15: [============================  ] 45/47 batches, loss: 0.0993Epoch 4/15: [============================= ] 46/47 batches, loss: 0.0987Epoch 4/15: [==============================] 47/47 batches, loss: 0.0989
[2025-05-07 21:58:03,707][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0989
[2025-05-07 21:58:04,017][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0594, Metrics: {'mse': 0.06301184743642807, 'rmse': 0.25102160750905106, 'r2': -0.33752143383026123}
[2025-05-07 21:58:04,018][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.0907Epoch 5/15: [=                             ] 2/47 batches, loss: 0.0811Epoch 5/15: [=                             ] 3/47 batches, loss: 0.0813Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0687Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0623Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0688Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0712Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0729Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0737Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0717Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0732Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0743Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0718Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0705Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0697Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0706Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0684Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0696Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0685Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0691Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0695Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0699Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0706Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0717Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0711Epoch 5/15: [================              ] 26/47 batches, loss: 0.0703Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0701Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0692Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0700Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0705Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0716Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0715Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0703Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0694Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0703Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0698Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0695Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0696Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0701Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0703Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0701Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0698Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0694Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0695Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0710Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0708Epoch 5/15: [==============================] 47/47 batches, loss: 0.0708
[2025-05-07 21:58:05,517][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0708
[2025-05-07 21:58:05,847][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0672, Metrics: {'mse': 0.07110831141471863, 'rmse': 0.2666614171842613, 'r2': -0.5093811750411987}
[2025-05-07 21:58:05,847][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.0856Epoch 6/15: [=                             ] 2/47 batches, loss: 0.0773Epoch 6/15: [=                             ] 3/47 batches, loss: 0.0861Epoch 6/15: [==                            ] 4/47 batches, loss: 0.0835Epoch 6/15: [===                           ] 5/47 batches, loss: 0.0919Epoch 6/15: [===                           ] 6/47 batches, loss: 0.0910Epoch 6/15: [====                          ] 7/47 batches, loss: 0.0845Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.0867Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.0818Epoch 6/15: [======                        ] 10/47 batches, loss: 0.0837Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.0886Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.0887Epoch 6/15: [========                      ] 13/47 batches, loss: 0.0841Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0808Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0799Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.0812Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.0828Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.0826Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0798Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0801Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0774Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0769Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0765Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0752Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0752Epoch 6/15: [================              ] 26/47 batches, loss: 0.0744Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0739Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0736Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0735Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0727Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0727Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0723Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0722Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0724Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0722Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0712Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0706Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0702Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0701Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0696Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0698Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0695Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0692Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0695Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0694Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0691Epoch 6/15: [==============================] 47/47 batches, loss: 0.0692
[2025-05-07 21:58:07,356][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0692
[2025-05-07 21:58:07,650][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0649, Metrics: {'mse': 0.06866516917943954, 'rmse': 0.2620403960831985, 'r2': -0.4575216770172119}
[2025-05-07 21:58:07,651][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 21:58:07,651][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-07 21:58:07,651][src.training.lm_trainer][INFO] - Training completed in 12.56 seconds
[2025-05-07 21:58:07,651][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 21:58:09,853][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.019038164988160133, 'rmse': 0.13797885703309812, 'r2': 0.1447957158088684}
[2025-05-07 21:58:09,854][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05791819095611572, 'rmse': 0.2406619848586721, 'r2': -0.22940099239349365}
[2025-05-07 21:58:09,854][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04128776490688324, 'rmse': 0.20319390962054754, 'r2': -0.2819925546646118}
[2025-05-07 21:58:11,542][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/ko/ko/model.pt
[2025-05-07 21:58:11,543][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▅▄▅▄
wandb:       train_loss █▄▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▃▂▃▃
wandb:          val_mse █▁▃▂▃▃
wandb:           val_r2 ▁█▆▇▆▆
wandb:         val_rmse █▁▃▂▄▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05485
wandb:     best_val_mse 0.05792
wandb:      best_val_r2 -0.2294
wandb:    best_val_rmse 0.24066
wandb: early_stop_epoch 6
wandb:            epoch 6
wandb:   final_test_mse 0.04129
wandb:    final_test_r2 -0.28199
wandb:  final_test_rmse 0.20319
wandb:  final_train_mse 0.01904
wandb:   final_train_r2 0.1448
wandb: final_train_rmse 0.13798
wandb:    final_val_mse 0.05792
wandb:     final_val_r2 -0.2294
wandb:   final_val_rmse 0.24066
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06922
wandb:       train_time 12.55921
wandb:         val_loss 0.06487
wandb:          val_mse 0.06867
wandb:           val_r2 -0.45752
wandb:         val_rmse 0.26204
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_215731-rz6zifcs
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_215731-rz6zifcs/logs
Experiment probe_layer2_complexity_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/ko/ko/results.json for layer 2
Running control probing experiments...
=======================
PROBING LAYER 2 (CONTROL EXPERIMENTS)
=======================
Running experiment: probe_layer2_complexity_control1_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_control1_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 21:58:41,897][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/ko
experiment_name: probe_layer2_complexity_control1_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 21:58:41,897][__main__][INFO] - Normalized task: complexity
[2025-05-07 21:58:41,897][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 21:58:41,897][__main__][INFO] - Determined Task Type: regression
[2025-05-07 21:58:41,901][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ko']
[2025-05-07 21:58:41,901][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 21:58:45,660][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 21:58:47,927][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 21:58:47,928][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:58:48,091][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-07 21:58:48,154][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-07 21:58:48,524][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 21:58:48,529][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:58:48,530][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 21:58:48,532][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:58:48,622][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:58:48,711][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:58:48,742][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 21:58:48,743][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:58:48,743][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 21:58:48,745][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:58:48,895][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:58:48,974][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:58:49,005][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 21:58:49,006][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:58:49,006][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 21:58:49,008][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 21:58:49,008][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 21:58:49,008][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 21:58:49,008][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 21:58:49,008][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 21:58:49,008][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:58:49,009][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-05-07 21:58:49,009][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 21:58:49,009][src.data.datasets][INFO] - Sample label: 0.2160857915878296
[2025-05-07 21:58:49,009][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 21:58:49,009][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 21:58:49,009][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 21:58:49,009][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 21:58:49,009][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:58:49,010][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-05-07 21:58:49,010][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 21:58:49,010][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-05-07 21:58:49,010][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 21:58:49,010][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 21:58:49,010][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 21:58:49,010][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 21:58:49,010][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:58:49,010][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-05-07 21:58:49,010][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 21:58:49,010][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-05-07 21:58:49,011][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 21:58:49,011][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 21:58:49,011][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 21:58:49,011][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 21:58:49,011][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 21:58:56,289][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 21:58:56,290][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 21:58:56,291][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 21:58:56,291][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 21:58:56,293][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 21:58:56,294][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 21:58:56,294][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 21:58:56,294][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 21:58:56,294][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 21:58:56,295][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 21:58:56,295][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.3186Epoch 1/15: [=                             ] 2/47 batches, loss: 0.4612Epoch 1/15: [=                             ] 3/47 batches, loss: 0.4459Epoch 1/15: [==                            ] 4/47 batches, loss: 0.4261Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4269Epoch 1/15: [===                           ] 6/47 batches, loss: 0.3923Epoch 1/15: [====                          ] 7/47 batches, loss: 0.3779Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.3912Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4081Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4074Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.3960Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4017Epoch 1/15: [========                      ] 13/47 batches, loss: 0.3903Epoch 1/15: [========                      ] 14/47 batches, loss: 0.3867Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.3769Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.3825Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.3742Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.3824Epoch 1/15: [============                  ] 19/47 batches, loss: 0.3735Epoch 1/15: [============                  ] 20/47 batches, loss: 0.3650Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.3763Epoch 1/15: [==============                ] 22/47 batches, loss: 0.3740Epoch 1/15: [==============                ] 23/47 batches, loss: 0.3651Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3569Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3555Epoch 1/15: [================              ] 26/47 batches, loss: 0.3534Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3531Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3465Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3481Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3451Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3384Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3336Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3339Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3339Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3306Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3288Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3240Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3224Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3199Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3225Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3217Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3201Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3179Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3169Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3171Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3115Epoch 1/15: [==============================] 47/47 batches, loss: 0.3162
[2025-05-07 21:59:02,687][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3162
[2025-05-07 21:59:02,967][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1190, Metrics: {'mse': 0.12604588270187378, 'rmse': 0.35502941103783753, 'r2': -1.6755139827728271}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2923Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2148Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2533Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2203Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2290Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2394Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2301Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2316Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2243Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2209Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2258Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2205Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2236Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2201Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2223Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2161Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2097Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2139Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2097Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2038Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2007Epoch 2/15: [==============                ] 22/47 batches, loss: 0.1996Epoch 2/15: [==============                ] 23/47 batches, loss: 0.1963Epoch 2/15: [===============               ] 24/47 batches, loss: 0.1958Epoch 2/15: [===============               ] 25/47 batches, loss: 0.1945Epoch 2/15: [================              ] 26/47 batches, loss: 0.1919Epoch 2/15: [=================             ] 27/47 batches, loss: 0.1886Epoch 2/15: [=================             ] 28/47 batches, loss: 0.1874Epoch 2/15: [==================            ] 29/47 batches, loss: 0.1849Epoch 2/15: [===================           ] 30/47 batches, loss: 0.1819Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1818Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1825Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1857Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1852Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1814Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1790Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1803Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1782Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1777Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1768Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1759Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1738Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1713Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1698Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1677Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1663Epoch 2/15: [==============================] 47/47 batches, loss: 0.1641
[2025-05-07 21:59:04,925][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1641
[2025-05-07 21:59:05,184][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0708, Metrics: {'mse': 0.07436778396368027, 'rmse': 0.2727045726856817, 'r2': -0.5785683393478394}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1878Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1516Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1304Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1174Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1330Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1528Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1478Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1393Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1435Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1405Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1432Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1466Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1491Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1421Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1400Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1407Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1435Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1454Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1417Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1386Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1415Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1408Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1406Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1420Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1399Epoch 3/15: [================              ] 26/47 batches, loss: 0.1391Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1390Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1370Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1343Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1328Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1334Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1323Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1328Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1316Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1337Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1342Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1329Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1327Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1313Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1312Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1299Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1302Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1299Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1312Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1323Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1315Epoch 3/15: [==============================] 47/47 batches, loss: 0.1329
[2025-05-07 21:59:07,081][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1329
[2025-05-07 21:59:07,394][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0927, Metrics: {'mse': 0.09820783883333206, 'rmse': 0.3133812994314308, 'r2': -1.0846095085144043}
[2025-05-07 21:59:07,395][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1752Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1321Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1159Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1174Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1148Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1088Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1133Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1279Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1268Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1237Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1333Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1277Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1273Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1210Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1200Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1160Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1152Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1157Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1161Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1184Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1173Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1169Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1163Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1158Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1189Epoch 4/15: [================              ] 26/47 batches, loss: 0.1180Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1174Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1182Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1196Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1177Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1196Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1211Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1187Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1194Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1194Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1184Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1189Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1173Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1159Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1149Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1148Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1143Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1130Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1133Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1120Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1125Epoch 4/15: [==============================] 47/47 batches, loss: 0.1138
[2025-05-07 21:59:08,861][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1138
[2025-05-07 21:59:09,126][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0810, Metrics: {'mse': 0.08588176220655441, 'rmse': 0.2930559028693236, 'r2': -0.822969913482666}
[2025-05-07 21:59:09,127][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.0946Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1123Epoch 5/15: [=                             ] 3/47 batches, loss: 0.1059Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0909Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0769Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0752Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0752Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0746Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0778Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0728Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0725Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0746Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0740Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0742Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0732Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0743Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0736Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0750Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0749Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0775Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0798Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0821Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0817Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0829Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0827Epoch 5/15: [================              ] 26/47 batches, loss: 0.0806Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0819Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0811Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0806Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0824Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0824Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0828Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0831Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0822Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0852Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0854Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0851Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0850Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0855Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0864Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0863Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0866Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0865Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0859Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0869Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0863Epoch 5/15: [==============================] 47/47 batches, loss: 0.0855
[2025-05-07 21:59:10,574][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0855
[2025-05-07 21:59:10,878][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0847, Metrics: {'mse': 0.08992665261030197, 'rmse': 0.2998777294336843, 'r2': -0.908828854560852}
[2025-05-07 21:59:10,880][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.1621Epoch 6/15: [=                             ] 2/47 batches, loss: 0.1138Epoch 6/15: [=                             ] 3/47 batches, loss: 0.1208Epoch 6/15: [==                            ] 4/47 batches, loss: 0.1136Epoch 6/15: [===                           ] 5/47 batches, loss: 0.1178Epoch 6/15: [===                           ] 6/47 batches, loss: 0.1139Epoch 6/15: [====                          ] 7/47 batches, loss: 0.1071Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.1066Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.1027Epoch 6/15: [======                        ] 10/47 batches, loss: 0.1035Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.1054Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.1069Epoch 6/15: [========                      ] 13/47 batches, loss: 0.1018Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0967Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0957Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.0970Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.0959Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.0953Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0914Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0891Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0866Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0855Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0852Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0840Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0821Epoch 6/15: [================              ] 26/47 batches, loss: 0.0812Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0800Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0803Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0814Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0803Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0812Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0797Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0803Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0797Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0800Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0795Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0800Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0792Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0788Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0784Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0782Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0784Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0787Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0785Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0786Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0782Epoch 6/15: [==============================] 47/47 batches, loss: 0.0779
[2025-05-07 21:59:12,399][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0779
[2025-05-07 21:59:12,711][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0822, Metrics: {'mse': 0.0872068926692009, 'rmse': 0.2953081317356515, 'r2': -0.851097822189331}
[2025-05-07 21:59:12,712][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 21:59:12,712][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-07 21:59:12,712][src.training.lm_trainer][INFO] - Training completed in 12.47 seconds
[2025-05-07 21:59:12,712][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 21:59:14,979][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.026596518233418465, 'rmse': 0.16308438991337726, 'r2': -0.19472956657409668}
[2025-05-07 21:59:14,979][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07436778396368027, 'rmse': 0.2727045726856817, 'r2': -0.5785683393478394}
[2025-05-07 21:59:14,979][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.05147784948348999, 'rmse': 0.2268873056904903, 'r2': -0.5983965396881104}
[2025-05-07 21:59:16,661][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/ko/ko/model.pt
[2025-05-07 21:59:16,662][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▆▄▅▄
wandb:       train_loss █▄▃▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▄▂▃▃
wandb:          val_mse █▁▄▃▃▃
wandb:           val_r2 ▁█▅▆▆▆
wandb:         val_rmse █▁▄▃▃▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07076
wandb:     best_val_mse 0.07437
wandb:      best_val_r2 -0.57857
wandb:    best_val_rmse 0.2727
wandb: early_stop_epoch 6
wandb:            epoch 6
wandb:   final_test_mse 0.05148
wandb:    final_test_r2 -0.5984
wandb:  final_test_rmse 0.22689
wandb:  final_train_mse 0.0266
wandb:   final_train_r2 -0.19473
wandb: final_train_rmse 0.16308
wandb:    final_val_mse 0.07437
wandb:     final_val_r2 -0.57857
wandb:   final_val_rmse 0.2727
wandb:    learning_rate 0.0001
wandb:       train_loss 0.0779
wandb:       train_time 12.46892
wandb:         val_loss 0.08223
wandb:          val_mse 0.08721
wandb:           val_r2 -0.8511
wandb:         val_rmse 0.29531
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_215841-8no4uhmo
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_215841-8no4uhmo/logs
Experiment probe_layer2_complexity_control1_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_complexity_control2_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_control2_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 21:59:44,643][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/ko
experiment_name: probe_layer2_complexity_control2_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 21:59:44,643][__main__][INFO] - Normalized task: complexity
[2025-05-07 21:59:44,643][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 21:59:44,643][__main__][INFO] - Determined Task Type: regression
[2025-05-07 21:59:44,647][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ko']
[2025-05-07 21:59:44,648][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 21:59:47,798][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 21:59:50,026][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 21:59:50,026][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:59:50,234][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-07 21:59:50,315][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-07 21:59:50,523][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 21:59:50,529][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:59:50,529][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 21:59:50,531][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:59:50,614][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:59:50,670][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:59:50,698][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 21:59:50,699][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:59:50,699][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 21:59:50,701][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:59:50,763][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:59:50,835][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:59:50,871][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 21:59:50,872][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:59:50,872][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 21:59:50,883][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 21:59:50,884][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 21:59:50,884][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 21:59:50,884][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 21:59:50,884][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 21:59:50,884][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:59:50,885][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-05-07 21:59:50,885][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 21:59:50,885][src.data.datasets][INFO] - Sample label: 0.4206434190273285
[2025-05-07 21:59:50,885][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 21:59:50,885][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 21:59:50,885][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 21:59:50,885][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 21:59:50,885][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:59:50,885][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-05-07 21:59:50,885][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 21:59:50,886][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-05-07 21:59:50,886][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 21:59:50,886][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 21:59:50,886][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 21:59:50,886][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 21:59:50,886][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:59:50,886][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-05-07 21:59:50,886][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 21:59:50,886][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-05-07 21:59:50,886][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 21:59:50,886][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 21:59:50,887][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 21:59:50,887][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 21:59:50,887][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 21:59:58,359][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 21:59:58,361][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 21:59:58,361][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 21:59:58,361][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 21:59:58,364][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 21:59:58,364][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 21:59:58,364][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 21:59:58,364][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 21:59:58,364][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 21:59:58,365][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 21:59:58,365][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.3339Epoch 1/15: [=                             ] 2/47 batches, loss: 0.4549Epoch 1/15: [=                             ] 3/47 batches, loss: 0.4554Epoch 1/15: [==                            ] 4/47 batches, loss: 0.4769Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4671Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4209Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4030Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4204Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4311Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4224Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4104Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4042Epoch 1/15: [========                      ] 13/47 batches, loss: 0.3941Epoch 1/15: [========                      ] 14/47 batches, loss: 0.3896Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.3768Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.3927Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.3836Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4000Epoch 1/15: [============                  ] 19/47 batches, loss: 0.3906Epoch 1/15: [============                  ] 20/47 batches, loss: 0.3843Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.3896Epoch 1/15: [==============                ] 22/47 batches, loss: 0.3877Epoch 1/15: [==============                ] 23/47 batches, loss: 0.3757Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3666Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3638Epoch 1/15: [================              ] 26/47 batches, loss: 0.3585Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3578Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3499Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3525Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3464Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3398Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3328Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3297Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3306Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3260Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3230Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3203Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3187Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3175Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3202Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3185Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3159Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3138Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3115Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3122Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3076Epoch 1/15: [==============================] 47/47 batches, loss: 0.3100
[2025-05-07 22:00:03,835][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3100
[2025-05-07 22:00:04,144][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1208, Metrics: {'mse': 0.1277722716331482, 'rmse': 0.3574524746496353, 'r2': -1.7121591567993164}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2618Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2137Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2303Epoch 2/15: [==                            ] 4/47 batches, loss: 0.1983Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2304Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2321Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2337Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2254Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2271Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2191Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2252Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2236Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2177Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2198Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2168Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2164Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2119Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2139Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2098Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2067Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2042Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2043Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2038Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2042Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2041Epoch 2/15: [================              ] 26/47 batches, loss: 0.2003Epoch 2/15: [=================             ] 27/47 batches, loss: 0.1972Epoch 2/15: [=================             ] 28/47 batches, loss: 0.1952Epoch 2/15: [==================            ] 29/47 batches, loss: 0.1914Epoch 2/15: [===================           ] 30/47 batches, loss: 0.1898Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1893Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1880Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1896Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1891Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1860Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1839Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1843Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1808Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1804Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1791Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1786Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1767Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1741Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1730Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1721Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1703Epoch 2/15: [==============================] 47/47 batches, loss: 0.1694
[2025-05-07 22:00:06,075][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1694
[2025-05-07 22:00:06,340][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0765, Metrics: {'mse': 0.08071161061525345, 'rmse': 0.28409788914255146, 'r2': -0.7132256031036377}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.2458Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1749Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1670Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1455Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1433Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1565Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1496Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1401Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1516Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1479Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1450Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1502Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1466Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1413Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1409Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1395Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1416Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1431Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1402Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1373Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1382Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1349Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1392Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1395Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1366Epoch 3/15: [================              ] 26/47 batches, loss: 0.1350Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1337Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1310Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1287Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1281Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1282Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1270Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1282Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1273Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1284Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1284Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1277Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1262Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1247Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1242Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1236Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1234Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1235Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1261Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1266Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1259Epoch 3/15: [==============================] 47/47 batches, loss: 0.1239
[2025-05-07 22:00:08,260][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1239
[2025-05-07 22:00:08,605][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0945, Metrics: {'mse': 0.10013221204280853, 'rmse': 0.3164367425613033, 'r2': -1.1254572868347168}
[2025-05-07 22:00:08,605][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1427Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1503Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1202Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1168Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1191Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1124Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1125Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1153Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1133Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1159Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1209Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1213Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1190Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1155Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1134Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1105Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1115Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1107Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1121Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1169Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1168Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1170Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1163Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1147Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1149Epoch 4/15: [================              ] 26/47 batches, loss: 0.1145Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1156Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1163Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1193Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1179Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1199Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1191Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1182Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1168Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1168Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1157Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1156Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1141Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1127Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1112Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1124Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1128Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1117Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1110Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1104Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1104Epoch 4/15: [==============================] 47/47 batches, loss: 0.1101
[2025-05-07 22:00:10,194][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1101
[2025-05-07 22:00:10,518][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0808, Metrics: {'mse': 0.08579577505588531, 'rmse': 0.2929091583680601, 'r2': -0.821144700050354}
[2025-05-07 22:00:10,519][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.0832Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1018Epoch 5/15: [=                             ] 3/47 batches, loss: 0.0949Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0785Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0759Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0753Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0757Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0739Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0744Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0710Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0722Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0755Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0722Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0730Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0728Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0747Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0737Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0749Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0744Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0765Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0774Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0774Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0768Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0795Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0788Epoch 5/15: [================              ] 26/47 batches, loss: 0.0783Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0768Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0760Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0766Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0786Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0793Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0794Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0787Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0795Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0803Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0803Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0795Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0808Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0817Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0828Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0823Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0820Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0816Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0818Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0828Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0825Epoch 5/15: [==============================] 47/47 batches, loss: 0.0816
[2025-05-07 22:00:12,016][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0816
[2025-05-07 22:00:12,275][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1111, Metrics: {'mse': 0.11768625676631927, 'rmse': 0.34305430585596686, 'r2': -1.4980683326721191}
[2025-05-07 22:00:12,276][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.0813Epoch 6/15: [=                             ] 2/47 batches, loss: 0.0731Epoch 6/15: [=                             ] 3/47 batches, loss: 0.0807Epoch 6/15: [==                            ] 4/47 batches, loss: 0.0818Epoch 6/15: [===                           ] 5/47 batches, loss: 0.0930Epoch 6/15: [===                           ] 6/47 batches, loss: 0.0901Epoch 6/15: [====                          ] 7/47 batches, loss: 0.0890Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.0880Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.0853Epoch 6/15: [======                        ] 10/47 batches, loss: 0.0835Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.0856Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.0910Epoch 6/15: [========                      ] 13/47 batches, loss: 0.0884Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0840Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0832Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.0832Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.0842Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.0848Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0824Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0808Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0784Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0787Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0775Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0762Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0752Epoch 6/15: [================              ] 26/47 batches, loss: 0.0758Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0769Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0772Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0784Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0789Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0791Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0786Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0788Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0781Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0777Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0773Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0780Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0776Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0782Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0778Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0781Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0798Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0795Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0798Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0803Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0802Epoch 6/15: [==============================] 47/47 batches, loss: 0.0790
[2025-05-07 22:00:13,789][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0790
[2025-05-07 22:00:14,058][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0872, Metrics: {'mse': 0.09255290031433105, 'rmse': 0.30422508166541934, 'r2': -0.9645748138427734}
[2025-05-07 22:00:14,059][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:00:14,059][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-07 22:00:14,059][src.training.lm_trainer][INFO] - Training completed in 12.52 seconds
[2025-05-07 22:00:14,059][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:00:16,328][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.028473570942878723, 'rmse': 0.16874113589424106, 'r2': -0.2790476083755493}
[2025-05-07 22:00:16,329][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.08071161061525345, 'rmse': 0.28409788914255146, 'r2': -0.7132256031036377}
[2025-05-07 22:00:16,329][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.05845830589532852, 'rmse': 0.24178152513235687, 'r2': -0.8151410818099976}
[2025-05-07 22:00:18,014][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/ko/ko/model.pt
[2025-05-07 22:00:18,016][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▅▄▅▂
wandb:       train_loss █▄▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▄▂▆▃
wandb:          val_mse █▁▄▂▆▃
wandb:           val_r2 ▁█▅▇▃▆
wandb:         val_rmse █▁▄▂▇▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07651
wandb:     best_val_mse 0.08071
wandb:      best_val_r2 -0.71323
wandb:    best_val_rmse 0.2841
wandb: early_stop_epoch 6
wandb:            epoch 6
wandb:   final_test_mse 0.05846
wandb:    final_test_r2 -0.81514
wandb:  final_test_rmse 0.24178
wandb:  final_train_mse 0.02847
wandb:   final_train_r2 -0.27905
wandb: final_train_rmse 0.16874
wandb:    final_val_mse 0.08071
wandb:     final_val_r2 -0.71323
wandb:   final_val_rmse 0.2841
wandb:    learning_rate 0.0001
wandb:       train_loss 0.079
wandb:       train_time 12.52063
wandb:         val_loss 0.0872
wandb:          val_mse 0.09255
wandb:           val_r2 -0.96457
wandb:         val_rmse 0.30423
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_215944-b6lut9o5
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_215944-b6lut9o5/logs
Experiment probe_layer2_complexity_control2_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_complexity_control3_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_control3_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:00:49,256][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/ko
experiment_name: probe_layer2_complexity_control3_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 22:00:49,256][__main__][INFO] - Normalized task: complexity
[2025-05-07 22:00:49,256][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:00:49,256][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:00:49,261][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ko']
[2025-05-07 22:00:49,261][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:00:53,068][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:00:55,405][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:00:55,406][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:00:55,722][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-07 22:00:55,886][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-07 22:00:56,232][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:00:56,238][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:00:56,238][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:00:56,241][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:00:56,342][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:00:56,452][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:00:56,498][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:00:56,499][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:00:56,499][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:00:56,512][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:00:56,631][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:00:56,788][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:00:56,822][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:00:56,823][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:00:56,823][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:00:56,827][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:00:56,828][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 22:00:56,828][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 22:00:56,828][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 22:00:56,828][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 22:00:56,828][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:00:56,829][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-05-07 22:00:56,829][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:00:56,829][src.data.datasets][INFO] - Sample label: 0.2868632674217224
[2025-05-07 22:00:56,829][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 22:00:56,829][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 22:00:56,829][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 22:00:56,829][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 22:00:56,829][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:00:56,829][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-05-07 22:00:56,830][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:00:56,830][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-05-07 22:00:56,830][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 22:00:56,830][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 22:00:56,830][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 22:00:56,830][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 22:00:56,830][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:00:56,830][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-05-07 22:00:56,830][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:00:56,830][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-05-07 22:00:56,830][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:00:56,831][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:00:56,831][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:00:56,831][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 22:00:56,831][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:01:04,938][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:01:04,939][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:01:04,939][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:01:04,939][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:01:04,942][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:01:04,943][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:01:04,943][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:01:04,943][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:01:04,943][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:01:04,944][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:01:04,944][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.3648Epoch 1/15: [=                             ] 2/47 batches, loss: 0.5422Epoch 1/15: [=                             ] 3/47 batches, loss: 0.5019Epoch 1/15: [==                            ] 4/47 batches, loss: 0.4971Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4881Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4347Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4193Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4204Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4330Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4386Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4259Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4176Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4102Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4140Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.4003Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4040Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.3949Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4165Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4126Epoch 1/15: [============                  ] 20/47 batches, loss: 0.4014Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4054Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4039Epoch 1/15: [==============                ] 23/47 batches, loss: 0.3937Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3830Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3797Epoch 1/15: [================              ] 26/47 batches, loss: 0.3755Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3743Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3659Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3704Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3646Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3564Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3509Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3491Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3508Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3456Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3452Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3400Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3375Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3332Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3335Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3327Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3303Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3285Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3260Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3257Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3204Epoch 1/15: [==============================] 47/47 batches, loss: 0.3219
[2025-05-07 22:01:11,399][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3219
[2025-05-07 22:01:11,693][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1290, Metrics: {'mse': 0.13675232231616974, 'rmse': 0.3698003817144727, 'r2': -1.9027743339538574}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2278Epoch 2/15: [=                             ] 2/47 batches, loss: 0.1965Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2280Epoch 2/15: [==                            ] 4/47 batches, loss: 0.1967Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2026Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2214Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2129Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2002Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2039Epoch 2/15: [======                        ] 10/47 batches, loss: 0.1974Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.1977Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.1973Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2002Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2014Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.1994Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2008Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.1954Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2062Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2027Epoch 2/15: [============                  ] 20/47 batches, loss: 0.1990Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.1964Epoch 2/15: [==============                ] 22/47 batches, loss: 0.1961Epoch 2/15: [==============                ] 23/47 batches, loss: 0.1940Epoch 2/15: [===============               ] 24/47 batches, loss: 0.1936Epoch 2/15: [===============               ] 25/47 batches, loss: 0.1936Epoch 2/15: [================              ] 26/47 batches, loss: 0.1915Epoch 2/15: [=================             ] 27/47 batches, loss: 0.1884Epoch 2/15: [=================             ] 28/47 batches, loss: 0.1852Epoch 2/15: [==================            ] 29/47 batches, loss: 0.1825Epoch 2/15: [===================           ] 30/47 batches, loss: 0.1804Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1802Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1786Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1801Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1800Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1772Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1766Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1785Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1753Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1730Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1719Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1704Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1686Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1662Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1643Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1651Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1639Epoch 2/15: [==============================] 47/47 batches, loss: 0.1622
[2025-05-07 22:01:13,577][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1622
[2025-05-07 22:01:13,850][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0717, Metrics: {'mse': 0.07564546167850494, 'rmse': 0.275037200535682, 'r2': -0.6056889295578003}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1673Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1311Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1159Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1121Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1228Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1431Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1414Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1369Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1429Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1384Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1361Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1392Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1410Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1357Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1357Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1353Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1374Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1355Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1337Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1325Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1340Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1311Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1306Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1320Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1282Epoch 3/15: [================              ] 26/47 batches, loss: 0.1274Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1279Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1256Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1241Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1220Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1238Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1236Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1235Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1232Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1245Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1238Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1227Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1211Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1191Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1193Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1186Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1185Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1183Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1207Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1222Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1225Epoch 3/15: [==============================] 47/47 batches, loss: 0.1244
[2025-05-07 22:01:15,776][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1244
[2025-05-07 22:01:16,110][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0868, Metrics: {'mse': 0.09220632910728455, 'rmse': 0.3036549507373205, 'r2': -0.9572184085845947}
[2025-05-07 22:01:16,110][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1335Epoch 4/15: [=                             ] 2/47 batches, loss: 0.0924Epoch 4/15: [=                             ] 3/47 batches, loss: 0.0773Epoch 4/15: [==                            ] 4/47 batches, loss: 0.0835Epoch 4/15: [===                           ] 5/47 batches, loss: 0.0943Epoch 4/15: [===                           ] 6/47 batches, loss: 0.0938Epoch 4/15: [====                          ] 7/47 batches, loss: 0.0951Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.0991Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1001Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1048Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1149Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1118Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1129Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1103Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1099Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1086Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1071Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1053Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1061Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1116Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1136Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1132Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1128Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1127Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1136Epoch 4/15: [================              ] 26/47 batches, loss: 0.1122Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1115Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1121Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1120Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1107Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1114Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1112Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1101Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1117Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1119Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1116Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1141Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1146Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1136Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1125Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1130Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1129Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1112Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1112Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1108Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1102Epoch 4/15: [==============================] 47/47 batches, loss: 0.1113
[2025-05-07 22:01:17,589][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1113
[2025-05-07 22:01:17,971][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0762, Metrics: {'mse': 0.08102137595415115, 'rmse': 0.284642540661355, 'r2': -0.7198008298873901}
[2025-05-07 22:01:17,972][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.1326Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1280Epoch 5/15: [=                             ] 3/47 batches, loss: 0.1135Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0986Epoch 5/15: [===                           ] 5/47 batches, loss: 0.1011Epoch 5/15: [===                           ] 6/47 batches, loss: 0.1011Epoch 5/15: [====                          ] 7/47 batches, loss: 0.1006Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.1011Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.1005Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0960Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0978Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0978Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0947Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0930Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0930Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0918Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0896Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0899Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0900Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0899Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0905Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0909Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0900Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0897Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0893Epoch 5/15: [================              ] 26/47 batches, loss: 0.0887Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0876Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0864Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0858Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0860Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0856Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0857Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0853Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0846Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0851Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0848Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0834Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0845Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0849Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0842Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0836Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0828Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0834Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0833Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0838Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0835Epoch 5/15: [==============================] 47/47 batches, loss: 0.0832
[2025-05-07 22:01:19,500][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0832
[2025-05-07 22:01:19,818][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0925, Metrics: {'mse': 0.0984337255358696, 'rmse': 0.31374149476259844, 'r2': -1.0894043445587158}
[2025-05-07 22:01:19,819][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.0839Epoch 6/15: [=                             ] 2/47 batches, loss: 0.0919Epoch 6/15: [=                             ] 3/47 batches, loss: 0.0915Epoch 6/15: [==                            ] 4/47 batches, loss: 0.0919Epoch 6/15: [===                           ] 5/47 batches, loss: 0.1055Epoch 6/15: [===                           ] 6/47 batches, loss: 0.1006Epoch 6/15: [====                          ] 7/47 batches, loss: 0.0987Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.1072Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.0999Epoch 6/15: [======                        ] 10/47 batches, loss: 0.1021Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.0969Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.1021Epoch 6/15: [========                      ] 13/47 batches, loss: 0.0974Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0930Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0900Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.0919Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.0918Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.0930Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0897Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0883Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0862Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0858Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0856Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0832Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0827Epoch 6/15: [================              ] 26/47 batches, loss: 0.0821Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0813Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0803Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0818Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0816Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0819Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0805Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0816Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0827Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0824Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0808Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0802Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0793Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0790Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0785Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0798Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0804Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0797Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0800Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0802Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0797Epoch 6/15: [==============================] 47/47 batches, loss: 0.0805
[2025-05-07 22:01:21,320][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0805
[2025-05-07 22:01:21,632][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0902, Metrics: {'mse': 0.09591837972402573, 'rmse': 0.30970692553448936, 'r2': -1.0360121726989746}
[2025-05-07 22:01:21,633][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:01:21,633][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-07 22:01:21,633][src.training.lm_trainer][INFO] - Training completed in 12.78 seconds
[2025-05-07 22:01:21,633][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:01:23,783][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.026140887290239334, 'rmse': 0.16168143767989984, 'r2': -0.1742624044418335}
[2025-05-07 22:01:23,784][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07564546167850494, 'rmse': 0.275037200535682, 'r2': -0.6056889295578003}
[2025-05-07 22:01:23,784][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.0531284473836422, 'rmse': 0.23049608973612154, 'r2': -0.6496479511260986}
[2025-05-07 22:01:25,479][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/ko/ko/model.pt
[2025-05-07 22:01:25,481][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▆▅▆▄
wandb:       train_loss █▃▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▃▂▄▃
wandb:          val_mse █▁▃▂▄▃
wandb:           val_r2 ▁█▆▇▅▆
wandb:         val_rmse █▁▃▂▄▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07175
wandb:     best_val_mse 0.07565
wandb:      best_val_r2 -0.60569
wandb:    best_val_rmse 0.27504
wandb: early_stop_epoch 6
wandb:            epoch 6
wandb:   final_test_mse 0.05313
wandb:    final_test_r2 -0.64965
wandb:  final_test_rmse 0.2305
wandb:  final_train_mse 0.02614
wandb:   final_train_r2 -0.17426
wandb: final_train_rmse 0.16168
wandb:    final_val_mse 0.07565
wandb:     final_val_r2 -0.60569
wandb:   final_val_rmse 0.27504
wandb:    learning_rate 0.0001
wandb:       train_loss 0.08054
wandb:       train_time 12.7764
wandb:         val_loss 0.09024
wandb:          val_mse 0.09592
wandb:           val_r2 -1.03601
wandb:         val_rmse 0.30971
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_220049-4xkovmzr
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_220049-4xkovmzr/logs
Experiment probe_layer2_complexity_control3_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/ko/ko/results.json for layer 2
Running submetric probing experiments...
=======================
PROBING LAYER 2 (SUBMETRIC EXPERIMENTS)
=======================
Running experiment: probe_layer2_avg_links_len_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/ko"         "wandb.mode=offline" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:01:54,857][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/ko
experiment_name: probe_layer2_avg_links_len_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:01:54,857][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:01:54,857][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 22:01:54,857][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:01:54,857][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:01:54,862][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:01:54,862][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 22:01:54,862][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:01:58,178][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:02:00,467][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:02:00,467][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:02:00,704][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:02:00,770][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:02:00,978][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:02:00,984][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:02:00,984][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:02:00,986][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:02:01,077][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:02:01,173][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:02:01,228][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:02:01,229][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:02:01,229][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:02:01,231][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:02:01,287][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:02:01,366][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:02:01,385][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:02:01,386][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:02:01,387][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:02:01,389][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:02:01,389][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:02:01,390][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 22:02:01,390][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 22:02:01,390][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 22:02:01,390][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:02:01,390][src.data.datasets][INFO] -   Mean: 0.1923, Std: 0.1477
[2025-05-07 22:02:01,390][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:02:01,390][src.data.datasets][INFO] - Sample label: 0.14300000667572021
[2025-05-07 22:02:01,390][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:02:01,390][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 22:02:01,391][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 22:02:01,391][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 22:02:01,391][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.5940
[2025-05-07 22:02:01,391][src.data.datasets][INFO] -   Mean: 0.2139, Std: 0.1419
[2025-05-07 22:02:01,391][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:02:01,391][src.data.datasets][INFO] - Sample label: 0.3160000145435333
[2025-05-07 22:02:01,391][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:02:01,391][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 22:02:01,391][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 22:02:01,391][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 22:02:01,392][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7720
[2025-05-07 22:02:01,392][src.data.datasets][INFO] -   Mean: 0.2570, Std: 0.1517
[2025-05-07 22:02:01,392][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:02:01,392][src.data.datasets][INFO] - Sample label: 0.1589999943971634
[2025-05-07 22:02:01,392][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:02:01,392][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:02:01,392][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:02:01,392][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 22:02:01,393][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:02:09,228][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:02:09,229][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:02:09,229][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:02:09,229][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:02:09,232][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:02:09,232][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:02:09,232][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:02:09,233][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:02:09,233][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:02:09,234][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:02:09,234][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.4508Epoch 1/15: [=                             ] 2/47 batches, loss: 0.5410Epoch 1/15: [=                             ] 3/47 batches, loss: 0.4991Epoch 1/15: [==                            ] 4/47 batches, loss: 0.5065Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4887Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4331Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4129Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4206Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4227Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4312Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4216Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4184Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4051Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4055Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.3934Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4002Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.3912Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4064Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4000Epoch 1/15: [============                  ] 20/47 batches, loss: 0.3943Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.3972Epoch 1/15: [==============                ] 22/47 batches, loss: 0.3998Epoch 1/15: [==============                ] 23/47 batches, loss: 0.3889Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3785Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3754Epoch 1/15: [================              ] 26/47 batches, loss: 0.3709Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3700Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3635Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3665Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3633Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3561Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3500Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3458Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3469Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3403Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3367Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3311Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3301Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3275Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3278Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3268Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3241Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3217Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3190Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3206Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3161Epoch 1/15: [==============================] 47/47 batches, loss: 0.3212
[2025-05-07 22:02:15,082][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3212
[2025-05-07 22:02:15,393][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0571, Metrics: {'mse': 0.06006038188934326, 'rmse': 0.24507219729978197, 'r2': -1.9842016696929932}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2617Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2216Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2496Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2143Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2219Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2274Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2170Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2083Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2074Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2010Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2038Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2014Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2023Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2075Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2036Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2016Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.1959Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2001Epoch 2/15: [============                  ] 19/47 batches, loss: 0.1981Epoch 2/15: [============                  ] 20/47 batches, loss: 0.1952Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.1931Epoch 2/15: [==============                ] 22/47 batches, loss: 0.1937Epoch 2/15: [==============                ] 23/47 batches, loss: 0.1923Epoch 2/15: [===============               ] 24/47 batches, loss: 0.1925Epoch 2/15: [===============               ] 25/47 batches, loss: 0.1913Epoch 2/15: [================              ] 26/47 batches, loss: 0.1882Epoch 2/15: [=================             ] 27/47 batches, loss: 0.1867Epoch 2/15: [=================             ] 28/47 batches, loss: 0.1855Epoch 2/15: [==================            ] 29/47 batches, loss: 0.1839Epoch 2/15: [===================           ] 30/47 batches, loss: 0.1842Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1840Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1835Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1860Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1854Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1830Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1810Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1817Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1784Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1760Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1745Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1730Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1706Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1680Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1663Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1665Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1660Epoch 2/15: [==============================] 47/47 batches, loss: 0.1650
[2025-05-07 22:02:17,265][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1650
[2025-05-07 22:02:17,552][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0199, Metrics: {'mse': 0.021099185571074486, 'rmse': 0.14525558705631425, 'r2': -0.04834866523742676}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1406Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1028Epoch 3/15: [=                             ] 3/47 batches, loss: 0.0975Epoch 3/15: [==                            ] 4/47 batches, loss: 0.0947Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1087Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1296Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1224Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1166Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1271Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1263Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1265Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1268Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1293Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1263Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1225Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1220Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1265Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1270Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1240Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1214Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1227Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1200Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1206Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1225Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1207Epoch 3/15: [================              ] 26/47 batches, loss: 0.1202Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1220Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1202Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1179Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1191Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1224Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1214Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1222Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1213Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1235Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1221Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1217Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1209Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1192Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1185Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1174Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1166Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1167Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1181Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1195Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1195Epoch 3/15: [==============================] 47/47 batches, loss: 0.1183
[2025-05-07 22:02:19,460][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1183
[2025-05-07 22:02:19,723][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0267, Metrics: {'mse': 0.028029322624206543, 'rmse': 0.1674196004779803, 'r2': -0.3926842212677002}
[2025-05-07 22:02:19,723][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1503Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1061Epoch 4/15: [=                             ] 3/47 batches, loss: 0.0956Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1106Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1062Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1023Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1022Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1098Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1083Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1128Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1205Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1184Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1166Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1133Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1110Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1067Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1069Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1053Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1045Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1100Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1099Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1096Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1086Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1090Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1099Epoch 4/15: [================              ] 26/47 batches, loss: 0.1084Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1081Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1081Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1090Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1075Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1085Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1105Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1089Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1096Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1096Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1083Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1077Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1065Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1054Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1061Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1062Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1061Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1051Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1048Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1044Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1038Epoch 4/15: [==============================] 47/47 batches, loss: 0.1039
[2025-05-07 22:02:21,183][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1039
[2025-05-07 22:02:21,447][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0217, Metrics: {'mse': 0.02288995496928692, 'rmse': 0.1512942661480828, 'r2': -0.13732612133026123}
[2025-05-07 22:02:21,448][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.0627Epoch 5/15: [=                             ] 2/47 batches, loss: 0.0726Epoch 5/15: [=                             ] 3/47 batches, loss: 0.0861Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0737Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0695Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0737Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0756Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0750Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0757Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0731Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0750Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0771Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0742Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0726Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0722Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0725Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0718Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0743Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0730Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0743Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0756Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0746Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0752Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0772Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0772Epoch 5/15: [================              ] 26/47 batches, loss: 0.0761Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0751Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0742Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0741Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0740Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0742Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0739Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0729Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0723Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0735Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0740Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0747Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0747Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0745Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0742Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0740Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0742Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0744Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0748Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0760Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0760Epoch 5/15: [==============================] 47/47 batches, loss: 0.0758
[2025-05-07 22:02:22,991][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0758
[2025-05-07 22:02:23,306][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0290, Metrics: {'mse': 0.030285613611340523, 'rmse': 0.17402762312730852, 'r2': -0.5047918558120728}
[2025-05-07 22:02:23,307][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.0814Epoch 6/15: [=                             ] 2/47 batches, loss: 0.0861Epoch 6/15: [=                             ] 3/47 batches, loss: 0.0895Epoch 6/15: [==                            ] 4/47 batches, loss: 0.0855Epoch 6/15: [===                           ] 5/47 batches, loss: 0.0887Epoch 6/15: [===                           ] 6/47 batches, loss: 0.0881Epoch 6/15: [====                          ] 7/47 batches, loss: 0.0836Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.0870Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.0834Epoch 6/15: [======                        ] 10/47 batches, loss: 0.0850Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.0871Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.0900Epoch 6/15: [========                      ] 13/47 batches, loss: 0.0869Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0845Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0826Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.0850Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.0862Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.0859Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0832Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0834Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0805Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0800Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0792Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0783Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0769Epoch 6/15: [================              ] 26/47 batches, loss: 0.0774Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0773Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0792Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0794Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0787Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0799Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0784Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0786Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0793Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0790Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0781Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0780Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0780Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0779Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0775Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0769Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0766Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0761Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0757Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0752Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0751Epoch 6/15: [==============================] 47/47 batches, loss: 0.0749
[2025-05-07 22:02:24,763][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0749
[2025-05-07 22:02:25,144][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0311, Metrics: {'mse': 0.03230223432183266, 'rmse': 0.1797282234982382, 'r2': -0.60499107837677}
[2025-05-07 22:02:25,145][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:02:25,145][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-07 22:02:25,145][src.training.lm_trainer][INFO] - Training completed in 12.36 seconds
[2025-05-07 22:02:25,145][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:02:27,479][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.02233613096177578, 'rmse': 0.14945277167645898, 'r2': -0.024329423904418945}
[2025-05-07 22:02:27,479][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.021099185571074486, 'rmse': 0.14525558705631425, 'r2': -0.04834866523742676}
[2025-05-07 22:02:27,479][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.040476616472005844, 'rmse': 0.20118801274431297, 'r2': -0.7589454650878906}
[2025-05-07 22:02:29,156][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/ko/ko/model.pt
[2025-05-07 22:02:29,158][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▇▆▇▆
wandb:       train_loss █▄▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▂▁▃▃
wandb:          val_mse █▁▂▁▃▃
wandb:           val_r2 ▁█▇█▆▆
wandb:         val_rmse █▁▃▁▃▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01991
wandb:     best_val_mse 0.0211
wandb:      best_val_r2 -0.04835
wandb:    best_val_rmse 0.14526
wandb: early_stop_epoch 6
wandb:            epoch 6
wandb:   final_test_mse 0.04048
wandb:    final_test_r2 -0.75895
wandb:  final_test_rmse 0.20119
wandb:  final_train_mse 0.02234
wandb:   final_train_r2 -0.02433
wandb: final_train_rmse 0.14945
wandb:    final_val_mse 0.0211
wandb:     final_val_r2 -0.04835
wandb:   final_val_rmse 0.14526
wandb:    learning_rate 0.0001
wandb:       train_loss 0.07492
wandb:       train_time 12.35641
wandb:         val_loss 0.03114
wandb:          val_mse 0.0323
wandb:           val_r2 -0.60499
wandb:         val_rmse 0.17973
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_220154-v06ez6ed
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_220154-v06ez6ed/logs
Experiment probe_layer2_avg_links_len_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_avg_max_depth_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_max_depth_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer2/ko"         "wandb.mode=offline" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:02:58,456][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer2/ko
experiment_name: probe_layer2_avg_max_depth_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:02:58,456][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:02:58,456][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 22:02:58,456][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:02:58,456][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:02:58,460][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:02:58,460][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 22:02:58,461][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:03:01,740][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:03:04,086][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:03:04,087][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:03:04,308][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:03:04,432][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:03:04,693][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:03:04,700][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:03:04,700][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:03:04,702][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:03:04,786][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:03:04,842][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:03:04,861][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:03:04,863][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:03:04,863][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:03:04,865][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:03:04,917][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:03:05,019][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:03:05,036][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:03:05,038][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:03:05,038][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:03:05,039][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:03:05,041][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:03:05,041][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 22:03:05,041][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 22:03:05,041][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 22:03:05,041][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:03:05,041][src.data.datasets][INFO] -   Mean: 0.2234, Std: 0.1709
[2025-05-07 22:03:05,041][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:03:05,042][src.data.datasets][INFO] - Sample label: 0.30000001192092896
[2025-05-07 22:03:05,042][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:03:05,042][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 22:03:05,042][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 22:03:05,042][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 22:03:05,042][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8890
[2025-05-07 22:03:05,042][src.data.datasets][INFO] -   Mean: 0.2502, Std: 0.1830
[2025-05-07 22:03:05,042][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:03:05,042][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 22:03:05,043][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:03:05,043][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 22:03:05,043][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 22:03:05,043][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 22:03:05,043][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8890
[2025-05-07 22:03:05,043][src.data.datasets][INFO] -   Mean: 0.2705, Std: 0.1869
[2025-05-07 22:03:05,043][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:03:05,043][src.data.datasets][INFO] - Sample label: 0.4440000057220459
[2025-05-07 22:03:05,043][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:03:05,043][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:03:05,044][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:03:05,044][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 22:03:05,044][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:03:12,292][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:03:12,293][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:03:12,293][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:03:12,293][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:03:12,297][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:03:12,297][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:03:12,297][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:03:12,297][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:03:12,298][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:03:12,298][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:03:12,299][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.4139Epoch 1/15: [=                             ] 2/47 batches, loss: 0.5654Epoch 1/15: [=                             ] 3/47 batches, loss: 0.5045Epoch 1/15: [==                            ] 4/47 batches, loss: 0.5169Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4914Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4459Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4269Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4291Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4286Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4202Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4086Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4133Epoch 1/15: [========                      ] 13/47 batches, loss: 0.3959Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4097Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.3992Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4030Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.3931Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4078Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4012Epoch 1/15: [============                  ] 20/47 batches, loss: 0.3951Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4015Epoch 1/15: [==============                ] 22/47 batches, loss: 0.3991Epoch 1/15: [==============                ] 23/47 batches, loss: 0.3863Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3771Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3732Epoch 1/15: [================              ] 26/47 batches, loss: 0.3686Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3679Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3596Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3662Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3610Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3543Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3478Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3454Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3474Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3425Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3403Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3347Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3305Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3267Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3279Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3273Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3258Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3240Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3210Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3230Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3183Epoch 1/15: [==============================] 47/47 batches, loss: 0.3175
[2025-05-07 22:03:17,903][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3175
[2025-05-07 22:03:18,160][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0532, Metrics: {'mse': 0.057196538895368576, 'rmse': 0.2391579789498326, 'r2': -0.7072329521179199}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2864Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2169Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2324Epoch 2/15: [==                            ] 4/47 batches, loss: 0.1988Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2193Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2405Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2214Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2189Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2202Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2108Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2147Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2198Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2169Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2161Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2142Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2087Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2043Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2110Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2089Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2061Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2032Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2039Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2038Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2037Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2037Epoch 2/15: [================              ] 26/47 batches, loss: 0.2001Epoch 2/15: [=================             ] 27/47 batches, loss: 0.1980Epoch 2/15: [=================             ] 28/47 batches, loss: 0.1975Epoch 2/15: [==================            ] 29/47 batches, loss: 0.1931Epoch 2/15: [===================           ] 30/47 batches, loss: 0.1932Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1927Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1929Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1952Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1955Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1917Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1882Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1874Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1845Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1822Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1808Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1799Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1788Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1764Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1744Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1749Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1731Epoch 2/15: [==============================] 47/47 batches, loss: 0.1727
[2025-05-07 22:03:20,028][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1727
[2025-05-07 22:03:20,320][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0279, Metrics: {'mse': 0.029902435839176178, 'rmse': 0.17292320792529897, 'r2': 0.10745608806610107}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1988Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1387Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1232Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1130Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1201Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1464Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1394Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1297Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1365Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1351Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1354Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1380Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1403Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1344Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1355Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1338Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1407Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1407Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1368Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1332Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1338Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1314Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1325Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1313Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1286Epoch 3/15: [================              ] 26/47 batches, loss: 0.1277Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1292Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1283Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1264Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1253Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1264Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1259Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1264Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1270Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1278Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1260Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1263Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1251Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1237Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1245Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1241Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1237Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1233Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1253Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1263Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1255Epoch 3/15: [==============================] 47/47 batches, loss: 0.1277
[2025-05-07 22:03:22,226][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1277
[2025-05-07 22:03:22,551][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0386, Metrics: {'mse': 0.04184827208518982, 'rmse': 0.20456850218249586, 'r2': -0.24910974502563477}
[2025-05-07 22:03:22,552][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1690Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1238Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1026Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1105Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1116Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1075Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1151Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1185Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1136Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1171Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1280Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1251Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1242Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1208Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1198Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1163Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1163Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1139Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1135Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1156Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1153Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1160Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1135Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1123Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1130Epoch 4/15: [================              ] 26/47 batches, loss: 0.1119Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1129Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1119Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1140Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1123Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1130Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1150Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1124Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1131Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1138Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1124Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1132Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1129Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1115Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1106Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1103Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1112Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1096Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1090Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1084Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1080Epoch 4/15: [==============================] 47/47 batches, loss: 0.1081
[2025-05-07 22:03:24,069][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1081
[2025-05-07 22:03:24,359][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0273, Metrics: {'mse': 0.029659589752554893, 'rmse': 0.172219597469495, 'r2': 0.1147046685218811}
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.0806Epoch 5/15: [=                             ] 2/47 batches, loss: 0.0986Epoch 5/15: [=                             ] 3/47 batches, loss: 0.0924Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0765Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0703Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0736Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0739Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0747Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0747Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0728Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0743Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0746Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0734Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0725Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0727Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0730Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0712Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0707Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0709Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0727Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0724Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0728Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0727Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0732Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0734Epoch 5/15: [================              ] 26/47 batches, loss: 0.0722Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0708Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0694Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0699Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0715Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0715Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0713Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0705Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0701Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0707Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0699Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0701Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0704Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0711Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0715Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0708Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0706Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0702Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0710Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0726Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0727Epoch 5/15: [==============================] 47/47 batches, loss: 0.0719
[2025-05-07 22:03:26,182][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0719
[2025-05-07 22:03:26,457][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0356, Metrics: {'mse': 0.038702622056007385, 'rmse': 0.19672981994605543, 'r2': -0.15521657466888428}
[2025-05-07 22:03:26,458][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.0993Epoch 6/15: [=                             ] 2/47 batches, loss: 0.0873Epoch 6/15: [=                             ] 3/47 batches, loss: 0.0869Epoch 6/15: [==                            ] 4/47 batches, loss: 0.0844Epoch 6/15: [===                           ] 5/47 batches, loss: 0.0977Epoch 6/15: [===                           ] 6/47 batches, loss: 0.0926Epoch 6/15: [====                          ] 7/47 batches, loss: 0.0858Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.0848Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.0813Epoch 6/15: [======                        ] 10/47 batches, loss: 0.0842Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.0901Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.0905Epoch 6/15: [========                      ] 13/47 batches, loss: 0.0870Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0829Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0809Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.0823Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.0839Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.0864Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0831Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0835Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0812Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0809Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0819Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0803Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0795Epoch 6/15: [================              ] 26/47 batches, loss: 0.0790Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0790Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0788Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0791Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0786Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0791Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0783Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0782Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0780Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0780Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0767Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0766Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0762Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0769Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0760Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0777Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0772Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0773Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0771Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0775Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0772Epoch 6/15: [==============================] 47/47 batches, loss: 0.0779
[2025-05-07 22:03:27,938][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0779
[2025-05-07 22:03:28,225][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0348, Metrics: {'mse': 0.03783610090613365, 'rmse': 0.1945150403082848, 'r2': -0.12935233116149902}
[2025-05-07 22:03:28,226][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/47 batches, loss: 0.0506Epoch 7/15: [=                             ] 2/47 batches, loss: 0.0638Epoch 7/15: [=                             ] 3/47 batches, loss: 0.0661Epoch 7/15: [==                            ] 4/47 batches, loss: 0.0627Epoch 7/15: [===                           ] 5/47 batches, loss: 0.0648Epoch 7/15: [===                           ] 6/47 batches, loss: 0.0643Epoch 7/15: [====                          ] 7/47 batches, loss: 0.0598Epoch 7/15: [=====                         ] 8/47 batches, loss: 0.0614Epoch 7/15: [=====                         ] 9/47 batches, loss: 0.0647Epoch 7/15: [======                        ] 10/47 batches, loss: 0.0635Epoch 7/15: [=======                       ] 11/47 batches, loss: 0.0596Epoch 7/15: [=======                       ] 12/47 batches, loss: 0.0591Epoch 7/15: [========                      ] 13/47 batches, loss: 0.0580Epoch 7/15: [========                      ] 14/47 batches, loss: 0.0570Epoch 7/15: [=========                     ] 15/47 batches, loss: 0.0589Epoch 7/15: [==========                    ] 16/47 batches, loss: 0.0596Epoch 7/15: [==========                    ] 17/47 batches, loss: 0.0599Epoch 7/15: [===========                   ] 18/47 batches, loss: 0.0616Epoch 7/15: [============                  ] 19/47 batches, loss: 0.0624Epoch 7/15: [============                  ] 20/47 batches, loss: 0.0613Epoch 7/15: [=============                 ] 21/47 batches, loss: 0.0608Epoch 7/15: [==============                ] 22/47 batches, loss: 0.0609Epoch 7/15: [==============                ] 23/47 batches, loss: 0.0606Epoch 7/15: [===============               ] 24/47 batches, loss: 0.0609Epoch 7/15: [===============               ] 25/47 batches, loss: 0.0626Epoch 7/15: [================              ] 26/47 batches, loss: 0.0633Epoch 7/15: [=================             ] 27/47 batches, loss: 0.0639Epoch 7/15: [=================             ] 28/47 batches, loss: 0.0650Epoch 7/15: [==================            ] 29/47 batches, loss: 0.0655Epoch 7/15: [===================           ] 30/47 batches, loss: 0.0658Epoch 7/15: [===================           ] 31/47 batches, loss: 0.0660Epoch 7/15: [====================          ] 32/47 batches, loss: 0.0661Epoch 7/15: [=====================         ] 33/47 batches, loss: 0.0653Epoch 7/15: [=====================         ] 34/47 batches, loss: 0.0663Epoch 7/15: [======================        ] 35/47 batches, loss: 0.0668Epoch 7/15: [======================        ] 36/47 batches, loss: 0.0674Epoch 7/15: [=======================       ] 37/47 batches, loss: 0.0673Epoch 7/15: [========================      ] 38/47 batches, loss: 0.0670Epoch 7/15: [========================      ] 39/47 batches, loss: 0.0668Epoch 7/15: [=========================     ] 40/47 batches, loss: 0.0661Epoch 7/15: [==========================    ] 41/47 batches, loss: 0.0659Epoch 7/15: [==========================    ] 42/47 batches, loss: 0.0659Epoch 7/15: [===========================   ] 43/47 batches, loss: 0.0654Epoch 7/15: [============================  ] 44/47 batches, loss: 0.0651Epoch 7/15: [============================  ] 45/47 batches, loss: 0.0648Epoch 7/15: [============================= ] 46/47 batches, loss: 0.0645Epoch 7/15: [==============================] 47/47 batches, loss: 0.0642
[2025-05-07 22:03:29,739][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0642
[2025-05-07 22:03:30,036][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0250, Metrics: {'mse': 0.02731594629585743, 'rmse': 0.16527536506042706, 'r2': 0.184658944606781}
Epoch 8/15: [Epoch 8/15: [                              ] 1/47 batches, loss: 0.0400Epoch 8/15: [=                             ] 2/47 batches, loss: 0.0708Epoch 8/15: [=                             ] 3/47 batches, loss: 0.0715Epoch 8/15: [==                            ] 4/47 batches, loss: 0.0733Epoch 8/15: [===                           ] 5/47 batches, loss: 0.0727Epoch 8/15: [===                           ] 6/47 batches, loss: 0.0686Epoch 8/15: [====                          ] 7/47 batches, loss: 0.0723Epoch 8/15: [=====                         ] 8/47 batches, loss: 0.0704Epoch 8/15: [=====                         ] 9/47 batches, loss: 0.0669Epoch 8/15: [======                        ] 10/47 batches, loss: 0.0647Epoch 8/15: [=======                       ] 11/47 batches, loss: 0.0630Epoch 8/15: [=======                       ] 12/47 batches, loss: 0.0630Epoch 8/15: [========                      ] 13/47 batches, loss: 0.0608Epoch 8/15: [========                      ] 14/47 batches, loss: 0.0626Epoch 8/15: [=========                     ] 15/47 batches, loss: 0.0620Epoch 8/15: [==========                    ] 16/47 batches, loss: 0.0608Epoch 8/15: [==========                    ] 17/47 batches, loss: 0.0613Epoch 8/15: [===========                   ] 18/47 batches, loss: 0.0600Epoch 8/15: [============                  ] 19/47 batches, loss: 0.0608Epoch 8/15: [============                  ] 20/47 batches, loss: 0.0598Epoch 8/15: [=============                 ] 21/47 batches, loss: 0.0607Epoch 8/15: [==============                ] 22/47 batches, loss: 0.0589Epoch 8/15: [==============                ] 23/47 batches, loss: 0.0595Epoch 8/15: [===============               ] 24/47 batches, loss: 0.0603Epoch 8/15: [===============               ] 25/47 batches, loss: 0.0603Epoch 8/15: [================              ] 26/47 batches, loss: 0.0608Epoch 8/15: [=================             ] 27/47 batches, loss: 0.0603Epoch 8/15: [=================             ] 28/47 batches, loss: 0.0610Epoch 8/15: [==================            ] 29/47 batches, loss: 0.0598Epoch 8/15: [===================           ] 30/47 batches, loss: 0.0597Epoch 8/15: [===================           ] 31/47 batches, loss: 0.0603Epoch 8/15: [====================          ] 32/47 batches, loss: 0.0607Epoch 8/15: [=====================         ] 33/47 batches, loss: 0.0613Epoch 8/15: [=====================         ] 34/47 batches, loss: 0.0615Epoch 8/15: [======================        ] 35/47 batches, loss: 0.0617Epoch 8/15: [======================        ] 36/47 batches, loss: 0.0612Epoch 8/15: [=======================       ] 37/47 batches, loss: 0.0624Epoch 8/15: [========================      ] 38/47 batches, loss: 0.0625Epoch 8/15: [========================      ] 39/47 batches, loss: 0.0632Epoch 8/15: [=========================     ] 40/47 batches, loss: 0.0620Epoch 8/15: [==========================    ] 41/47 batches, loss: 0.0613Epoch 8/15: [==========================    ] 42/47 batches, loss: 0.0604Epoch 8/15: [===========================   ] 43/47 batches, loss: 0.0601Epoch 8/15: [============================  ] 44/47 batches, loss: 0.0605Epoch 8/15: [============================  ] 45/47 batches, loss: 0.0601Epoch 8/15: [============================= ] 46/47 batches, loss: 0.0608Epoch 8/15: [==============================] 47/47 batches, loss: 0.0597
[2025-05-07 22:03:31,909][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0597
[2025-05-07 22:03:32,181][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0216, Metrics: {'mse': 0.023482918739318848, 'rmse': 0.15324137411064562, 'r2': 0.29906922578811646}
Epoch 9/15: [Epoch 9/15: [                              ] 1/47 batches, loss: 0.0416Epoch 9/15: [=                             ] 2/47 batches, loss: 0.0618Epoch 9/15: [=                             ] 3/47 batches, loss: 0.0676Epoch 9/15: [==                            ] 4/47 batches, loss: 0.0604Epoch 9/15: [===                           ] 5/47 batches, loss: 0.0531Epoch 9/15: [===                           ] 6/47 batches, loss: 0.0564Epoch 9/15: [====                          ] 7/47 batches, loss: 0.0531Epoch 9/15: [=====                         ] 8/47 batches, loss: 0.0568Epoch 9/15: [=====                         ] 9/47 batches, loss: 0.0557Epoch 9/15: [======                        ] 10/47 batches, loss: 0.0557Epoch 9/15: [=======                       ] 11/47 batches, loss: 0.0542Epoch 9/15: [=======                       ] 12/47 batches, loss: 0.0530Epoch 9/15: [========                      ] 13/47 batches, loss: 0.0523Epoch 9/15: [========                      ] 14/47 batches, loss: 0.0541Epoch 9/15: [=========                     ] 15/47 batches, loss: 0.0533Epoch 9/15: [==========                    ] 16/47 batches, loss: 0.0540Epoch 9/15: [==========                    ] 17/47 batches, loss: 0.0542Epoch 9/15: [===========                   ] 18/47 batches, loss: 0.0539Epoch 9/15: [============                  ] 19/47 batches, loss: 0.0545Epoch 9/15: [============                  ] 20/47 batches, loss: 0.0541Epoch 9/15: [=============                 ] 21/47 batches, loss: 0.0549Epoch 9/15: [==============                ] 22/47 batches, loss: 0.0554Epoch 9/15: [==============                ] 23/47 batches, loss: 0.0545Epoch 9/15: [===============               ] 24/47 batches, loss: 0.0538Epoch 9/15: [===============               ] 25/47 batches, loss: 0.0560Epoch 9/15: [================              ] 26/47 batches, loss: 0.0585Epoch 9/15: [=================             ] 27/47 batches, loss: 0.0576Epoch 9/15: [=================             ] 28/47 batches, loss: 0.0579Epoch 9/15: [==================            ] 29/47 batches, loss: 0.0577Epoch 9/15: [===================           ] 30/47 batches, loss: 0.0565Epoch 9/15: [===================           ] 31/47 batches, loss: 0.0560Epoch 9/15: [====================          ] 32/47 batches, loss: 0.0558Epoch 9/15: [=====================         ] 33/47 batches, loss: 0.0557Epoch 9/15: [=====================         ] 34/47 batches, loss: 0.0551Epoch 9/15: [======================        ] 35/47 batches, loss: 0.0547Epoch 9/15: [======================        ] 36/47 batches, loss: 0.0550Epoch 9/15: [=======================       ] 37/47 batches, loss: 0.0567Epoch 9/15: [========================      ] 38/47 batches, loss: 0.0562Epoch 9/15: [========================      ] 39/47 batches, loss: 0.0566Epoch 9/15: [=========================     ] 40/47 batches, loss: 0.0567Epoch 9/15: [==========================    ] 41/47 batches, loss: 0.0567Epoch 9/15: [==========================    ] 42/47 batches, loss: 0.0579Epoch 9/15: [===========================   ] 43/47 batches, loss: 0.0576Epoch 9/15: [============================  ] 44/47 batches, loss: 0.0573Epoch 9/15: [============================  ] 45/47 batches, loss: 0.0583Epoch 9/15: [============================= ] 46/47 batches, loss: 0.0577Epoch 9/15: [==============================] 47/47 batches, loss: 0.0578
[2025-05-07 22:03:34,114][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0578
[2025-05-07 22:03:34,383][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0303, Metrics: {'mse': 0.032999489456415176, 'rmse': 0.18165761601544586, 'r2': 0.015013515949249268}
[2025-05-07 22:03:34,383][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/47 batches, loss: 0.0216Epoch 10/15: [=                             ] 2/47 batches, loss: 0.0364Epoch 10/15: [=                             ] 3/47 batches, loss: 0.0396Epoch 10/15: [==                            ] 4/47 batches, loss: 0.0409Epoch 10/15: [===                           ] 5/47 batches, loss: 0.0456Epoch 10/15: [===                           ] 6/47 batches, loss: 0.0497Epoch 10/15: [====                          ] 7/47 batches, loss: 0.0488Epoch 10/15: [=====                         ] 8/47 batches, loss: 0.0544Epoch 10/15: [=====                         ] 9/47 batches, loss: 0.0560Epoch 10/15: [======                        ] 10/47 batches, loss: 0.0554Epoch 10/15: [=======                       ] 11/47 batches, loss: 0.0561Epoch 10/15: [=======                       ] 12/47 batches, loss: 0.0538Epoch 10/15: [========                      ] 13/47 batches, loss: 0.0541Epoch 10/15: [========                      ] 14/47 batches, loss: 0.0538Epoch 10/15: [=========                     ] 15/47 batches, loss: 0.0532Epoch 10/15: [==========                    ] 16/47 batches, loss: 0.0544Epoch 10/15: [==========                    ] 17/47 batches, loss: 0.0536Epoch 10/15: [===========                   ] 18/47 batches, loss: 0.0532Epoch 10/15: [============                  ] 19/47 batches, loss: 0.0544Epoch 10/15: [============                  ] 20/47 batches, loss: 0.0551Epoch 10/15: [=============                 ] 21/47 batches, loss: 0.0549Epoch 10/15: [==============                ] 22/47 batches, loss: 0.0548Epoch 10/15: [==============                ] 23/47 batches, loss: 0.0536Epoch 10/15: [===============               ] 24/47 batches, loss: 0.0561Epoch 10/15: [===============               ] 25/47 batches, loss: 0.0557Epoch 10/15: [================              ] 26/47 batches, loss: 0.0546Epoch 10/15: [=================             ] 27/47 batches, loss: 0.0534Epoch 10/15: [=================             ] 28/47 batches, loss: 0.0542Epoch 10/15: [==================            ] 29/47 batches, loss: 0.0541Epoch 10/15: [===================           ] 30/47 batches, loss: 0.0543Epoch 10/15: [===================           ] 31/47 batches, loss: 0.0540Epoch 10/15: [====================          ] 32/47 batches, loss: 0.0541Epoch 10/15: [=====================         ] 33/47 batches, loss: 0.0544Epoch 10/15: [=====================         ] 34/47 batches, loss: 0.0539Epoch 10/15: [======================        ] 35/47 batches, loss: 0.0529Epoch 10/15: [======================        ] 36/47 batches, loss: 0.0531Epoch 10/15: [=======================       ] 37/47 batches, loss: 0.0531Epoch 10/15: [========================      ] 38/47 batches, loss: 0.0546Epoch 10/15: [========================      ] 39/47 batches, loss: 0.0543Epoch 10/15: [=========================     ] 40/47 batches, loss: 0.0556Epoch 10/15: [==========================    ] 41/47 batches, loss: 0.0549Epoch 10/15: [==========================    ] 42/47 batches, loss: 0.0551Epoch 10/15: [===========================   ] 43/47 batches, loss: 0.0553Epoch 10/15: [============================  ] 44/47 batches, loss: 0.0552Epoch 10/15: [============================  ] 45/47 batches, loss: 0.0550Epoch 10/15: [============================= ] 46/47 batches, loss: 0.0552Epoch 10/15: [==============================] 47/47 batches, loss: 0.0550
[2025-05-07 22:03:35,900][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0550
[2025-05-07 22:03:36,191][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0238, Metrics: {'mse': 0.025912366807460785, 'rmse': 0.1609731866102575, 'r2': 0.22655373811721802}
[2025-05-07 22:03:36,191][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/47 batches, loss: 0.0170Epoch 11/15: [=                             ] 2/47 batches, loss: 0.0290Epoch 11/15: [=                             ] 3/47 batches, loss: 0.0416Epoch 11/15: [==                            ] 4/47 batches, loss: 0.0613Epoch 11/15: [===                           ] 5/47 batches, loss: 0.0603Epoch 11/15: [===                           ] 6/47 batches, loss: 0.0554Epoch 11/15: [====                          ] 7/47 batches, loss: 0.0583Epoch 11/15: [=====                         ] 8/47 batches, loss: 0.0576Epoch 11/15: [=====                         ] 9/47 batches, loss: 0.0560Epoch 11/15: [======                        ] 10/47 batches, loss: 0.0524Epoch 11/15: [=======                       ] 11/47 batches, loss: 0.0516Epoch 11/15: [=======                       ] 12/47 batches, loss: 0.0494Epoch 11/15: [========                      ] 13/47 batches, loss: 0.0532Epoch 11/15: [========                      ] 14/47 batches, loss: 0.0533Epoch 11/15: [=========                     ] 15/47 batches, loss: 0.0519Epoch 11/15: [==========                    ] 16/47 batches, loss: 0.0522Epoch 11/15: [==========                    ] 17/47 batches, loss: 0.0516Epoch 11/15: [===========                   ] 18/47 batches, loss: 0.0508Epoch 11/15: [============                  ] 19/47 batches, loss: 0.0498Epoch 11/15: [============                  ] 20/47 batches, loss: 0.0502Epoch 11/15: [=============                 ] 21/47 batches, loss: 0.0487Epoch 11/15: [==============                ] 22/47 batches, loss: 0.0496Epoch 11/15: [==============                ] 23/47 batches, loss: 0.0490Epoch 11/15: [===============               ] 24/47 batches, loss: 0.0505Epoch 11/15: [===============               ] 25/47 batches, loss: 0.0491Epoch 11/15: [================              ] 26/47 batches, loss: 0.0500Epoch 11/15: [=================             ] 27/47 batches, loss: 0.0492Epoch 11/15: [=================             ] 28/47 batches, loss: 0.0500Epoch 11/15: [==================            ] 29/47 batches, loss: 0.0498Epoch 11/15: [===================           ] 30/47 batches, loss: 0.0492Epoch 11/15: [===================           ] 31/47 batches, loss: 0.0487Epoch 11/15: [====================          ] 32/47 batches, loss: 0.0481Epoch 11/15: [=====================         ] 33/47 batches, loss: 0.0480Epoch 11/15: [=====================         ] 34/47 batches, loss: 0.0477Epoch 11/15: [======================        ] 35/47 batches, loss: 0.0478Epoch 11/15: [======================        ] 36/47 batches, loss: 0.0475Epoch 11/15: [=======================       ] 37/47 batches, loss: 0.0472Epoch 11/15: [========================      ] 38/47 batches, loss: 0.0476Epoch 11/15: [========================      ] 39/47 batches, loss: 0.0474Epoch 11/15: [=========================     ] 40/47 batches, loss: 0.0478Epoch 11/15: [==========================    ] 41/47 batches, loss: 0.0477Epoch 11/15: [==========================    ] 42/47 batches, loss: 0.0476Epoch 11/15: [===========================   ] 43/47 batches, loss: 0.0472Epoch 11/15: [============================  ] 44/47 batches, loss: 0.0474Epoch 11/15: [============================  ] 45/47 batches, loss: 0.0471Epoch 11/15: [============================= ] 46/47 batches, loss: 0.0470Epoch 11/15: [==============================] 47/47 batches, loss: 0.0468
[2025-05-07 22:03:37,712][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0468
[2025-05-07 22:03:37,990][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0207, Metrics: {'mse': 0.022392164915800095, 'rmse': 0.14964011800249322, 'r2': 0.3316265940666199}
Epoch 12/15: [Epoch 12/15: [                              ] 1/47 batches, loss: 0.0173Epoch 12/15: [=                             ] 2/47 batches, loss: 0.0249Epoch 12/15: [=                             ] 3/47 batches, loss: 0.0312Epoch 12/15: [==                            ] 4/47 batches, loss: 0.0338Epoch 12/15: [===                           ] 5/47 batches, loss: 0.0347Epoch 12/15: [===                           ] 6/47 batches, loss: 0.0376Epoch 12/15: [====                          ] 7/47 batches, loss: 0.0362Epoch 12/15: [=====                         ] 8/47 batches, loss: 0.0397Epoch 12/15: [=====                         ] 9/47 batches, loss: 0.0413Epoch 12/15: [======                        ] 10/47 batches, loss: 0.0387Epoch 12/15: [=======                       ] 11/47 batches, loss: 0.0373Epoch 12/15: [=======                       ] 12/47 batches, loss: 0.0384Epoch 12/15: [========                      ] 13/47 batches, loss: 0.0397Epoch 12/15: [========                      ] 14/47 batches, loss: 0.0402Epoch 12/15: [=========                     ] 15/47 batches, loss: 0.0432Epoch 12/15: [==========                    ] 16/47 batches, loss: 0.0449Epoch 12/15: [==========                    ] 17/47 batches, loss: 0.0438Epoch 12/15: [===========                   ] 18/47 batches, loss: 0.0426Epoch 12/15: [============                  ] 19/47 batches, loss: 0.0462Epoch 12/15: [============                  ] 20/47 batches, loss: 0.0455Epoch 12/15: [=============                 ] 21/47 batches, loss: 0.0463Epoch 12/15: [==============                ] 22/47 batches, loss: 0.0447Epoch 12/15: [==============                ] 23/47 batches, loss: 0.0453Epoch 12/15: [===============               ] 24/47 batches, loss: 0.0459Epoch 12/15: [===============               ] 25/47 batches, loss: 0.0457Epoch 12/15: [================              ] 26/47 batches, loss: 0.0449Epoch 12/15: [=================             ] 27/47 batches, loss: 0.0444Epoch 12/15: [=================             ] 28/47 batches, loss: 0.0443Epoch 12/15: [==================            ] 29/47 batches, loss: 0.0437Epoch 12/15: [===================           ] 30/47 batches, loss: 0.0433Epoch 12/15: [===================           ] 31/47 batches, loss: 0.0435Epoch 12/15: [====================          ] 32/47 batches, loss: 0.0431Epoch 12/15: [=====================         ] 33/47 batches, loss: 0.0428Epoch 12/15: [=====================         ] 34/47 batches, loss: 0.0429Epoch 12/15: [======================        ] 35/47 batches, loss: 0.0427Epoch 12/15: [======================        ] 36/47 batches, loss: 0.0434Epoch 12/15: [=======================       ] 37/47 batches, loss: 0.0433Epoch 12/15: [========================      ] 38/47 batches, loss: 0.0435Epoch 12/15: [========================      ] 39/47 batches, loss: 0.0435Epoch 12/15: [=========================     ] 40/47 batches, loss: 0.0441Epoch 12/15: [==========================    ] 41/47 batches, loss: 0.0435Epoch 12/15: [==========================    ] 42/47 batches, loss: 0.0436Epoch 12/15: [===========================   ] 43/47 batches, loss: 0.0435Epoch 12/15: [============================  ] 44/47 batches, loss: 0.0431Epoch 12/15: [============================  ] 45/47 batches, loss: 0.0433Epoch 12/15: [============================= ] 46/47 batches, loss: 0.0433Epoch 12/15: [==============================] 47/47 batches, loss: 0.0428
[2025-05-07 22:03:39,862][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0428
[2025-05-07 22:03:40,163][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0273, Metrics: {'mse': 0.029672585427761078, 'rmse': 0.17225732329210586, 'r2': 0.11431670188903809}
[2025-05-07 22:03:40,163][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/47 batches, loss: 0.0383Epoch 13/15: [=                             ] 2/47 batches, loss: 0.0322Epoch 13/15: [=                             ] 3/47 batches, loss: 0.0347Epoch 13/15: [==                            ] 4/47 batches, loss: 0.0344Epoch 13/15: [===                           ] 5/47 batches, loss: 0.0345Epoch 13/15: [===                           ] 6/47 batches, loss: 0.0336Epoch 13/15: [====                          ] 7/47 batches, loss: 0.0357Epoch 13/15: [=====                         ] 8/47 batches, loss: 0.0353Epoch 13/15: [=====                         ] 9/47 batches, loss: 0.0354Epoch 13/15: [======                        ] 10/47 batches, loss: 0.0348Epoch 13/15: [=======                       ] 11/47 batches, loss: 0.0340Epoch 13/15: [=======                       ] 12/47 batches, loss: 0.0339Epoch 13/15: [========                      ] 13/47 batches, loss: 0.0363Epoch 13/15: [========                      ] 14/47 batches, loss: 0.0361Epoch 13/15: [=========                     ] 15/47 batches, loss: 0.0360Epoch 13/15: [==========                    ] 16/47 batches, loss: 0.0352Epoch 13/15: [==========                    ] 17/47 batches, loss: 0.0348Epoch 13/15: [===========                   ] 18/47 batches, loss: 0.0365Epoch 13/15: [============                  ] 19/47 batches, loss: 0.0368Epoch 13/15: [============                  ] 20/47 batches, loss: 0.0356Epoch 13/15: [=============                 ] 21/47 batches, loss: 0.0364Epoch 13/15: [==============                ] 22/47 batches, loss: 0.0365Epoch 13/15: [==============                ] 23/47 batches, loss: 0.0383Epoch 13/15: [===============               ] 24/47 batches, loss: 0.0375Epoch 13/15: [===============               ] 25/47 batches, loss: 0.0375Epoch 13/15: [================              ] 26/47 batches, loss: 0.0374Epoch 13/15: [=================             ] 27/47 batches, loss: 0.0383Epoch 13/15: [=================             ] 28/47 batches, loss: 0.0375Epoch 13/15: [==================            ] 29/47 batches, loss: 0.0384Epoch 13/15: [===================           ] 30/47 batches, loss: 0.0380Epoch 13/15: [===================           ] 31/47 batches, loss: 0.0389Epoch 13/15: [====================          ] 32/47 batches, loss: 0.0390Epoch 13/15: [=====================         ] 33/47 batches, loss: 0.0396Epoch 13/15: [=====================         ] 34/47 batches, loss: 0.0398Epoch 13/15: [======================        ] 35/47 batches, loss: 0.0395Epoch 13/15: [======================        ] 36/47 batches, loss: 0.0389Epoch 13/15: [=======================       ] 37/47 batches, loss: 0.0386Epoch 13/15: [========================      ] 38/47 batches, loss: 0.0384Epoch 13/15: [========================      ] 39/47 batches, loss: 0.0387Epoch 13/15: [=========================     ] 40/47 batches, loss: 0.0387Epoch 13/15: [==========================    ] 41/47 batches, loss: 0.0383Epoch 13/15: [==========================    ] 42/47 batches, loss: 0.0381Epoch 13/15: [===========================   ] 43/47 batches, loss: 0.0378Epoch 13/15: [============================  ] 44/47 batches, loss: 0.0379Epoch 13/15: [============================  ] 45/47 batches, loss: 0.0381Epoch 13/15: [============================= ] 46/47 batches, loss: 0.0379Epoch 13/15: [==============================] 47/47 batches, loss: 0.0377
[2025-05-07 22:03:41,666][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0377
[2025-05-07 22:03:41,965][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0307, Metrics: {'mse': 0.03328986465930939, 'rmse': 0.1824551031330979, 'r2': 0.006346285343170166}
[2025-05-07 22:03:41,966][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/47 batches, loss: 0.0300Epoch 14/15: [=                             ] 2/47 batches, loss: 0.0334Epoch 14/15: [=                             ] 3/47 batches, loss: 0.0316Epoch 14/15: [==                            ] 4/47 batches, loss: 0.0295Epoch 14/15: [===                           ] 5/47 batches, loss: 0.0329Epoch 14/15: [===                           ] 6/47 batches, loss: 0.0339Epoch 14/15: [====                          ] 7/47 batches, loss: 0.0381Epoch 14/15: [=====                         ] 8/47 batches, loss: 0.0377Epoch 14/15: [=====                         ] 9/47 batches, loss: 0.0408Epoch 14/15: [======                        ] 10/47 batches, loss: 0.0389Epoch 14/15: [=======                       ] 11/47 batches, loss: 0.0386Epoch 14/15: [=======                       ] 12/47 batches, loss: 0.0413Epoch 14/15: [========                      ] 13/47 batches, loss: 0.0412Epoch 14/15: [========                      ] 14/47 batches, loss: 0.0424Epoch 14/15: [=========                     ] 15/47 batches, loss: 0.0424Epoch 14/15: [==========                    ] 16/47 batches, loss: 0.0416Epoch 14/15: [==========                    ] 17/47 batches, loss: 0.0407Epoch 14/15: [===========                   ] 18/47 batches, loss: 0.0407Epoch 14/15: [============                  ] 19/47 batches, loss: 0.0399Epoch 14/15: [============                  ] 20/47 batches, loss: 0.0402Epoch 14/15: [=============                 ] 21/47 batches, loss: 0.0394Epoch 14/15: [==============                ] 22/47 batches, loss: 0.0382Epoch 14/15: [==============                ] 23/47 batches, loss: 0.0387Epoch 14/15: [===============               ] 24/47 batches, loss: 0.0381Epoch 14/15: [===============               ] 25/47 batches, loss: 0.0391Epoch 14/15: [================              ] 26/47 batches, loss: 0.0391Epoch 14/15: [=================             ] 27/47 batches, loss: 0.0385Epoch 14/15: [=================             ] 28/47 batches, loss: 0.0393Epoch 14/15: [==================            ] 29/47 batches, loss: 0.0392Epoch 14/15: [===================           ] 30/47 batches, loss: 0.0391Epoch 14/15: [===================           ] 31/47 batches, loss: 0.0388Epoch 14/15: [====================          ] 32/47 batches, loss: 0.0386Epoch 14/15: [=====================         ] 33/47 batches, loss: 0.0391Epoch 14/15: [=====================         ] 34/47 batches, loss: 0.0389Epoch 14/15: [======================        ] 35/47 batches, loss: 0.0385Epoch 14/15: [======================        ] 36/47 batches, loss: 0.0379Epoch 14/15: [=======================       ] 37/47 batches, loss: 0.0378Epoch 14/15: [========================      ] 38/47 batches, loss: 0.0377Epoch 14/15: [========================      ] 39/47 batches, loss: 0.0391Epoch 14/15: [=========================     ] 40/47 batches, loss: 0.0387Epoch 14/15: [==========================    ] 41/47 batches, loss: 0.0383Epoch 14/15: [==========================    ] 42/47 batches, loss: 0.0390Epoch 14/15: [===========================   ] 43/47 batches, loss: 0.0386Epoch 14/15: [============================  ] 44/47 batches, loss: 0.0384Epoch 14/15: [============================  ] 45/47 batches, loss: 0.0383Epoch 14/15: [============================= ] 46/47 batches, loss: 0.0383Epoch 14/15: [==============================] 47/47 batches, loss: 0.0376
[2025-05-07 22:03:43,531][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0376
[2025-05-07 22:03:43,838][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0180, Metrics: {'mse': 0.01954057812690735, 'rmse': 0.13978761793130087, 'r2': 0.41674232482910156}
Epoch 15/15: [Epoch 15/15: [                              ] 1/47 batches, loss: 0.0243Epoch 15/15: [=                             ] 2/47 batches, loss: 0.0452Epoch 15/15: [=                             ] 3/47 batches, loss: 0.0450Epoch 15/15: [==                            ] 4/47 batches, loss: 0.0411Epoch 15/15: [===                           ] 5/47 batches, loss: 0.0405Epoch 15/15: [===                           ] 6/47 batches, loss: 0.0432Epoch 15/15: [====                          ] 7/47 batches, loss: 0.0406Epoch 15/15: [=====                         ] 8/47 batches, loss: 0.0427Epoch 15/15: [=====                         ] 9/47 batches, loss: 0.0459Epoch 15/15: [======                        ] 10/47 batches, loss: 0.0438Epoch 15/15: [=======                       ] 11/47 batches, loss: 0.0425Epoch 15/15: [=======                       ] 12/47 batches, loss: 0.0426Epoch 15/15: [========                      ] 13/47 batches, loss: 0.0431Epoch 15/15: [========                      ] 14/47 batches, loss: 0.0423Epoch 15/15: [=========                     ] 15/47 batches, loss: 0.0434Epoch 15/15: [==========                    ] 16/47 batches, loss: 0.0423Epoch 15/15: [==========                    ] 17/47 batches, loss: 0.0417Epoch 15/15: [===========                   ] 18/47 batches, loss: 0.0412Epoch 15/15: [============                  ] 19/47 batches, loss: 0.0416Epoch 15/15: [============                  ] 20/47 batches, loss: 0.0415Epoch 15/15: [=============                 ] 21/47 batches, loss: 0.0418Epoch 15/15: [==============                ] 22/47 batches, loss: 0.0427Epoch 15/15: [==============                ] 23/47 batches, loss: 0.0425Epoch 15/15: [===============               ] 24/47 batches, loss: 0.0430Epoch 15/15: [===============               ] 25/47 batches, loss: 0.0429Epoch 15/15: [================              ] 26/47 batches, loss: 0.0425Epoch 15/15: [=================             ] 27/47 batches, loss: 0.0418Epoch 15/15: [=================             ] 28/47 batches, loss: 0.0426Epoch 15/15: [==================            ] 29/47 batches, loss: 0.0419Epoch 15/15: [===================           ] 30/47 batches, loss: 0.0414Epoch 15/15: [===================           ] 31/47 batches, loss: 0.0408Epoch 15/15: [====================          ] 32/47 batches, loss: 0.0409Epoch 15/15: [=====================         ] 33/47 batches, loss: 0.0407Epoch 15/15: [=====================         ] 34/47 batches, loss: 0.0411Epoch 15/15: [======================        ] 35/47 batches, loss: 0.0424Epoch 15/15: [======================        ] 36/47 batches, loss: 0.0422Epoch 15/15: [=======================       ] 37/47 batches, loss: 0.0421Epoch 15/15: [========================      ] 38/47 batches, loss: 0.0418Epoch 15/15: [========================      ] 39/47 batches, loss: 0.0416Epoch 15/15: [=========================     ] 40/47 batches, loss: 0.0421Epoch 15/15: [==========================    ] 41/47 batches, loss: 0.0419Epoch 15/15: [==========================    ] 42/47 batches, loss: 0.0414Epoch 15/15: [===========================   ] 43/47 batches, loss: 0.0418Epoch 15/15: [============================  ] 44/47 batches, loss: 0.0418Epoch 15/15: [============================  ] 45/47 batches, loss: 0.0418Epoch 15/15: [============================= ] 46/47 batches, loss: 0.0414Epoch 15/15: [==============================] 47/47 batches, loss: 0.0408
[2025-05-07 22:03:45,720][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0408
[2025-05-07 22:03:46,029][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0175, Metrics: {'mse': 0.018943268805742264, 'rmse': 0.13763454800936523, 'r2': 0.43457114696502686}
[2025-05-07 22:03:46,403][src.training.lm_trainer][INFO] - Training completed in 30.75 seconds
[2025-05-07 22:03:46,403][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:03:48,695][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.014927730895578861, 'rmse': 0.12217909352904392, 'r2': 0.4889066219329834}
[2025-05-07 22:03:48,695][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.018943268805742264, 'rmse': 0.13763454800936523, 'r2': 0.43457114696502686}
[2025-05-07 22:03:48,695][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.020173467695713043, 'rmse': 0.14203333304444082, 'r2': 0.42221128940582275}
[2025-05-07 22:03:50,407][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer2/ko/ko/model.pt
[2025-05-07 22:03:50,409][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▃▂▂▂▁▁
wandb:     best_val_mse █▃▃▃▂▂▁▁
wandb:      best_val_r2 ▁▆▆▆▇▇██
wandb:    best_val_rmse █▃▃▃▂▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▆▄▆▅▅▆▇▆▇▇▆▆▇
wandb:       train_loss █▄▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▅▃▅▄▂▂▄▂▂▃▄▁▁
wandb:          val_mse █▃▅▃▅▄▃▂▄▂▂▃▄▁▁
wandb:           val_r2 ▁▆▄▆▄▅▆▇▅▇▇▆▅██
wandb:         val_rmse █▃▆▃▅▅▃▂▄▃▂▃▄▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01748
wandb:     best_val_mse 0.01894
wandb:      best_val_r2 0.43457
wandb:    best_val_rmse 0.13763
wandb:            epoch 15
wandb:   final_test_mse 0.02017
wandb:    final_test_r2 0.42221
wandb:  final_test_rmse 0.14203
wandb:  final_train_mse 0.01493
wandb:   final_train_r2 0.48891
wandb: final_train_rmse 0.12218
wandb:    final_val_mse 0.01894
wandb:     final_val_r2 0.43457
wandb:   final_val_rmse 0.13763
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04083
wandb:       train_time 30.75479
wandb:         val_loss 0.01748
wandb:          val_mse 0.01894
wandb:           val_r2 0.43457
wandb:         val_rmse 0.13763
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_220258-ujbmryei
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_220258-ujbmryei/logs
Experiment probe_layer2_avg_max_depth_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_avg_subordinate_chain_len_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_subordinate_chain_len_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/ko"         "wandb.mode=offline" "experiment.submetric=avg_subordinate_chain_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:04:16,728][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/ko
experiment_name: probe_layer2_avg_subordinate_chain_len_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:04:16,728][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:04:16,728][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 22:04:16,728][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:04:16,728][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:04:16,733][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:04:16,733][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 22:04:16,734][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:04:19,777][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:04:22,290][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:04:22,291][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:04:22,544][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:04:22,635][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:04:22,872][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:04:22,880][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:04:22,881][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:04:22,882][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:04:22,945][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:04:23,030][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:04:23,057][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:04:23,059][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:04:23,059][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:04:23,060][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:04:23,128][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:04:23,207][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:04:23,231][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:04:23,234][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:04:23,234][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:04:23,235][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:04:23,236][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:04:23,236][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 22:04:23,237][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 22:04:23,237][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 22:04:23,237][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:04:23,237][src.data.datasets][INFO] -   Mean: 0.1845, Std: 0.2535
[2025-05-07 22:04:23,237][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:04:23,237][src.data.datasets][INFO] - Sample label: 0.5
[2025-05-07 22:04:23,237][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:04:23,238][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 22:04:23,238][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 22:04:23,238][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 22:04:23,238][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:04:23,238][src.data.datasets][INFO] -   Mean: 0.2673, Std: 0.2561
[2025-05-07 22:04:23,238][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:04:23,238][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 22:04:23,238][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:04:23,238][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 22:04:23,238][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 22:04:23,239][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 22:04:23,239][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6670
[2025-05-07 22:04:23,239][src.data.datasets][INFO] -   Mean: 0.2435, Std: 0.2248
[2025-05-07 22:04:23,239][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:04:23,239][src.data.datasets][INFO] - Sample label: 0.5
[2025-05-07 22:04:23,239][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:04:23,239][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:04:23,239][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:04:23,240][__main__][INFO] - Using model type: lm_probe for submetric avg_subordinate_chain_len
[2025-05-07 22:04:23,240][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:04:30,045][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:04:30,046][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:04:30,046][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:04:30,046][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:04:30,050][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:04:30,050][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:04:30,050][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:04:30,051][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:04:30,051][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:04:30,052][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:04:30,052][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.5066Epoch 1/15: [=                             ] 2/47 batches, loss: 0.6352Epoch 1/15: [=                             ] 3/47 batches, loss: 0.5578Epoch 1/15: [==                            ] 4/47 batches, loss: 0.5418Epoch 1/15: [===                           ] 5/47 batches, loss: 0.5161Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4742Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4663Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4790Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4826Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4625Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4574Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4523Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4319Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4346Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.4261Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4263Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.4194Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4348Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4272Epoch 1/15: [============                  ] 20/47 batches, loss: 0.4246Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4345Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4315Epoch 1/15: [==============                ] 23/47 batches, loss: 0.4186Epoch 1/15: [===============               ] 24/47 batches, loss: 0.4102Epoch 1/15: [===============               ] 25/47 batches, loss: 0.4080Epoch 1/15: [================              ] 26/47 batches, loss: 0.4005Epoch 1/15: [=================             ] 27/47 batches, loss: 0.4000Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3949Epoch 1/15: [==================            ] 29/47 batches, loss: 0.4076Epoch 1/15: [===================           ] 30/47 batches, loss: 0.4039Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3951Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3889Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3862Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3874Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3827Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3797Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3735Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3711Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3669Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3675Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3681Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3665Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3644Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3625Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3653Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3597Epoch 1/15: [==============================] 47/47 batches, loss: 0.3554
[2025-05-07 22:04:35,781][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3554
[2025-05-07 22:04:36,093][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1280, Metrics: {'mse': 0.13053782284259796, 'rmse': 0.36130018383969575, 'r2': -0.9907567501068115}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.3397Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2412Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2507Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2127Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2589Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2972Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2744Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2696Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2715Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2591Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2664Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2722Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2695Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2692Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2671Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2555Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2508Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2597Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2576Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2511Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2499Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2477Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2487Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2503Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2503Epoch 2/15: [================              ] 26/47 batches, loss: 0.2460Epoch 2/15: [=================             ] 27/47 batches, loss: 0.2425Epoch 2/15: [=================             ] 28/47 batches, loss: 0.2431Epoch 2/15: [==================            ] 29/47 batches, loss: 0.2373Epoch 2/15: [===================           ] 30/47 batches, loss: 0.2399Epoch 2/15: [===================           ] 31/47 batches, loss: 0.2378Epoch 2/15: [====================          ] 32/47 batches, loss: 0.2388Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.2392Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.2402Epoch 2/15: [======================        ] 35/47 batches, loss: 0.2358Epoch 2/15: [======================        ] 36/47 batches, loss: 0.2329Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.2327Epoch 2/15: [========================      ] 38/47 batches, loss: 0.2292Epoch 2/15: [========================      ] 39/47 batches, loss: 0.2264Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.2258Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.2234Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.2224Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.2198Epoch 2/15: [============================  ] 44/47 batches, loss: 0.2179Epoch 2/15: [============================  ] 45/47 batches, loss: 0.2171Epoch 2/15: [============================= ] 46/47 batches, loss: 0.2150Epoch 2/15: [==============================] 47/47 batches, loss: 0.2166
[2025-05-07 22:04:38,029][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.2166
[2025-05-07 22:04:38,363][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0869, Metrics: {'mse': 0.08829537034034729, 'rmse': 0.29714536903735733, 'r2': -0.3465414047241211}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.2587Epoch 3/15: [=                             ] 2/47 batches, loss: 0.2022Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1771Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1650Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1634Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1896Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1931Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1845Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1885Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1861Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1867Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1914Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1916Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1856Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1885Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1855Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1917Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1903Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1856Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1824Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1823Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1791Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1793Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1784Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1740Epoch 3/15: [================              ] 26/47 batches, loss: 0.1747Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1756Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1751Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1747Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1746Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1730Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1706Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1694Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1689Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1693Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1664Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1664Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1654Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1642Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1632Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1621Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1611Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1610Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1636Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1647Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1645Epoch 3/15: [==============================] 47/47 batches, loss: 0.1697
[2025-05-07 22:04:40,248][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1697
[2025-05-07 22:04:40,517][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0889, Metrics: {'mse': 0.09046885371208191, 'rmse': 0.3007804077929311, 'r2': -0.379688024520874}
[2025-05-07 22:04:40,517][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1979Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1607Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1887Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1836Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1779Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1731Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1802Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1795Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1726Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1782Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1833Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1838Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1770Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1691Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1642Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1597Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1605Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1592Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1554Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1598Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1584Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1571Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1541Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1537Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1526Epoch 4/15: [================              ] 26/47 batches, loss: 0.1505Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1512Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1499Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1509Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1495Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1530Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1575Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1542Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1553Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1551Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1524Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1533Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1529Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1518Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1506Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1491Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1483Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1463Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1464Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1447Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1437Epoch 4/15: [==============================] 47/47 batches, loss: 0.1449
[2025-05-07 22:04:42,090][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1449
[2025-05-07 22:04:42,402][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0815, Metrics: {'mse': 0.08313968777656555, 'rmse': 0.28833953557666275, 'r2': -0.26791512966156006}
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.2170Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1585Epoch 5/15: [=                             ] 3/47 batches, loss: 0.1416Epoch 5/15: [==                            ] 4/47 batches, loss: 0.1228Epoch 5/15: [===                           ] 5/47 batches, loss: 0.1141Epoch 5/15: [===                           ] 6/47 batches, loss: 0.1170Epoch 5/15: [====                          ] 7/47 batches, loss: 0.1213Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.1204Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.1248Epoch 5/15: [======                        ] 10/47 batches, loss: 0.1207Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.1222Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.1215Epoch 5/15: [========                      ] 13/47 batches, loss: 0.1199Epoch 5/15: [========                      ] 14/47 batches, loss: 0.1164Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.1145Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.1140Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.1112Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.1110Epoch 5/15: [============                  ] 19/47 batches, loss: 0.1093Epoch 5/15: [============                  ] 20/47 batches, loss: 0.1105Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.1097Epoch 5/15: [==============                ] 22/47 batches, loss: 0.1121Epoch 5/15: [==============                ] 23/47 batches, loss: 0.1134Epoch 5/15: [===============               ] 24/47 batches, loss: 0.1146Epoch 5/15: [===============               ] 25/47 batches, loss: 0.1131Epoch 5/15: [================              ] 26/47 batches, loss: 0.1120Epoch 5/15: [=================             ] 27/47 batches, loss: 0.1128Epoch 5/15: [=================             ] 28/47 batches, loss: 0.1119Epoch 5/15: [==================            ] 29/47 batches, loss: 0.1123Epoch 5/15: [===================           ] 30/47 batches, loss: 0.1131Epoch 5/15: [===================           ] 31/47 batches, loss: 0.1140Epoch 5/15: [====================          ] 32/47 batches, loss: 0.1140Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.1124Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.1114Epoch 5/15: [======================        ] 35/47 batches, loss: 0.1112Epoch 5/15: [======================        ] 36/47 batches, loss: 0.1101Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.1094Epoch 5/15: [========================      ] 38/47 batches, loss: 0.1103Epoch 5/15: [========================      ] 39/47 batches, loss: 0.1094Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.1097Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.1096Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.1101Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.1092Epoch 5/15: [============================  ] 44/47 batches, loss: 0.1084Epoch 5/15: [============================  ] 45/47 batches, loss: 0.1096Epoch 5/15: [============================= ] 46/47 batches, loss: 0.1089Epoch 5/15: [==============================] 47/47 batches, loss: 0.1079
[2025-05-07 22:04:44,282][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1079
[2025-05-07 22:04:44,580][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0880, Metrics: {'mse': 0.08969707041978836, 'rmse': 0.2994946918056952, 'r2': -0.3679180145263672}
[2025-05-07 22:04:44,580][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.1293Epoch 6/15: [=                             ] 2/47 batches, loss: 0.1083Epoch 6/15: [=                             ] 3/47 batches, loss: 0.1231Epoch 6/15: [==                            ] 4/47 batches, loss: 0.1181Epoch 6/15: [===                           ] 5/47 batches, loss: 0.1343Epoch 6/15: [===                           ] 6/47 batches, loss: 0.1376Epoch 6/15: [====                          ] 7/47 batches, loss: 0.1274Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.1299Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.1207Epoch 6/15: [======                        ] 10/47 batches, loss: 0.1272Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.1288Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.1288Epoch 6/15: [========                      ] 13/47 batches, loss: 0.1220Epoch 6/15: [========                      ] 14/47 batches, loss: 0.1185Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.1195Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.1207Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.1231Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.1232Epoch 6/15: [============                  ] 19/47 batches, loss: 0.1203Epoch 6/15: [============                  ] 20/47 batches, loss: 0.1208Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.1182Epoch 6/15: [==============                ] 22/47 batches, loss: 0.1179Epoch 6/15: [==============                ] 23/47 batches, loss: 0.1168Epoch 6/15: [===============               ] 24/47 batches, loss: 0.1148Epoch 6/15: [===============               ] 25/47 batches, loss: 0.1162Epoch 6/15: [================              ] 26/47 batches, loss: 0.1167Epoch 6/15: [=================             ] 27/47 batches, loss: 0.1169Epoch 6/15: [=================             ] 28/47 batches, loss: 0.1172Epoch 6/15: [==================            ] 29/47 batches, loss: 0.1163Epoch 6/15: [===================           ] 30/47 batches, loss: 0.1160Epoch 6/15: [===================           ] 31/47 batches, loss: 0.1168Epoch 6/15: [====================          ] 32/47 batches, loss: 0.1162Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.1157Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.1141Epoch 6/15: [======================        ] 35/47 batches, loss: 0.1142Epoch 6/15: [======================        ] 36/47 batches, loss: 0.1121Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.1113Epoch 6/15: [========================      ] 38/47 batches, loss: 0.1110Epoch 6/15: [========================      ] 39/47 batches, loss: 0.1104Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.1097Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.1092Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.1094Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.1093Epoch 6/15: [============================  ] 44/47 batches, loss: 0.1090Epoch 6/15: [============================  ] 45/47 batches, loss: 0.1090Epoch 6/15: [============================= ] 46/47 batches, loss: 0.1079Epoch 6/15: [==============================] 47/47 batches, loss: 0.1074
[2025-05-07 22:04:46,058][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1074
[2025-05-07 22:04:46,388][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0741, Metrics: {'mse': 0.07564198970794678, 'rmse': 0.2750308886433427, 'r2': -0.15357208251953125}
Epoch 7/15: [Epoch 7/15: [                              ] 1/47 batches, loss: 0.1121Epoch 7/15: [=                             ] 2/47 batches, loss: 0.1229Epoch 7/15: [=                             ] 3/47 batches, loss: 0.1087Epoch 7/15: [==                            ] 4/47 batches, loss: 0.1042Epoch 7/15: [===                           ] 5/47 batches, loss: 0.1074Epoch 7/15: [===                           ] 6/47 batches, loss: 0.1109Epoch 7/15: [====                          ] 7/47 batches, loss: 0.1021Epoch 7/15: [=====                         ] 8/47 batches, loss: 0.1014Epoch 7/15: [=====                         ] 9/47 batches, loss: 0.1000Epoch 7/15: [======                        ] 10/47 batches, loss: 0.1022Epoch 7/15: [=======                       ] 11/47 batches, loss: 0.0995Epoch 7/15: [=======                       ] 12/47 batches, loss: 0.0994Epoch 7/15: [========                      ] 13/47 batches, loss: 0.0995Epoch 7/15: [========                      ] 14/47 batches, loss: 0.0980Epoch 7/15: [=========                     ] 15/47 batches, loss: 0.0953Epoch 7/15: [==========                    ] 16/47 batches, loss: 0.0932Epoch 7/15: [==========                    ] 17/47 batches, loss: 0.0924Epoch 7/15: [===========                   ] 18/47 batches, loss: 0.0942Epoch 7/15: [============                  ] 19/47 batches, loss: 0.0940Epoch 7/15: [============                  ] 20/47 batches, loss: 0.0927Epoch 7/15: [=============                 ] 21/47 batches, loss: 0.0907Epoch 7/15: [==============                ] 22/47 batches, loss: 0.0901Epoch 7/15: [==============                ] 23/47 batches, loss: 0.0902Epoch 7/15: [===============               ] 24/47 batches, loss: 0.0927Epoch 7/15: [===============               ] 25/47 batches, loss: 0.0945Epoch 7/15: [================              ] 26/47 batches, loss: 0.0963Epoch 7/15: [=================             ] 27/47 batches, loss: 0.0970Epoch 7/15: [=================             ] 28/47 batches, loss: 0.0970Epoch 7/15: [==================            ] 29/47 batches, loss: 0.0970Epoch 7/15: [===================           ] 30/47 batches, loss: 0.0970Epoch 7/15: [===================           ] 31/47 batches, loss: 0.0973Epoch 7/15: [====================          ] 32/47 batches, loss: 0.0979Epoch 7/15: [=====================         ] 33/47 batches, loss: 0.0973Epoch 7/15: [=====================         ] 34/47 batches, loss: 0.0980Epoch 7/15: [======================        ] 35/47 batches, loss: 0.0980Epoch 7/15: [======================        ] 36/47 batches, loss: 0.0990Epoch 7/15: [=======================       ] 37/47 batches, loss: 0.0974Epoch 7/15: [========================      ] 38/47 batches, loss: 0.1005Epoch 7/15: [========================      ] 39/47 batches, loss: 0.1001Epoch 7/15: [=========================     ] 40/47 batches, loss: 0.0996Epoch 7/15: [==========================    ] 41/47 batches, loss: 0.0988Epoch 7/15: [==========================    ] 42/47 batches, loss: 0.0982Epoch 7/15: [===========================   ] 43/47 batches, loss: 0.0985Epoch 7/15: [============================  ] 44/47 batches, loss: 0.0984Epoch 7/15: [============================  ] 45/47 batches, loss: 0.0980Epoch 7/15: [============================= ] 46/47 batches, loss: 0.0977Epoch 7/15: [==============================] 47/47 batches, loss: 0.0965
[2025-05-07 22:04:48,241][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0965
[2025-05-07 22:04:48,517][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0687, Metrics: {'mse': 0.07008928805589676, 'rmse': 0.26474381589736284, 'r2': -0.06889104843139648}
Epoch 8/15: [Epoch 8/15: [                              ] 1/47 batches, loss: 0.0379Epoch 8/15: [=                             ] 2/47 batches, loss: 0.0786Epoch 8/15: [=                             ] 3/47 batches, loss: 0.0783Epoch 8/15: [==                            ] 4/47 batches, loss: 0.1022Epoch 8/15: [===                           ] 5/47 batches, loss: 0.1055Epoch 8/15: [===                           ] 6/47 batches, loss: 0.1068Epoch 8/15: [====                          ] 7/47 batches, loss: 0.1097Epoch 8/15: [=====                         ] 8/47 batches, loss: 0.1064Epoch 8/15: [=====                         ] 9/47 batches, loss: 0.1030Epoch 8/15: [======                        ] 10/47 batches, loss: 0.1029Epoch 8/15: [=======                       ] 11/47 batches, loss: 0.1009Epoch 8/15: [=======                       ] 12/47 batches, loss: 0.1006Epoch 8/15: [========                      ] 13/47 batches, loss: 0.0998Epoch 8/15: [========                      ] 14/47 batches, loss: 0.1035Epoch 8/15: [=========                     ] 15/47 batches, loss: 0.1022Epoch 8/15: [==========                    ] 16/47 batches, loss: 0.1019Epoch 8/15: [==========                    ] 17/47 batches, loss: 0.1027Epoch 8/15: [===========                   ] 18/47 batches, loss: 0.1018Epoch 8/15: [============                  ] 19/47 batches, loss: 0.1020Epoch 8/15: [============                  ] 20/47 batches, loss: 0.1034Epoch 8/15: [=============                 ] 21/47 batches, loss: 0.1023Epoch 8/15: [==============                ] 22/47 batches, loss: 0.0998Epoch 8/15: [==============                ] 23/47 batches, loss: 0.1018Epoch 8/15: [===============               ] 24/47 batches, loss: 0.1019Epoch 8/15: [===============               ] 25/47 batches, loss: 0.1045Epoch 8/15: [================              ] 26/47 batches, loss: 0.1048Epoch 8/15: [=================             ] 27/47 batches, loss: 0.1025Epoch 8/15: [=================             ] 28/47 batches, loss: 0.1027Epoch 8/15: [==================            ] 29/47 batches, loss: 0.1004Epoch 8/15: [===================           ] 30/47 batches, loss: 0.0995Epoch 8/15: [===================           ] 31/47 batches, loss: 0.1002Epoch 8/15: [====================          ] 32/47 batches, loss: 0.1009Epoch 8/15: [=====================         ] 33/47 batches, loss: 0.1012Epoch 8/15: [=====================         ] 34/47 batches, loss: 0.1021Epoch 8/15: [======================        ] 35/47 batches, loss: 0.1016Epoch 8/15: [======================        ] 36/47 batches, loss: 0.1011Epoch 8/15: [=======================       ] 37/47 batches, loss: 0.1014Epoch 8/15: [========================      ] 38/47 batches, loss: 0.1005Epoch 8/15: [========================      ] 39/47 batches, loss: 0.1013Epoch 8/15: [=========================     ] 40/47 batches, loss: 0.1000Epoch 8/15: [==========================    ] 41/47 batches, loss: 0.0996Epoch 8/15: [==========================    ] 42/47 batches, loss: 0.0987Epoch 8/15: [===========================   ] 43/47 batches, loss: 0.0985Epoch 8/15: [============================  ] 44/47 batches, loss: 0.0981Epoch 8/15: [============================  ] 45/47 batches, loss: 0.0976Epoch 8/15: [============================= ] 46/47 batches, loss: 0.0983Epoch 8/15: [==============================] 47/47 batches, loss: 0.0967
[2025-05-07 22:04:50,362][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0967
[2025-05-07 22:04:50,633][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0624, Metrics: {'mse': 0.06361807882785797, 'rmse': 0.2522262453192728, 'r2': 0.029797613620758057}
Epoch 9/15: [Epoch 9/15: [                              ] 1/47 batches, loss: 0.0740Epoch 9/15: [=                             ] 2/47 batches, loss: 0.0824Epoch 9/15: [=                             ] 3/47 batches, loss: 0.0846Epoch 9/15: [==                            ] 4/47 batches, loss: 0.0856Epoch 9/15: [===                           ] 5/47 batches, loss: 0.0805Epoch 9/15: [===                           ] 6/47 batches, loss: 0.0873Epoch 9/15: [====                          ] 7/47 batches, loss: 0.0906Epoch 9/15: [=====                         ] 8/47 batches, loss: 0.0938Epoch 9/15: [=====                         ] 9/47 batches, loss: 0.0934Epoch 9/15: [======                        ] 10/47 batches, loss: 0.0915Epoch 9/15: [=======                       ] 11/47 batches, loss: 0.0877Epoch 9/15: [=======                       ] 12/47 batches, loss: 0.0907Epoch 9/15: [========                      ] 13/47 batches, loss: 0.0910Epoch 9/15: [========                      ] 14/47 batches, loss: 0.0933Epoch 9/15: [=========                     ] 15/47 batches, loss: 0.0941Epoch 9/15: [==========                    ] 16/47 batches, loss: 0.0938Epoch 9/15: [==========                    ] 17/47 batches, loss: 0.0953Epoch 9/15: [===========                   ] 18/47 batches, loss: 0.0920Epoch 9/15: [============                  ] 19/47 batches, loss: 0.0919Epoch 9/15: [============                  ] 20/47 batches, loss: 0.0918Epoch 9/15: [=============                 ] 21/47 batches, loss: 0.0941Epoch 9/15: [==============                ] 22/47 batches, loss: 0.0930Epoch 9/15: [==============                ] 23/47 batches, loss: 0.0916Epoch 9/15: [===============               ] 24/47 batches, loss: 0.0913Epoch 9/15: [===============               ] 25/47 batches, loss: 0.0943Epoch 9/15: [================              ] 26/47 batches, loss: 0.0950Epoch 9/15: [=================             ] 27/47 batches, loss: 0.0967Epoch 9/15: [=================             ] 28/47 batches, loss: 0.0987Epoch 9/15: [==================            ] 29/47 batches, loss: 0.0981Epoch 9/15: [===================           ] 30/47 batches, loss: 0.0969Epoch 9/15: [===================           ] 31/47 batches, loss: 0.0965Epoch 9/15: [====================          ] 32/47 batches, loss: 0.0953Epoch 9/15: [=====================         ] 33/47 batches, loss: 0.0951Epoch 9/15: [=====================         ] 34/47 batches, loss: 0.0937Epoch 9/15: [======================        ] 35/47 batches, loss: 0.0929Epoch 9/15: [======================        ] 36/47 batches, loss: 0.0934Epoch 9/15: [=======================       ] 37/47 batches, loss: 0.0942Epoch 9/15: [========================      ] 38/47 batches, loss: 0.0939Epoch 9/15: [========================      ] 39/47 batches, loss: 0.0941Epoch 9/15: [=========================     ] 40/47 batches, loss: 0.0940Epoch 9/15: [==========================    ] 41/47 batches, loss: 0.0941Epoch 9/15: [==========================    ] 42/47 batches, loss: 0.0953Epoch 9/15: [===========================   ] 43/47 batches, loss: 0.0951Epoch 9/15: [============================  ] 44/47 batches, loss: 0.0952Epoch 9/15: [============================  ] 45/47 batches, loss: 0.0973Epoch 9/15: [============================= ] 46/47 batches, loss: 0.0962Epoch 9/15: [==============================] 47/47 batches, loss: 0.0958
[2025-05-07 22:04:52,597][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0958
[2025-05-07 22:04:52,888][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0777, Metrics: {'mse': 0.07900430262088776, 'rmse': 0.28107704036596043, 'r2': -0.2048487663269043}
[2025-05-07 22:04:52,889][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/47 batches, loss: 0.0597Epoch 10/15: [=                             ] 2/47 batches, loss: 0.0720Epoch 10/15: [=                             ] 3/47 batches, loss: 0.0744Epoch 10/15: [==                            ] 4/47 batches, loss: 0.0776Epoch 10/15: [===                           ] 5/47 batches, loss: 0.0858Epoch 10/15: [===                           ] 6/47 batches, loss: 0.0850Epoch 10/15: [====                          ] 7/47 batches, loss: 0.0787Epoch 10/15: [=====                         ] 8/47 batches, loss: 0.0834Epoch 10/15: [=====                         ] 9/47 batches, loss: 0.0855Epoch 10/15: [======                        ] 10/47 batches, loss: 0.0876Epoch 10/15: [=======                       ] 11/47 batches, loss: 0.0873Epoch 10/15: [=======                       ] 12/47 batches, loss: 0.0898Epoch 10/15: [========                      ] 13/47 batches, loss: 0.0894Epoch 10/15: [========                      ] 14/47 batches, loss: 0.0878Epoch 10/15: [=========                     ] 15/47 batches, loss: 0.0878Epoch 10/15: [==========                    ] 16/47 batches, loss: 0.0872Epoch 10/15: [==========                    ] 17/47 batches, loss: 0.0842Epoch 10/15: [===========                   ] 18/47 batches, loss: 0.0888Epoch 10/15: [============                  ] 19/47 batches, loss: 0.0893Epoch 10/15: [============                  ] 20/47 batches, loss: 0.0885Epoch 10/15: [=============                 ] 21/47 batches, loss: 0.0906Epoch 10/15: [==============                ] 22/47 batches, loss: 0.0913Epoch 10/15: [==============                ] 23/47 batches, loss: 0.0894Epoch 10/15: [===============               ] 24/47 batches, loss: 0.0913Epoch 10/15: [===============               ] 25/47 batches, loss: 0.0909Epoch 10/15: [================              ] 26/47 batches, loss: 0.0896Epoch 10/15: [=================             ] 27/47 batches, loss: 0.0877Epoch 10/15: [=================             ] 28/47 batches, loss: 0.0891Epoch 10/15: [==================            ] 29/47 batches, loss: 0.0879Epoch 10/15: [===================           ] 30/47 batches, loss: 0.0872Epoch 10/15: [===================           ] 31/47 batches, loss: 0.0873Epoch 10/15: [====================          ] 32/47 batches, loss: 0.0865Epoch 10/15: [=====================         ] 33/47 batches, loss: 0.0876Epoch 10/15: [=====================         ] 34/47 batches, loss: 0.0868Epoch 10/15: [======================        ] 35/47 batches, loss: 0.0867Epoch 10/15: [======================        ] 36/47 batches, loss: 0.0867Epoch 10/15: [=======================       ] 37/47 batches, loss: 0.0864Epoch 10/15: [========================      ] 38/47 batches, loss: 0.0880Epoch 10/15: [========================      ] 39/47 batches, loss: 0.0873Epoch 10/15: [=========================     ] 40/47 batches, loss: 0.0875Epoch 10/15: [==========================    ] 41/47 batches, loss: 0.0862Epoch 10/15: [==========================    ] 42/47 batches, loss: 0.0864Epoch 10/15: [===========================   ] 43/47 batches, loss: 0.0861Epoch 10/15: [============================  ] 44/47 batches, loss: 0.0857Epoch 10/15: [============================  ] 45/47 batches, loss: 0.0859Epoch 10/15: [============================= ] 46/47 batches, loss: 0.0850Epoch 10/15: [==============================] 47/47 batches, loss: 0.0847
[2025-05-07 22:04:54,379][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0847
[2025-05-07 22:04:54,647][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0599, Metrics: {'mse': 0.061069149523973465, 'rmse': 0.24712173017355935, 'r2': 0.06866979598999023}
Epoch 11/15: [Epoch 11/15: [                              ] 1/47 batches, loss: 0.0444Epoch 11/15: [=                             ] 2/47 batches, loss: 0.0581Epoch 11/15: [=                             ] 3/47 batches, loss: 0.0648Epoch 11/15: [==                            ] 4/47 batches, loss: 0.0849Epoch 11/15: [===                           ] 5/47 batches, loss: 0.0878Epoch 11/15: [===                           ] 6/47 batches, loss: 0.0842Epoch 11/15: [====                          ] 7/47 batches, loss: 0.0862Epoch 11/15: [=====                         ] 8/47 batches, loss: 0.0855Epoch 11/15: [=====                         ] 9/47 batches, loss: 0.0828Epoch 11/15: [======                        ] 10/47 batches, loss: 0.0789Epoch 11/15: [=======                       ] 11/47 batches, loss: 0.0789Epoch 11/15: [=======                       ] 12/47 batches, loss: 0.0792Epoch 11/15: [========                      ] 13/47 batches, loss: 0.0872Epoch 11/15: [========                      ] 14/47 batches, loss: 0.0889Epoch 11/15: [=========                     ] 15/47 batches, loss: 0.0856Epoch 11/15: [==========                    ] 16/47 batches, loss: 0.0844Epoch 11/15: [==========                    ] 17/47 batches, loss: 0.0825Epoch 11/15: [===========                   ] 18/47 batches, loss: 0.0822Epoch 11/15: [============                  ] 19/47 batches, loss: 0.0811Epoch 11/15: [============                  ] 20/47 batches, loss: 0.0825Epoch 11/15: [=============                 ] 21/47 batches, loss: 0.0816Epoch 11/15: [==============                ] 22/47 batches, loss: 0.0827Epoch 11/15: [==============                ] 23/47 batches, loss: 0.0834Epoch 11/15: [===============               ] 24/47 batches, loss: 0.0833Epoch 11/15: [===============               ] 25/47 batches, loss: 0.0816Epoch 11/15: [================              ] 26/47 batches, loss: 0.0828Epoch 11/15: [=================             ] 27/47 batches, loss: 0.0813Epoch 11/15: [=================             ] 28/47 batches, loss: 0.0826Epoch 11/15: [==================            ] 29/47 batches, loss: 0.0828Epoch 11/15: [===================           ] 30/47 batches, loss: 0.0828Epoch 11/15: [===================           ] 31/47 batches, loss: 0.0826Epoch 11/15: [====================          ] 32/47 batches, loss: 0.0844Epoch 11/15: [=====================         ] 33/47 batches, loss: 0.0841Epoch 11/15: [=====================         ] 34/47 batches, loss: 0.0837Epoch 11/15: [======================        ] 35/47 batches, loss: 0.0829Epoch 11/15: [======================        ] 36/47 batches, loss: 0.0824Epoch 11/15: [=======================       ] 37/47 batches, loss: 0.0835Epoch 11/15: [========================      ] 38/47 batches, loss: 0.0835Epoch 11/15: [========================      ] 39/47 batches, loss: 0.0835Epoch 11/15: [=========================     ] 40/47 batches, loss: 0.0834Epoch 11/15: [==========================    ] 41/47 batches, loss: 0.0834Epoch 11/15: [==========================    ] 42/47 batches, loss: 0.0828Epoch 11/15: [===========================   ] 43/47 batches, loss: 0.0827Epoch 11/15: [============================  ] 44/47 batches, loss: 0.0822Epoch 11/15: [============================  ] 45/47 batches, loss: 0.0821Epoch 11/15: [============================= ] 46/47 batches, loss: 0.0819Epoch 11/15: [==============================] 47/47 batches, loss: 0.0818
[2025-05-07 22:04:56,523][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0818
[2025-05-07 22:04:56,802][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0583, Metrics: {'mse': 0.0595075823366642, 'rmse': 0.24394176013275012, 'r2': 0.09248435497283936}
Epoch 12/15: [Epoch 12/15: [                              ] 1/47 batches, loss: 0.0578Epoch 12/15: [=                             ] 2/47 batches, loss: 0.0731Epoch 12/15: [=                             ] 3/47 batches, loss: 0.0638Epoch 12/15: [==                            ] 4/47 batches, loss: 0.0609Epoch 12/15: [===                           ] 5/47 batches, loss: 0.0624Epoch 12/15: [===                           ] 6/47 batches, loss: 0.0711Epoch 12/15: [====                          ] 7/47 batches, loss: 0.0697Epoch 12/15: [=====                         ] 8/47 batches, loss: 0.0709Epoch 12/15: [=====                         ] 9/47 batches, loss: 0.0686Epoch 12/15: [======                        ] 10/47 batches, loss: 0.0688Epoch 12/15: [=======                       ] 11/47 batches, loss: 0.0684Epoch 12/15: [=======                       ] 12/47 batches, loss: 0.0682Epoch 12/15: [========                      ] 13/47 batches, loss: 0.0690Epoch 12/15: [========                      ] 14/47 batches, loss: 0.0686Epoch 12/15: [=========                     ] 15/47 batches, loss: 0.0714Epoch 12/15: [==========                    ] 16/47 batches, loss: 0.0766Epoch 12/15: [==========                    ] 17/47 batches, loss: 0.0744Epoch 12/15: [===========                   ] 18/47 batches, loss: 0.0733Epoch 12/15: [============                  ] 19/47 batches, loss: 0.0730Epoch 12/15: [============                  ] 20/47 batches, loss: 0.0720Epoch 12/15: [=============                 ] 21/47 batches, loss: 0.0722Epoch 12/15: [==============                ] 22/47 batches, loss: 0.0712Epoch 12/15: [==============                ] 23/47 batches, loss: 0.0715Epoch 12/15: [===============               ] 24/47 batches, loss: 0.0743Epoch 12/15: [===============               ] 25/47 batches, loss: 0.0734Epoch 12/15: [================              ] 26/47 batches, loss: 0.0729Epoch 12/15: [=================             ] 27/47 batches, loss: 0.0726Epoch 12/15: [=================             ] 28/47 batches, loss: 0.0735Epoch 12/15: [==================            ] 29/47 batches, loss: 0.0736Epoch 12/15: [===================           ] 30/47 batches, loss: 0.0739Epoch 12/15: [===================           ] 31/47 batches, loss: 0.0747Epoch 12/15: [====================          ] 32/47 batches, loss: 0.0747Epoch 12/15: [=====================         ] 33/47 batches, loss: 0.0752Epoch 12/15: [=====================         ] 34/47 batches, loss: 0.0750Epoch 12/15: [======================        ] 35/47 batches, loss: 0.0750Epoch 12/15: [======================        ] 36/47 batches, loss: 0.0753Epoch 12/15: [=======================       ] 37/47 batches, loss: 0.0750Epoch 12/15: [========================      ] 38/47 batches, loss: 0.0762Epoch 12/15: [========================      ] 39/47 batches, loss: 0.0753Epoch 12/15: [=========================     ] 40/47 batches, loss: 0.0756Epoch 12/15: [==========================    ] 41/47 batches, loss: 0.0747Epoch 12/15: [==========================    ] 42/47 batches, loss: 0.0741Epoch 12/15: [===========================   ] 43/47 batches, loss: 0.0755Epoch 12/15: [============================  ] 44/47 batches, loss: 0.0751Epoch 12/15: [============================  ] 45/47 batches, loss: 0.0751Epoch 12/15: [============================= ] 46/47 batches, loss: 0.0749Epoch 12/15: [==============================] 47/47 batches, loss: 0.0759
[2025-05-07 22:04:58,649][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0759
[2025-05-07 22:04:58,919][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0712, Metrics: {'mse': 0.07242265343666077, 'rmse': 0.26911457306630715, 'r2': -0.10447585582733154}
[2025-05-07 22:04:58,920][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/47 batches, loss: 0.0691Epoch 13/15: [=                             ] 2/47 batches, loss: 0.0588Epoch 13/15: [=                             ] 3/47 batches, loss: 0.0731Epoch 13/15: [==                            ] 4/47 batches, loss: 0.0760Epoch 13/15: [===                           ] 5/47 batches, loss: 0.0735Epoch 13/15: [===                           ] 6/47 batches, loss: 0.0717Epoch 13/15: [====                          ] 7/47 batches, loss: 0.0715Epoch 13/15: [=====                         ] 8/47 batches, loss: 0.0712Epoch 13/15: [=====                         ] 9/47 batches, loss: 0.0688Epoch 13/15: [======                        ] 10/47 batches, loss: 0.0695Epoch 13/15: [=======                       ] 11/47 batches, loss: 0.0724Epoch 13/15: [=======                       ] 12/47 batches, loss: 0.0724Epoch 13/15: [========                      ] 13/47 batches, loss: 0.0763Epoch 13/15: [========                      ] 14/47 batches, loss: 0.0780Epoch 13/15: [=========                     ] 15/47 batches, loss: 0.0764Epoch 13/15: [==========                    ] 16/47 batches, loss: 0.0747Epoch 13/15: [==========                    ] 17/47 batches, loss: 0.0715Epoch 13/15: [===========                   ] 18/47 batches, loss: 0.0735Epoch 13/15: [============                  ] 19/47 batches, loss: 0.0739Epoch 13/15: [============                  ] 20/47 batches, loss: 0.0731Epoch 13/15: [=============                 ] 21/47 batches, loss: 0.0734Epoch 13/15: [==============                ] 22/47 batches, loss: 0.0736Epoch 13/15: [==============                ] 23/47 batches, loss: 0.0728Epoch 13/15: [===============               ] 24/47 batches, loss: 0.0732Epoch 13/15: [===============               ] 25/47 batches, loss: 0.0738Epoch 13/15: [================              ] 26/47 batches, loss: 0.0738Epoch 13/15: [=================             ] 27/47 batches, loss: 0.0726Epoch 13/15: [=================             ] 28/47 batches, loss: 0.0719Epoch 13/15: [==================            ] 29/47 batches, loss: 0.0742Epoch 13/15: [===================           ] 30/47 batches, loss: 0.0739Epoch 13/15: [===================           ] 31/47 batches, loss: 0.0760Epoch 13/15: [====================          ] 32/47 batches, loss: 0.0761Epoch 13/15: [=====================         ] 33/47 batches, loss: 0.0767Epoch 13/15: [=====================         ] 34/47 batches, loss: 0.0771Epoch 13/15: [======================        ] 35/47 batches, loss: 0.0763Epoch 13/15: [======================        ] 36/47 batches, loss: 0.0760Epoch 13/15: [=======================       ] 37/47 batches, loss: 0.0755Epoch 13/15: [========================      ] 38/47 batches, loss: 0.0758Epoch 13/15: [========================      ] 39/47 batches, loss: 0.0755Epoch 13/15: [=========================     ] 40/47 batches, loss: 0.0753Epoch 13/15: [==========================    ] 41/47 batches, loss: 0.0755Epoch 13/15: [==========================    ] 42/47 batches, loss: 0.0747Epoch 13/15: [===========================   ] 43/47 batches, loss: 0.0740Epoch 13/15: [============================  ] 44/47 batches, loss: 0.0740Epoch 13/15: [============================  ] 45/47 batches, loss: 0.0744Epoch 13/15: [============================= ] 46/47 batches, loss: 0.0746Epoch 13/15: [==============================] 47/47 batches, loss: 0.0740
[2025-05-07 22:05:00,406][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0740
[2025-05-07 22:05:02,782][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0816, Metrics: {'mse': 0.08274798840284348, 'rmse': 0.2876595008040643, 'r2': -0.2619415521621704}
[2025-05-07 22:05:02,782][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/47 batches, loss: 0.0585Epoch 14/15: [=                             ] 2/47 batches, loss: 0.0689Epoch 14/15: [=                             ] 3/47 batches, loss: 0.0869Epoch 14/15: [==                            ] 4/47 batches, loss: 0.0802Epoch 14/15: [===                           ] 5/47 batches, loss: 0.0806Epoch 14/15: [===                           ] 6/47 batches, loss: 0.0931Epoch 14/15: [====                          ] 7/47 batches, loss: 0.0978Epoch 14/15: [=====                         ] 8/47 batches, loss: 0.0951Epoch 14/15: [=====                         ] 9/47 batches, loss: 0.0931Epoch 14/15: [======                        ] 10/47 batches, loss: 0.0904Epoch 14/15: [=======                       ] 11/47 batches, loss: 0.0873Epoch 14/15: [=======                       ] 12/47 batches, loss: 0.0854Epoch 14/15: [========                      ] 13/47 batches, loss: 0.0867Epoch 14/15: [========                      ] 14/47 batches, loss: 0.0865Epoch 14/15: [=========                     ] 15/47 batches, loss: 0.0870Epoch 14/15: [==========                    ] 16/47 batches, loss: 0.0843Epoch 14/15: [==========                    ] 17/47 batches, loss: 0.0820Epoch 14/15: [===========                   ] 18/47 batches, loss: 0.0804Epoch 14/15: [============                  ] 19/47 batches, loss: 0.0782Epoch 14/15: [============                  ] 20/47 batches, loss: 0.0772Epoch 14/15: [=============                 ] 21/47 batches, loss: 0.0773Epoch 14/15: [==============                ] 22/47 batches, loss: 0.0763Epoch 14/15: [==============                ] 23/47 batches, loss: 0.0772Epoch 14/15: [===============               ] 24/47 batches, loss: 0.0758Epoch 14/15: [===============               ] 25/47 batches, loss: 0.0782Epoch 14/15: [================              ] 26/47 batches, loss: 0.0781Epoch 14/15: [=================             ] 27/47 batches, loss: 0.0764Epoch 14/15: [=================             ] 28/47 batches, loss: 0.0774Epoch 14/15: [==================            ] 29/47 batches, loss: 0.0771Epoch 14/15: [===================           ] 30/47 batches, loss: 0.0762Epoch 14/15: [===================           ] 31/47 batches, loss: 0.0754Epoch 14/15: [====================          ] 32/47 batches, loss: 0.0762Epoch 14/15: [=====================         ] 33/47 batches, loss: 0.0754Epoch 14/15: [=====================         ] 34/47 batches, loss: 0.0754Epoch 14/15: [======================        ] 35/47 batches, loss: 0.0760Epoch 14/15: [======================        ] 36/47 batches, loss: 0.0753Epoch 14/15: [=======================       ] 37/47 batches, loss: 0.0751Epoch 14/15: [========================      ] 38/47 batches, loss: 0.0748Epoch 14/15: [========================      ] 39/47 batches, loss: 0.0746Epoch 14/15: [=========================     ] 40/47 batches, loss: 0.0748Epoch 14/15: [==========================    ] 41/47 batches, loss: 0.0745Epoch 14/15: [==========================    ] 42/47 batches, loss: 0.0737Epoch 14/15: [===========================   ] 43/47 batches, loss: 0.0730Epoch 14/15: [============================  ] 44/47 batches, loss: 0.0727Epoch 14/15: [============================  ] 45/47 batches, loss: 0.0722Epoch 14/15: [============================= ] 46/47 batches, loss: 0.0726Epoch 14/15: [==============================] 47/47 batches, loss: 0.0716
[2025-05-07 22:05:04,271][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0716
[2025-05-07 22:05:04,579][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0562, Metrics: {'mse': 0.05728505551815033, 'rmse': 0.23934296630181204, 'r2': 0.1263788342475891}
Epoch 15/15: [Epoch 15/15: [                              ] 1/47 batches, loss: 0.0805Epoch 15/15: [=                             ] 2/47 batches, loss: 0.0902Epoch 15/15: [=                             ] 3/47 batches, loss: 0.0852Epoch 15/15: [==                            ] 4/47 batches, loss: 0.0801Epoch 15/15: [===                           ] 5/47 batches, loss: 0.0735Epoch 15/15: [===                           ] 6/47 batches, loss: 0.0772Epoch 15/15: [====                          ] 7/47 batches, loss: 0.0740Epoch 15/15: [=====                         ] 8/47 batches, loss: 0.0753Epoch 15/15: [=====                         ] 9/47 batches, loss: 0.0744Epoch 15/15: [======                        ] 10/47 batches, loss: 0.0723Epoch 15/15: [=======                       ] 11/47 batches, loss: 0.0735Epoch 15/15: [=======                       ] 12/47 batches, loss: 0.0753Epoch 15/15: [========                      ] 13/47 batches, loss: 0.0752Epoch 15/15: [========                      ] 14/47 batches, loss: 0.0759Epoch 15/15: [=========                     ] 15/47 batches, loss: 0.0735Epoch 15/15: [==========                    ] 16/47 batches, loss: 0.0709Epoch 15/15: [==========                    ] 17/47 batches, loss: 0.0729Epoch 15/15: [===========                   ] 18/47 batches, loss: 0.0735Epoch 15/15: [============                  ] 19/47 batches, loss: 0.0759Epoch 15/15: [============                  ] 20/47 batches, loss: 0.0752Epoch 15/15: [=============                 ] 21/47 batches, loss: 0.0757Epoch 15/15: [==============                ] 22/47 batches, loss: 0.0755Epoch 15/15: [==============                ] 23/47 batches, loss: 0.0754Epoch 15/15: [===============               ] 24/47 batches, loss: 0.0756Epoch 15/15: [===============               ] 25/47 batches, loss: 0.0751Epoch 15/15: [================              ] 26/47 batches, loss: 0.0741Epoch 15/15: [=================             ] 27/47 batches, loss: 0.0730Epoch 15/15: [=================             ] 28/47 batches, loss: 0.0730Epoch 15/15: [==================            ] 29/47 batches, loss: 0.0722Epoch 15/15: [===================           ] 30/47 batches, loss: 0.0722Epoch 15/15: [===================           ] 31/47 batches, loss: 0.0724Epoch 15/15: [====================          ] 32/47 batches, loss: 0.0743Epoch 15/15: [=====================         ] 33/47 batches, loss: 0.0747Epoch 15/15: [=====================         ] 34/47 batches, loss: 0.0737Epoch 15/15: [======================        ] 35/47 batches, loss: 0.0746Epoch 15/15: [======================        ] 36/47 batches, loss: 0.0747Epoch 15/15: [=======================       ] 37/47 batches, loss: 0.0739Epoch 15/15: [========================      ] 38/47 batches, loss: 0.0733Epoch 15/15: [========================      ] 39/47 batches, loss: 0.0731Epoch 15/15: [=========================     ] 40/47 batches, loss: 0.0733Epoch 15/15: [==========================    ] 41/47 batches, loss: 0.0727Epoch 15/15: [==========================    ] 42/47 batches, loss: 0.0720Epoch 15/15: [===========================   ] 43/47 batches, loss: 0.0733Epoch 15/15: [============================  ] 44/47 batches, loss: 0.0722Epoch 15/15: [============================  ] 45/47 batches, loss: 0.0715Epoch 15/15: [============================= ] 46/47 batches, loss: 0.0720Epoch 15/15: [==============================] 47/47 batches, loss: 0.0709
[2025-05-07 22:05:06,464][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0709
[2025-05-07 22:05:06,779][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0494, Metrics: {'mse': 0.05043551325798035, 'rmse': 0.22457852359025862, 'r2': 0.23083722591400146}
[2025-05-07 22:05:07,191][src.training.lm_trainer][INFO] - Training completed in 33.71 seconds
[2025-05-07 22:05:07,191][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:05:09,563][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.043069012463092804, 'rmse': 0.2075307506445558, 'r2': 0.32996588945388794}
[2025-05-07 22:05:09,563][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05043551325798035, 'rmse': 0.22457852359025862, 'r2': 0.23083722591400146}
[2025-05-07 22:05:09,563][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.050424788147211075, 'rmse': 0.2245546440116772, 'r2': 0.002089858055114746}
[2025-05-07 22:05:11,274][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/ko/ko/model.pt
[2025-05-07 22:05:11,276][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▄▃▃▂▂▂▂▁
wandb:     best_val_mse █▄▄▃▃▂▂▂▂▁
wandb:      best_val_r2 ▁▅▅▆▆▇▇▇▇█
wandb:    best_val_rmse █▅▄▄▃▂▂▂▂▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▅▅▅▅▆▆▆▅▆▇▆▅▇
wandb:       train_loss █▅▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▅▄▄▃▃▂▄▂▂▃▄▂▁
wandb:          val_mse █▄▄▄▄▃▃▂▃▂▂▃▄▂▁
wandb:           val_r2 ▁▅▅▅▅▆▆▇▆▇▇▆▅▇█
wandb:         val_rmse █▅▅▄▅▄▃▂▄▂▂▃▄▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04944
wandb:     best_val_mse 0.05044
wandb:      best_val_r2 0.23084
wandb:    best_val_rmse 0.22458
wandb:            epoch 15
wandb:   final_test_mse 0.05042
wandb:    final_test_r2 0.00209
wandb:  final_test_rmse 0.22455
wandb:  final_train_mse 0.04307
wandb:   final_train_r2 0.32997
wandb: final_train_rmse 0.20753
wandb:    final_val_mse 0.05044
wandb:     final_val_r2 0.23084
wandb:   final_val_rmse 0.22458
wandb:    learning_rate 0.0001
wandb:       train_loss 0.07092
wandb:       train_time 33.71158
wandb:         val_loss 0.04944
wandb:          val_mse 0.05044
wandb:           val_r2 0.23084
wandb:         val_rmse 0.22458
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_220416-cscuwyjk
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_220416-cscuwyjk/logs
Experiment probe_layer2_avg_subordinate_chain_len_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_avg_verb_edges_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_verb_edges_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/layer2/ko"         "wandb.mode=offline" "experiment.submetric=avg_verb_edges"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:05:38,312][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/layer2/ko
experiment_name: probe_layer2_avg_verb_edges_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:05:38,312][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:05:38,312][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 22:05:38,312][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:05:38,312][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:05:38,317][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:05:38,317][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 22:05:38,317][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:05:41,188][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:05:43,697][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:05:43,697][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:05:43,871][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:05:43,959][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:05:44,143][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:05:44,150][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:05:44,150][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:05:44,151][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:05:44,187][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:05:44,260][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:05:44,286][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:05:44,287][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:05:44,287][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:05:44,288][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:05:44,350][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:05:44,417][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:05:44,463][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:05:44,464][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:05:44,464][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:05:44,466][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:05:44,466][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:05:44,466][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 22:05:44,466][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 22:05:44,467][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 22:05:44,467][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:05:44,467][src.data.datasets][INFO] -   Mean: 0.3982, Std: 0.1920
[2025-05-07 22:05:44,467][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:05:44,467][src.data.datasets][INFO] - Sample label: 0.4000000059604645
[2025-05-07 22:05:44,467][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:05:44,467][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 22:05:44,467][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 22:05:44,468][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 22:05:44,468][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-05-07 22:05:44,468][src.data.datasets][INFO] -   Mean: 0.3198, Std: 0.1567
[2025-05-07 22:05:44,468][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:05:44,468][src.data.datasets][INFO] - Sample label: 0.27799999713897705
[2025-05-07 22:05:44,468][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:05:44,468][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 22:05:44,468][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 22:05:44,468][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 22:05:44,468][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-05-07 22:05:44,469][src.data.datasets][INFO] -   Mean: 0.3201, Std: 0.1863
[2025-05-07 22:05:44,469][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:05:44,469][src.data.datasets][INFO] - Sample label: 0.25
[2025-05-07 22:05:44,469][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:05:44,469][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:05:44,469][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:05:44,469][__main__][INFO] - Using model type: lm_probe for submetric avg_verb_edges
[2025-05-07 22:05:44,470][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:05:51,390][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:05:51,392][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:05:51,392][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:05:51,392][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:05:51,395][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:05:51,395][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:05:51,395][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:05:51,396][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:05:51,396][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:05:51,397][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:05:51,397][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.3553Epoch 1/15: [=                             ] 2/47 batches, loss: 0.5485Epoch 1/15: [=                             ] 3/47 batches, loss: 0.5012Epoch 1/15: [==                            ] 4/47 batches, loss: 0.4874Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4708Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4416Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4224Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4347Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4421Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4452Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4346Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4273Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4169Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4177Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.4052Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4138Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.4029Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4202Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4153Epoch 1/15: [============                  ] 20/47 batches, loss: 0.4114Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4143Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4178Epoch 1/15: [==============                ] 23/47 batches, loss: 0.4064Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3956Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3957Epoch 1/15: [================              ] 26/47 batches, loss: 0.3912Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3929Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3854Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3869Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3841Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3779Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3722Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3693Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3691Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3627Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3597Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3540Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3542Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3511Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3517Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3496Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3482Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3453Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3426Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3419Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3361Epoch 1/15: [==============================] 47/47 batches, loss: 0.3506
[2025-05-07 22:05:56,869][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3506
[2025-05-07 22:05:57,163][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0341, Metrics: {'mse': 0.03559610992670059, 'rmse': 0.18866931368587894, 'r2': -0.4488532543182373}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2563Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2015Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2502Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2150Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2258Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2403Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2454Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2405Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2451Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2360Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2467Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2350Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2353Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2350Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2316Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2292Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2226Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2243Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2259Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2195Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2189Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2181Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2162Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2176Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2162Epoch 2/15: [================              ] 26/47 batches, loss: 0.2117Epoch 2/15: [=================             ] 27/47 batches, loss: 0.2113Epoch 2/15: [=================             ] 28/47 batches, loss: 0.2098Epoch 2/15: [==================            ] 29/47 batches, loss: 0.2082Epoch 2/15: [===================           ] 30/47 batches, loss: 0.2076Epoch 2/15: [===================           ] 31/47 batches, loss: 0.2080Epoch 2/15: [====================          ] 32/47 batches, loss: 0.2056Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.2075Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.2074Epoch 2/15: [======================        ] 35/47 batches, loss: 0.2058Epoch 2/15: [======================        ] 36/47 batches, loss: 0.2041Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.2057Epoch 2/15: [========================      ] 38/47 batches, loss: 0.2025Epoch 2/15: [========================      ] 39/47 batches, loss: 0.2006Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1996Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1977Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1956Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1937Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1913Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1900Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1906Epoch 2/15: [==============================] 47/47 batches, loss: 0.1891
[2025-05-07 22:05:59,019][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1891
[2025-05-07 22:05:59,299][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0272, Metrics: {'mse': 0.027563996613025665, 'rmse': 0.1660240844366433, 'r2': -0.12192559242248535}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1319Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1412Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1300Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1164Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1283Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1469Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1394Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1336Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1426Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1403Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1397Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1391Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1440Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1420Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1377Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1386Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1413Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1435Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1419Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1392Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1401Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1378Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1380Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1393Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1382Epoch 3/15: [================              ] 26/47 batches, loss: 0.1399Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1421Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1391Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1362Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1388Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1436Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1422Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1430Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1407Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1418Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1407Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1418Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1404Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1380Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1377Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1362Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1356Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1364Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1392Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1409Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1410Epoch 3/15: [==============================] 47/47 batches, loss: 0.1391
[2025-05-07 22:06:01,199][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1391
[2025-05-07 22:06:01,492][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0257, Metrics: {'mse': 0.02632882446050644, 'rmse': 0.16226159268448723, 'r2': -0.07165086269378662}
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1559Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1377Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1122Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1270Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1293Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1306Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1290Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1278Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1297Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1287Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1343Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1317Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1311Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1308Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1269Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1236Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1227Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1213Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1221Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1303Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1310Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1309Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1314Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1322Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1315Epoch 4/15: [================              ] 26/47 batches, loss: 0.1300Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1318Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1351Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1353Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1318Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1343Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1342Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1322Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1314Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1313Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1297Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1292Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1280Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1269Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1270Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1274Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1264Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1252Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1254Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1241Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1247Epoch 4/15: [==============================] 47/47 batches, loss: 0.1235
[2025-05-07 22:06:03,384][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1235
[2025-05-07 22:06:03,684][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0252, Metrics: {'mse': 0.02570117823779583, 'rmse': 0.16031587019941548, 'r2': -0.0461040735244751}
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.0810Epoch 5/15: [=                             ] 2/47 batches, loss: 0.0874Epoch 5/15: [=                             ] 3/47 batches, loss: 0.0994Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0858Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0855Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0910Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0879Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0938Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0971Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0920Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0948Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0956Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0908Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0917Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0917Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0929Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0921Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0973Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0954Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0952Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0982Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0972Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0965Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0997Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0998Epoch 5/15: [================              ] 26/47 batches, loss: 0.0990Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0994Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0982Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0983Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0989Epoch 5/15: [===================           ] 31/47 batches, loss: 0.1014Epoch 5/15: [====================          ] 32/47 batches, loss: 0.1014Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.1016Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.1007Epoch 5/15: [======================        ] 35/47 batches, loss: 0.1031Epoch 5/15: [======================        ] 36/47 batches, loss: 0.1033Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.1027Epoch 5/15: [========================      ] 38/47 batches, loss: 0.1022Epoch 5/15: [========================      ] 39/47 batches, loss: 0.1022Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.1020Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.1010Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0995Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0986Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0989Epoch 5/15: [============================  ] 45/47 batches, loss: 0.1003Epoch 5/15: [============================= ] 46/47 batches, loss: 0.1006Epoch 5/15: [==============================] 47/47 batches, loss: 0.1040
[2025-05-07 22:06:05,520][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1040
[2025-05-07 22:06:05,780][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0247, Metrics: {'mse': 0.025056017562747, 'rmse': 0.15829092697544922, 'r2': -0.019844412803649902}
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.1049Epoch 6/15: [=                             ] 2/47 batches, loss: 0.1030Epoch 6/15: [=                             ] 3/47 batches, loss: 0.1133Epoch 6/15: [==                            ] 4/47 batches, loss: 0.1110Epoch 6/15: [===                           ] 5/47 batches, loss: 0.1182Epoch 6/15: [===                           ] 6/47 batches, loss: 0.1122Epoch 6/15: [====                          ] 7/47 batches, loss: 0.1051Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.1100Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.1054Epoch 6/15: [======                        ] 10/47 batches, loss: 0.1034Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.1089Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.1110Epoch 6/15: [========                      ] 13/47 batches, loss: 0.1050Epoch 6/15: [========                      ] 14/47 batches, loss: 0.1017Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.1024Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.1070Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.1056Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.1072Epoch 6/15: [============                  ] 19/47 batches, loss: 0.1036Epoch 6/15: [============                  ] 20/47 batches, loss: 0.1037Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.1021Epoch 6/15: [==============                ] 22/47 batches, loss: 0.1021Epoch 6/15: [==============                ] 23/47 batches, loss: 0.1013Epoch 6/15: [===============               ] 24/47 batches, loss: 0.1002Epoch 6/15: [===============               ] 25/47 batches, loss: 0.1003Epoch 6/15: [================              ] 26/47 batches, loss: 0.0998Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0989Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0998Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0988Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0973Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0977Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0967Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0961Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0973Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0974Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0972Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0989Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0997Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0994Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0997Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0996Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0991Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0985Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0979Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0971Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0970Epoch 6/15: [==============================] 47/47 batches, loss: 0.0969
[2025-05-07 22:06:07,675][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0969
[2025-05-07 22:06:07,946][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0242, Metrics: {'mse': 0.02444622665643692, 'rmse': 0.15635289142333417, 'r2': 0.004975736141204834}
Epoch 7/15: [Epoch 7/15: [                              ] 1/47 batches, loss: 0.0598Epoch 7/15: [=                             ] 2/47 batches, loss: 0.0691Epoch 7/15: [=                             ] 3/47 batches, loss: 0.0678Epoch 7/15: [==                            ] 4/47 batches, loss: 0.0551Epoch 7/15: [===                           ] 5/47 batches, loss: 0.0782Epoch 7/15: [===                           ] 6/47 batches, loss: 0.0757Epoch 7/15: [====                          ] 7/47 batches, loss: 0.0740Epoch 7/15: [=====                         ] 8/47 batches, loss: 0.0706Epoch 7/15: [=====                         ] 9/47 batches, loss: 0.0733Epoch 7/15: [======                        ] 10/47 batches, loss: 0.0783Epoch 7/15: [=======                       ] 11/47 batches, loss: 0.0774Epoch 7/15: [=======                       ] 12/47 batches, loss: 0.0785Epoch 7/15: [========                      ] 13/47 batches, loss: 0.0759Epoch 7/15: [========                      ] 14/47 batches, loss: 0.0733Epoch 7/15: [=========                     ] 15/47 batches, loss: 0.0736Epoch 7/15: [==========                    ] 16/47 batches, loss: 0.0770Epoch 7/15: [==========                    ] 17/47 batches, loss: 0.0790Epoch 7/15: [===========                   ] 18/47 batches, loss: 0.0835Epoch 7/15: [============                  ] 19/47 batches, loss: 0.0850Epoch 7/15: [============                  ] 20/47 batches, loss: 0.0837Epoch 7/15: [=============                 ] 21/47 batches, loss: 0.0838Epoch 7/15: [==============                ] 22/47 batches, loss: 0.0840Epoch 7/15: [==============                ] 23/47 batches, loss: 0.0842Epoch 7/15: [===============               ] 24/47 batches, loss: 0.0829Epoch 7/15: [===============               ] 25/47 batches, loss: 0.0833Epoch 7/15: [================              ] 26/47 batches, loss: 0.0833Epoch 7/15: [=================             ] 27/47 batches, loss: 0.0828Epoch 7/15: [=================             ] 28/47 batches, loss: 0.0828Epoch 7/15: [==================            ] 29/47 batches, loss: 0.0837Epoch 7/15: [===================           ] 30/47 batches, loss: 0.0832Epoch 7/15: [===================           ] 31/47 batches, loss: 0.0833Epoch 7/15: [====================          ] 32/47 batches, loss: 0.0835Epoch 7/15: [=====================         ] 33/47 batches, loss: 0.0821Epoch 7/15: [=====================         ] 34/47 batches, loss: 0.0817Epoch 7/15: [======================        ] 35/47 batches, loss: 0.0817Epoch 7/15: [======================        ] 36/47 batches, loss: 0.0829Epoch 7/15: [=======================       ] 37/47 batches, loss: 0.0829Epoch 7/15: [========================      ] 38/47 batches, loss: 0.0816Epoch 7/15: [========================      ] 39/47 batches, loss: 0.0820Epoch 7/15: [=========================     ] 40/47 batches, loss: 0.0814Epoch 7/15: [==========================    ] 41/47 batches, loss: 0.0811Epoch 7/15: [==========================    ] 42/47 batches, loss: 0.0805Epoch 7/15: [===========================   ] 43/47 batches, loss: 0.0801Epoch 7/15: [============================  ] 44/47 batches, loss: 0.0797Epoch 7/15: [============================  ] 45/47 batches, loss: 0.0801Epoch 7/15: [============================= ] 46/47 batches, loss: 0.0797Epoch 7/15: [==============================] 47/47 batches, loss: 0.0812
[2025-05-07 22:06:09,779][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0812
[2025-05-07 22:06:10,104][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0234, Metrics: {'mse': 0.02356533706188202, 'rmse': 0.15351005524682096, 'r2': 0.040830135345458984}
Epoch 8/15: [Epoch 8/15: [                              ] 1/47 batches, loss: 0.0298Epoch 8/15: [=                             ] 2/47 batches, loss: 0.0414Epoch 8/15: [=                             ] 3/47 batches, loss: 0.0493Epoch 8/15: [==                            ] 4/47 batches, loss: 0.0574Epoch 8/15: [===                           ] 5/47 batches, loss: 0.0700Epoch 8/15: [===                           ] 6/47 batches, loss: 0.0644Epoch 8/15: [====                          ] 7/47 batches, loss: 0.0643Epoch 8/15: [=====                         ] 8/47 batches, loss: 0.0658Epoch 8/15: [=====                         ] 9/47 batches, loss: 0.0680Epoch 8/15: [======                        ] 10/47 batches, loss: 0.0656Epoch 8/15: [=======                       ] 11/47 batches, loss: 0.0699Epoch 8/15: [=======                       ] 12/47 batches, loss: 0.0724Epoch 8/15: [========                      ] 13/47 batches, loss: 0.0707Epoch 8/15: [========                      ] 14/47 batches, loss: 0.0701Epoch 8/15: [=========                     ] 15/47 batches, loss: 0.0706Epoch 8/15: [==========                    ] 16/47 batches, loss: 0.0689Epoch 8/15: [==========                    ] 17/47 batches, loss: 0.0681Epoch 8/15: [===========                   ] 18/47 batches, loss: 0.0679Epoch 8/15: [============                  ] 19/47 batches, loss: 0.0689Epoch 8/15: [============                  ] 20/47 batches, loss: 0.0683Epoch 8/15: [=============                 ] 21/47 batches, loss: 0.0692Epoch 8/15: [==============                ] 22/47 batches, loss: 0.0685Epoch 8/15: [==============                ] 23/47 batches, loss: 0.0683Epoch 8/15: [===============               ] 24/47 batches, loss: 0.0697Epoch 8/15: [===============               ] 25/47 batches, loss: 0.0681Epoch 8/15: [================              ] 26/47 batches, loss: 0.0670Epoch 8/15: [=================             ] 27/47 batches, loss: 0.0671Epoch 8/15: [=================             ] 28/47 batches, loss: 0.0693Epoch 8/15: [==================            ] 29/47 batches, loss: 0.0688Epoch 8/15: [===================           ] 30/47 batches, loss: 0.0711Epoch 8/15: [===================           ] 31/47 batches, loss: 0.0725Epoch 8/15: [====================          ] 32/47 batches, loss: 0.0720Epoch 8/15: [=====================         ] 33/47 batches, loss: 0.0724Epoch 8/15: [=====================         ] 34/47 batches, loss: 0.0716Epoch 8/15: [======================        ] 35/47 batches, loss: 0.0716Epoch 8/15: [======================        ] 36/47 batches, loss: 0.0713Epoch 8/15: [=======================       ] 37/47 batches, loss: 0.0705Epoch 8/15: [========================      ] 38/47 batches, loss: 0.0704Epoch 8/15: [========================      ] 39/47 batches, loss: 0.0702Epoch 8/15: [=========================     ] 40/47 batches, loss: 0.0692Epoch 8/15: [==========================    ] 41/47 batches, loss: 0.0694Epoch 8/15: [==========================    ] 42/47 batches, loss: 0.0685Epoch 8/15: [===========================   ] 43/47 batches, loss: 0.0687Epoch 8/15: [============================  ] 44/47 batches, loss: 0.0685Epoch 8/15: [============================  ] 45/47 batches, loss: 0.0688Epoch 8/15: [============================= ] 46/47 batches, loss: 0.0685Epoch 8/15: [==============================] 47/47 batches, loss: 0.0678
[2025-05-07 22:06:12,006][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0678
[2025-05-07 22:06:12,321][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0233, Metrics: {'mse': 0.02337515354156494, 'rmse': 0.15288935064799294, 'r2': 0.048571109771728516}
Epoch 9/15: [Epoch 9/15: [                              ] 1/47 batches, loss: 0.0984Epoch 9/15: [=                             ] 2/47 batches, loss: 0.0861Epoch 9/15: [=                             ] 3/47 batches, loss: 0.0776Epoch 9/15: [==                            ] 4/47 batches, loss: 0.0828Epoch 9/15: [===                           ] 5/47 batches, loss: 0.0731Epoch 9/15: [===                           ] 6/47 batches, loss: 0.0739Epoch 9/15: [====                          ] 7/47 batches, loss: 0.0742Epoch 9/15: [=====                         ] 8/47 batches, loss: 0.0801Epoch 9/15: [=====                         ] 9/47 batches, loss: 0.0801Epoch 9/15: [======                        ] 10/47 batches, loss: 0.0760Epoch 9/15: [=======                       ] 11/47 batches, loss: 0.0786Epoch 9/15: [=======                       ] 12/47 batches, loss: 0.0766Epoch 9/15: [========                      ] 13/47 batches, loss: 0.0730Epoch 9/15: [========                      ] 14/47 batches, loss: 0.0743Epoch 9/15: [=========                     ] 15/47 batches, loss: 0.0726Epoch 9/15: [==========                    ] 16/47 batches, loss: 0.0713Epoch 9/15: [==========                    ] 17/47 batches, loss: 0.0700Epoch 9/15: [===========                   ] 18/47 batches, loss: 0.0680Epoch 9/15: [============                  ] 19/47 batches, loss: 0.0668Epoch 9/15: [============                  ] 20/47 batches, loss: 0.0676Epoch 9/15: [=============                 ] 21/47 batches, loss: 0.0667Epoch 9/15: [==============                ] 22/47 batches, loss: 0.0662Epoch 9/15: [==============                ] 23/47 batches, loss: 0.0660Epoch 9/15: [===============               ] 24/47 batches, loss: 0.0658Epoch 9/15: [===============               ] 25/47 batches, loss: 0.0680Epoch 9/15: [================              ] 26/47 batches, loss: 0.0705Epoch 9/15: [=================             ] 27/47 batches, loss: 0.0710Epoch 9/15: [=================             ] 28/47 batches, loss: 0.0705Epoch 9/15: [==================            ] 29/47 batches, loss: 0.0700Epoch 9/15: [===================           ] 30/47 batches, loss: 0.0697Epoch 9/15: [===================           ] 31/47 batches, loss: 0.0700Epoch 9/15: [====================          ] 32/47 batches, loss: 0.0689Epoch 9/15: [=====================         ] 33/47 batches, loss: 0.0679Epoch 9/15: [=====================         ] 34/47 batches, loss: 0.0677Epoch 9/15: [======================        ] 35/47 batches, loss: 0.0673Epoch 9/15: [======================        ] 36/47 batches, loss: 0.0678Epoch 9/15: [=======================       ] 37/47 batches, loss: 0.0695Epoch 9/15: [========================      ] 38/47 batches, loss: 0.0689Epoch 9/15: [========================      ] 39/47 batches, loss: 0.0686Epoch 9/15: [=========================     ] 40/47 batches, loss: 0.0694Epoch 9/15: [==========================    ] 41/47 batches, loss: 0.0697Epoch 9/15: [==========================    ] 42/47 batches, loss: 0.0698Epoch 9/15: [===========================   ] 43/47 batches, loss: 0.0695Epoch 9/15: [============================  ] 44/47 batches, loss: 0.0690Epoch 9/15: [============================  ] 45/47 batches, loss: 0.0696Epoch 9/15: [============================= ] 46/47 batches, loss: 0.0701Epoch 9/15: [==============================] 47/47 batches, loss: 0.0707
[2025-05-07 22:06:14,240][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0707
[2025-05-07 22:06:14,513][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0235, Metrics: {'mse': 0.023530688136816025, 'rmse': 0.15339715817711885, 'r2': 0.0422404408454895}
[2025-05-07 22:06:14,514][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/47 batches, loss: 0.0774Epoch 10/15: [=                             ] 2/47 batches, loss: 0.0930Epoch 10/15: [=                             ] 3/47 batches, loss: 0.0734Epoch 10/15: [==                            ] 4/47 batches, loss: 0.0823Epoch 10/15: [===                           ] 5/47 batches, loss: 0.0771Epoch 10/15: [===                           ] 6/47 batches, loss: 0.0748Epoch 10/15: [====                          ] 7/47 batches, loss: 0.0748Epoch 10/15: [=====                         ] 8/47 batches, loss: 0.0715Epoch 10/15: [=====                         ] 9/47 batches, loss: 0.0720Epoch 10/15: [======                        ] 10/47 batches, loss: 0.0713Epoch 10/15: [=======                       ] 11/47 batches, loss: 0.0730Epoch 10/15: [=======                       ] 12/47 batches, loss: 0.0711Epoch 10/15: [========                      ] 13/47 batches, loss: 0.0691Epoch 10/15: [========                      ] 14/47 batches, loss: 0.0709Epoch 10/15: [=========                     ] 15/47 batches, loss: 0.0695Epoch 10/15: [==========                    ] 16/47 batches, loss: 0.0709Epoch 10/15: [==========                    ] 17/47 batches, loss: 0.0693Epoch 10/15: [===========                   ] 18/47 batches, loss: 0.0702Epoch 10/15: [============                  ] 19/47 batches, loss: 0.0706Epoch 10/15: [============                  ] 20/47 batches, loss: 0.0704Epoch 10/15: [=============                 ] 21/47 batches, loss: 0.0693Epoch 10/15: [==============                ] 22/47 batches, loss: 0.0679Epoch 10/15: [==============                ] 23/47 batches, loss: 0.0668Epoch 10/15: [===============               ] 24/47 batches, loss: 0.0691Epoch 10/15: [===============               ] 25/47 batches, loss: 0.0690Epoch 10/15: [================              ] 26/47 batches, loss: 0.0680Epoch 10/15: [=================             ] 27/47 batches, loss: 0.0674Epoch 10/15: [=================             ] 28/47 batches, loss: 0.0678Epoch 10/15: [==================            ] 29/47 batches, loss: 0.0673Epoch 10/15: [===================           ] 30/47 batches, loss: 0.0670Epoch 10/15: [===================           ] 31/47 batches, loss: 0.0662Epoch 10/15: [====================          ] 32/47 batches, loss: 0.0660Epoch 10/15: [=====================         ] 33/47 batches, loss: 0.0658Epoch 10/15: [=====================         ] 34/47 batches, loss: 0.0664Epoch 10/15: [======================        ] 35/47 batches, loss: 0.0663Epoch 10/15: [======================        ] 36/47 batches, loss: 0.0654Epoch 10/15: [=======================       ] 37/47 batches, loss: 0.0663Epoch 10/15: [========================      ] 38/47 batches, loss: 0.0660Epoch 10/15: [========================      ] 39/47 batches, loss: 0.0661Epoch 10/15: [=========================     ] 40/47 batches, loss: 0.0669Epoch 10/15: [==========================    ] 41/47 batches, loss: 0.0665Epoch 10/15: [==========================    ] 42/47 batches, loss: 0.0663Epoch 10/15: [===========================   ] 43/47 batches, loss: 0.0664Epoch 10/15: [============================  ] 44/47 batches, loss: 0.0659Epoch 10/15: [============================  ] 45/47 batches, loss: 0.0666Epoch 10/15: [============================= ] 46/47 batches, loss: 0.0662Epoch 10/15: [==============================] 47/47 batches, loss: 0.0665
[2025-05-07 22:06:16,000][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0665
[2025-05-07 22:06:16,280][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0235, Metrics: {'mse': 0.02352258935570717, 'rmse': 0.15337075782464912, 'r2': 0.04257011413574219}
[2025-05-07 22:06:16,281][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/47 batches, loss: 0.0542Epoch 11/15: [=                             ] 2/47 batches, loss: 0.0596Epoch 11/15: [=                             ] 3/47 batches, loss: 0.0648Epoch 11/15: [==                            ] 4/47 batches, loss: 0.0791Epoch 11/15: [===                           ] 5/47 batches, loss: 0.0689Epoch 11/15: [===                           ] 6/47 batches, loss: 0.0747Epoch 11/15: [====                          ] 7/47 batches, loss: 0.0711Epoch 11/15: [=====                         ] 8/47 batches, loss: 0.0679Epoch 11/15: [=====                         ] 9/47 batches, loss: 0.0663Epoch 11/15: [======                        ] 10/47 batches, loss: 0.0663Epoch 11/15: [=======                       ] 11/47 batches, loss: 0.0654Epoch 11/15: [=======                       ] 12/47 batches, loss: 0.0648Epoch 11/15: [========                      ] 13/47 batches, loss: 0.0667Epoch 11/15: [========                      ] 14/47 batches, loss: 0.0641Epoch 11/15: [=========                     ] 15/47 batches, loss: 0.0671Epoch 11/15: [==========                    ] 16/47 batches, loss: 0.0713Epoch 11/15: [==========                    ] 17/47 batches, loss: 0.0703Epoch 11/15: [===========                   ] 18/47 batches, loss: 0.0702Epoch 11/15: [============                  ] 19/47 batches, loss: 0.0702Epoch 11/15: [============                  ] 20/47 batches, loss: 0.0682Epoch 11/15: [=============                 ] 21/47 batches, loss: 0.0682Epoch 11/15: [==============                ] 22/47 batches, loss: 0.0682Epoch 11/15: [==============                ] 23/47 batches, loss: 0.0674Epoch 11/15: [===============               ] 24/47 batches, loss: 0.0669Epoch 11/15: [===============               ] 25/47 batches, loss: 0.0653Epoch 11/15: [================              ] 26/47 batches, loss: 0.0670Epoch 11/15: [=================             ] 27/47 batches, loss: 0.0679Epoch 11/15: [=================             ] 28/47 batches, loss: 0.0670Epoch 11/15: [==================            ] 29/47 batches, loss: 0.0673Epoch 11/15: [===================           ] 30/47 batches, loss: 0.0661Epoch 11/15: [===================           ] 31/47 batches, loss: 0.0653Epoch 11/15: [====================          ] 32/47 batches, loss: 0.0648Epoch 11/15: [=====================         ] 33/47 batches, loss: 0.0647Epoch 11/15: [=====================         ] 34/47 batches, loss: 0.0642Epoch 11/15: [======================        ] 35/47 batches, loss: 0.0650Epoch 11/15: [======================        ] 36/47 batches, loss: 0.0653Epoch 11/15: [=======================       ] 37/47 batches, loss: 0.0652Epoch 11/15: [========================      ] 38/47 batches, loss: 0.0649Epoch 11/15: [========================      ] 39/47 batches, loss: 0.0651Epoch 11/15: [=========================     ] 40/47 batches, loss: 0.0649Epoch 11/15: [==========================    ] 41/47 batches, loss: 0.0646Epoch 11/15: [==========================    ] 42/47 batches, loss: 0.0648Epoch 11/15: [===========================   ] 43/47 batches, loss: 0.0640Epoch 11/15: [============================  ] 44/47 batches, loss: 0.0640Epoch 11/15: [============================  ] 45/47 batches, loss: 0.0640Epoch 11/15: [============================= ] 46/47 batches, loss: 0.0640Epoch 11/15: [==============================] 47/47 batches, loss: 0.0645
[2025-05-07 22:06:17,779][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0645
[2025-05-07 22:06:18,078][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0235, Metrics: {'mse': 0.02345690317451954, 'rmse': 0.15315646631637705, 'r2': 0.045243680477142334}
[2025-05-07 22:06:18,079][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/47 batches, loss: 0.0446Epoch 12/15: [=                             ] 2/47 batches, loss: 0.0407Epoch 12/15: [=                             ] 3/47 batches, loss: 0.0650Epoch 12/15: [==                            ] 4/47 batches, loss: 0.0624Epoch 12/15: [===                           ] 5/47 batches, loss: 0.0542Epoch 12/15: [===                           ] 6/47 batches, loss: 0.0547Epoch 12/15: [====                          ] 7/47 batches, loss: 0.0529Epoch 12/15: [=====                         ] 8/47 batches, loss: 0.0585Epoch 12/15: [=====                         ] 9/47 batches, loss: 0.0557Epoch 12/15: [======                        ] 10/47 batches, loss: 0.0550Epoch 12/15: [=======                       ] 11/47 batches, loss: 0.0558Epoch 12/15: [=======                       ] 12/47 batches, loss: 0.0541Epoch 12/15: [========                      ] 13/47 batches, loss: 0.0585Epoch 12/15: [========                      ] 14/47 batches, loss: 0.0574Epoch 12/15: [=========                     ] 15/47 batches, loss: 0.0591Epoch 12/15: [==========                    ] 16/47 batches, loss: 0.0601Epoch 12/15: [==========                    ] 17/47 batches, loss: 0.0608Epoch 12/15: [===========                   ] 18/47 batches, loss: 0.0607Epoch 12/15: [============                  ] 19/47 batches, loss: 0.0607Epoch 12/15: [============                  ] 20/47 batches, loss: 0.0615Epoch 12/15: [=============                 ] 21/47 batches, loss: 0.0625Epoch 12/15: [==============                ] 22/47 batches, loss: 0.0617Epoch 12/15: [==============                ] 23/47 batches, loss: 0.0612Epoch 12/15: [===============               ] 24/47 batches, loss: 0.0608Epoch 12/15: [===============               ] 25/47 batches, loss: 0.0609Epoch 12/15: [================              ] 26/47 batches, loss: 0.0599Epoch 12/15: [=================             ] 27/47 batches, loss: 0.0596Epoch 12/15: [=================             ] 28/47 batches, loss: 0.0600Epoch 12/15: [==================            ] 29/47 batches, loss: 0.0595Epoch 12/15: [===================           ] 30/47 batches, loss: 0.0613Epoch 12/15: [===================           ] 31/47 batches, loss: 0.0610Epoch 12/15: [====================          ] 32/47 batches, loss: 0.0608Epoch 12/15: [=====================         ] 33/47 batches, loss: 0.0600Epoch 12/15: [=====================         ] 34/47 batches, loss: 0.0593Epoch 12/15: [======================        ] 35/47 batches, loss: 0.0596Epoch 12/15: [======================        ] 36/47 batches, loss: 0.0595Epoch 12/15: [=======================       ] 37/47 batches, loss: 0.0596Epoch 12/15: [========================      ] 38/47 batches, loss: 0.0590Epoch 12/15: [========================      ] 39/47 batches, loss: 0.0580Epoch 12/15: [=========================     ] 40/47 batches, loss: 0.0577Epoch 12/15: [==========================    ] 41/47 batches, loss: 0.0573Epoch 12/15: [==========================    ] 42/47 batches, loss: 0.0572Epoch 12/15: [===========================   ] 43/47 batches, loss: 0.0570Epoch 12/15: [============================  ] 44/47 batches, loss: 0.0568Epoch 12/15: [============================  ] 45/47 batches, loss: 0.0575Epoch 12/15: [============================= ] 46/47 batches, loss: 0.0581Epoch 12/15: [==============================] 47/47 batches, loss: 0.0572
[2025-05-07 22:06:19,577][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0572
[2025-05-07 22:06:19,850][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0259, Metrics: {'mse': 0.02595788985490799, 'rmse': 0.16111452403463813, 'r2': -0.056552886962890625}
[2025-05-07 22:06:19,850][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:06:19,850][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 12
[2025-05-07 22:06:19,850][src.training.lm_trainer][INFO] - Training completed in 25.20 seconds
[2025-05-07 22:06:19,851][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:06:22,071][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.036267273128032684, 'rmse': 0.1904396837007263, 'r2': 0.016528546810150146}
[2025-05-07 22:06:22,072][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.02337515354156494, 'rmse': 0.15288935064799294, 'r2': 0.048571109771728516}
[2025-05-07 22:06:22,072][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.035098135471343994, 'rmse': 0.18734496382701082, 'r2': -0.011643648147583008}
[2025-05-07 22:06:23,795][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/layer2/ko/ko/model.pt
[2025-05-07 22:06:23,796][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▃▂▂▂▁▁
wandb:     best_val_mse █▃▃▂▂▂▁▁
wandb:      best_val_r2 ▁▆▆▇▇▇██
wandb:    best_val_rmse █▄▃▂▂▂▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▄▄▄▄▄▅▅▅▅▅
wandb:       train_loss █▄▃▃▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▃▂▂▂▁▁▁▁▁▃
wandb:          val_mse █▃▃▂▂▂▁▁▁▁▁▂
wandb:           val_r2 ▁▆▆▇▇▇█████▇
wandb:         val_rmse █▄▃▂▂▂▁▁▁▁▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02328
wandb:     best_val_mse 0.02338
wandb:      best_val_r2 0.04857
wandb:    best_val_rmse 0.15289
wandb: early_stop_epoch 12
wandb:            epoch 12
wandb:   final_test_mse 0.0351
wandb:    final_test_r2 -0.01164
wandb:  final_test_rmse 0.18734
wandb:  final_train_mse 0.03627
wandb:   final_train_r2 0.01653
wandb: final_train_rmse 0.19044
wandb:    final_val_mse 0.02338
wandb:     final_val_r2 0.04857
wandb:   final_val_rmse 0.15289
wandb:    learning_rate 0.0001
wandb:       train_loss 0.05719
wandb:       train_time 25.19903
wandb:         val_loss 0.02591
wandb:          val_mse 0.02596
wandb:           val_r2 -0.05655
wandb:         val_rmse 0.16111
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_220538-efg75nsl
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_220538-efg75nsl/logs
Experiment probe_layer2_avg_verb_edges_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_lexical_density_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_lexical_density_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/layer2/ko"         "wandb.mode=offline" "experiment.submetric=lexical_density"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:06:53,249][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/layer2/ko
experiment_name: probe_layer2_lexical_density_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: lexical_density
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:06:53,250][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:06:53,250][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 22:06:53,250][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:06:53,250][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:06:53,254][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:06:53,254][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 22:06:53,254][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:06:57,048][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'lexical_density'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:06:59,535][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:06:59,536][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:06:59,811][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:06:59,901][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:07:00,200][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:07:00,207][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:07:00,207][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:07:00,222][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:07:00,293][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:07:00,414][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:07:00,454][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:07:00,455][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:07:00,455][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:07:00,458][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:07:00,525][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:07:00,604][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:07:00,623][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:07:00,625][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:07:00,625][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:07:00,628][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:07:00,628][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:07:00,629][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 22:07:00,629][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 22:07:00,629][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 22:07:00,629][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:07:00,629][src.data.datasets][INFO] -   Mean: 0.8900, Std: 0.1851
[2025-05-07 22:07:00,629][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:07:00,629][src.data.datasets][INFO] - Sample label: 1.0
[2025-05-07 22:07:00,629][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:07:00,630][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 22:07:00,630][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 22:07:00,630][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 22:07:00,630][src.data.datasets][INFO] -   Min: 0.2860, Max: 1.0000
[2025-05-07 22:07:00,630][src.data.datasets][INFO] -   Mean: 0.8166, Std: 0.2038
[2025-05-07 22:07:00,630][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:07:00,630][src.data.datasets][INFO] - Sample label: 0.6669999957084656
[2025-05-07 22:07:00,630][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:07:00,630][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 22:07:00,630][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 22:07:00,631][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 22:07:00,631][src.data.datasets][INFO] -   Min: 0.1000, Max: 1.0000
[2025-05-07 22:07:00,631][src.data.datasets][INFO] -   Mean: 0.6309, Std: 0.1999
[2025-05-07 22:07:00,631][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:07:00,631][src.data.datasets][INFO] - Sample label: 0.8640000224113464
[2025-05-07 22:07:00,631][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:07:00,631][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:07:00,631][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:07:00,632][__main__][INFO] - Using model type: lm_probe for submetric lexical_density
[2025-05-07 22:07:00,632][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:07:07,731][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:07:07,732][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:07:07,733][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:07:07,733][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:07:07,735][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:07:07,736][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:07:07,736][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:07:07,736][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:07:07,736][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:07:07,737][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:07:07,737][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.6334Epoch 1/15: [=                             ] 2/47 batches, loss: 0.7427Epoch 1/15: [=                             ] 3/47 batches, loss: 0.6596Epoch 1/15: [==                            ] 4/47 batches, loss: 0.6009Epoch 1/15: [===                           ] 5/47 batches, loss: 0.5684Epoch 1/15: [===                           ] 6/47 batches, loss: 0.5001Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4655Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4715Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4838Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4727Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4530Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4457Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4295Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4345Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.4266Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4296Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.4201Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4343Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4230Epoch 1/15: [============                  ] 20/47 batches, loss: 0.4140Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4204Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4162Epoch 1/15: [==============                ] 23/47 batches, loss: 0.4042Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3989Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3955Epoch 1/15: [================              ] 26/47 batches, loss: 0.3920Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3902Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3803Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3820Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3772Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3694Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3624Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3631Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3621Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3599Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3618Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3592Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3578Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3532Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3535Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3511Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3468Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3449Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3426Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3404Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3357Epoch 1/15: [==============================] 47/47 batches, loss: 0.3389
[2025-05-07 22:07:13,131][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3389
[2025-05-07 22:07:13,403][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0539, Metrics: {'mse': 0.0490366630256176, 'rmse': 0.22144223406030206, 'r2': -0.18005383014678955}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.3539Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2418Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2509Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2002Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2163Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2347Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2404Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2386Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2365Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2322Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2336Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2342Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2368Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2390Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2386Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2296Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2260Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2341Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2320Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2274Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2246Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2216Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2196Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2193Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2180Epoch 2/15: [================              ] 26/47 batches, loss: 0.2161Epoch 2/15: [=================             ] 27/47 batches, loss: 0.2115Epoch 2/15: [=================             ] 28/47 batches, loss: 0.2088Epoch 2/15: [==================            ] 29/47 batches, loss: 0.2057Epoch 2/15: [===================           ] 30/47 batches, loss: 0.2051Epoch 2/15: [===================           ] 31/47 batches, loss: 0.2033Epoch 2/15: [====================          ] 32/47 batches, loss: 0.2029Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.2042Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.2070Epoch 2/15: [======================        ] 35/47 batches, loss: 0.2026Epoch 2/15: [======================        ] 36/47 batches, loss: 0.2013Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.2033Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1995Epoch 2/15: [========================      ] 39/47 batches, loss: 0.2025Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.2020Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.2000Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1986Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1978Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1950Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1933Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1923Epoch 2/15: [==============================] 47/47 batches, loss: 0.1915
[2025-05-07 22:07:15,295][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1915
[2025-05-07 22:07:15,561][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0478, Metrics: {'mse': 0.04374344274401665, 'rmse': 0.20914933120623802, 'r2': -0.052673935890197754}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1677Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1236Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1506Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1342Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1294Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1414Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1429Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1414Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1438Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1364Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1340Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1354Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1371Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1328Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1400Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1385Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1406Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1421Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1439Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1427Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1432Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1409Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1421Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1433Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1411Epoch 3/15: [================              ] 26/47 batches, loss: 0.1398Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1404Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1384Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1369Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1381Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1394Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1384Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1368Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1361Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1371Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1365Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1365Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1349Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1328Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1336Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1328Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1327Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1319Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1343Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1346Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1328Epoch 3/15: [==============================] 47/47 batches, loss: 0.1359
[2025-05-07 22:07:17,425][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1359
[2025-05-07 22:07:17,674][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0425, Metrics: {'mse': 0.039458017796278, 'rmse': 0.19864042336915716, 'r2': 0.05045360326766968}
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.0985Epoch 4/15: [=                             ] 2/47 batches, loss: 0.0917Epoch 4/15: [=                             ] 3/47 batches, loss: 0.0832Epoch 4/15: [==                            ] 4/47 batches, loss: 0.0886Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1030Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1158Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1136Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1201Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1163Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1147Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1187Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1178Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1180Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1147Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1166Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1134Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1146Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1116Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1137Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1180Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1177Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1172Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1180Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1232Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1240Epoch 4/15: [================              ] 26/47 batches, loss: 0.1233Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1232Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1265Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1265Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1256Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1293Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1281Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1253Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1246Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1259Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1248Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1278Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1268Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1266Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1253Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1254Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1242Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1233Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1229Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1214Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1211Epoch 4/15: [==============================] 47/47 batches, loss: 0.1202
[2025-05-07 22:07:19,509][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1202
[2025-05-07 22:07:19,755][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0420, Metrics: {'mse': 0.03919485583901405, 'rmse': 0.19797690733773485, 'r2': 0.056786537170410156}
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.1096Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1126Epoch 5/15: [=                             ] 3/47 batches, loss: 0.1010Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0898Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0812Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0846Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0947Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0959Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0960Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0904Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0916Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0951Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0941Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0954Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0930Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0974Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0949Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0949Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0974Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0955Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0979Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0987Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0984Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0997Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0979Epoch 5/15: [================              ] 26/47 batches, loss: 0.0960Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0969Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0956Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0960Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0978Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0991Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0995Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0986Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0974Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0984Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0975Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0965Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0972Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0998Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.1002Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.1005Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0993Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0996Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0997Epoch 5/15: [============================  ] 45/47 batches, loss: 0.1009Epoch 5/15: [============================= ] 46/47 batches, loss: 0.1014Epoch 5/15: [==============================] 47/47 batches, loss: 0.1012
[2025-05-07 22:07:21,584][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1012
[2025-05-07 22:07:21,853][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0413, Metrics: {'mse': 0.03898514062166214, 'rmse': 0.19744655130354175, 'r2': 0.06183326244354248}
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.1133Epoch 6/15: [=                             ] 2/47 batches, loss: 0.0962Epoch 6/15: [=                             ] 3/47 batches, loss: 0.1222Epoch 6/15: [==                            ] 4/47 batches, loss: 0.1084Epoch 6/15: [===                           ] 5/47 batches, loss: 0.1079Epoch 6/15: [===                           ] 6/47 batches, loss: 0.1044Epoch 6/15: [====                          ] 7/47 batches, loss: 0.0972Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.0985Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.0959Epoch 6/15: [======                        ] 10/47 batches, loss: 0.0977Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.1019Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.1027Epoch 6/15: [========                      ] 13/47 batches, loss: 0.0992Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0960Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0925Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.0936Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.0942Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.0926Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0896Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0884Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0859Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0856Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0838Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0832Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0831Epoch 6/15: [================              ] 26/47 batches, loss: 0.0822Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0809Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0791Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0807Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0810Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0801Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0801Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0817Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0817Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0815Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0811Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0805Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0793Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0790Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0788Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0793Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0803Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0803Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0829Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0823Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0826Epoch 6/15: [==============================] 47/47 batches, loss: 0.0821
[2025-05-07 22:07:23,704][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0821
[2025-05-07 22:07:24,057][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0416, Metrics: {'mse': 0.03949742019176483, 'rmse': 0.1987395788255697, 'r2': 0.04950535297393799}
[2025-05-07 22:07:24,057][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/47 batches, loss: 0.0908Epoch 7/15: [=                             ] 2/47 batches, loss: 0.0938Epoch 7/15: [=                             ] 3/47 batches, loss: 0.0871Epoch 7/15: [==                            ] 4/47 batches, loss: 0.0775Epoch 7/15: [===                           ] 5/47 batches, loss: 0.0827Epoch 7/15: [===                           ] 6/47 batches, loss: 0.0822Epoch 7/15: [====                          ] 7/47 batches, loss: 0.0820Epoch 7/15: [=====                         ] 8/47 batches, loss: 0.0823Epoch 7/15: [=====                         ] 9/47 batches, loss: 0.0803Epoch 7/15: [======                        ] 10/47 batches, loss: 0.0812Epoch 7/15: [=======                       ] 11/47 batches, loss: 0.0787Epoch 7/15: [=======                       ] 12/47 batches, loss: 0.0806Epoch 7/15: [========                      ] 13/47 batches, loss: 0.0809Epoch 7/15: [========                      ] 14/47 batches, loss: 0.0774Epoch 7/15: [=========                     ] 15/47 batches, loss: 0.0781Epoch 7/15: [==========                    ] 16/47 batches, loss: 0.0802Epoch 7/15: [==========                    ] 17/47 batches, loss: 0.0798Epoch 7/15: [===========                   ] 18/47 batches, loss: 0.0803Epoch 7/15: [============                  ] 19/47 batches, loss: 0.0823Epoch 7/15: [============                  ] 20/47 batches, loss: 0.0806Epoch 7/15: [=============                 ] 21/47 batches, loss: 0.0801Epoch 7/15: [==============                ] 22/47 batches, loss: 0.0801Epoch 7/15: [==============                ] 23/47 batches, loss: 0.0820Epoch 7/15: [===============               ] 24/47 batches, loss: 0.0803Epoch 7/15: [===============               ] 25/47 batches, loss: 0.0833Epoch 7/15: [================              ] 26/47 batches, loss: 0.0834Epoch 7/15: [=================             ] 27/47 batches, loss: 0.0831Epoch 7/15: [=================             ] 28/47 batches, loss: 0.0833Epoch 7/15: [==================            ] 29/47 batches, loss: 0.0841Epoch 7/15: [===================           ] 30/47 batches, loss: 0.0847Epoch 7/15: [===================           ] 31/47 batches, loss: 0.0847Epoch 7/15: [====================          ] 32/47 batches, loss: 0.0852Epoch 7/15: [=====================         ] 33/47 batches, loss: 0.0852Epoch 7/15: [=====================         ] 34/47 batches, loss: 0.0854Epoch 7/15: [======================        ] 35/47 batches, loss: 0.0848Epoch 7/15: [======================        ] 36/47 batches, loss: 0.0861Epoch 7/15: [=======================       ] 37/47 batches, loss: 0.0857Epoch 7/15: [========================      ] 38/47 batches, loss: 0.0851Epoch 7/15: [========================      ] 39/47 batches, loss: 0.0845Epoch 7/15: [=========================     ] 40/47 batches, loss: 0.0839Epoch 7/15: [==========================    ] 41/47 batches, loss: 0.0837Epoch 7/15: [==========================    ] 42/47 batches, loss: 0.0829Epoch 7/15: [===========================   ] 43/47 batches, loss: 0.0830Epoch 7/15: [============================  ] 44/47 batches, loss: 0.0830Epoch 7/15: [============================  ] 45/47 batches, loss: 0.0829Epoch 7/15: [============================= ] 46/47 batches, loss: 0.0819Epoch 7/15: [==============================] 47/47 batches, loss: 0.0839
[2025-05-07 22:07:25,537][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0839
[2025-05-07 22:07:25,801][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0400, Metrics: {'mse': 0.03772834315896034, 'rmse': 0.19423785202416222, 'r2': 0.09207773208618164}
Epoch 8/15: [Epoch 8/15: [                              ] 1/47 batches, loss: 0.0474Epoch 8/15: [=                             ] 2/47 batches, loss: 0.0609Epoch 8/15: [=                             ] 3/47 batches, loss: 0.0674Epoch 8/15: [==                            ] 4/47 batches, loss: 0.0699Epoch 8/15: [===                           ] 5/47 batches, loss: 0.0792Epoch 8/15: [===                           ] 6/47 batches, loss: 0.0722Epoch 8/15: [====                          ] 7/47 batches, loss: 0.0771Epoch 8/15: [=====                         ] 8/47 batches, loss: 0.0745Epoch 8/15: [=====                         ] 9/47 batches, loss: 0.0707Epoch 8/15: [======                        ] 10/47 batches, loss: 0.0685Epoch 8/15: [=======                       ] 11/47 batches, loss: 0.0697Epoch 8/15: [=======                       ] 12/47 batches, loss: 0.0717Epoch 8/15: [========                      ] 13/47 batches, loss: 0.0705Epoch 8/15: [========                      ] 14/47 batches, loss: 0.0684Epoch 8/15: [=========                     ] 15/47 batches, loss: 0.0705Epoch 8/15: [==========                    ] 16/47 batches, loss: 0.0690Epoch 8/15: [==========                    ] 17/47 batches, loss: 0.0714Epoch 8/15: [===========                   ] 18/47 batches, loss: 0.0700Epoch 8/15: [============                  ] 19/47 batches, loss: 0.0707Epoch 8/15: [============                  ] 20/47 batches, loss: 0.0708Epoch 8/15: [=============                 ] 21/47 batches, loss: 0.0716Epoch 8/15: [==============                ] 22/47 batches, loss: 0.0700Epoch 8/15: [==============                ] 23/47 batches, loss: 0.0706Epoch 8/15: [===============               ] 24/47 batches, loss: 0.0712Epoch 8/15: [===============               ] 25/47 batches, loss: 0.0716Epoch 8/15: [================              ] 26/47 batches, loss: 0.0715Epoch 8/15: [=================             ] 27/47 batches, loss: 0.0712Epoch 8/15: [=================             ] 28/47 batches, loss: 0.0702Epoch 8/15: [==================            ] 29/47 batches, loss: 0.0690Epoch 8/15: [===================           ] 30/47 batches, loss: 0.0690Epoch 8/15: [===================           ] 31/47 batches, loss: 0.0689Epoch 8/15: [====================          ] 32/47 batches, loss: 0.0683Epoch 8/15: [=====================         ] 33/47 batches, loss: 0.0685Epoch 8/15: [=====================         ] 34/47 batches, loss: 0.0678Epoch 8/15: [======================        ] 35/47 batches, loss: 0.0672Epoch 8/15: [======================        ] 36/47 batches, loss: 0.0671Epoch 8/15: [=======================       ] 37/47 batches, loss: 0.0666Epoch 8/15: [========================      ] 38/47 batches, loss: 0.0671Epoch 8/15: [========================      ] 39/47 batches, loss: 0.0666Epoch 8/15: [=========================     ] 40/47 batches, loss: 0.0670Epoch 8/15: [==========================    ] 41/47 batches, loss: 0.0666Epoch 8/15: [==========================    ] 42/47 batches, loss: 0.0662Epoch 8/15: [===========================   ] 43/47 batches, loss: 0.0674Epoch 8/15: [============================  ] 44/47 batches, loss: 0.0677Epoch 8/15: [============================  ] 45/47 batches, loss: 0.0675Epoch 8/15: [============================= ] 46/47 batches, loss: 0.0679Epoch 8/15: [==============================] 47/47 batches, loss: 0.0667
[2025-05-07 22:07:27,625][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0667
[2025-05-07 22:07:27,909][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0405, Metrics: {'mse': 0.03784093260765076, 'rmse': 0.1945274597779212, 'r2': 0.08936828374862671}
[2025-05-07 22:07:27,909][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/47 batches, loss: 0.1260Epoch 9/15: [=                             ] 2/47 batches, loss: 0.1003Epoch 9/15: [=                             ] 3/47 batches, loss: 0.0808Epoch 9/15: [==                            ] 4/47 batches, loss: 0.0854Epoch 9/15: [===                           ] 5/47 batches, loss: 0.0762Epoch 9/15: [===                           ] 6/47 batches, loss: 0.0807Epoch 9/15: [====                          ] 7/47 batches, loss: 0.0785Epoch 9/15: [=====                         ] 8/47 batches, loss: 0.0797Epoch 9/15: [=====                         ] 9/47 batches, loss: 0.0840Epoch 9/15: [======                        ] 10/47 batches, loss: 0.0807Epoch 9/15: [=======                       ] 11/47 batches, loss: 0.0797Epoch 9/15: [=======                       ] 12/47 batches, loss: 0.0802Epoch 9/15: [========                      ] 13/47 batches, loss: 0.0787Epoch 9/15: [========                      ] 14/47 batches, loss: 0.0779Epoch 9/15: [=========                     ] 15/47 batches, loss: 0.0774Epoch 9/15: [==========                    ] 16/47 batches, loss: 0.0776Epoch 9/15: [==========                    ] 17/47 batches, loss: 0.0782Epoch 9/15: [===========                   ] 18/47 batches, loss: 0.0756Epoch 9/15: [============                  ] 19/47 batches, loss: 0.0748Epoch 9/15: [============                  ] 20/47 batches, loss: 0.0755Epoch 9/15: [=============                 ] 21/47 batches, loss: 0.0778Epoch 9/15: [==============                ] 22/47 batches, loss: 0.0756Epoch 9/15: [==============                ] 23/47 batches, loss: 0.0737Epoch 9/15: [===============               ] 24/47 batches, loss: 0.0745Epoch 9/15: [===============               ] 25/47 batches, loss: 0.0743Epoch 9/15: [================              ] 26/47 batches, loss: 0.0752Epoch 9/15: [=================             ] 27/47 batches, loss: 0.0761Epoch 9/15: [=================             ] 28/47 batches, loss: 0.0761Epoch 9/15: [==================            ] 29/47 batches, loss: 0.0759Epoch 9/15: [===================           ] 30/47 batches, loss: 0.0755Epoch 9/15: [===================           ] 31/47 batches, loss: 0.0739Epoch 9/15: [====================          ] 32/47 batches, loss: 0.0742Epoch 9/15: [=====================         ] 33/47 batches, loss: 0.0745Epoch 9/15: [=====================         ] 34/47 batches, loss: 0.0744Epoch 9/15: [======================        ] 35/47 batches, loss: 0.0736Epoch 9/15: [======================        ] 36/47 batches, loss: 0.0724Epoch 9/15: [=======================       ] 37/47 batches, loss: 0.0737Epoch 9/15: [========================      ] 38/47 batches, loss: 0.0729Epoch 9/15: [========================      ] 39/47 batches, loss: 0.0726Epoch 9/15: [=========================     ] 40/47 batches, loss: 0.0720Epoch 9/15: [==========================    ] 41/47 batches, loss: 0.0712Epoch 9/15: [==========================    ] 42/47 batches, loss: 0.0708Epoch 9/15: [===========================   ] 43/47 batches, loss: 0.0712Epoch 9/15: [============================  ] 44/47 batches, loss: 0.0713Epoch 9/15: [============================  ] 45/47 batches, loss: 0.0726Epoch 9/15: [============================= ] 46/47 batches, loss: 0.0721Epoch 9/15: [==============================] 47/47 batches, loss: 0.0723
[2025-05-07 22:07:29,382][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0723
[2025-05-07 22:07:29,633][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0397, Metrics: {'mse': 0.03718917816877365, 'rmse': 0.19284495888867215, 'r2': 0.10505259037017822}
Epoch 10/15: [Epoch 10/15: [                              ] 1/47 batches, loss: 0.0601Epoch 10/15: [=                             ] 2/47 batches, loss: 0.0727Epoch 10/15: [=                             ] 3/47 batches, loss: 0.0643Epoch 10/15: [==                            ] 4/47 batches, loss: 0.0707Epoch 10/15: [===                           ] 5/47 batches, loss: 0.0723Epoch 10/15: [===                           ] 6/47 batches, loss: 0.0693Epoch 10/15: [====                          ] 7/47 batches, loss: 0.0709Epoch 10/15: [=====                         ] 8/47 batches, loss: 0.0671Epoch 10/15: [=====                         ] 9/47 batches, loss: 0.0743Epoch 10/15: [======                        ] 10/47 batches, loss: 0.0725Epoch 10/15: [=======                       ] 11/47 batches, loss: 0.0759Epoch 10/15: [=======                       ] 12/47 batches, loss: 0.0754Epoch 10/15: [========                      ] 13/47 batches, loss: 0.0740Epoch 10/15: [========                      ] 14/47 batches, loss: 0.0716Epoch 10/15: [=========                     ] 15/47 batches, loss: 0.0704Epoch 10/15: [==========                    ] 16/47 batches, loss: 0.0704Epoch 10/15: [==========                    ] 17/47 batches, loss: 0.0689Epoch 10/15: [===========                   ] 18/47 batches, loss: 0.0679Epoch 10/15: [============                  ] 19/47 batches, loss: 0.0687Epoch 10/15: [============                  ] 20/47 batches, loss: 0.0679Epoch 10/15: [=============                 ] 21/47 batches, loss: 0.0664Epoch 10/15: [==============                ] 22/47 batches, loss: 0.0642Epoch 10/15: [==============                ] 23/47 batches, loss: 0.0641Epoch 10/15: [===============               ] 24/47 batches, loss: 0.0651Epoch 10/15: [===============               ] 25/47 batches, loss: 0.0652Epoch 10/15: [================              ] 26/47 batches, loss: 0.0642Epoch 10/15: [=================             ] 27/47 batches, loss: 0.0639Epoch 10/15: [=================             ] 28/47 batches, loss: 0.0636Epoch 10/15: [==================            ] 29/47 batches, loss: 0.0628Epoch 10/15: [===================           ] 30/47 batches, loss: 0.0618Epoch 10/15: [===================           ] 31/47 batches, loss: 0.0612Epoch 10/15: [====================          ] 32/47 batches, loss: 0.0613Epoch 10/15: [=====================         ] 33/47 batches, loss: 0.0608Epoch 10/15: [=====================         ] 34/47 batches, loss: 0.0610Epoch 10/15: [======================        ] 35/47 batches, loss: 0.0611Epoch 10/15: [======================        ] 36/47 batches, loss: 0.0612Epoch 10/15: [=======================       ] 37/47 batches, loss: 0.0612Epoch 10/15: [========================      ] 38/47 batches, loss: 0.0624Epoch 10/15: [========================      ] 39/47 batches, loss: 0.0633Epoch 10/15: [=========================     ] 40/47 batches, loss: 0.0636Epoch 10/15: [==========================    ] 41/47 batches, loss: 0.0642Epoch 10/15: [==========================    ] 42/47 batches, loss: 0.0642Epoch 10/15: [===========================   ] 43/47 batches, loss: 0.0638Epoch 10/15: [============================  ] 44/47 batches, loss: 0.0639Epoch 10/15: [============================  ] 45/47 batches, loss: 0.0639Epoch 10/15: [============================= ] 46/47 batches, loss: 0.0638Epoch 10/15: [==============================] 47/47 batches, loss: 0.0626
[2025-05-07 22:07:31,545][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0626
[2025-05-07 22:07:31,804][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0404, Metrics: {'mse': 0.03803384304046631, 'rmse': 0.19502267314460212, 'r2': 0.08472597599029541}
[2025-05-07 22:07:31,805][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/47 batches, loss: 0.0282Epoch 11/15: [=                             ] 2/47 batches, loss: 0.0326Epoch 11/15: [=                             ] 3/47 batches, loss: 0.0458Epoch 11/15: [==                            ] 4/47 batches, loss: 0.0524Epoch 11/15: [===                           ] 5/47 batches, loss: 0.0597Epoch 11/15: [===                           ] 6/47 batches, loss: 0.0566Epoch 11/15: [====                          ] 7/47 batches, loss: 0.0622Epoch 11/15: [=====                         ] 8/47 batches, loss: 0.0610Epoch 11/15: [=====                         ] 9/47 batches, loss: 0.0621Epoch 11/15: [======                        ] 10/47 batches, loss: 0.0641Epoch 11/15: [=======                       ] 11/47 batches, loss: 0.0612Epoch 11/15: [=======                       ] 12/47 batches, loss: 0.0608Epoch 11/15: [========                      ] 13/47 batches, loss: 0.0615Epoch 11/15: [========                      ] 14/47 batches, loss: 0.0592Epoch 11/15: [=========                     ] 15/47 batches, loss: 0.0574Epoch 11/15: [==========                    ] 16/47 batches, loss: 0.0570Epoch 11/15: [==========                    ] 17/47 batches, loss: 0.0584Epoch 11/15: [===========                   ] 18/47 batches, loss: 0.0578Epoch 11/15: [============                  ] 19/47 batches, loss: 0.0604Epoch 11/15: [============                  ] 20/47 batches, loss: 0.0615Epoch 11/15: [=============                 ] 21/47 batches, loss: 0.0613Epoch 11/15: [==============                ] 22/47 batches, loss: 0.0607Epoch 11/15: [==============                ] 23/47 batches, loss: 0.0612Epoch 11/15: [===============               ] 24/47 batches, loss: 0.0615Epoch 11/15: [===============               ] 25/47 batches, loss: 0.0600Epoch 11/15: [================              ] 26/47 batches, loss: 0.0594Epoch 11/15: [=================             ] 27/47 batches, loss: 0.0595Epoch 11/15: [=================             ] 28/47 batches, loss: 0.0599Epoch 11/15: [==================            ] 29/47 batches, loss: 0.0590Epoch 11/15: [===================           ] 30/47 batches, loss: 0.0577Epoch 11/15: [===================           ] 31/47 batches, loss: 0.0580Epoch 11/15: [====================          ] 32/47 batches, loss: 0.0571Epoch 11/15: [=====================         ] 33/47 batches, loss: 0.0571Epoch 11/15: [=====================         ] 34/47 batches, loss: 0.0576Epoch 11/15: [======================        ] 35/47 batches, loss: 0.0570Epoch 11/15: [======================        ] 36/47 batches, loss: 0.0569Epoch 11/15: [=======================       ] 37/47 batches, loss: 0.0562Epoch 11/15: [========================      ] 38/47 batches, loss: 0.0558Epoch 11/15: [========================      ] 39/47 batches, loss: 0.0560Epoch 11/15: [=========================     ] 40/47 batches, loss: 0.0561Epoch 11/15: [==========================    ] 41/47 batches, loss: 0.0563Epoch 11/15: [==========================    ] 42/47 batches, loss: 0.0568Epoch 11/15: [===========================   ] 43/47 batches, loss: 0.0566Epoch 11/15: [============================  ] 44/47 batches, loss: 0.0563Epoch 11/15: [============================  ] 45/47 batches, loss: 0.0563Epoch 11/15: [============================= ] 46/47 batches, loss: 0.0562Epoch 11/15: [==============================] 47/47 batches, loss: 0.0581
[2025-05-07 22:07:33,289][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0581
[2025-05-07 22:07:33,578][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0402, Metrics: {'mse': 0.03769752010703087, 'rmse': 0.19415849223516046, 'r2': 0.09281951189041138}
[2025-05-07 22:07:33,579][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/47 batches, loss: 0.0422Epoch 12/15: [=                             ] 2/47 batches, loss: 0.0471Epoch 12/15: [=                             ] 3/47 batches, loss: 0.0504Epoch 12/15: [==                            ] 4/47 batches, loss: 0.0492Epoch 12/15: [===                           ] 5/47 batches, loss: 0.0459Epoch 12/15: [===                           ] 6/47 batches, loss: 0.0485Epoch 12/15: [====                          ] 7/47 batches, loss: 0.0472Epoch 12/15: [=====                         ] 8/47 batches, loss: 0.0505Epoch 12/15: [=====                         ] 9/47 batches, loss: 0.0517Epoch 12/15: [======                        ] 10/47 batches, loss: 0.0498Epoch 12/15: [=======                       ] 11/47 batches, loss: 0.0525Epoch 12/15: [=======                       ] 12/47 batches, loss: 0.0518Epoch 12/15: [========                      ] 13/47 batches, loss: 0.0532Epoch 12/15: [========                      ] 14/47 batches, loss: 0.0564Epoch 12/15: [=========                     ] 15/47 batches, loss: 0.0577Epoch 12/15: [==========                    ] 16/47 batches, loss: 0.0597Epoch 12/15: [==========                    ] 17/47 batches, loss: 0.0595Epoch 12/15: [===========                   ] 18/47 batches, loss: 0.0580Epoch 12/15: [============                  ] 19/47 batches, loss: 0.0584Epoch 12/15: [============                  ] 20/47 batches, loss: 0.0585Epoch 12/15: [=============                 ] 21/47 batches, loss: 0.0588Epoch 12/15: [==============                ] 22/47 batches, loss: 0.0583Epoch 12/15: [==============                ] 23/47 batches, loss: 0.0579Epoch 12/15: [===============               ] 24/47 batches, loss: 0.0579Epoch 12/15: [===============               ] 25/47 batches, loss: 0.0571Epoch 12/15: [================              ] 26/47 batches, loss: 0.0559Epoch 12/15: [=================             ] 27/47 batches, loss: 0.0544Epoch 12/15: [=================             ] 28/47 batches, loss: 0.0544Epoch 12/15: [==================            ] 29/47 batches, loss: 0.0543Epoch 12/15: [===================           ] 30/47 batches, loss: 0.0541Epoch 12/15: [===================           ] 31/47 batches, loss: 0.0540Epoch 12/15: [====================          ] 32/47 batches, loss: 0.0531Epoch 12/15: [=====================         ] 33/47 batches, loss: 0.0531Epoch 12/15: [=====================         ] 34/47 batches, loss: 0.0531Epoch 12/15: [======================        ] 35/47 batches, loss: 0.0527Epoch 12/15: [======================        ] 36/47 batches, loss: 0.0532Epoch 12/15: [=======================       ] 37/47 batches, loss: 0.0531Epoch 12/15: [========================      ] 38/47 batches, loss: 0.0560Epoch 12/15: [========================      ] 39/47 batches, loss: 0.0553Epoch 12/15: [=========================     ] 40/47 batches, loss: 0.0555Epoch 12/15: [==========================    ] 41/47 batches, loss: 0.0547Epoch 12/15: [==========================    ] 42/47 batches, loss: 0.0539Epoch 12/15: [===========================   ] 43/47 batches, loss: 0.0547Epoch 12/15: [============================  ] 44/47 batches, loss: 0.0545Epoch 12/15: [============================  ] 45/47 batches, loss: 0.0545Epoch 12/15: [============================= ] 46/47 batches, loss: 0.0546Epoch 12/15: [==============================] 47/47 batches, loss: 0.0539
[2025-05-07 22:07:35,066][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0539
[2025-05-07 22:07:35,338][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0433, Metrics: {'mse': 0.04128040373325348, 'rmse': 0.2031757951461086, 'r2': 0.006598412990570068}
[2025-05-07 22:07:35,339][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/47 batches, loss: 0.0531Epoch 13/15: [=                             ] 2/47 batches, loss: 0.0527Epoch 13/15: [=                             ] 3/47 batches, loss: 0.0533Epoch 13/15: [==                            ] 4/47 batches, loss: 0.0455Epoch 13/15: [===                           ] 5/47 batches, loss: 0.0444Epoch 13/15: [===                           ] 6/47 batches, loss: 0.0478Epoch 13/15: [====                          ] 7/47 batches, loss: 0.0459Epoch 13/15: [=====                         ] 8/47 batches, loss: 0.0479Epoch 13/15: [=====                         ] 9/47 batches, loss: 0.0467Epoch 13/15: [======                        ] 10/47 batches, loss: 0.0451Epoch 13/15: [=======                       ] 11/47 batches, loss: 0.0472Epoch 13/15: [=======                       ] 12/47 batches, loss: 0.0487Epoch 13/15: [========                      ] 13/47 batches, loss: 0.0519Epoch 13/15: [========                      ] 14/47 batches, loss: 0.0517Epoch 13/15: [=========                     ] 15/47 batches, loss: 0.0523Epoch 13/15: [==========                    ] 16/47 batches, loss: 0.0509Epoch 13/15: [==========                    ] 17/47 batches, loss: 0.0507Epoch 13/15: [===========                   ] 18/47 batches, loss: 0.0501Epoch 13/15: [============                  ] 19/47 batches, loss: 0.0485Epoch 13/15: [============                  ] 20/47 batches, loss: 0.0474Epoch 13/15: [=============                 ] 21/47 batches, loss: 0.0484Epoch 13/15: [==============                ] 22/47 batches, loss: 0.0481Epoch 13/15: [==============                ] 23/47 batches, loss: 0.0492Epoch 13/15: [===============               ] 24/47 batches, loss: 0.0482Epoch 13/15: [===============               ] 25/47 batches, loss: 0.0493Epoch 13/15: [================              ] 26/47 batches, loss: 0.0506Epoch 13/15: [=================             ] 27/47 batches, loss: 0.0498Epoch 13/15: [=================             ] 28/47 batches, loss: 0.0494Epoch 13/15: [==================            ] 29/47 batches, loss: 0.0507Epoch 13/15: [===================           ] 30/47 batches, loss: 0.0517Epoch 13/15: [===================           ] 31/47 batches, loss: 0.0525Epoch 13/15: [====================          ] 32/47 batches, loss: 0.0522Epoch 13/15: [=====================         ] 33/47 batches, loss: 0.0532Epoch 13/15: [=====================         ] 34/47 batches, loss: 0.0534Epoch 13/15: [======================        ] 35/47 batches, loss: 0.0536Epoch 13/15: [======================        ] 36/47 batches, loss: 0.0537Epoch 13/15: [=======================       ] 37/47 batches, loss: 0.0540Epoch 13/15: [========================      ] 38/47 batches, loss: 0.0542Epoch 13/15: [========================      ] 39/47 batches, loss: 0.0544Epoch 13/15: [=========================     ] 40/47 batches, loss: 0.0542Epoch 13/15: [==========================    ] 41/47 batches, loss: 0.0547Epoch 13/15: [==========================    ] 42/47 batches, loss: 0.0547Epoch 13/15: [===========================   ] 43/47 batches, loss: 0.0549Epoch 13/15: [============================  ] 44/47 batches, loss: 0.0552Epoch 13/15: [============================  ] 45/47 batches, loss: 0.0557Epoch 13/15: [============================= ] 46/47 batches, loss: 0.0553Epoch 13/15: [==============================] 47/47 batches, loss: 0.0545
[2025-05-07 22:07:36,830][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0545
[2025-05-07 22:07:37,097][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0424, Metrics: {'mse': 0.04038279131054878, 'rmse': 0.20095469964782806, 'r2': 0.02819913625717163}
[2025-05-07 22:07:37,098][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:07:37,098][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 13
[2025-05-07 22:07:37,098][src.training.lm_trainer][INFO] - Training completed in 26.19 seconds
[2025-05-07 22:07:37,099][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:07:39,318][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03070899099111557, 'rmse': 0.1752398099494392, 'r2': 0.10327976942062378}
[2025-05-07 22:07:39,318][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03718917816877365, 'rmse': 0.19284495888867215, 'r2': 0.10505259037017822}
[2025-05-07 22:07:39,319][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06301616132259369, 'rmse': 0.2510302000210208, 'r2': -0.576407790184021}
[2025-05-07 22:07:40,981][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/layer2/ko/ko/model.pt
[2025-05-07 22:07:40,983][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▂▂▂▁▁
wandb:     best_val_mse █▅▂▂▂▁▁
wandb:      best_val_r2 ▁▄▇▇▇██
wandb:    best_val_rmse █▅▂▂▂▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▄▄▄▄▄▄▄▄▄▃
wandb:       train_loss █▄▃▃▂▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▂▂▂▂▁▁▁▁▁▃▂
wandb:          val_mse █▅▂▂▂▂▁▁▁▁▁▃▃
wandb:           val_r2 ▁▄▇▇▇▇█████▆▆
wandb:         val_rmse █▅▂▂▂▂▁▁▁▂▁▄▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03972
wandb:     best_val_mse 0.03719
wandb:      best_val_r2 0.10505
wandb:    best_val_rmse 0.19284
wandb: early_stop_epoch 13
wandb:            epoch 13
wandb:   final_test_mse 0.06302
wandb:    final_test_r2 -0.57641
wandb:  final_test_rmse 0.25103
wandb:  final_train_mse 0.03071
wandb:   final_train_r2 0.10328
wandb: final_train_rmse 0.17524
wandb:    final_val_mse 0.03719
wandb:     final_val_r2 0.10505
wandb:   final_val_rmse 0.19284
wandb:    learning_rate 0.0001
wandb:       train_loss 0.05447
wandb:       train_time 26.18642
wandb:         val_loss 0.04241
wandb:          val_mse 0.04038
wandb:           val_r2 0.0282
wandb:         val_rmse 0.20095
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_220653-jf6eycii
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_220653-jf6eycii/logs
Experiment probe_layer2_lexical_density_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_n_tokens_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_n_tokens_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/layer2/ko"         "wandb.mode=offline" "experiment.submetric=n_tokens"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:08:09,732][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/layer2/ko
experiment_name: probe_layer2_n_tokens_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: n_tokens
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:08:09,733][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:08:09,733][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 22:08:09,733][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:08:09,733][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:08:09,737][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:08:09,737][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 22:08:09,737][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:08:13,436][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'n_tokens'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:08:15,859][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:08:15,859][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:08:16,032][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:08:16,133][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:08:16,430][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:08:16,436][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:08:16,437][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:08:16,438][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:08:16,523][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:08:16,603][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:08:16,628][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:08:16,629][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:08:16,630][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:08:16,632][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:08:16,700][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:08:16,819][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:08:16,853][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:08:16,855][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:08:16,855][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:08:16,858][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:08:16,859][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:08:16,859][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 22:08:16,859][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 22:08:16,859][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 22:08:16,859][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:08:16,859][src.data.datasets][INFO] -   Mean: 0.0860, Std: 0.0995
[2025-05-07 22:08:16,859][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:08:16,860][src.data.datasets][INFO] - Sample label: 0.12800000607967377
[2025-05-07 22:08:16,860][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:08:16,860][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 22:08:16,860][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 22:08:16,860][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 22:08:16,860][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8890
[2025-05-07 22:08:16,860][src.data.datasets][INFO] -   Mean: 0.1868, Std: 0.2076
[2025-05-07 22:08:16,860][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:08:16,860][src.data.datasets][INFO] - Sample label: 0.22200000286102295
[2025-05-07 22:08:16,860][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:08:16,861][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 22:08:16,861][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 22:08:16,861][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 22:08:16,861][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9440
[2025-05-07 22:08:16,861][src.data.datasets][INFO] -   Mean: 0.2782, Std: 0.2042
[2025-05-07 22:08:16,861][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:08:16,861][src.data.datasets][INFO] - Sample label: 0.4440000057220459
[2025-05-07 22:08:16,861][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:08:16,861][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:08:16,862][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:08:16,862][__main__][INFO] - Using model type: lm_probe for submetric n_tokens
[2025-05-07 22:08:16,862][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:08:24,163][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:08:24,165][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:08:24,165][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:08:24,165][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:08:24,168][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:08:24,168][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:08:24,168][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:08:24,169][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:08:24,169][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:08:24,170][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:08:24,170][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.4122Epoch 1/15: [=                             ] 2/47 batches, loss: 0.5202Epoch 1/15: [=                             ] 3/47 batches, loss: 0.4750Epoch 1/15: [==                            ] 4/47 batches, loss: 0.4754Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4580Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4148Epoch 1/15: [====                          ] 7/47 batches, loss: 0.3990Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4119Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4163Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4146Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4039Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4020Epoch 1/15: [========                      ] 13/47 batches, loss: 0.3884Epoch 1/15: [========                      ] 14/47 batches, loss: 0.3939Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.3812Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.3879Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.3790Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.3935Epoch 1/15: [============                  ] 19/47 batches, loss: 0.3885Epoch 1/15: [============                  ] 20/47 batches, loss: 0.3817Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.3867Epoch 1/15: [==============                ] 22/47 batches, loss: 0.3882Epoch 1/15: [==============                ] 23/47 batches, loss: 0.3769Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3664Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3639Epoch 1/15: [================              ] 26/47 batches, loss: 0.3593Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3585Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3511Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3577Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3526Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3452Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3391Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3355Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3360Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3300Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3270Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3212Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3189Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3153Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3158Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3149Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3128Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3107Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3079Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3096Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3051Epoch 1/15: [==============================] 47/47 batches, loss: 0.3075
[2025-05-07 22:08:29,293][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3075
[2025-05-07 22:08:29,575][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1183, Metrics: {'mse': 0.12832045555114746, 'rmse': 0.35821844669300246, 'r2': -1.9776246547698975}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2803Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2171Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2356Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2029Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2168Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2316Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2168Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2088Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2095Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2006Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2013Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2016Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2000Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2020Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.1996Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.1950Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.1907Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.1963Epoch 2/15: [============                  ] 19/47 batches, loss: 0.1945Epoch 2/15: [============                  ] 20/47 batches, loss: 0.1907Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.1897Epoch 2/15: [==============                ] 22/47 batches, loss: 0.1895Epoch 2/15: [==============                ] 23/47 batches, loss: 0.1887Epoch 2/15: [===============               ] 24/47 batches, loss: 0.1886Epoch 2/15: [===============               ] 25/47 batches, loss: 0.1877Epoch 2/15: [================              ] 26/47 batches, loss: 0.1843Epoch 2/15: [=================             ] 27/47 batches, loss: 0.1819Epoch 2/15: [=================             ] 28/47 batches, loss: 0.1805Epoch 2/15: [==================            ] 29/47 batches, loss: 0.1776Epoch 2/15: [===================           ] 30/47 batches, loss: 0.1773Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1771Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1768Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1781Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1778Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1749Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1720Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1722Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1693Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1670Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1660Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1647Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1631Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1605Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1587Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1585Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1571Epoch 2/15: [==============================] 47/47 batches, loss: 0.1564
[2025-05-07 22:08:31,432][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1564
[2025-05-07 22:08:31,689][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0589, Metrics: {'mse': 0.06448598206043243, 'rmse': 0.2539409026927967, 'r2': -0.4963712692260742}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1492Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1076Epoch 3/15: [=                             ] 3/47 batches, loss: 0.0963Epoch 3/15: [==                            ] 4/47 batches, loss: 0.0915Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1032Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1278Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1201Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1121Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1182Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1168Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1185Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1200Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1221Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1180Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1176Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1162Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1220Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1215Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1182Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1158Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1172Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1143Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1147Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1155Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1138Epoch 3/15: [================              ] 26/47 batches, loss: 0.1126Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1138Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1130Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1108Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1105Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1120Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1115Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1118Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1130Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1141Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1128Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1128Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1117Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1100Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1098Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1089Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1082Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1082Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1097Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1109Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1107Epoch 3/15: [==============================] 47/47 batches, loss: 0.1113
[2025-05-07 22:08:33,560][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1113
[2025-05-07 22:08:33,830][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0794, Metrics: {'mse': 0.08659103512763977, 'rmse': 0.29426354705882235, 'r2': -1.009310007095337}
[2025-05-07 22:08:33,831][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1409Epoch 4/15: [=                             ] 2/47 batches, loss: 0.0999Epoch 4/15: [=                             ] 3/47 batches, loss: 0.0896Epoch 4/15: [==                            ] 4/47 batches, loss: 0.0992Epoch 4/15: [===                           ] 5/47 batches, loss: 0.0977Epoch 4/15: [===                           ] 6/47 batches, loss: 0.0953Epoch 4/15: [====                          ] 7/47 batches, loss: 0.0966Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1001Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.0983Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1030Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1094Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1071Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1044Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1020Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1003Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.0970Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.0982Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.0969Epoch 4/15: [============                  ] 19/47 batches, loss: 0.0970Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1007Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1002Epoch 4/15: [==============                ] 22/47 batches, loss: 0.0997Epoch 4/15: [==============                ] 23/47 batches, loss: 0.0979Epoch 4/15: [===============               ] 24/47 batches, loss: 0.0970Epoch 4/15: [===============               ] 25/47 batches, loss: 0.0976Epoch 4/15: [================              ] 26/47 batches, loss: 0.0967Epoch 4/15: [=================             ] 27/47 batches, loss: 0.0973Epoch 4/15: [=================             ] 28/47 batches, loss: 0.0969Epoch 4/15: [==================            ] 29/47 batches, loss: 0.0978Epoch 4/15: [===================           ] 30/47 batches, loss: 0.0967Epoch 4/15: [===================           ] 31/47 batches, loss: 0.0971Epoch 4/15: [====================          ] 32/47 batches, loss: 0.0983Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.0965Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.0975Epoch 4/15: [======================        ] 35/47 batches, loss: 0.0979Epoch 4/15: [======================        ] 36/47 batches, loss: 0.0968Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.0970Epoch 4/15: [========================      ] 38/47 batches, loss: 0.0959Epoch 4/15: [========================      ] 39/47 batches, loss: 0.0946Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.0942Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.0943Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.0945Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.0932Epoch 4/15: [============================  ] 44/47 batches, loss: 0.0931Epoch 4/15: [============================  ] 45/47 batches, loss: 0.0922Epoch 4/15: [============================= ] 46/47 batches, loss: 0.0918Epoch 4/15: [==============================] 47/47 batches, loss: 0.0921
[2025-05-07 22:08:35,317][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0921
[2025-05-07 22:08:35,606][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0582, Metrics: {'mse': 0.06387310475111008, 'rmse': 0.2527312896162841, 'r2': -0.48214972019195557}
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.0597Epoch 5/15: [=                             ] 2/47 batches, loss: 0.0752Epoch 5/15: [=                             ] 3/47 batches, loss: 0.0761Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0619Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0578Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0596Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0596Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0589Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0607Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0589Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0620Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0632Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0615Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0604Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0617Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0617Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0600Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0601Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0601Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0616Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0620Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0621Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0623Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0621Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0620Epoch 5/15: [================              ] 26/47 batches, loss: 0.0610Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0602Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0590Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0588Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0598Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0601Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0600Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0594Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0588Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0595Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0595Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0592Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0596Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0601Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0602Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0600Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0598Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0598Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0603Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0616Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0616Epoch 5/15: [==============================] 47/47 batches, loss: 0.0613
[2025-05-07 22:08:37,438][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0613
[2025-05-07 22:08:37,721][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0703, Metrics: {'mse': 0.07681875675916672, 'rmse': 0.2771619684573746, 'r2': -0.7825484275817871}
[2025-05-07 22:08:37,721][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.0742Epoch 6/15: [=                             ] 2/47 batches, loss: 0.0676Epoch 6/15: [=                             ] 3/47 batches, loss: 0.0765Epoch 6/15: [==                            ] 4/47 batches, loss: 0.0728Epoch 6/15: [===                           ] 5/47 batches, loss: 0.0786Epoch 6/15: [===                           ] 6/47 batches, loss: 0.0754Epoch 6/15: [====                          ] 7/47 batches, loss: 0.0694Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.0718Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.0682Epoch 6/15: [======                        ] 10/47 batches, loss: 0.0699Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.0726Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.0753Epoch 6/15: [========                      ] 13/47 batches, loss: 0.0728Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0700Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0693Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.0714Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.0722Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.0731Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0704Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0696Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0673Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0669Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0661Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0648Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0641Epoch 6/15: [================              ] 26/47 batches, loss: 0.0637Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0640Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0640Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0646Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0642Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0646Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0634Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0637Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0640Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0638Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0628Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0628Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0627Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0627Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0622Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0622Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0619Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0621Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0619Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0617Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0615Epoch 6/15: [==============================] 47/47 batches, loss: 0.0618
[2025-05-07 22:08:39,227][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0618
[2025-05-07 22:08:39,511][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0663, Metrics: {'mse': 0.07261423021554947, 'rmse': 0.26947027705398136, 'r2': -0.6849840879440308}
[2025-05-07 22:08:39,512][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/47 batches, loss: 0.0415Epoch 7/15: [=                             ] 2/47 batches, loss: 0.0566Epoch 7/15: [=                             ] 3/47 batches, loss: 0.0544Epoch 7/15: [==                            ] 4/47 batches, loss: 0.0477Epoch 7/15: [===                           ] 5/47 batches, loss: 0.0510Epoch 7/15: [===                           ] 6/47 batches, loss: 0.0499Epoch 7/15: [====                          ] 7/47 batches, loss: 0.0485Epoch 7/15: [=====                         ] 8/47 batches, loss: 0.0473Epoch 7/15: [=====                         ] 9/47 batches, loss: 0.0493Epoch 7/15: [======                        ] 10/47 batches, loss: 0.0503Epoch 7/15: [=======                       ] 11/47 batches, loss: 0.0496Epoch 7/15: [=======                       ] 12/47 batches, loss: 0.0495Epoch 7/15: [========                      ] 13/47 batches, loss: 0.0482Epoch 7/15: [========                      ] 14/47 batches, loss: 0.0472Epoch 7/15: [=========                     ] 15/47 batches, loss: 0.0506Epoch 7/15: [==========                    ] 16/47 batches, loss: 0.0508Epoch 7/15: [==========                    ] 17/47 batches, loss: 0.0514Epoch 7/15: [===========                   ] 18/47 batches, loss: 0.0528Epoch 7/15: [============                  ] 19/47 batches, loss: 0.0530Epoch 7/15: [============                  ] 20/47 batches, loss: 0.0524Epoch 7/15: [=============                 ] 21/47 batches, loss: 0.0516Epoch 7/15: [==============                ] 22/47 batches, loss: 0.0515Epoch 7/15: [==============                ] 23/47 batches, loss: 0.0516Epoch 7/15: [===============               ] 24/47 batches, loss: 0.0510Epoch 7/15: [===============               ] 25/47 batches, loss: 0.0521Epoch 7/15: [================              ] 26/47 batches, loss: 0.0525Epoch 7/15: [=================             ] 27/47 batches, loss: 0.0520Epoch 7/15: [=================             ] 28/47 batches, loss: 0.0526Epoch 7/15: [==================            ] 29/47 batches, loss: 0.0529Epoch 7/15: [===================           ] 30/47 batches, loss: 0.0532Epoch 7/15: [===================           ] 31/47 batches, loss: 0.0528Epoch 7/15: [====================          ] 32/47 batches, loss: 0.0526Epoch 7/15: [=====================         ] 33/47 batches, loss: 0.0516Epoch 7/15: [=====================         ] 34/47 batches, loss: 0.0515Epoch 7/15: [======================        ] 35/47 batches, loss: 0.0517Epoch 7/15: [======================        ] 36/47 batches, loss: 0.0524Epoch 7/15: [=======================       ] 37/47 batches, loss: 0.0520Epoch 7/15: [========================      ] 38/47 batches, loss: 0.0518Epoch 7/15: [========================      ] 39/47 batches, loss: 0.0517Epoch 7/15: [=========================     ] 40/47 batches, loss: 0.0513Epoch 7/15: [==========================    ] 41/47 batches, loss: 0.0510Epoch 7/15: [==========================    ] 42/47 batches, loss: 0.0511Epoch 7/15: [===========================   ] 43/47 batches, loss: 0.0506Epoch 7/15: [============================  ] 44/47 batches, loss: 0.0501Epoch 7/15: [============================  ] 45/47 batches, loss: 0.0500Epoch 7/15: [============================= ] 46/47 batches, loss: 0.0498Epoch 7/15: [==============================] 47/47 batches, loss: 0.0499
[2025-05-07 22:08:41,005][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0499
[2025-05-07 22:08:41,273][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0548, Metrics: {'mse': 0.060249753296375275, 'rmse': 0.24545825163635318, 'r2': -0.3980712890625}
Epoch 8/15: [Epoch 8/15: [                              ] 1/47 batches, loss: 0.0287Epoch 8/15: [=                             ] 2/47 batches, loss: 0.0404Epoch 8/15: [=                             ] 3/47 batches, loss: 0.0459Epoch 8/15: [==                            ] 4/47 batches, loss: 0.0491Epoch 8/15: [===                           ] 5/47 batches, loss: 0.0505Epoch 8/15: [===                           ] 6/47 batches, loss: 0.0485Epoch 8/15: [====                          ] 7/47 batches, loss: 0.0528Epoch 8/15: [=====                         ] 8/47 batches, loss: 0.0524Epoch 8/15: [=====                         ] 9/47 batches, loss: 0.0498Epoch 8/15: [======                        ] 10/47 batches, loss: 0.0493Epoch 8/15: [=======                       ] 11/47 batches, loss: 0.0479Epoch 8/15: [=======                       ] 12/47 batches, loss: 0.0482Epoch 8/15: [========                      ] 13/47 batches, loss: 0.0460Epoch 8/15: [========                      ] 14/47 batches, loss: 0.0474Epoch 8/15: [=========                     ] 15/47 batches, loss: 0.0472Epoch 8/15: [==========                    ] 16/47 batches, loss: 0.0466Epoch 8/15: [==========                    ] 17/47 batches, loss: 0.0466Epoch 8/15: [===========                   ] 18/47 batches, loss: 0.0462Epoch 8/15: [============                  ] 19/47 batches, loss: 0.0464Epoch 8/15: [============                  ] 20/47 batches, loss: 0.0457Epoch 8/15: [=============                 ] 21/47 batches, loss: 0.0475Epoch 8/15: [==============                ] 22/47 batches, loss: 0.0464Epoch 8/15: [==============                ] 23/47 batches, loss: 0.0460Epoch 8/15: [===============               ] 24/47 batches, loss: 0.0465Epoch 8/15: [===============               ] 25/47 batches, loss: 0.0466Epoch 8/15: [================              ] 26/47 batches, loss: 0.0469Epoch 8/15: [=================             ] 27/47 batches, loss: 0.0463Epoch 8/15: [=================             ] 28/47 batches, loss: 0.0473Epoch 8/15: [==================            ] 29/47 batches, loss: 0.0466Epoch 8/15: [===================           ] 30/47 batches, loss: 0.0465Epoch 8/15: [===================           ] 31/47 batches, loss: 0.0467Epoch 8/15: [====================          ] 32/47 batches, loss: 0.0469Epoch 8/15: [=====================         ] 33/47 batches, loss: 0.0474Epoch 8/15: [=====================         ] 34/47 batches, loss: 0.0474Epoch 8/15: [======================        ] 35/47 batches, loss: 0.0473Epoch 8/15: [======================        ] 36/47 batches, loss: 0.0470Epoch 8/15: [=======================       ] 37/47 batches, loss: 0.0468Epoch 8/15: [========================      ] 38/47 batches, loss: 0.0465Epoch 8/15: [========================      ] 39/47 batches, loss: 0.0466Epoch 8/15: [=========================     ] 40/47 batches, loss: 0.0458Epoch 8/15: [==========================    ] 41/47 batches, loss: 0.0455Epoch 8/15: [==========================    ] 42/47 batches, loss: 0.0451Epoch 8/15: [===========================   ] 43/47 batches, loss: 0.0448Epoch 8/15: [============================  ] 44/47 batches, loss: 0.0447Epoch 8/15: [============================  ] 45/47 batches, loss: 0.0443Epoch 8/15: [============================= ] 46/47 batches, loss: 0.0445Epoch 8/15: [==============================] 47/47 batches, loss: 0.0440
[2025-05-07 22:08:43,152][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0440
[2025-05-07 22:08:43,453][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0459, Metrics: {'mse': 0.050674550235271454, 'rmse': 0.22511008470362107, 'r2': -0.17588257789611816}
Epoch 9/15: [Epoch 9/15: [                              ] 1/47 batches, loss: 0.0465Epoch 9/15: [=                             ] 2/47 batches, loss: 0.0464Epoch 9/15: [=                             ] 3/47 batches, loss: 0.0499Epoch 9/15: [==                            ] 4/47 batches, loss: 0.0489Epoch 9/15: [===                           ] 5/47 batches, loss: 0.0426Epoch 9/15: [===                           ] 6/47 batches, loss: 0.0437Epoch 9/15: [====                          ] 7/47 batches, loss: 0.0421Epoch 9/15: [=====                         ] 8/47 batches, loss: 0.0454Epoch 9/15: [=====                         ] 9/47 batches, loss: 0.0449Epoch 9/15: [======                        ] 10/47 batches, loss: 0.0433Epoch 9/15: [=======                       ] 11/47 batches, loss: 0.0429Epoch 9/15: [=======                       ] 12/47 batches, loss: 0.0419Epoch 9/15: [========                      ] 13/47 batches, loss: 0.0405Epoch 9/15: [========                      ] 14/47 batches, loss: 0.0421Epoch 9/15: [=========                     ] 15/47 batches, loss: 0.0422Epoch 9/15: [==========                    ] 16/47 batches, loss: 0.0417Epoch 9/15: [==========                    ] 17/47 batches, loss: 0.0417Epoch 9/15: [===========                   ] 18/47 batches, loss: 0.0404Epoch 9/15: [============                  ] 19/47 batches, loss: 0.0403Epoch 9/15: [============                  ] 20/47 batches, loss: 0.0404Epoch 9/15: [=============                 ] 21/47 batches, loss: 0.0404Epoch 9/15: [==============                ] 22/47 batches, loss: 0.0400Epoch 9/15: [==============                ] 23/47 batches, loss: 0.0398Epoch 9/15: [===============               ] 24/47 batches, loss: 0.0394Epoch 9/15: [===============               ] 25/47 batches, loss: 0.0405Epoch 9/15: [================              ] 26/47 batches, loss: 0.0418Epoch 9/15: [=================             ] 27/47 batches, loss: 0.0414Epoch 9/15: [=================             ] 28/47 batches, loss: 0.0410Epoch 9/15: [==================            ] 29/47 batches, loss: 0.0410Epoch 9/15: [===================           ] 30/47 batches, loss: 0.0404Epoch 9/15: [===================           ] 31/47 batches, loss: 0.0404Epoch 9/15: [====================          ] 32/47 batches, loss: 0.0401Epoch 9/15: [=====================         ] 33/47 batches, loss: 0.0399Epoch 9/15: [=====================         ] 34/47 batches, loss: 0.0397Epoch 9/15: [======================        ] 35/47 batches, loss: 0.0396Epoch 9/15: [======================        ] 36/47 batches, loss: 0.0396Epoch 9/15: [=======================       ] 37/47 batches, loss: 0.0409Epoch 9/15: [========================      ] 38/47 batches, loss: 0.0406Epoch 9/15: [========================      ] 39/47 batches, loss: 0.0406Epoch 9/15: [=========================     ] 40/47 batches, loss: 0.0407Epoch 9/15: [==========================    ] 41/47 batches, loss: 0.0405Epoch 9/15: [==========================    ] 42/47 batches, loss: 0.0415Epoch 9/15: [===========================   ] 43/47 batches, loss: 0.0413Epoch 9/15: [============================  ] 44/47 batches, loss: 0.0411Epoch 9/15: [============================  ] 45/47 batches, loss: 0.0418Epoch 9/15: [============================= ] 46/47 batches, loss: 0.0416Epoch 9/15: [==============================] 47/47 batches, loss: 0.0415
[2025-05-07 22:08:45,420][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0415
[2025-05-07 22:08:45,769][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0578, Metrics: {'mse': 0.06341627240180969, 'rmse': 0.2518258771488937, 'r2': -0.47154903411865234}
[2025-05-07 22:08:45,770][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/47 batches, loss: 0.0196Epoch 10/15: [=                             ] 2/47 batches, loss: 0.0274Epoch 10/15: [=                             ] 3/47 batches, loss: 0.0288Epoch 10/15: [==                            ] 4/47 batches, loss: 0.0322Epoch 10/15: [===                           ] 5/47 batches, loss: 0.0350Epoch 10/15: [===                           ] 6/47 batches, loss: 0.0362Epoch 10/15: [====                          ] 7/47 batches, loss: 0.0375Epoch 10/15: [=====                         ] 8/47 batches, loss: 0.0363Epoch 10/15: [=====                         ] 9/47 batches, loss: 0.0369Epoch 10/15: [======                        ] 10/47 batches, loss: 0.0367Epoch 10/15: [=======                       ] 11/47 batches, loss: 0.0377Epoch 10/15: [=======                       ] 12/47 batches, loss: 0.0364Epoch 10/15: [========                      ] 13/47 batches, loss: 0.0364Epoch 10/15: [========                      ] 14/47 batches, loss: 0.0371Epoch 10/15: [=========                     ] 15/47 batches, loss: 0.0369Epoch 10/15: [==========                    ] 16/47 batches, loss: 0.0376Epoch 10/15: [==========                    ] 17/47 batches, loss: 0.0367Epoch 10/15: [===========                   ] 18/47 batches, loss: 0.0369Epoch 10/15: [============                  ] 19/47 batches, loss: 0.0369Epoch 10/15: [============                  ] 20/47 batches, loss: 0.0377Epoch 10/15: [=============                 ] 21/47 batches, loss: 0.0378Epoch 10/15: [==============                ] 22/47 batches, loss: 0.0376Epoch 10/15: [==============                ] 23/47 batches, loss: 0.0367Epoch 10/15: [===============               ] 24/47 batches, loss: 0.0392Epoch 10/15: [===============               ] 25/47 batches, loss: 0.0389Epoch 10/15: [================              ] 26/47 batches, loss: 0.0379Epoch 10/15: [=================             ] 27/47 batches, loss: 0.0375Epoch 10/15: [=================             ] 28/47 batches, loss: 0.0380Epoch 10/15: [==================            ] 29/47 batches, loss: 0.0374Epoch 10/15: [===================           ] 30/47 batches, loss: 0.0375Epoch 10/15: [===================           ] 31/47 batches, loss: 0.0372Epoch 10/15: [====================          ] 32/47 batches, loss: 0.0372Epoch 10/15: [=====================         ] 33/47 batches, loss: 0.0372Epoch 10/15: [=====================         ] 34/47 batches, loss: 0.0371Epoch 10/15: [======================        ] 35/47 batches, loss: 0.0367Epoch 10/15: [======================        ] 36/47 batches, loss: 0.0365Epoch 10/15: [=======================       ] 37/47 batches, loss: 0.0362Epoch 10/15: [========================      ] 38/47 batches, loss: 0.0374Epoch 10/15: [========================      ] 39/47 batches, loss: 0.0367Epoch 10/15: [=========================     ] 40/47 batches, loss: 0.0374Epoch 10/15: [==========================    ] 41/47 batches, loss: 0.0369Epoch 10/15: [==========================    ] 42/47 batches, loss: 0.0374Epoch 10/15: [===========================   ] 43/47 batches, loss: 0.0375Epoch 10/15: [============================  ] 44/47 batches, loss: 0.0374Epoch 10/15: [============================  ] 45/47 batches, loss: 0.0373Epoch 10/15: [============================= ] 46/47 batches, loss: 0.0374Epoch 10/15: [==============================] 47/47 batches, loss: 0.0376
[2025-05-07 22:08:47,287][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0376
[2025-05-07 22:08:47,560][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0532, Metrics: {'mse': 0.05847962573170662, 'rmse': 0.2418256101650663, 'r2': -0.3569962978363037}
[2025-05-07 22:08:47,560][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/47 batches, loss: 0.0152Epoch 11/15: [=                             ] 2/47 batches, loss: 0.0229Epoch 11/15: [=                             ] 3/47 batches, loss: 0.0304Epoch 11/15: [==                            ] 4/47 batches, loss: 0.0399Epoch 11/15: [===                           ] 5/47 batches, loss: 0.0427Epoch 11/15: [===                           ] 6/47 batches, loss: 0.0390Epoch 11/15: [====                          ] 7/47 batches, loss: 0.0398Epoch 11/15: [=====                         ] 8/47 batches, loss: 0.0396Epoch 11/15: [=====                         ] 9/47 batches, loss: 0.0386Epoch 11/15: [======                        ] 10/47 batches, loss: 0.0379Epoch 11/15: [=======                       ] 11/47 batches, loss: 0.0373Epoch 11/15: [=======                       ] 12/47 batches, loss: 0.0359Epoch 11/15: [========                      ] 13/47 batches, loss: 0.0383Epoch 11/15: [========                      ] 14/47 batches, loss: 0.0377Epoch 11/15: [=========                     ] 15/47 batches, loss: 0.0370Epoch 11/15: [==========                    ] 16/47 batches, loss: 0.0364Epoch 11/15: [==========                    ] 17/47 batches, loss: 0.0360Epoch 11/15: [===========                   ] 18/47 batches, loss: 0.0352Epoch 11/15: [============                  ] 19/47 batches, loss: 0.0350Epoch 11/15: [============                  ] 20/47 batches, loss: 0.0363Epoch 11/15: [=============                 ] 21/47 batches, loss: 0.0354Epoch 11/15: [==============                ] 22/47 batches, loss: 0.0358Epoch 11/15: [==============                ] 23/47 batches, loss: 0.0353Epoch 11/15: [===============               ] 24/47 batches, loss: 0.0353Epoch 11/15: [===============               ] 25/47 batches, loss: 0.0343Epoch 11/15: [================              ] 26/47 batches, loss: 0.0353Epoch 11/15: [=================             ] 27/47 batches, loss: 0.0351Epoch 11/15: [=================             ] 28/47 batches, loss: 0.0356Epoch 11/15: [==================            ] 29/47 batches, loss: 0.0354Epoch 11/15: [===================           ] 30/47 batches, loss: 0.0351Epoch 11/15: [===================           ] 31/47 batches, loss: 0.0347Epoch 11/15: [====================          ] 32/47 batches, loss: 0.0343Epoch 11/15: [=====================         ] 33/47 batches, loss: 0.0340Epoch 11/15: [=====================         ] 34/47 batches, loss: 0.0340Epoch 11/15: [======================        ] 35/47 batches, loss: 0.0346Epoch 11/15: [======================        ] 36/47 batches, loss: 0.0347Epoch 11/15: [=======================       ] 37/47 batches, loss: 0.0346Epoch 11/15: [========================      ] 38/47 batches, loss: 0.0343Epoch 11/15: [========================      ] 39/47 batches, loss: 0.0340Epoch 11/15: [=========================     ] 40/47 batches, loss: 0.0340Epoch 11/15: [==========================    ] 41/47 batches, loss: 0.0339Epoch 11/15: [==========================    ] 42/47 batches, loss: 0.0336Epoch 11/15: [===========================   ] 43/47 batches, loss: 0.0332Epoch 11/15: [============================  ] 44/47 batches, loss: 0.0336Epoch 11/15: [============================  ] 45/47 batches, loss: 0.0333Epoch 11/15: [============================= ] 46/47 batches, loss: 0.0331Epoch 11/15: [==============================] 47/47 batches, loss: 0.0329
[2025-05-07 22:08:49,048][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0329
[2025-05-07 22:08:49,316][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0463, Metrics: {'mse': 0.05103674903512001, 'rmse': 0.22591314489227937, 'r2': -0.18428730964660645}
[2025-05-07 22:08:49,317][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/47 batches, loss: 0.0074Epoch 12/15: [=                             ] 2/47 batches, loss: 0.0120Epoch 12/15: [=                             ] 3/47 batches, loss: 0.0171Epoch 12/15: [==                            ] 4/47 batches, loss: 0.0198Epoch 12/15: [===                           ] 5/47 batches, loss: 0.0207Epoch 12/15: [===                           ] 6/47 batches, loss: 0.0225Epoch 12/15: [====                          ] 7/47 batches, loss: 0.0237Epoch 12/15: [=====                         ] 8/47 batches, loss: 0.0264Epoch 12/15: [=====                         ] 9/47 batches, loss: 0.0259Epoch 12/15: [======                        ] 10/47 batches, loss: 0.0247Epoch 12/15: [=======                       ] 11/47 batches, loss: 0.0258Epoch 12/15: [=======                       ] 12/47 batches, loss: 0.0258Epoch 12/15: [========                      ] 13/47 batches, loss: 0.0267Epoch 12/15: [========                      ] 14/47 batches, loss: 0.0266Epoch 12/15: [=========                     ] 15/47 batches, loss: 0.0281Epoch 12/15: [==========                    ] 16/47 batches, loss: 0.0293Epoch 12/15: [==========                    ] 17/47 batches, loss: 0.0295Epoch 12/15: [===========                   ] 18/47 batches, loss: 0.0289Epoch 12/15: [============                  ] 19/47 batches, loss: 0.0303Epoch 12/15: [============                  ] 20/47 batches, loss: 0.0299Epoch 12/15: [=============                 ] 21/47 batches, loss: 0.0303Epoch 12/15: [==============                ] 22/47 batches, loss: 0.0295Epoch 12/15: [==============                ] 23/47 batches, loss: 0.0299Epoch 12/15: [===============               ] 24/47 batches, loss: 0.0306Epoch 12/15: [===============               ] 25/47 batches, loss: 0.0307Epoch 12/15: [================              ] 26/47 batches, loss: 0.0303Epoch 12/15: [=================             ] 27/47 batches, loss: 0.0299Epoch 12/15: [=================             ] 28/47 batches, loss: 0.0300Epoch 12/15: [==================            ] 29/47 batches, loss: 0.0296Epoch 12/15: [===================           ] 30/47 batches, loss: 0.0294Epoch 12/15: [===================           ] 31/47 batches, loss: 0.0294Epoch 12/15: [====================          ] 32/47 batches, loss: 0.0294Epoch 12/15: [=====================         ] 33/47 batches, loss: 0.0291Epoch 12/15: [=====================         ] 34/47 batches, loss: 0.0292Epoch 12/15: [======================        ] 35/47 batches, loss: 0.0288Epoch 12/15: [======================        ] 36/47 batches, loss: 0.0293Epoch 12/15: [=======================       ] 37/47 batches, loss: 0.0297Epoch 12/15: [========================      ] 38/47 batches, loss: 0.0299Epoch 12/15: [========================      ] 39/47 batches, loss: 0.0296Epoch 12/15: [=========================     ] 40/47 batches, loss: 0.0296Epoch 12/15: [==========================    ] 41/47 batches, loss: 0.0293Epoch 12/15: [==========================    ] 42/47 batches, loss: 0.0291Epoch 12/15: [===========================   ] 43/47 batches, loss: 0.0293Epoch 12/15: [============================  ] 44/47 batches, loss: 0.0292Epoch 12/15: [============================  ] 45/47 batches, loss: 0.0292Epoch 12/15: [============================= ] 46/47 batches, loss: 0.0292Epoch 12/15: [==============================] 47/47 batches, loss: 0.0289
[2025-05-07 22:08:50,798][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0289
[2025-05-07 22:08:51,075][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0599, Metrics: {'mse': 0.065650075674057, 'rmse': 0.25622270717884665, 'r2': -0.5233837366104126}
[2025-05-07 22:08:51,075][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:08:51,076][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 12
[2025-05-07 22:08:51,076][src.training.lm_trainer][INFO] - Training completed in 24.03 seconds
[2025-05-07 22:08:51,076][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:08:53,335][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.006705073639750481, 'rmse': 0.08188451404112063, 'r2': 0.32336294651031494}
[2025-05-07 22:08:53,336][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.050674550235271454, 'rmse': 0.22511008470362107, 'r2': -0.17588257789611816}
[2025-05-07 22:08:53,336][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.08305183798074722, 'rmse': 0.2881871579039344, 'r2': -0.9916235208511353}
[2025-05-07 22:08:55,015][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/layer2/ko/ko/model.pt
[2025-05-07 22:08:55,017][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▂▂▁
wandb:     best_val_mse █▂▂▂▁
wandb:      best_val_r2 ▁▇▇▇█
wandb:    best_val_rmse █▃▂▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▆▅▆▆▆▆▇▆▇▇
wandb:       train_loss █▄▃▃▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▂▄▂▃▃▂▁▂▂▁▂
wandb:          val_mse █▂▄▂▃▃▂▁▂▂▁▂
wandb:           val_r2 ▁▇▅▇▆▆▇█▇▇█▇
wandb:         val_rmse █▃▅▂▄▃▂▁▂▂▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0459
wandb:     best_val_mse 0.05067
wandb:      best_val_r2 -0.17588
wandb:    best_val_rmse 0.22511
wandb: early_stop_epoch 12
wandb:            epoch 12
wandb:   final_test_mse 0.08305
wandb:    final_test_r2 -0.99162
wandb:  final_test_rmse 0.28819
wandb:  final_train_mse 0.00671
wandb:   final_train_r2 0.32336
wandb: final_train_rmse 0.08188
wandb:    final_val_mse 0.05067
wandb:     final_val_r2 -0.17588
wandb:   final_val_rmse 0.22511
wandb:    learning_rate 0.0001
wandb:       train_loss 0.02888
wandb:       train_time 24.02922
wandb:         val_loss 0.0599
wandb:          val_mse 0.06565
wandb:           val_r2 -0.52338
wandb:         val_rmse 0.25622
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_220809-ep5k7y0h
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_220809-ep5k7y0h/logs
Experiment probe_layer2_n_tokens_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/layer2/ko/ko/results.json for layer 2
Running control submetric probing experiments...
=======================
PROBING LAYER 2 (SUBMETRIC CONTROL EXPERIMENTS)
=======================
Running experiment: probe_layer2_avg_links_len_control1_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_control1_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:09:22,935][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/ko
experiment_name: probe_layer2_avg_links_len_control1_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:09:22,935][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:09:22,935][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 22:09:22,935][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:09:22,935][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:09:22,940][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:09:22,940][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 22:09:22,940][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:09:26,024][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:09:28,293][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:09:28,293][src.data.datasets][INFO] - Loading 'control_avg_links_len_seed1' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:09:28,491][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_links_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:14:19 2025).
[2025-05-07 22:09:28,574][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_links_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:14:19 2025).
[2025-05-07 22:09:28,911][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:09:28,917][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:09:28,918][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:09:28,920][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:09:28,959][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:09:29,023][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:09:29,044][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:09:29,045][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:09:29,046][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:09:29,048][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:09:29,135][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:09:29,200][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:09:29,250][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:09:29,251][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:09:29,251][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:09:29,253][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:09:29,253][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:09:29,253][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 22:09:29,253][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 22:09:29,253][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 22:09:29,253][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:09:29,254][src.data.datasets][INFO] -   Mean: 0.1923, Std: 0.1477
[2025-05-07 22:09:29,254][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:09:29,254][src.data.datasets][INFO] - Sample label: 0.22200000286102295
[2025-05-07 22:09:29,254][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:09:29,254][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 22:09:29,254][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 22:09:29,254][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 22:09:29,254][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.5940
[2025-05-07 22:09:29,254][src.data.datasets][INFO] -   Mean: 0.2139, Std: 0.1419
[2025-05-07 22:09:29,255][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:09:29,255][src.data.datasets][INFO] - Sample label: 0.3160000145435333
[2025-05-07 22:09:29,255][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:09:29,255][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 22:09:29,255][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 22:09:29,255][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 22:09:29,255][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7720
[2025-05-07 22:09:29,255][src.data.datasets][INFO] -   Mean: 0.2570, Std: 0.1517
[2025-05-07 22:09:29,255][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:09:29,255][src.data.datasets][INFO] - Sample label: 0.1589999943971634
[2025-05-07 22:09:29,255][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:09:29,256][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:09:29,256][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:09:29,256][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 22:09:29,256][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:09:36,324][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:09:36,326][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:09:36,326][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:09:36,326][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:09:36,329][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:09:36,329][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:09:36,329][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:09:36,330][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:09:36,330][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:09:36,331][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:09:36,331][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.3446Epoch 1/15: [=                             ] 2/47 batches, loss: 0.4329Epoch 1/15: [=                             ] 3/47 batches, loss: 0.4303Epoch 1/15: [==                            ] 4/47 batches, loss: 0.4099Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4058Epoch 1/15: [===                           ] 6/47 batches, loss: 0.3746Epoch 1/15: [====                          ] 7/47 batches, loss: 0.3598Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.3803Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.3934Epoch 1/15: [======                        ] 10/47 batches, loss: 0.3958Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.3845Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.3884Epoch 1/15: [========                      ] 13/47 batches, loss: 0.3787Epoch 1/15: [========                      ] 14/47 batches, loss: 0.3796Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.3684Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.3759Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.3710Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.3841Epoch 1/15: [============                  ] 19/47 batches, loss: 0.3765Epoch 1/15: [============                  ] 20/47 batches, loss: 0.3682Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.3770Epoch 1/15: [==============                ] 22/47 batches, loss: 0.3795Epoch 1/15: [==============                ] 23/47 batches, loss: 0.3683Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3599Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3590Epoch 1/15: [================              ] 26/47 batches, loss: 0.3546Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3546Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3479Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3532Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3488Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3430Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3389Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3397Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3432Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3387Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3375Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3321Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3317Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3286Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3291Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3272Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3256Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3233Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3212Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3207Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3152Epoch 1/15: [==============================] 47/47 batches, loss: 0.3175
[2025-05-07 22:09:41,718][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3175
[2025-05-07 22:09:42,003][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0752, Metrics: {'mse': 0.0797661691904068, 'rmse': 0.28242905160483545, 'r2': -2.9633169174194336}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2869Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2322Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2819Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2391Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2451Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2586Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2494Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2388Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2372Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2324Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2328Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2260Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2266Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2257Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2266Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2201Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2148Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2203Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2161Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2118Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2089Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2072Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2037Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2038Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2020Epoch 2/15: [================              ] 26/47 batches, loss: 0.1992Epoch 2/15: [=================             ] 27/47 batches, loss: 0.1966Epoch 2/15: [=================             ] 28/47 batches, loss: 0.1980Epoch 2/15: [==================            ] 29/47 batches, loss: 0.1950Epoch 2/15: [===================           ] 30/47 batches, loss: 0.1930Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1932Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1957Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1970Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1964Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1919Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1904Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1918Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1895Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1901Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1890Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1889Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1874Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1842Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1823Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1807Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1789Epoch 2/15: [==============================] 47/47 batches, loss: 0.1782
[2025-05-07 22:09:43,864][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1782
[2025-05-07 22:09:44,154][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0310, Metrics: {'mse': 0.03326885402202606, 'rmse': 0.1823975164908395, 'r2': -0.6530191898345947}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.2051Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1549Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1278Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1139Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1285Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1590Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1498Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1455Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1486Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1425Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1459Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1500Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1526Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1461Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1469Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1460Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1475Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1465Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1428Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1391Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1425Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1415Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1406Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1432Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1416Epoch 3/15: [================              ] 26/47 batches, loss: 0.1412Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1405Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1383Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1365Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1363Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1359Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1353Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1356Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1340Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1353Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1372Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1359Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1344Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1330Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1322Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1309Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1313Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1308Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1311Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1319Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1312Epoch 3/15: [==============================] 47/47 batches, loss: 0.1317
[2025-05-07 22:09:46,105][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1317
[2025-05-07 22:09:46,393][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0371, Metrics: {'mse': 0.039677850902080536, 'rmse': 0.19919299912918761, 'r2': -0.9714610576629639}
[2025-05-07 22:09:46,394][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.2094Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1351Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1116Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1090Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1121Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1044Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1076Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1142Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1128Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1157Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1218Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1200Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1199Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1146Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1124Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1078Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1098Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1075Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1099Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1128Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1124Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1127Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1126Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1120Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1144Epoch 4/15: [================              ] 26/47 batches, loss: 0.1147Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1149Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1168Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1177Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1163Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1187Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1188Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1170Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1187Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1193Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1178Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1188Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1169Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1159Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1152Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1159Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1160Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1143Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1141Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1124Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1130Epoch 4/15: [==============================] 47/47 batches, loss: 0.1181
[2025-05-07 22:09:47,890][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1181
[2025-05-07 22:09:48,238][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0333, Metrics: {'mse': 0.03559456765651703, 'rmse': 0.18866522641047828, 'r2': -0.7685763835906982}
[2025-05-07 22:09:48,238][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.0948Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1338Epoch 5/15: [=                             ] 3/47 batches, loss: 0.1135Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0935Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0848Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0890Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0867Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0827Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0836Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0786Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0777Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0803Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0782Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0751Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0744Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0755Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0770Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0771Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0769Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0789Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0807Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0809Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0799Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0822Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0820Epoch 5/15: [================              ] 26/47 batches, loss: 0.0796Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0802Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0792Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0780Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0784Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0792Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0786Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0789Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0786Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0812Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0812Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0809Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0811Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0819Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0825Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0831Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0829Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0832Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0826Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0857Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0851Epoch 5/15: [==============================] 47/47 batches, loss: 0.0840
[2025-05-07 22:09:49,710][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0840
[2025-05-07 22:09:49,971][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0353, Metrics: {'mse': 0.03753865882754326, 'rmse': 0.1937489582618272, 'r2': -0.8651716709136963}
[2025-05-07 22:09:49,972][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.1364Epoch 6/15: [=                             ] 2/47 batches, loss: 0.1120Epoch 6/15: [=                             ] 3/47 batches, loss: 0.1162Epoch 6/15: [==                            ] 4/47 batches, loss: 0.1068Epoch 6/15: [===                           ] 5/47 batches, loss: 0.1086Epoch 6/15: [===                           ] 6/47 batches, loss: 0.1110Epoch 6/15: [====                          ] 7/47 batches, loss: 0.1026Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.1035Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.0991Epoch 6/15: [======                        ] 10/47 batches, loss: 0.0987Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.1004Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.1043Epoch 6/15: [========                      ] 13/47 batches, loss: 0.0990Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0950Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0931Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.0937Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.0927Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.0936Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0899Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0872Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0844Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0843Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0843Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0829Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0814Epoch 6/15: [================              ] 26/47 batches, loss: 0.0810Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0804Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0797Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0811Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0802Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0807Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0794Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0797Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0795Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0790Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0782Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0787Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0780Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0782Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0779Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0766Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0770Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0773Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0771Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0769Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0766Epoch 6/15: [==============================] 47/47 batches, loss: 0.0769
[2025-05-07 22:09:51,453][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0769
[2025-05-07 22:09:51,713][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0485, Metrics: {'mse': 0.05114156752824783, 'rmse': 0.22614501437849086, 'r2': -1.5410552024841309}
[2025-05-07 22:09:51,713][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:09:51,713][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-07 22:09:51,714][src.training.lm_trainer][INFO] - Training completed in 12.31 seconds
[2025-05-07 22:09:51,714][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:09:53,867][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.028491852805018425, 'rmse': 0.1687952985275906, 'r2': -0.3066293001174927}
[2025-05-07 22:09:53,867][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03326885402202606, 'rmse': 0.1823975164908395, 'r2': -0.6530191898345947}
[2025-05-07 22:09:53,867][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.051258865743875504, 'rmse': 0.22640420875919137, 'r2': -1.2274971008300781}
[2025-05-07 22:09:55,562][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/ko/ko/model.pt
[2025-05-07 22:09:55,563][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▇▆▇▆
wandb:       train_loss █▄▃▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▂▁▂▄
wandb:          val_mse █▁▂▁▂▄
wandb:           val_r2 ▁█▇█▇▅
wandb:         val_rmse █▁▂▁▂▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03098
wandb:     best_val_mse 0.03327
wandb:      best_val_r2 -0.65302
wandb:    best_val_rmse 0.1824
wandb: early_stop_epoch 6
wandb:            epoch 6
wandb:   final_test_mse 0.05126
wandb:    final_test_r2 -1.2275
wandb:  final_test_rmse 0.2264
wandb:  final_train_mse 0.02849
wandb:   final_train_r2 -0.30663
wandb: final_train_rmse 0.1688
wandb:    final_val_mse 0.03327
wandb:     final_val_r2 -0.65302
wandb:   final_val_rmse 0.1824
wandb:    learning_rate 0.0001
wandb:       train_loss 0.07692
wandb:       train_time 12.31217
wandb:         val_loss 0.04853
wandb:          val_mse 0.05114
wandb:           val_r2 -1.54106
wandb:         val_rmse 0.22615
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_220922-vrhclc4l
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_220922-vrhclc4l/logs
Experiment probe_layer2_avg_links_len_control1_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_avg_links_len_control2_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_control2_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:10:15,699][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/ko
experiment_name: probe_layer2_avg_links_len_control2_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:10:15,699][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:10:15,699][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 22:10:15,699][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:10:15,699][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:10:15,704][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:10:15,704][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 22:10:15,704][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:10:17,810][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:10:20,039][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:10:20,039][src.data.datasets][INFO] - Loading 'control_avg_links_len_seed2' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:10:20,120][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_links_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:16:02 2025).
[2025-05-07 22:10:20,208][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_links_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:16:02 2025).
[2025-05-07 22:10:20,428][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:10:20,435][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:10:20,435][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:10:20,448][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:10:20,519][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:10:20,597][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:10:20,632][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:10:20,634][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:10:20,634][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:10:20,637][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:10:20,702][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:10:20,805][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:10:20,834][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:10:20,835][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:10:20,836][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:10:20,837][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:10:20,838][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:10:20,839][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 22:10:20,839][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 22:10:20,839][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 22:10:20,839][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:10:20,839][src.data.datasets][INFO] -   Mean: 0.1923, Std: 0.1477
[2025-05-07 22:10:20,839][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:10:20,839][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 22:10:20,839][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:10:20,840][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 22:10:20,840][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 22:10:20,840][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 22:10:20,840][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.5940
[2025-05-07 22:10:20,840][src.data.datasets][INFO] -   Mean: 0.2139, Std: 0.1419
[2025-05-07 22:10:20,840][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:10:20,840][src.data.datasets][INFO] - Sample label: 0.3160000145435333
[2025-05-07 22:10:20,840][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:10:20,840][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 22:10:20,840][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 22:10:20,841][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 22:10:20,841][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7720
[2025-05-07 22:10:20,841][src.data.datasets][INFO] -   Mean: 0.2570, Std: 0.1517
[2025-05-07 22:10:20,841][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:10:20,841][src.data.datasets][INFO] - Sample label: 0.1589999943971634
[2025-05-07 22:10:20,841][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:10:20,841][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:10:20,841][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:10:20,841][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 22:10:20,842][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:10:26,897][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:10:26,898][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:10:26,899][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:10:26,899][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:10:26,902][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:10:26,902][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:10:26,902][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:10:26,902][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:10:26,903][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:10:26,903][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:10:26,903][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.3872Epoch 1/15: [=                             ] 2/47 batches, loss: 0.4888Epoch 1/15: [=                             ] 3/47 batches, loss: 0.4786Epoch 1/15: [==                            ] 4/47 batches, loss: 0.4856Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4762Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4330Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4171Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4335Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4430Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4322Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4174Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4095Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4018Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4032Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.3906Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4080Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.3997Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4201Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4117Epoch 1/15: [============                  ] 20/47 batches, loss: 0.4050Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4108Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4086Epoch 1/15: [==============                ] 23/47 batches, loss: 0.3976Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3877Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3855Epoch 1/15: [================              ] 26/47 batches, loss: 0.3815Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3809Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3725Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3746Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3677Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3609Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3527Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3494Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3488Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3439Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3398Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3374Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3347Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3317Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3323Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3322Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3305Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3291Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3260Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3259Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3213Epoch 1/15: [==============================] 47/47 batches, loss: 0.3226
[2025-05-07 22:10:38,055][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3226
[2025-05-07 22:10:38,349][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0671, Metrics: {'mse': 0.07136935740709305, 'rmse': 0.26715043965356494, 'r2': -2.546107053756714}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2432Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2380Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2482Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2101Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2368Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2428Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2347Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2273Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2285Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2193Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2256Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2255Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2200Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2252Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2210Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2191Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2144Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2170Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2127Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2112Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2114Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2121Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2110Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2119Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2101Epoch 2/15: [================              ] 26/47 batches, loss: 0.2078Epoch 2/15: [=================             ] 27/47 batches, loss: 0.2037Epoch 2/15: [=================             ] 28/47 batches, loss: 0.2010Epoch 2/15: [==================            ] 29/47 batches, loss: 0.1976Epoch 2/15: [===================           ] 30/47 batches, loss: 0.1951Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1934Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1920Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1951Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1945Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1917Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1891Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1895Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1860Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1844Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1840Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1828Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1810Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1786Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1777Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1765Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1743Epoch 2/15: [==============================] 47/47 batches, loss: 0.1739
[2025-05-07 22:10:40,267][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1739
[2025-05-07 22:10:40,522][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0306, Metrics: {'mse': 0.03297153115272522, 'rmse': 0.18158064641564975, 'r2': -0.6382462978363037}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1631Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1343Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1340Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1250Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1320Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1518Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1474Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1380Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1515Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1486Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1493Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1517Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1499Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1447Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1443Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1402Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1443Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1444Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1404Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1369Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1374Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1336Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1366Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1370Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1349Epoch 3/15: [================              ] 26/47 batches, loss: 0.1340Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1322Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1302Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1276Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1270Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1261Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1250Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1267Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1262Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1267Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1254Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1245Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1237Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1220Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1220Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1213Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1210Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1204Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1226Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1241Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1238Epoch 3/15: [==============================] 47/47 batches, loss: 0.1221
[2025-05-07 22:10:42,424][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1221
[2025-05-07 22:10:42,685][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0394, Metrics: {'mse': 0.042036570608615875, 'rmse': 0.2050282190543923, 'r2': -1.088658094406128}
[2025-05-07 22:10:42,685][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1409Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1614Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1248Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1231Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1215Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1134Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1113Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1097Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1058Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1096Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1147Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1142Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1159Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1122Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1116Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1083Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1089Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1086Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1090Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1139Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1135Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1129Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1107Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1099Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1095Epoch 4/15: [================              ] 26/47 batches, loss: 0.1084Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1093Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1123Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1151Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1140Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1170Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1172Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1161Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1153Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1141Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1125Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1127Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1111Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1096Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1085Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1088Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1099Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1085Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1083Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1071Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1070Epoch 4/15: [==============================] 47/47 batches, loss: 0.1077
[2025-05-07 22:10:44,158][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1077
[2025-05-07 22:10:44,496][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0403, Metrics: {'mse': 0.04285643994808197, 'rmse': 0.20701797010907524, 'r2': -1.12939453125}
[2025-05-07 22:10:44,497][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.1119Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1095Epoch 5/15: [=                             ] 3/47 batches, loss: 0.1027Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0926Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0839Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0823Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0829Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0821Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0824Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0775Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0776Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0782Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0768Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0774Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0759Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0753Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0757Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0781Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0774Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0791Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0811Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0813Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0805Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0832Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0830Epoch 5/15: [================              ] 26/47 batches, loss: 0.0830Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0809Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0801Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0804Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0818Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0829Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0822Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0815Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0818Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0819Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0823Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0815Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0830Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0837Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0849Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0845Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0850Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0851Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0849Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0856Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0856Epoch 5/15: [==============================] 47/47 batches, loss: 0.0842
[2025-05-07 22:10:46,015][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0842
[2025-05-07 22:10:46,285][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0467, Metrics: {'mse': 0.04932308942079544, 'rmse': 0.2220880217859474, 'r2': -1.4507009983062744}
[2025-05-07 22:10:46,286][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.0928Epoch 6/15: [=                             ] 2/47 batches, loss: 0.0662Epoch 6/15: [=                             ] 3/47 batches, loss: 0.0877Epoch 6/15: [==                            ] 4/47 batches, loss: 0.0859Epoch 6/15: [===                           ] 5/47 batches, loss: 0.0893Epoch 6/15: [===                           ] 6/47 batches, loss: 0.0848Epoch 6/15: [====                          ] 7/47 batches, loss: 0.0809Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.0807Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.0771Epoch 6/15: [======                        ] 10/47 batches, loss: 0.0809Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.0824Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.0879Epoch 6/15: [========                      ] 13/47 batches, loss: 0.0864Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0835Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0818Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.0828Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.0834Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.0844Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0817Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0789Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0767Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0764Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0751Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0741Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0734Epoch 6/15: [================              ] 26/47 batches, loss: 0.0730Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0749Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0762Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0780Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0783Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0796Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0785Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0794Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0785Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0778Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0765Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0775Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0771Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0783Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0774Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0779Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0791Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0793Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0792Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0796Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0794Epoch 6/15: [==============================] 47/47 batches, loss: 0.0786
[2025-05-07 22:10:47,742][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0786
[2025-05-07 22:10:48,023][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0380, Metrics: {'mse': 0.04041922464966774, 'rmse': 0.20104532983799384, 'r2': -1.0082974433898926}
[2025-05-07 22:10:48,024][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:10:48,024][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-07 22:10:48,024][src.training.lm_trainer][INFO] - Training completed in 12.27 seconds
[2025-05-07 22:10:48,024][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:10:50,244][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.027757396921515465, 'rmse': 0.16660551287852232, 'r2': -0.27294719219207764}
[2025-05-07 22:10:50,244][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03297153115272522, 'rmse': 0.18158064641564975, 'r2': -0.6382462978363037}
[2025-05-07 22:10:50,244][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04875278100371361, 'rmse': 0.22080031930165683, 'r2': -1.1185929775238037}
[2025-05-07 22:10:51,928][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/ko/ko/model.pt
[2025-05-07 22:10:51,930][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▇▆▅▅
wandb:       train_loss █▄▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▃▃▄▂
wandb:          val_mse █▁▃▃▄▂
wandb:           val_r2 ▁█▆▆▅▇
wandb:         val_rmse █▁▃▃▄▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03062
wandb:     best_val_mse 0.03297
wandb:      best_val_r2 -0.63825
wandb:    best_val_rmse 0.18158
wandb: early_stop_epoch 6
wandb:            epoch 6
wandb:   final_test_mse 0.04875
wandb:    final_test_r2 -1.11859
wandb:  final_test_rmse 0.2208
wandb:  final_train_mse 0.02776
wandb:   final_train_r2 -0.27295
wandb: final_train_rmse 0.16661
wandb:    final_val_mse 0.03297
wandb:     final_val_r2 -0.63825
wandb:   final_val_rmse 0.18158
wandb:    learning_rate 0.0001
wandb:       train_loss 0.07856
wandb:       train_time 12.26924
wandb:         val_loss 0.03796
wandb:          val_mse 0.04042
wandb:           val_r2 -1.0083
wandb:         val_rmse 0.20105
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_221015-5zbkj9fn
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_221015-5zbkj9fn/logs
Experiment probe_layer2_avg_links_len_control2_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_avg_links_len_control3_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_control3_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control3/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:11:12,934][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control3/layer2/ko
experiment_name: probe_layer2_avg_links_len_control3_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:11:12,935][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:11:12,935][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 22:11:12,935][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:11:12,935][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:11:12,939][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:11:12,939][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 22:11:12,939][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:11:16,620][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:11:18,923][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:11:18,923][src.data.datasets][INFO] - Loading 'control_avg_links_len_seed3' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:11:19,059][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_links_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:17:58 2025).
[2025-05-07 22:11:19,129][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_links_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:17:58 2025).
[2025-05-07 22:11:19,334][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:11:19,340][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:11:19,340][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:11:19,342][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:11:19,456][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:11:19,539][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:11:19,595][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:11:19,596][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:11:19,597][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:11:19,598][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:11:19,706][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:11:19,752][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:11:19,776][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:11:19,777][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:11:19,778][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:11:19,779][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:11:19,780][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:11:19,780][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 22:11:19,780][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 22:11:19,780][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 22:11:19,780][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:11:19,780][src.data.datasets][INFO] -   Mean: 0.1923, Std: 0.1477
[2025-05-07 22:11:19,780][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:11:19,781][src.data.datasets][INFO] - Sample label: 0.11100000143051147
[2025-05-07 22:11:19,781][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:11:19,781][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 22:11:19,781][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 22:11:19,781][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 22:11:19,781][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.5940
[2025-05-07 22:11:19,781][src.data.datasets][INFO] -   Mean: 0.2139, Std: 0.1419
[2025-05-07 22:11:19,781][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:11:19,781][src.data.datasets][INFO] - Sample label: 0.3160000145435333
[2025-05-07 22:11:19,781][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:11:19,782][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 22:11:19,782][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 22:11:19,782][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 22:11:19,782][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7720
[2025-05-07 22:11:19,782][src.data.datasets][INFO] -   Mean: 0.2570, Std: 0.1517
[2025-05-07 22:11:19,782][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:11:19,782][src.data.datasets][INFO] - Sample label: 0.1589999943971634
[2025-05-07 22:11:19,782][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:11:19,782][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:11:19,783][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:11:19,783][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 22:11:19,783][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:11:25,905][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:11:25,906][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:11:25,906][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:11:25,906][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:11:25,909][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:11:25,909][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:11:25,909][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:11:25,909][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:11:25,909][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:11:25,910][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:11:25,910][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.4015Epoch 1/15: [=                             ] 2/47 batches, loss: 0.5460Epoch 1/15: [=                             ] 3/47 batches, loss: 0.5099Epoch 1/15: [==                            ] 4/47 batches, loss: 0.5017Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4742Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4230Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4079Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4090Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4234Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4235Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4110Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4013Epoch 1/15: [========                      ] 13/47 batches, loss: 0.3941Epoch 1/15: [========                      ] 14/47 batches, loss: 0.3972Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.3845Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.3941Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.3843Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4096Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4062Epoch 1/15: [============                  ] 20/47 batches, loss: 0.3953Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4030Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4034Epoch 1/15: [==============                ] 23/47 batches, loss: 0.3935Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3823Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3804Epoch 1/15: [================              ] 26/47 batches, loss: 0.3742Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3757Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3677Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3737Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3665Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3586Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3515Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3491Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3500Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3446Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3431Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3387Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3368Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3326Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3336Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3317Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3292Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3285Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3261Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3255Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3205Epoch 1/15: [==============================] 47/47 batches, loss: 0.3226
[2025-05-07 22:11:31,469][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3226
[2025-05-07 22:11:31,714][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0834, Metrics: {'mse': 0.0882822647690773, 'rmse': 0.2971233157614483, 'r2': -3.386453628540039}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2674Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2334Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2511Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2071Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2207Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2349Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2236Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2087Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2131Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2067Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2068Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2052Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2060Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2077Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2070Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2060Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2012Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2122Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2103Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2070Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2058Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2040Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2010Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2009Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2001Epoch 2/15: [================              ] 26/47 batches, loss: 0.1985Epoch 2/15: [=================             ] 27/47 batches, loss: 0.1947Epoch 2/15: [=================             ] 28/47 batches, loss: 0.1922Epoch 2/15: [==================            ] 29/47 batches, loss: 0.1901Epoch 2/15: [===================           ] 30/47 batches, loss: 0.1888Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1882Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1886Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1888Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1890Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1863Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1839Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1844Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1810Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1794Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1788Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1777Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1755Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1726Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1703Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1706Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1696Epoch 2/15: [==============================] 47/47 batches, loss: 0.1679
[2025-05-07 22:11:33,628][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1679
[2025-05-07 22:11:33,888][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0303, Metrics: {'mse': 0.03260878100991249, 'rmse': 0.1805790159733752, 'r2': -0.6202223300933838}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1769Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1256Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1253Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1251Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1312Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1473Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1458Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1413Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1451Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1420Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1386Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1434Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1446Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1410Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1445Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1428Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1452Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1417Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1383Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1368Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1383Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1357Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1359Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1365Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1341Epoch 3/15: [================              ] 26/47 batches, loss: 0.1325Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1326Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1302Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1286Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1270Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1279Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1273Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1264Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1271Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1277Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1263Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1257Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1247Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1230Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1228Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1218Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1213Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1214Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1235Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1240Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1245Epoch 3/15: [==============================] 47/47 batches, loss: 0.1237
[2025-05-07 22:11:35,795][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1237
[2025-05-07 22:11:36,059][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0370, Metrics: {'mse': 0.039658211171627045, 'rmse': 0.19914369478250382, 'r2': -0.9704853296279907}
[2025-05-07 22:11:36,060][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1457Epoch 4/15: [=                             ] 2/47 batches, loss: 0.0910Epoch 4/15: [=                             ] 3/47 batches, loss: 0.0725Epoch 4/15: [==                            ] 4/47 batches, loss: 0.0823Epoch 4/15: [===                           ] 5/47 batches, loss: 0.0849Epoch 4/15: [===                           ] 6/47 batches, loss: 0.0894Epoch 4/15: [====                          ] 7/47 batches, loss: 0.0920Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.0947Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.0927Epoch 4/15: [======                        ] 10/47 batches, loss: 0.0989Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1099Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1112Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1100Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1069Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1054Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1055Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1052Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1025Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1047Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1087Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1097Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1091Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1097Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1098Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1112Epoch 4/15: [================              ] 26/47 batches, loss: 0.1108Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1100Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1116Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1121Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1116Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1135Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1129Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1117Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1123Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1119Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1104Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1120Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1117Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1103Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1094Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1103Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1103Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1093Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1090Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1080Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1076Epoch 4/15: [==============================] 47/47 batches, loss: 0.1090
[2025-05-07 22:11:37,537][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1090
[2025-05-07 22:11:37,892][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0277, Metrics: {'mse': 0.02987045794725418, 'rmse': 0.17283072049625373, 'r2': -0.48416411876678467}
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.1348Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1323Epoch 5/15: [=                             ] 3/47 batches, loss: 0.1191Epoch 5/15: [==                            ] 4/47 batches, loss: 0.1034Epoch 5/15: [===                           ] 5/47 batches, loss: 0.1164Epoch 5/15: [===                           ] 6/47 batches, loss: 0.1117Epoch 5/15: [====                          ] 7/47 batches, loss: 0.1085Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.1054Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.1051Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0988Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0968Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0979Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0954Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0944Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0951Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0928Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0904Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0898Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0888Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0888Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0881Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0912Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0895Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0892Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0894Epoch 5/15: [================              ] 26/47 batches, loss: 0.0892Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0880Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0862Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0850Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0851Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0843Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0841Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0839Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0839Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0845Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0848Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0838Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0855Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0857Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0845Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0839Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0837Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0834Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0834Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0834Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0832Epoch 5/15: [==============================] 47/47 batches, loss: 0.0817
[2025-05-07 22:11:39,839][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0817
[2025-05-07 22:11:40,166][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0393, Metrics: {'mse': 0.041844312101602554, 'rmse': 0.20455882308422327, 'r2': -1.0791053771972656}
[2025-05-07 22:11:40,166][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.0826Epoch 6/15: [=                             ] 2/47 batches, loss: 0.1026Epoch 6/15: [=                             ] 3/47 batches, loss: 0.1044Epoch 6/15: [==                            ] 4/47 batches, loss: 0.0987Epoch 6/15: [===                           ] 5/47 batches, loss: 0.1121Epoch 6/15: [===                           ] 6/47 batches, loss: 0.1055Epoch 6/15: [====                          ] 7/47 batches, loss: 0.1045Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.1069Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.0978Epoch 6/15: [======                        ] 10/47 batches, loss: 0.0981Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.0944Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.1002Epoch 6/15: [========                      ] 13/47 batches, loss: 0.0960Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0932Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0893Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.0894Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.0898Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.0913Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0881Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0872Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0881Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0867Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0865Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0842Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0830Epoch 6/15: [================              ] 26/47 batches, loss: 0.0819Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0822Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0812Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0818Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0811Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0820Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0811Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0817Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0818Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0818Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0808Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0803Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0799Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0799Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0792Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0800Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0805Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0805Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0803Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0801Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0798Epoch 6/15: [==============================] 47/47 batches, loss: 0.0804
[2025-05-07 22:11:41,623][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0804
[2025-05-07 22:11:41,890][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0356, Metrics: {'mse': 0.03811590373516083, 'rmse': 0.19523294736073835, 'r2': -0.8938531875610352}
[2025-05-07 22:11:41,890][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/47 batches, loss: 0.0362Epoch 7/15: [=                             ] 2/47 batches, loss: 0.0546Epoch 7/15: [=                             ] 3/47 batches, loss: 0.0618Epoch 7/15: [==                            ] 4/47 batches, loss: 0.0684Epoch 7/15: [===                           ] 5/47 batches, loss: 0.0738Epoch 7/15: [===                           ] 6/47 batches, loss: 0.0760Epoch 7/15: [====                          ] 7/47 batches, loss: 0.0750Epoch 7/15: [=====                         ] 8/47 batches, loss: 0.0796Epoch 7/15: [=====                         ] 9/47 batches, loss: 0.0781Epoch 7/15: [======                        ] 10/47 batches, loss: 0.0780Epoch 7/15: [=======                       ] 11/47 batches, loss: 0.0725Epoch 7/15: [=======                       ] 12/47 batches, loss: 0.0713Epoch 7/15: [========                      ] 13/47 batches, loss: 0.0689Epoch 7/15: [========                      ] 14/47 batches, loss: 0.0666Epoch 7/15: [=========                     ] 15/47 batches, loss: 0.0678Epoch 7/15: [==========                    ] 16/47 batches, loss: 0.0681Epoch 7/15: [==========                    ] 17/47 batches, loss: 0.0671Epoch 7/15: [===========                   ] 18/47 batches, loss: 0.0683Epoch 7/15: [============                  ] 19/47 batches, loss: 0.0712Epoch 7/15: [============                  ] 20/47 batches, loss: 0.0694Epoch 7/15: [=============                 ] 21/47 batches, loss: 0.0686Epoch 7/15: [==============                ] 22/47 batches, loss: 0.0695Epoch 7/15: [==============                ] 23/47 batches, loss: 0.0694Epoch 7/15: [===============               ] 24/47 batches, loss: 0.0680Epoch 7/15: [===============               ] 25/47 batches, loss: 0.0695Epoch 7/15: [================              ] 26/47 batches, loss: 0.0694Epoch 7/15: [=================             ] 27/47 batches, loss: 0.0688Epoch 7/15: [=================             ] 28/47 batches, loss: 0.0679Epoch 7/15: [==================            ] 29/47 batches, loss: 0.0669Epoch 7/15: [===================           ] 30/47 batches, loss: 0.0678Epoch 7/15: [===================           ] 31/47 batches, loss: 0.0676Epoch 7/15: [====================          ] 32/47 batches, loss: 0.0671Epoch 7/15: [=====================         ] 33/47 batches, loss: 0.0662Epoch 7/15: [=====================         ] 34/47 batches, loss: 0.0665Epoch 7/15: [======================        ] 35/47 batches, loss: 0.0668Epoch 7/15: [======================        ] 36/47 batches, loss: 0.0679Epoch 7/15: [=======================       ] 37/47 batches, loss: 0.0671Epoch 7/15: [========================      ] 38/47 batches, loss: 0.0669Epoch 7/15: [========================      ] 39/47 batches, loss: 0.0665Epoch 7/15: [=========================     ] 40/47 batches, loss: 0.0679Epoch 7/15: [==========================    ] 41/47 batches, loss: 0.0669Epoch 7/15: [==========================    ] 42/47 batches, loss: 0.0682Epoch 7/15: [===========================   ] 43/47 batches, loss: 0.0679Epoch 7/15: [============================  ] 44/47 batches, loss: 0.0679Epoch 7/15: [============================  ] 45/47 batches, loss: 0.0675Epoch 7/15: [============================= ] 46/47 batches, loss: 0.0680Epoch 7/15: [==============================] 47/47 batches, loss: 0.0677
[2025-05-07 22:11:43,390][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0677
[2025-05-07 22:11:43,698][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0260, Metrics: {'mse': 0.02806815132498741, 'rmse': 0.1675355225765193, 'r2': -0.39461350440979004}
Epoch 8/15: [Epoch 8/15: [                              ] 1/47 batches, loss: 0.0283Epoch 8/15: [=                             ] 2/47 batches, loss: 0.0453Epoch 8/15: [=                             ] 3/47 batches, loss: 0.0609Epoch 8/15: [==                            ] 4/47 batches, loss: 0.0660Epoch 8/15: [===                           ] 5/47 batches, loss: 0.0665Epoch 8/15: [===                           ] 6/47 batches, loss: 0.0598Epoch 8/15: [====                          ] 7/47 batches, loss: 0.0616Epoch 8/15: [=====                         ] 8/47 batches, loss: 0.0622Epoch 8/15: [=====                         ] 9/47 batches, loss: 0.0624Epoch 8/15: [======                        ] 10/47 batches, loss: 0.0635Epoch 8/15: [=======                       ] 11/47 batches, loss: 0.0651Epoch 8/15: [=======                       ] 12/47 batches, loss: 0.0671Epoch 8/15: [========                      ] 13/47 batches, loss: 0.0651Epoch 8/15: [========                      ] 14/47 batches, loss: 0.0634Epoch 8/15: [=========                     ] 15/47 batches, loss: 0.0615Epoch 8/15: [==========                    ] 16/47 batches, loss: 0.0612Epoch 8/15: [==========                    ] 17/47 batches, loss: 0.0633Epoch 8/15: [===========                   ] 18/47 batches, loss: 0.0631Epoch 8/15: [============                  ] 19/47 batches, loss: 0.0632Epoch 8/15: [============                  ] 20/47 batches, loss: 0.0613Epoch 8/15: [=============                 ] 21/47 batches, loss: 0.0636Epoch 8/15: [==============                ] 22/47 batches, loss: 0.0621Epoch 8/15: [==============                ] 23/47 batches, loss: 0.0617Epoch 8/15: [===============               ] 24/47 batches, loss: 0.0615Epoch 8/15: [===============               ] 25/47 batches, loss: 0.0611Epoch 8/15: [================              ] 26/47 batches, loss: 0.0611Epoch 8/15: [=================             ] 27/47 batches, loss: 0.0600Epoch 8/15: [=================             ] 28/47 batches, loss: 0.0630Epoch 8/15: [==================            ] 29/47 batches, loss: 0.0629Epoch 8/15: [===================           ] 30/47 batches, loss: 0.0624Epoch 8/15: [===================           ] 31/47 batches, loss: 0.0624Epoch 8/15: [====================          ] 32/47 batches, loss: 0.0631Epoch 8/15: [=====================         ] 33/47 batches, loss: 0.0630Epoch 8/15: [=====================         ] 34/47 batches, loss: 0.0635Epoch 8/15: [======================        ] 35/47 batches, loss: 0.0632Epoch 8/15: [======================        ] 36/47 batches, loss: 0.0627Epoch 8/15: [=======================       ] 37/47 batches, loss: 0.0621Epoch 8/15: [========================      ] 38/47 batches, loss: 0.0620Epoch 8/15: [========================      ] 39/47 batches, loss: 0.0609Epoch 8/15: [=========================     ] 40/47 batches, loss: 0.0598Epoch 8/15: [==========================    ] 41/47 batches, loss: 0.0597Epoch 8/15: [==========================    ] 42/47 batches, loss: 0.0588Epoch 8/15: [===========================   ] 43/47 batches, loss: 0.0586Epoch 8/15: [============================  ] 44/47 batches, loss: 0.0581Epoch 8/15: [============================  ] 45/47 batches, loss: 0.0581Epoch 8/15: [============================= ] 46/47 batches, loss: 0.0579Epoch 8/15: [==============================] 47/47 batches, loss: 0.0588
[2025-05-07 22:11:45,576][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0588
[2025-05-07 22:11:45,825][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0241, Metrics: {'mse': 0.026035018265247345, 'rmse': 0.16135370545868274, 'r2': -0.2935938835144043}
Epoch 9/15: [Epoch 9/15: [                              ] 1/47 batches, loss: 0.0578Epoch 9/15: [=                             ] 2/47 batches, loss: 0.0528Epoch 9/15: [=                             ] 3/47 batches, loss: 0.0514Epoch 9/15: [==                            ] 4/47 batches, loss: 0.0483Epoch 9/15: [===                           ] 5/47 batches, loss: 0.0501Epoch 9/15: [===                           ] 6/47 batches, loss: 0.0477Epoch 9/15: [====                          ] 7/47 batches, loss: 0.0534Epoch 9/15: [=====                         ] 8/47 batches, loss: 0.0544Epoch 9/15: [=====                         ] 9/47 batches, loss: 0.0554Epoch 9/15: [======                        ] 10/47 batches, loss: 0.0534Epoch 9/15: [=======                       ] 11/47 batches, loss: 0.0561Epoch 9/15: [=======                       ] 12/47 batches, loss: 0.0568Epoch 9/15: [========                      ] 13/47 batches, loss: 0.0552Epoch 9/15: [========                      ] 14/47 batches, loss: 0.0580Epoch 9/15: [=========                     ] 15/47 batches, loss: 0.0582Epoch 9/15: [==========                    ] 16/47 batches, loss: 0.0594Epoch 9/15: [==========                    ] 17/47 batches, loss: 0.0593Epoch 9/15: [===========                   ] 18/47 batches, loss: 0.0586Epoch 9/15: [============                  ] 19/47 batches, loss: 0.0570Epoch 9/15: [============                  ] 20/47 batches, loss: 0.0571Epoch 9/15: [=============                 ] 21/47 batches, loss: 0.0577Epoch 9/15: [==============                ] 22/47 batches, loss: 0.0576Epoch 9/15: [==============                ] 23/47 batches, loss: 0.0581Epoch 9/15: [===============               ] 24/47 batches, loss: 0.0597Epoch 9/15: [===============               ] 25/47 batches, loss: 0.0607Epoch 9/15: [================              ] 26/47 batches, loss: 0.0630Epoch 9/15: [=================             ] 27/47 batches, loss: 0.0627Epoch 9/15: [=================             ] 28/47 batches, loss: 0.0620Epoch 9/15: [==================            ] 29/47 batches, loss: 0.0616Epoch 9/15: [===================           ] 30/47 batches, loss: 0.0608Epoch 9/15: [===================           ] 31/47 batches, loss: 0.0604Epoch 9/15: [====================          ] 32/47 batches, loss: 0.0597Epoch 9/15: [=====================         ] 33/47 batches, loss: 0.0597Epoch 9/15: [=====================         ] 34/47 batches, loss: 0.0593Epoch 9/15: [======================        ] 35/47 batches, loss: 0.0604Epoch 9/15: [======================        ] 36/47 batches, loss: 0.0597Epoch 9/15: [=======================       ] 37/47 batches, loss: 0.0607Epoch 9/15: [========================      ] 38/47 batches, loss: 0.0611Epoch 9/15: [========================      ] 39/47 batches, loss: 0.0604Epoch 9/15: [=========================     ] 40/47 batches, loss: 0.0611Epoch 9/15: [==========================    ] 41/47 batches, loss: 0.0607Epoch 9/15: [==========================    ] 42/47 batches, loss: 0.0624Epoch 9/15: [===========================   ] 43/47 batches, loss: 0.0625Epoch 9/15: [============================  ] 44/47 batches, loss: 0.0626Epoch 9/15: [============================  ] 45/47 batches, loss: 0.0630Epoch 9/15: [============================= ] 46/47 batches, loss: 0.0625Epoch 9/15: [==============================] 47/47 batches, loss: 0.0621
[2025-05-07 22:11:47,797][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0621
[2025-05-07 22:11:48,081][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0290, Metrics: {'mse': 0.03102635033428669, 'rmse': 0.17614298264275727, 'r2': -0.5415966510772705}
[2025-05-07 22:11:48,082][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/47 batches, loss: 0.0379Epoch 10/15: [=                             ] 2/47 batches, loss: 0.0392Epoch 10/15: [=                             ] 3/47 batches, loss: 0.0397Epoch 10/15: [==                            ] 4/47 batches, loss: 0.0417Epoch 10/15: [===                           ] 5/47 batches, loss: 0.0427Epoch 10/15: [===                           ] 6/47 batches, loss: 0.0402Epoch 10/15: [====                          ] 7/47 batches, loss: 0.0412Epoch 10/15: [=====                         ] 8/47 batches, loss: 0.0401Epoch 10/15: [=====                         ] 9/47 batches, loss: 0.0445Epoch 10/15: [======                        ] 10/47 batches, loss: 0.0467Epoch 10/15: [=======                       ] 11/47 batches, loss: 0.0506Epoch 10/15: [=======                       ] 12/47 batches, loss: 0.0491Epoch 10/15: [========                      ] 13/47 batches, loss: 0.0501Epoch 10/15: [========                      ] 14/47 batches, loss: 0.0497Epoch 10/15: [=========                     ] 15/47 batches, loss: 0.0494Epoch 10/15: [==========                    ] 16/47 batches, loss: 0.0479Epoch 10/15: [==========                    ] 17/47 batches, loss: 0.0474Epoch 10/15: [===========                   ] 18/47 batches, loss: 0.0472Epoch 10/15: [============                  ] 19/47 batches, loss: 0.0467Epoch 10/15: [============                  ] 20/47 batches, loss: 0.0478Epoch 10/15: [=============                 ] 21/47 batches, loss: 0.0483Epoch 10/15: [==============                ] 22/47 batches, loss: 0.0478Epoch 10/15: [==============                ] 23/47 batches, loss: 0.0470Epoch 10/15: [===============               ] 24/47 batches, loss: 0.0476Epoch 10/15: [===============               ] 25/47 batches, loss: 0.0475Epoch 10/15: [================              ] 26/47 batches, loss: 0.0466Epoch 10/15: [=================             ] 27/47 batches, loss: 0.0467Epoch 10/15: [=================             ] 28/47 batches, loss: 0.0474Epoch 10/15: [==================            ] 29/47 batches, loss: 0.0470Epoch 10/15: [===================           ] 30/47 batches, loss: 0.0473Epoch 10/15: [===================           ] 31/47 batches, loss: 0.0466Epoch 10/15: [====================          ] 32/47 batches, loss: 0.0468Epoch 10/15: [=====================         ] 33/47 batches, loss: 0.0471Epoch 10/15: [=====================         ] 34/47 batches, loss: 0.0473Epoch 10/15: [======================        ] 35/47 batches, loss: 0.0498Epoch 10/15: [======================        ] 36/47 batches, loss: 0.0494Epoch 10/15: [=======================       ] 37/47 batches, loss: 0.0501Epoch 10/15: [========================      ] 38/47 batches, loss: 0.0509Epoch 10/15: [========================      ] 39/47 batches, loss: 0.0505Epoch 10/15: [=========================     ] 40/47 batches, loss: 0.0501Epoch 10/15: [==========================    ] 41/47 batches, loss: 0.0502Epoch 10/15: [==========================    ] 42/47 batches, loss: 0.0502Epoch 10/15: [===========================   ] 43/47 batches, loss: 0.0502Epoch 10/15: [============================  ] 44/47 batches, loss: 0.0505Epoch 10/15: [============================  ] 45/47 batches, loss: 0.0504Epoch 10/15: [============================= ] 46/47 batches, loss: 0.0504Epoch 10/15: [==============================] 47/47 batches, loss: 0.0514
[2025-05-07 22:11:49,643][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0514
[2025-05-07 22:11:50,000][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0251, Metrics: {'mse': 0.026988552883267403, 'rmse': 0.16428193109185016, 'r2': -0.34097182750701904}
[2025-05-07 22:11:50,001][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/47 batches, loss: 0.0306Epoch 11/15: [=                             ] 2/47 batches, loss: 0.0643Epoch 11/15: [=                             ] 3/47 batches, loss: 0.0605Epoch 11/15: [==                            ] 4/47 batches, loss: 0.0575Epoch 11/15: [===                           ] 5/47 batches, loss: 0.0563Epoch 11/15: [===                           ] 6/47 batches, loss: 0.0516Epoch 11/15: [====                          ] 7/47 batches, loss: 0.0483Epoch 11/15: [=====                         ] 8/47 batches, loss: 0.0570Epoch 11/15: [=====                         ] 9/47 batches, loss: 0.0570Epoch 11/15: [======                        ] 10/47 batches, loss: 0.0557Epoch 11/15: [=======                       ] 11/47 batches, loss: 0.0535Epoch 11/15: [=======                       ] 12/47 batches, loss: 0.0513Epoch 11/15: [========                      ] 13/47 batches, loss: 0.0530Epoch 11/15: [========                      ] 14/47 batches, loss: 0.0525Epoch 11/15: [=========                     ] 15/47 batches, loss: 0.0527Epoch 11/15: [==========                    ] 16/47 batches, loss: 0.0520Epoch 11/15: [==========                    ] 17/47 batches, loss: 0.0529Epoch 11/15: [===========                   ] 18/47 batches, loss: 0.0512Epoch 11/15: [============                  ] 19/47 batches, loss: 0.0525Epoch 11/15: [============                  ] 20/47 batches, loss: 0.0529Epoch 11/15: [=============                 ] 21/47 batches, loss: 0.0517Epoch 11/15: [==============                ] 22/47 batches, loss: 0.0508Epoch 11/15: [==============                ] 23/47 batches, loss: 0.0519Epoch 11/15: [===============               ] 24/47 batches, loss: 0.0528Epoch 11/15: [===============               ] 25/47 batches, loss: 0.0516Epoch 11/15: [================              ] 26/47 batches, loss: 0.0531Epoch 11/15: [=================             ] 27/47 batches, loss: 0.0544Epoch 11/15: [=================             ] 28/47 batches, loss: 0.0548Epoch 11/15: [==================            ] 29/47 batches, loss: 0.0535Epoch 11/15: [===================           ] 30/47 batches, loss: 0.0529Epoch 11/15: [===================           ] 31/47 batches, loss: 0.0532Epoch 11/15: [====================          ] 32/47 batches, loss: 0.0524Epoch 11/15: [=====================         ] 33/47 batches, loss: 0.0518Epoch 11/15: [=====================         ] 34/47 batches, loss: 0.0513Epoch 11/15: [======================        ] 35/47 batches, loss: 0.0510Epoch 11/15: [======================        ] 36/47 batches, loss: 0.0512Epoch 11/15: [=======================       ] 37/47 batches, loss: 0.0510Epoch 11/15: [========================      ] 38/47 batches, loss: 0.0503Epoch 11/15: [========================      ] 39/47 batches, loss: 0.0501Epoch 11/15: [=========================     ] 40/47 batches, loss: 0.0500Epoch 11/15: [==========================    ] 41/47 batches, loss: 0.0498Epoch 11/15: [==========================    ] 42/47 batches, loss: 0.0496Epoch 11/15: [===========================   ] 43/47 batches, loss: 0.0494Epoch 11/15: [============================  ] 44/47 batches, loss: 0.0494Epoch 11/15: [============================  ] 45/47 batches, loss: 0.0490Epoch 11/15: [============================= ] 46/47 batches, loss: 0.0486Epoch 11/15: [==============================] 47/47 batches, loss: 0.0482
[2025-05-07 22:11:51,543][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0482
[2025-05-07 22:11:51,861][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0255, Metrics: {'mse': 0.027403729036450386, 'rmse': 0.1655407171557813, 'r2': -0.36160051822662354}
[2025-05-07 22:11:51,862][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/47 batches, loss: 0.0187Epoch 12/15: [=                             ] 2/47 batches, loss: 0.0198Epoch 12/15: [=                             ] 3/47 batches, loss: 0.0316Epoch 12/15: [==                            ] 4/47 batches, loss: 0.0349Epoch 12/15: [===                           ] 5/47 batches, loss: 0.0395Epoch 12/15: [===                           ] 6/47 batches, loss: 0.0409Epoch 12/15: [====                          ] 7/47 batches, loss: 0.0389Epoch 12/15: [=====                         ] 8/47 batches, loss: 0.0405Epoch 12/15: [=====                         ] 9/47 batches, loss: 0.0381Epoch 12/15: [======                        ] 10/47 batches, loss: 0.0364Epoch 12/15: [=======                       ] 11/47 batches, loss: 0.0409Epoch 12/15: [=======                       ] 12/47 batches, loss: 0.0393Epoch 12/15: [========                      ] 13/47 batches, loss: 0.0447Epoch 12/15: [========                      ] 14/47 batches, loss: 0.0457Epoch 12/15: [=========                     ] 15/47 batches, loss: 0.0493Epoch 12/15: [==========                    ] 16/47 batches, loss: 0.0506Epoch 12/15: [==========                    ] 17/47 batches, loss: 0.0500Epoch 12/15: [===========                   ] 18/47 batches, loss: 0.0505Epoch 12/15: [============                  ] 19/47 batches, loss: 0.0514Epoch 12/15: [============                  ] 20/47 batches, loss: 0.0504Epoch 12/15: [=============                 ] 21/47 batches, loss: 0.0495Epoch 12/15: [==============                ] 22/47 batches, loss: 0.0484Epoch 12/15: [==============                ] 23/47 batches, loss: 0.0482Epoch 12/15: [===============               ] 24/47 batches, loss: 0.0483Epoch 12/15: [===============               ] 25/47 batches, loss: 0.0493Epoch 12/15: [================              ] 26/47 batches, loss: 0.0488Epoch 12/15: [=================             ] 27/47 batches, loss: 0.0481Epoch 12/15: [=================             ] 28/47 batches, loss: 0.0479Epoch 12/15: [==================            ] 29/47 batches, loss: 0.0482Epoch 12/15: [===================           ] 30/47 batches, loss: 0.0475Epoch 12/15: [===================           ] 31/47 batches, loss: 0.0468Epoch 12/15: [====================          ] 32/47 batches, loss: 0.0468Epoch 12/15: [=====================         ] 33/47 batches, loss: 0.0468Epoch 12/15: [=====================         ] 34/47 batches, loss: 0.0467Epoch 12/15: [======================        ] 35/47 batches, loss: 0.0464Epoch 12/15: [======================        ] 36/47 batches, loss: 0.0460Epoch 12/15: [=======================       ] 37/47 batches, loss: 0.0460Epoch 12/15: [========================      ] 38/47 batches, loss: 0.0459Epoch 12/15: [========================      ] 39/47 batches, loss: 0.0459Epoch 12/15: [=========================     ] 40/47 batches, loss: 0.0454Epoch 12/15: [==========================    ] 41/47 batches, loss: 0.0453Epoch 12/15: [==========================    ] 42/47 batches, loss: 0.0454Epoch 12/15: [===========================   ] 43/47 batches, loss: 0.0455Epoch 12/15: [============================  ] 44/47 batches, loss: 0.0448Epoch 12/15: [============================  ] 45/47 batches, loss: 0.0449Epoch 12/15: [============================= ] 46/47 batches, loss: 0.0446Epoch 12/15: [==============================] 47/47 batches, loss: 0.0440
[2025-05-07 22:11:53,352][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0440
[2025-05-07 22:11:53,671][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0312, Metrics: {'mse': 0.033126842230558395, 'rmse': 0.1820078081582172, 'r2': -0.6459630727767944}
[2025-05-07 22:11:53,672][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:11:53,672][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 12
[2025-05-07 22:11:53,672][src.training.lm_trainer][INFO] - Training completed in 24.60 seconds
[2025-05-07 22:11:53,672][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:11:56,106][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.02235163189470768, 'rmse': 0.14950462164999342, 'r2': -0.025040388107299805}
[2025-05-07 22:11:56,107][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.026035018265247345, 'rmse': 0.16135370545868274, 'r2': -0.2935938835144043}
[2025-05-07 22:11:56,107][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03388763219118118, 'rmse': 0.18408593697287468, 'r2': -0.4726155996322632}
[2025-05-07 22:11:57,748][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control3/layer2/ko/ko/model.pt
[2025-05-07 22:11:57,749][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▁▁▁
wandb:     best_val_mse █▂▁▁▁
wandb:      best_val_r2 ▁▇███
wandb:    best_val_rmse █▂▂▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▇▇▇▆▇▇▇▇▇▇
wandb:       train_loss █▄▃▃▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▂▃▁▃▂▁▁▂▁▁▂
wandb:          val_mse █▂▃▁▃▂▁▁▂▁▁▂
wandb:           val_r2 ▁▇▆█▆▇██▇██▇
wandb:         val_rmse █▂▃▂▃▃▁▁▂▁▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02406
wandb:     best_val_mse 0.02604
wandb:      best_val_r2 -0.29359
wandb:    best_val_rmse 0.16135
wandb: early_stop_epoch 12
wandb:            epoch 12
wandb:   final_test_mse 0.03389
wandb:    final_test_r2 -0.47262
wandb:  final_test_rmse 0.18409
wandb:  final_train_mse 0.02235
wandb:   final_train_r2 -0.02504
wandb: final_train_rmse 0.1495
wandb:    final_val_mse 0.02604
wandb:     final_val_r2 -0.29359
wandb:   final_val_rmse 0.16135
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04399
wandb:       train_time 24.60115
wandb:         val_loss 0.03119
wandb:          val_mse 0.03313
wandb:           val_r2 -0.64596
wandb:         val_rmse 0.18201
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_221112-6gj45rgr
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_221112-6gj45rgr/logs
Experiment probe_layer2_avg_links_len_control3_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control3/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_avg_max_depth_control1_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_max_depth_control1_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:12:26,268][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer2/ko
experiment_name: probe_layer2_avg_max_depth_control1_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:12:26,268][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:12:26,268][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 22:12:26,268][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:12:26,268][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:12:26,272][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:12:26,272][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 22:12:26,273][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:12:29,219][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:12:31,519][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:12:31,519][src.data.datasets][INFO] - Loading 'control_avg_max_depth_seed1' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:12:31,617][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_max_depth_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:19:23 2025).
[2025-05-07 22:12:31,678][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_max_depth_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:19:23 2025).
[2025-05-07 22:12:31,919][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:12:31,925][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:12:31,925][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:12:31,926][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:12:31,993][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:12:32,090][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:12:32,127][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:12:32,128][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:12:32,128][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:12:32,129][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:12:32,198][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:12:32,249][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:12:32,276][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:12:32,277][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:12:32,277][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:12:32,279][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:12:32,280][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:12:32,280][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 22:12:32,280][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 22:12:32,280][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 22:12:32,280][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:12:32,280][src.data.datasets][INFO] -   Mean: 0.2234, Std: 0.1709
[2025-05-07 22:12:32,280][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:12:32,281][src.data.datasets][INFO] - Sample label: 0.10000000149011612
[2025-05-07 22:12:32,281][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:12:32,281][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 22:12:32,281][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 22:12:32,281][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 22:12:32,281][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8890
[2025-05-07 22:12:32,281][src.data.datasets][INFO] -   Mean: 0.2502, Std: 0.1830
[2025-05-07 22:12:32,281][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:12:32,281][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 22:12:32,282][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:12:32,282][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 22:12:32,282][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 22:12:32,282][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 22:12:32,282][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8890
[2025-05-07 22:12:32,282][src.data.datasets][INFO] -   Mean: 0.2705, Std: 0.1869
[2025-05-07 22:12:32,282][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:12:32,282][src.data.datasets][INFO] - Sample label: 0.4440000057220459
[2025-05-07 22:12:32,282][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:12:32,282][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:12:32,283][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:12:32,283][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 22:12:32,283][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:12:39,250][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:12:39,250][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:12:39,250][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:12:39,250][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:12:39,253][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:12:39,254][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:12:39,254][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:12:39,254][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:12:39,254][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:12:39,255][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:12:39,255][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.3627Epoch 1/15: [=                             ] 2/47 batches, loss: 0.4523Epoch 1/15: [=                             ] 3/47 batches, loss: 0.4253Epoch 1/15: [==                            ] 4/47 batches, loss: 0.4265Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4274Epoch 1/15: [===                           ] 6/47 batches, loss: 0.3954Epoch 1/15: [====                          ] 7/47 batches, loss: 0.3764Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.3863Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4015Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4015Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.3902Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.3986Epoch 1/15: [========                      ] 13/47 batches, loss: 0.3912Epoch 1/15: [========                      ] 14/47 batches, loss: 0.3873Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.3763Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.3819Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.3745Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.3901Epoch 1/15: [============                  ] 19/47 batches, loss: 0.3849Epoch 1/15: [============                  ] 20/47 batches, loss: 0.3744Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.3831Epoch 1/15: [==============                ] 22/47 batches, loss: 0.3815Epoch 1/15: [==============                ] 23/47 batches, loss: 0.3725Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3630Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3625Epoch 1/15: [================              ] 26/47 batches, loss: 0.3605Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3589Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3525Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3553Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3496Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3430Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3371Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3347Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3352Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3315Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3308Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3260Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3248Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3222Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3266Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3253Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3245Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3224Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3216Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3223Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3173Epoch 1/15: [==============================] 47/47 batches, loss: 0.3210
[2025-05-07 22:12:44,499][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3210
[2025-05-07 22:12:44,745][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0722, Metrics: {'mse': 0.07661682367324829, 'rmse': 0.2767974415944777, 'r2': -1.2869000434875488}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.3169Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2249Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2460Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2321Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2380Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2478Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2419Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2377Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2287Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2230Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2302Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2287Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2326Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2288Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2281Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2217Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2165Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2182Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2140Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2080Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2061Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2045Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2009Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2008Epoch 2/15: [===============               ] 25/47 batches, loss: 0.1994Epoch 2/15: [================              ] 26/47 batches, loss: 0.1959Epoch 2/15: [=================             ] 27/47 batches, loss: 0.1929Epoch 2/15: [=================             ] 28/47 batches, loss: 0.1902Epoch 2/15: [==================            ] 29/47 batches, loss: 0.1868Epoch 2/15: [===================           ] 30/47 batches, loss: 0.1835Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1840Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1832Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1866Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1858Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1829Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1807Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1825Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1801Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1799Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1790Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1782Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1760Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1740Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1724Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1712Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1702Epoch 2/15: [==============================] 47/47 batches, loss: 0.1678
[2025-05-07 22:12:46,632][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1678
[2025-05-07 22:12:46,902][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0448, Metrics: {'mse': 0.046207234263420105, 'rmse': 0.21495868036304117, 'r2': -0.37921833992004395}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1545Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1377Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1163Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1088Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1272Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1408Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1410Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1348Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1353Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1339Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1375Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1440Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1466Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1415Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1404Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1407Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1427Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1439Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1402Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1373Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1399Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1390Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1399Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1402Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1392Epoch 3/15: [================              ] 26/47 batches, loss: 0.1390Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1394Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1376Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1344Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1339Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1340Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1341Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1335Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1327Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1354Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1358Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1361Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1347Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1333Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1329Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1318Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1322Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1325Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1343Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1357Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1354Epoch 3/15: [==============================] 47/47 batches, loss: 0.1357
[2025-05-07 22:12:48,843][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1357
[2025-05-07 22:12:49,100][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0598, Metrics: {'mse': 0.06374665349721909, 'rmse': 0.25248099630906695, 'r2': -0.9027442932128906}
[2025-05-07 22:12:49,101][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.2357Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1518Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1344Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1327Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1227Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1202Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1172Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1373Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1352Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1338Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1412Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1352Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1374Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1309Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1293Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1259Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1248Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1236Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1261Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1289Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1264Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1251Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1239Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1240Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1261Epoch 4/15: [================              ] 26/47 batches, loss: 0.1259Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1247Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1247Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1263Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1242Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1247Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1264Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1237Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1245Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1246Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1245Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1243Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1224Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1204Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1202Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1196Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1201Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1191Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1195Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1177Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1184Epoch 4/15: [==============================] 47/47 batches, loss: 0.1189
[2025-05-07 22:12:50,604][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1189
[2025-05-07 22:12:50,966][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0475, Metrics: {'mse': 0.050473421812057495, 'rmse': 0.22466290706758313, 'r2': -0.506557822227478}
[2025-05-07 22:12:50,967][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.0805Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1050Epoch 5/15: [=                             ] 3/47 batches, loss: 0.1153Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0989Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0839Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0819Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0804Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0799Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0823Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0766Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0774Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0798Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0783Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0798Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0797Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0792Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0794Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0819Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0822Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0863Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0873Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0895Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0897Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0900Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0895Epoch 5/15: [================              ] 26/47 batches, loss: 0.0875Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0886Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0874Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0864Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0888Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0894Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0900Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0915Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0907Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0926Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0921Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0925Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0930Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0926Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0947Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0943Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0943Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0936Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0934Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0937Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0936Epoch 5/15: [==============================] 47/47 batches, loss: 0.0923
[2025-05-07 22:12:52,526][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0923
[2025-05-07 22:12:52,884][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0464, Metrics: {'mse': 0.0493612140417099, 'rmse': 0.22217383743751176, 'r2': -0.47335994243621826}
[2025-05-07 22:12:52,885][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.2045Epoch 6/15: [=                             ] 2/47 batches, loss: 0.1384Epoch 6/15: [=                             ] 3/47 batches, loss: 0.1270Epoch 6/15: [==                            ] 4/47 batches, loss: 0.1168Epoch 6/15: [===                           ] 5/47 batches, loss: 0.1228Epoch 6/15: [===                           ] 6/47 batches, loss: 0.1188Epoch 6/15: [====                          ] 7/47 batches, loss: 0.1137Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.1128Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.1065Epoch 6/15: [======                        ] 10/47 batches, loss: 0.1086Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.1074Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.1092Epoch 6/15: [========                      ] 13/47 batches, loss: 0.1053Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0990Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.1000Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.1004Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.1002Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.1001Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0962Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0940Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0917Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0911Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0906Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0889Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0874Epoch 6/15: [================              ] 26/47 batches, loss: 0.0862Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0851Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0852Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0855Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0845Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0861Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0855Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0856Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0844Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0843Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0839Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0844Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0841Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0834Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0827Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0824Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0821Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0826Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0829Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0827Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0825Epoch 6/15: [==============================] 47/47 batches, loss: 0.0822
[2025-05-07 22:12:54,390][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0822
[2025-05-07 22:12:54,641][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0448, Metrics: {'mse': 0.04730646684765816, 'rmse': 0.2175004984997923, 'r2': -0.4120289087295532}
[2025-05-07 22:12:54,641][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:12:54,641][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-07 22:12:54,641][src.training.lm_trainer][INFO] - Training completed in 12.31 seconds
[2025-05-07 22:12:54,642][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:12:56,872][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03341086581349373, 'rmse': 0.1827863939506815, 'r2': -0.14391613006591797}
[2025-05-07 22:12:56,872][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.046207234263420105, 'rmse': 0.21495868036304117, 'r2': -0.37921833992004395}
[2025-05-07 22:12:56,873][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.049529630690813065, 'rmse': 0.22255253467622665, 'r2': -0.4185791015625}
[2025-05-07 22:12:58,538][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer2/ko/ko/model.pt
[2025-05-07 22:12:58,540][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▅▃▅▅
wandb:       train_loss █▄▃▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▅▂▁▁
wandb:          val_mse █▁▅▂▂▁
wandb:           val_r2 ▁█▄▇▇█
wandb:         val_rmse █▁▅▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04476
wandb:     best_val_mse 0.04621
wandb:      best_val_r2 -0.37922
wandb:    best_val_rmse 0.21496
wandb: early_stop_epoch 6
wandb:            epoch 6
wandb:   final_test_mse 0.04953
wandb:    final_test_r2 -0.41858
wandb:  final_test_rmse 0.22255
wandb:  final_train_mse 0.03341
wandb:   final_train_r2 -0.14392
wandb: final_train_rmse 0.18279
wandb:    final_val_mse 0.04621
wandb:     final_val_r2 -0.37922
wandb:   final_val_rmse 0.21496
wandb:    learning_rate 0.0001
wandb:       train_loss 0.08216
wandb:       train_time 12.31034
wandb:         val_loss 0.04477
wandb:          val_mse 0.04731
wandb:           val_r2 -0.41203
wandb:         val_rmse 0.2175
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_221226-yb1qlzaq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_221226-yb1qlzaq/logs
Experiment probe_layer2_avg_max_depth_control1_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_avg_max_depth_control2_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_max_depth_control2_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:13:22,787][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer2/ko
experiment_name: probe_layer2_avg_max_depth_control2_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:13:22,787][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:13:22,787][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 22:13:22,787][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:13:22,787][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:13:22,792][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:13:22,792][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 22:13:22,792][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:13:24,696][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:13:27,010][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:13:27,011][src.data.datasets][INFO] - Loading 'control_avg_max_depth_seed2' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:13:27,092][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_max_depth_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:21:04 2025).
[2025-05-07 22:13:27,162][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_max_depth_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:21:04 2025).
[2025-05-07 22:13:27,331][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:13:27,337][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:13:27,338][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:13:27,340][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:13:27,420][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:13:27,491][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:13:27,509][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:13:27,510][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:13:27,510][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:13:27,512][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:13:27,576][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:13:27,660][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:13:27,676][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:13:27,677][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:13:27,678][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:13:27,679][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:13:27,680][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:13:27,680][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 22:13:27,680][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 22:13:27,680][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 22:13:27,680][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:13:27,681][src.data.datasets][INFO] -   Mean: 0.2234, Std: 0.1709
[2025-05-07 22:13:27,681][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:13:27,681][src.data.datasets][INFO] - Sample label: 0.30000001192092896
[2025-05-07 22:13:27,681][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:13:27,681][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 22:13:27,681][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 22:13:27,681][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 22:13:27,681][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8890
[2025-05-07 22:13:27,681][src.data.datasets][INFO] -   Mean: 0.2502, Std: 0.1830
[2025-05-07 22:13:27,681][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:13:27,682][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 22:13:27,682][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:13:27,682][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 22:13:27,682][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 22:13:27,682][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 22:13:27,682][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8890
[2025-05-07 22:13:27,682][src.data.datasets][INFO] -   Mean: 0.2705, Std: 0.1869
[2025-05-07 22:13:27,682][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:13:27,682][src.data.datasets][INFO] - Sample label: 0.4440000057220459
[2025-05-07 22:13:27,682][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:13:27,682][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:13:27,683][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:13:27,683][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 22:13:27,683][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:13:33,514][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:13:33,515][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:13:33,515][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:13:33,515][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:13:33,518][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:13:33,519][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:13:33,519][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:13:33,519][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:13:33,519][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:13:33,520][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:13:33,520][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.3959Epoch 1/15: [=                             ] 2/47 batches, loss: 0.4828Epoch 1/15: [=                             ] 3/47 batches, loss: 0.5015Epoch 1/15: [==                            ] 4/47 batches, loss: 0.5354Epoch 1/15: [===                           ] 5/47 batches, loss: 0.5279Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4784Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4530Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4663Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4796Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4717Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4561Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4490Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4346Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4300Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.4160Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4310Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.4180Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4327Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4214Epoch 1/15: [============                  ] 20/47 batches, loss: 0.4135Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4173Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4148Epoch 1/15: [==============                ] 23/47 batches, loss: 0.4015Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3931Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3879Epoch 1/15: [================              ] 26/47 batches, loss: 0.3813Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3812Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3726Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3760Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3695Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3612Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3540Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3514Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3512Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3456Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3432Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3392Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3378Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3361Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3390Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3377Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3353Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3328Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3301Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3295Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3250Epoch 1/15: [==============================] 47/47 batches, loss: 0.3269
[2025-05-07 22:13:42,123][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3269
[2025-05-07 22:13:42,386][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0742, Metrics: {'mse': 0.07896789908409119, 'rmse': 0.28101227568220427, 'r2': -1.3570764064788818}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2712Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2069Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2077Epoch 2/15: [==                            ] 4/47 batches, loss: 0.1962Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2269Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2353Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2363Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2279Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2292Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2191Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2228Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2292Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2236Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2270Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2274Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2255Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2207Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2223Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2193Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2177Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2146Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2146Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2137Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2121Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2109Epoch 2/15: [================              ] 26/47 batches, loss: 0.2073Epoch 2/15: [=================             ] 27/47 batches, loss: 0.2047Epoch 2/15: [=================             ] 28/47 batches, loss: 0.2030Epoch 2/15: [==================            ] 29/47 batches, loss: 0.1990Epoch 2/15: [===================           ] 30/47 batches, loss: 0.1979Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1959Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1953Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1963Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1965Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1932Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1907Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1908Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1872Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1870Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1856Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1858Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1839Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1811Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1801Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1795Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1774Epoch 2/15: [==============================] 47/47 batches, loss: 0.1770
[2025-05-07 22:13:44,291][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1770
[2025-05-07 22:13:44,580][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0494, Metrics: {'mse': 0.05194462090730667, 'rmse': 0.22791362597990203, 'r2': -0.5504709482192993}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.2627Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1670Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1627Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1391Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1457Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1676Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1535Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1436Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1624Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1594Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1583Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1612Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1591Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1538Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1523Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1490Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1522Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1516Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1487Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1453Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1468Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1432Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1494Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1503Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1467Epoch 3/15: [================              ] 26/47 batches, loss: 0.1445Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1433Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1407Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1381Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1374Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1376Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1367Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1379Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1369Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1381Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1386Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1379Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1360Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1343Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1344Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1336Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1345Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1345Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1357Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1364Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1355Epoch 3/15: [==============================] 47/47 batches, loss: 0.1344
[2025-05-07 22:13:46,510][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1344
[2025-05-07 22:13:46,777][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0539, Metrics: {'mse': 0.057379260659217834, 'rmse': 0.2395396849359576, 'r2': -0.7126868963241577}
[2025-05-07 22:13:46,778][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1664Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1409Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1228Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1157Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1203Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1114Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1136Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1180Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1169Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1217Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1282Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1310Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1305Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1281Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1249Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1220Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1258Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1240Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1245Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1274Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1268Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1271Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1270Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1248Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1242Epoch 4/15: [================              ] 26/47 batches, loss: 0.1233Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1260Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1264Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1276Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1272Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1289Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1285Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1285Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1270Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1275Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1262Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1268Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1253Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1235Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1222Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1230Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1230Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1216Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1210Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1202Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1204Epoch 4/15: [==============================] 47/47 batches, loss: 0.1208
[2025-05-07 22:13:48,299][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1208
[2025-05-07 22:13:48,600][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0479, Metrics: {'mse': 0.051135480403900146, 'rmse': 0.22613155552443392, 'r2': -0.5263192653656006}
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.0868Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1107Epoch 5/15: [=                             ] 3/47 batches, loss: 0.0977Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0812Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0766Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0832Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0825Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0803Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0836Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0809Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0798Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0835Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0800Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0807Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0813Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0827Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0818Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0827Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0821Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0858Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0850Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0845Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0841Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0865Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0857Epoch 5/15: [================              ] 26/47 batches, loss: 0.0851Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0840Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0843Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0846Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0864Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0873Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0868Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0868Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0881Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0884Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0885Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0882Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0882Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0888Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0897Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0894Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0890Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0889Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0887Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0906Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0901Epoch 5/15: [==============================] 47/47 batches, loss: 0.0898
[2025-05-07 22:13:50,534][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0898
[2025-05-07 22:13:50,802][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0668, Metrics: {'mse': 0.07159645110368729, 'rmse': 0.26757513169890673, 'r2': -1.1370491981506348}
[2025-05-07 22:13:50,803][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.1001Epoch 6/15: [=                             ] 2/47 batches, loss: 0.1021Epoch 6/15: [=                             ] 3/47 batches, loss: 0.0911Epoch 6/15: [==                            ] 4/47 batches, loss: 0.0916Epoch 6/15: [===                           ] 5/47 batches, loss: 0.1049Epoch 6/15: [===                           ] 6/47 batches, loss: 0.1008Epoch 6/15: [====                          ] 7/47 batches, loss: 0.0956Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.0955Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.0930Epoch 6/15: [======                        ] 10/47 batches, loss: 0.0912Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.0933Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.1044Epoch 6/15: [========                      ] 13/47 batches, loss: 0.1010Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0974Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0957Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.0949Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.0949Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.0958Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0920Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0894Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0874Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0883Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0863Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0848Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0842Epoch 6/15: [================              ] 26/47 batches, loss: 0.0867Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0866Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0865Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0865Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0866Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0880Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0871Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0882Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0873Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0864Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0854Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0855Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0858Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0855Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0849Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0853Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0869Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0869Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0868Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0868Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0862Epoch 6/15: [==============================] 47/47 batches, loss: 0.0850
[2025-05-07 22:13:52,297][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0850
[2025-05-07 22:13:52,560][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0495, Metrics: {'mse': 0.052821896970272064, 'rmse': 0.2298301480882612, 'r2': -0.5766563415527344}
[2025-05-07 22:13:52,560][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/47 batches, loss: 0.0772Epoch 7/15: [=                             ] 2/47 batches, loss: 0.1094Epoch 7/15: [=                             ] 3/47 batches, loss: 0.0994Epoch 7/15: [==                            ] 4/47 batches, loss: 0.0913Epoch 7/15: [===                           ] 5/47 batches, loss: 0.0960Epoch 7/15: [===                           ] 6/47 batches, loss: 0.0911Epoch 7/15: [====                          ] 7/47 batches, loss: 0.0825Epoch 7/15: [=====                         ] 8/47 batches, loss: 0.0785Epoch 7/15: [=====                         ] 9/47 batches, loss: 0.0804Epoch 7/15: [======                        ] 10/47 batches, loss: 0.0816Epoch 7/15: [=======                       ] 11/47 batches, loss: 0.0771Epoch 7/15: [=======                       ] 12/47 batches, loss: 0.0774Epoch 7/15: [========                      ] 13/47 batches, loss: 0.0758Epoch 7/15: [========                      ] 14/47 batches, loss: 0.0722Epoch 7/15: [=========                     ] 15/47 batches, loss: 0.0703Epoch 7/15: [==========                    ] 16/47 batches, loss: 0.0705Epoch 7/15: [==========                    ] 17/47 batches, loss: 0.0697Epoch 7/15: [===========                   ] 18/47 batches, loss: 0.0707Epoch 7/15: [============                  ] 19/47 batches, loss: 0.0735Epoch 7/15: [============                  ] 20/47 batches, loss: 0.0727Epoch 7/15: [=============                 ] 21/47 batches, loss: 0.0708Epoch 7/15: [==============                ] 22/47 batches, loss: 0.0706Epoch 7/15: [==============                ] 23/47 batches, loss: 0.0702Epoch 7/15: [===============               ] 24/47 batches, loss: 0.0710Epoch 7/15: [===============               ] 25/47 batches, loss: 0.0725Epoch 7/15: [================              ] 26/47 batches, loss: 0.0725Epoch 7/15: [=================             ] 27/47 batches, loss: 0.0721Epoch 7/15: [=================             ] 28/47 batches, loss: 0.0718Epoch 7/15: [==================            ] 29/47 batches, loss: 0.0716Epoch 7/15: [===================           ] 30/47 batches, loss: 0.0708Epoch 7/15: [===================           ] 31/47 batches, loss: 0.0709Epoch 7/15: [====================          ] 32/47 batches, loss: 0.0731Epoch 7/15: [=====================         ] 33/47 batches, loss: 0.0718Epoch 7/15: [=====================         ] 34/47 batches, loss: 0.0724Epoch 7/15: [======================        ] 35/47 batches, loss: 0.0730Epoch 7/15: [======================        ] 36/47 batches, loss: 0.0731Epoch 7/15: [=======================       ] 37/47 batches, loss: 0.0736Epoch 7/15: [========================      ] 38/47 batches, loss: 0.0736Epoch 7/15: [========================      ] 39/47 batches, loss: 0.0739Epoch 7/15: [=========================     ] 40/47 batches, loss: 0.0735Epoch 7/15: [==========================    ] 41/47 batches, loss: 0.0726Epoch 7/15: [==========================    ] 42/47 batches, loss: 0.0733Epoch 7/15: [===========================   ] 43/47 batches, loss: 0.0727Epoch 7/15: [============================  ] 44/47 batches, loss: 0.0729Epoch 7/15: [============================  ] 45/47 batches, loss: 0.0741Epoch 7/15: [============================= ] 46/47 batches, loss: 0.0740Epoch 7/15: [==============================] 47/47 batches, loss: 0.0727
[2025-05-07 22:13:54,054][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0727
[2025-05-07 22:13:54,319][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0431, Metrics: {'mse': 0.045920610427856445, 'rmse': 0.21429094807727284, 'r2': -0.370663046836853}
Epoch 8/15: [Epoch 8/15: [                              ] 1/47 batches, loss: 0.0231Epoch 8/15: [=                             ] 2/47 batches, loss: 0.0361Epoch 8/15: [=                             ] 3/47 batches, loss: 0.0552Epoch 8/15: [==                            ] 4/47 batches, loss: 0.0889Epoch 8/15: [===                           ] 5/47 batches, loss: 0.0838Epoch 8/15: [===                           ] 6/47 batches, loss: 0.0848Epoch 8/15: [====                          ] 7/47 batches, loss: 0.0877Epoch 8/15: [=====                         ] 8/47 batches, loss: 0.0841Epoch 8/15: [=====                         ] 9/47 batches, loss: 0.0795Epoch 8/15: [======                        ] 10/47 batches, loss: 0.0883Epoch 8/15: [=======                       ] 11/47 batches, loss: 0.0874Epoch 8/15: [=======                       ] 12/47 batches, loss: 0.0867Epoch 8/15: [========                      ] 13/47 batches, loss: 0.0819Epoch 8/15: [========                      ] 14/47 batches, loss: 0.0833Epoch 8/15: [=========                     ] 15/47 batches, loss: 0.0855Epoch 8/15: [==========                    ] 16/47 batches, loss: 0.0821Epoch 8/15: [==========                    ] 17/47 batches, loss: 0.0828Epoch 8/15: [===========                   ] 18/47 batches, loss: 0.0838Epoch 8/15: [============                  ] 19/47 batches, loss: 0.0814Epoch 8/15: [============                  ] 20/47 batches, loss: 0.0796Epoch 8/15: [=============                 ] 21/47 batches, loss: 0.0785Epoch 8/15: [==============                ] 22/47 batches, loss: 0.0778Epoch 8/15: [==============                ] 23/47 batches, loss: 0.0784Epoch 8/15: [===============               ] 24/47 batches, loss: 0.0771Epoch 8/15: [===============               ] 25/47 batches, loss: 0.0761Epoch 8/15: [================              ] 26/47 batches, loss: 0.0772Epoch 8/15: [=================             ] 27/47 batches, loss: 0.0769Epoch 8/15: [=================             ] 28/47 batches, loss: 0.0779Epoch 8/15: [==================            ] 29/47 batches, loss: 0.0774Epoch 8/15: [===================           ] 30/47 batches, loss: 0.0779Epoch 8/15: [===================           ] 31/47 batches, loss: 0.0774Epoch 8/15: [====================          ] 32/47 batches, loss: 0.0769Epoch 8/15: [=====================         ] 33/47 batches, loss: 0.0776Epoch 8/15: [=====================         ] 34/47 batches, loss: 0.0778Epoch 8/15: [======================        ] 35/47 batches, loss: 0.0775Epoch 8/15: [======================        ] 36/47 batches, loss: 0.0777Epoch 8/15: [=======================       ] 37/47 batches, loss: 0.0775Epoch 8/15: [========================      ] 38/47 batches, loss: 0.0780Epoch 8/15: [========================      ] 39/47 batches, loss: 0.0781Epoch 8/15: [=========================     ] 40/47 batches, loss: 0.0772Epoch 8/15: [==========================    ] 41/47 batches, loss: 0.0766Epoch 8/15: [==========================    ] 42/47 batches, loss: 0.0758Epoch 8/15: [===========================   ] 43/47 batches, loss: 0.0759Epoch 8/15: [============================  ] 44/47 batches, loss: 0.0764Epoch 8/15: [============================  ] 45/47 batches, loss: 0.0755Epoch 8/15: [============================= ] 46/47 batches, loss: 0.0753Epoch 8/15: [==============================] 47/47 batches, loss: 0.0741
[2025-05-07 22:13:56,185][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0741
[2025-05-07 22:13:56,480][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0372, Metrics: {'mse': 0.03886527195572853, 'rmse': 0.19714277048811232, 'r2': -0.1600714921951294}
Epoch 9/15: [Epoch 9/15: [                              ] 1/47 batches, loss: 0.0986Epoch 9/15: [=                             ] 2/47 batches, loss: 0.0793Epoch 9/15: [=                             ] 3/47 batches, loss: 0.0892Epoch 9/15: [==                            ] 4/47 batches, loss: 0.0795Epoch 9/15: [===                           ] 5/47 batches, loss: 0.0746Epoch 9/15: [===                           ] 6/47 batches, loss: 0.0698Epoch 9/15: [====                          ] 7/47 batches, loss: 0.0728Epoch 9/15: [=====                         ] 8/47 batches, loss: 0.0766Epoch 9/15: [=====                         ] 9/47 batches, loss: 0.0765Epoch 9/15: [======                        ] 10/47 batches, loss: 0.0773Epoch 9/15: [=======                       ] 11/47 batches, loss: 0.0749Epoch 9/15: [=======                       ] 12/47 batches, loss: 0.0724Epoch 9/15: [========                      ] 13/47 batches, loss: 0.0696Epoch 9/15: [========                      ] 14/47 batches, loss: 0.0706Epoch 9/15: [=========                     ] 15/47 batches, loss: 0.0714Epoch 9/15: [==========                    ] 16/47 batches, loss: 0.0686Epoch 9/15: [==========                    ] 17/47 batches, loss: 0.0705Epoch 9/15: [===========                   ] 18/47 batches, loss: 0.0681Epoch 9/15: [============                  ] 19/47 batches, loss: 0.0658Epoch 9/15: [============                  ] 20/47 batches, loss: 0.0648Epoch 9/15: [=============                 ] 21/47 batches, loss: 0.0648Epoch 9/15: [==============                ] 22/47 batches, loss: 0.0645Epoch 9/15: [==============                ] 23/47 batches, loss: 0.0640Epoch 9/15: [===============               ] 24/47 batches, loss: 0.0635Epoch 9/15: [===============               ] 25/47 batches, loss: 0.0636Epoch 9/15: [================              ] 26/47 batches, loss: 0.0640Epoch 9/15: [=================             ] 27/47 batches, loss: 0.0652Epoch 9/15: [=================             ] 28/47 batches, loss: 0.0664Epoch 9/15: [==================            ] 29/47 batches, loss: 0.0671Epoch 9/15: [===================           ] 30/47 batches, loss: 0.0661Epoch 9/15: [===================           ] 31/47 batches, loss: 0.0659Epoch 9/15: [====================          ] 32/47 batches, loss: 0.0659Epoch 9/15: [=====================         ] 33/47 batches, loss: 0.0652Epoch 9/15: [=====================         ] 34/47 batches, loss: 0.0662Epoch 9/15: [======================        ] 35/47 batches, loss: 0.0660Epoch 9/15: [======================        ] 36/47 batches, loss: 0.0672Epoch 9/15: [=======================       ] 37/47 batches, loss: 0.0677Epoch 9/15: [========================      ] 38/47 batches, loss: 0.0668Epoch 9/15: [========================      ] 39/47 batches, loss: 0.0668Epoch 9/15: [=========================     ] 40/47 batches, loss: 0.0677Epoch 9/15: [==========================    ] 41/47 batches, loss: 0.0687Epoch 9/15: [==========================    ] 42/47 batches, loss: 0.0685Epoch 9/15: [===========================   ] 43/47 batches, loss: 0.0686Epoch 9/15: [============================  ] 44/47 batches, loss: 0.0687Epoch 9/15: [============================  ] 45/47 batches, loss: 0.0687Epoch 9/15: [============================= ] 46/47 batches, loss: 0.0679Epoch 9/15: [==============================] 47/47 batches, loss: 0.0672
[2025-05-07 22:13:58,479][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0672
[2025-05-07 22:13:58,803][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0393, Metrics: {'mse': 0.04153997078537941, 'rmse': 0.20381356869791425, 'r2': -0.2399073839187622}
[2025-05-07 22:13:58,804][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/47 batches, loss: 0.0350Epoch 10/15: [=                             ] 2/47 batches, loss: 0.0654Epoch 10/15: [=                             ] 3/47 batches, loss: 0.0656Epoch 10/15: [==                            ] 4/47 batches, loss: 0.0663Epoch 10/15: [===                           ] 5/47 batches, loss: 0.0688Epoch 10/15: [===                           ] 6/47 batches, loss: 0.0664Epoch 10/15: [====                          ] 7/47 batches, loss: 0.0739Epoch 10/15: [=====                         ] 8/47 batches, loss: 0.0801Epoch 10/15: [=====                         ] 9/47 batches, loss: 0.0756Epoch 10/15: [======                        ] 10/47 batches, loss: 0.0756Epoch 10/15: [=======                       ] 11/47 batches, loss: 0.0741Epoch 10/15: [=======                       ] 12/47 batches, loss: 0.0739Epoch 10/15: [========                      ] 13/47 batches, loss: 0.0751Epoch 10/15: [========                      ] 14/47 batches, loss: 0.0734Epoch 10/15: [=========                     ] 15/47 batches, loss: 0.0710Epoch 10/15: [==========                    ] 16/47 batches, loss: 0.0703Epoch 10/15: [==========                    ] 17/47 batches, loss: 0.0691Epoch 10/15: [===========                   ] 18/47 batches, loss: 0.0678Epoch 10/15: [============                  ] 19/47 batches, loss: 0.0670Epoch 10/15: [============                  ] 20/47 batches, loss: 0.0708Epoch 10/15: [=============                 ] 21/47 batches, loss: 0.0692Epoch 10/15: [==============                ] 22/47 batches, loss: 0.0696Epoch 10/15: [==============                ] 23/47 batches, loss: 0.0687Epoch 10/15: [===============               ] 24/47 batches, loss: 0.0693Epoch 10/15: [===============               ] 25/47 batches, loss: 0.0685Epoch 10/15: [================              ] 26/47 batches, loss: 0.0674Epoch 10/15: [=================             ] 27/47 batches, loss: 0.0671Epoch 10/15: [=================             ] 28/47 batches, loss: 0.0662Epoch 10/15: [==================            ] 29/47 batches, loss: 0.0663Epoch 10/15: [===================           ] 30/47 batches, loss: 0.0677Epoch 10/15: [===================           ] 31/47 batches, loss: 0.0683Epoch 10/15: [====================          ] 32/47 batches, loss: 0.0673Epoch 10/15: [=====================         ] 33/47 batches, loss: 0.0663Epoch 10/15: [=====================         ] 34/47 batches, loss: 0.0656Epoch 10/15: [======================        ] 35/47 batches, loss: 0.0654Epoch 10/15: [======================        ] 36/47 batches, loss: 0.0655Epoch 10/15: [=======================       ] 37/47 batches, loss: 0.0651Epoch 10/15: [========================      ] 38/47 batches, loss: 0.0655Epoch 10/15: [========================      ] 39/47 batches, loss: 0.0656Epoch 10/15: [=========================     ] 40/47 batches, loss: 0.0649Epoch 10/15: [==========================    ] 41/47 batches, loss: 0.0641Epoch 10/15: [==========================    ] 42/47 batches, loss: 0.0637Epoch 10/15: [===========================   ] 43/47 batches, loss: 0.0639Epoch 10/15: [============================  ] 44/47 batches, loss: 0.0636Epoch 10/15: [============================  ] 45/47 batches, loss: 0.0635Epoch 10/15: [============================= ] 46/47 batches, loss: 0.0634Epoch 10/15: [==============================] 47/47 batches, loss: 0.0631
[2025-05-07 22:14:00,320][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0631
[2025-05-07 22:14:00,628][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0434, Metrics: {'mse': 0.04624985158443451, 'rmse': 0.21505778661660802, 'r2': -0.38049042224884033}
[2025-05-07 22:14:00,628][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/47 batches, loss: 0.0600Epoch 11/15: [=                             ] 2/47 batches, loss: 0.0511Epoch 11/15: [=                             ] 3/47 batches, loss: 0.0513Epoch 11/15: [==                            ] 4/47 batches, loss: 0.0464Epoch 11/15: [===                           ] 5/47 batches, loss: 0.0496Epoch 11/15: [===                           ] 6/47 batches, loss: 0.0494Epoch 11/15: [====                          ] 7/47 batches, loss: 0.0489Epoch 11/15: [=====                         ] 8/47 batches, loss: 0.0487Epoch 11/15: [=====                         ] 9/47 batches, loss: 0.0505Epoch 11/15: [======                        ] 10/47 batches, loss: 0.0505Epoch 11/15: [=======                       ] 11/47 batches, loss: 0.0520Epoch 11/15: [=======                       ] 12/47 batches, loss: 0.0553Epoch 11/15: [========                      ] 13/47 batches, loss: 0.0557Epoch 11/15: [========                      ] 14/47 batches, loss: 0.0538Epoch 11/15: [=========                     ] 15/47 batches, loss: 0.0535Epoch 11/15: [==========                    ] 16/47 batches, loss: 0.0527Epoch 11/15: [==========                    ] 17/47 batches, loss: 0.0519Epoch 11/15: [===========                   ] 18/47 batches, loss: 0.0515Epoch 11/15: [============                  ] 19/47 batches, loss: 0.0509Epoch 11/15: [============                  ] 20/47 batches, loss: 0.0518Epoch 11/15: [=============                 ] 21/47 batches, loss: 0.0508Epoch 11/15: [==============                ] 22/47 batches, loss: 0.0528Epoch 11/15: [==============                ] 23/47 batches, loss: 0.0530Epoch 11/15: [===============               ] 24/47 batches, loss: 0.0525Epoch 11/15: [===============               ] 25/47 batches, loss: 0.0515Epoch 11/15: [================              ] 26/47 batches, loss: 0.0524Epoch 11/15: [=================             ] 27/47 batches, loss: 0.0519Epoch 11/15: [=================             ] 28/47 batches, loss: 0.0526Epoch 11/15: [==================            ] 29/47 batches, loss: 0.0521Epoch 11/15: [===================           ] 30/47 batches, loss: 0.0515Epoch 11/15: [===================           ] 31/47 batches, loss: 0.0518Epoch 11/15: [====================          ] 32/47 batches, loss: 0.0525Epoch 11/15: [=====================         ] 33/47 batches, loss: 0.0521Epoch 11/15: [=====================         ] 34/47 batches, loss: 0.0518Epoch 11/15: [======================        ] 35/47 batches, loss: 0.0522Epoch 11/15: [======================        ] 36/47 batches, loss: 0.0522Epoch 11/15: [=======================       ] 37/47 batches, loss: 0.0518Epoch 11/15: [========================      ] 38/47 batches, loss: 0.0515Epoch 11/15: [========================      ] 39/47 batches, loss: 0.0516Epoch 11/15: [=========================     ] 40/47 batches, loss: 0.0532Epoch 11/15: [==========================    ] 41/47 batches, loss: 0.0534Epoch 11/15: [==========================    ] 42/47 batches, loss: 0.0529Epoch 11/15: [===========================   ] 43/47 batches, loss: 0.0524Epoch 11/15: [============================  ] 44/47 batches, loss: 0.0535Epoch 11/15: [============================  ] 45/47 batches, loss: 0.0531Epoch 11/15: [============================= ] 46/47 batches, loss: 0.0535Epoch 11/15: [==============================] 47/47 batches, loss: 0.0530
[2025-05-07 22:14:02,102][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0530
[2025-05-07 22:14:02,542][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0430, Metrics: {'mse': 0.045654088258743286, 'rmse': 0.21366817324707787, 'r2': -0.3627077341079712}
[2025-05-07 22:14:02,543][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/47 batches, loss: 0.0208Epoch 12/15: [=                             ] 2/47 batches, loss: 0.0549Epoch 12/15: [=                             ] 3/47 batches, loss: 0.0482Epoch 12/15: [==                            ] 4/47 batches, loss: 0.0427Epoch 12/15: [===                           ] 5/47 batches, loss: 0.0464Epoch 12/15: [===                           ] 6/47 batches, loss: 0.0493Epoch 12/15: [====                          ] 7/47 batches, loss: 0.0479Epoch 12/15: [=====                         ] 8/47 batches, loss: 0.0503Epoch 12/15: [=====                         ] 9/47 batches, loss: 0.0499Epoch 12/15: [======                        ] 10/47 batches, loss: 0.0489Epoch 12/15: [=======                       ] 11/47 batches, loss: 0.0505Epoch 12/15: [=======                       ] 12/47 batches, loss: 0.0546Epoch 12/15: [========                      ] 13/47 batches, loss: 0.0585Epoch 12/15: [========                      ] 14/47 batches, loss: 0.0592Epoch 12/15: [=========                     ] 15/47 batches, loss: 0.0579Epoch 12/15: [==========                    ] 16/47 batches, loss: 0.0613Epoch 12/15: [==========                    ] 17/47 batches, loss: 0.0598Epoch 12/15: [===========                   ] 18/47 batches, loss: 0.0584Epoch 12/15: [============                  ] 19/47 batches, loss: 0.0596Epoch 12/15: [============                  ] 20/47 batches, loss: 0.0605Epoch 12/15: [=============                 ] 21/47 batches, loss: 0.0599Epoch 12/15: [==============                ] 22/47 batches, loss: 0.0594Epoch 12/15: [==============                ] 23/47 batches, loss: 0.0598Epoch 12/15: [===============               ] 24/47 batches, loss: 0.0604Epoch 12/15: [===============               ] 25/47 batches, loss: 0.0613Epoch 12/15: [================              ] 26/47 batches, loss: 0.0601Epoch 12/15: [=================             ] 27/47 batches, loss: 0.0586Epoch 12/15: [=================             ] 28/47 batches, loss: 0.0579Epoch 12/15: [==================            ] 29/47 batches, loss: 0.0579Epoch 12/15: [===================           ] 30/47 batches, loss: 0.0576Epoch 12/15: [===================           ] 31/47 batches, loss: 0.0571Epoch 12/15: [====================          ] 32/47 batches, loss: 0.0575Epoch 12/15: [=====================         ] 33/47 batches, loss: 0.0572Epoch 12/15: [=====================         ] 34/47 batches, loss: 0.0567Epoch 12/15: [======================        ] 35/47 batches, loss: 0.0563Epoch 12/15: [======================        ] 36/47 batches, loss: 0.0571Epoch 12/15: [=======================       ] 37/47 batches, loss: 0.0565Epoch 12/15: [========================      ] 38/47 batches, loss: 0.0562Epoch 12/15: [========================      ] 39/47 batches, loss: 0.0560Epoch 12/15: [=========================     ] 40/47 batches, loss: 0.0553Epoch 12/15: [==========================    ] 41/47 batches, loss: 0.0560Epoch 12/15: [==========================    ] 42/47 batches, loss: 0.0558Epoch 12/15: [===========================   ] 43/47 batches, loss: 0.0557Epoch 12/15: [============================  ] 44/47 batches, loss: 0.0553Epoch 12/15: [============================  ] 45/47 batches, loss: 0.0550Epoch 12/15: [============================= ] 46/47 batches, loss: 0.0545Epoch 12/15: [==============================] 47/47 batches, loss: 0.0540
[2025-05-07 22:14:04,092][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0540
[2025-05-07 22:14:04,455][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0496, Metrics: {'mse': 0.05310754105448723, 'rmse': 0.23045073454968032, 'r2': -0.5851824283599854}
[2025-05-07 22:14:04,455][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:14:04,455][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 12
[2025-05-07 22:14:04,456][src.training.lm_trainer][INFO] - Training completed in 24.63 seconds
[2025-05-07 22:14:04,456][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:14:06,703][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.029749460518360138, 'rmse': 0.17248031922036827, 'r2': -0.01855754852294922}
[2025-05-07 22:14:06,704][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03886527195572853, 'rmse': 0.19714277048811232, 'r2': -0.1600714921951294}
[2025-05-07 22:14:06,704][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04023488983511925, 'rmse': 0.20058636502793317, 'r2': -0.15236830711364746}
[2025-05-07 22:14:08,371][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer2/ko/ko/model.pt
[2025-05-07 22:14:08,372][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▃▂▁
wandb:     best_val_mse █▃▃▂▁
wandb:      best_val_r2 ▁▆▆▇█
wandb:    best_val_rmse █▄▃▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▅▄▅▂▅▆▆▆▆▆
wandb:       train_loss █▄▃▃▂▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▄▃▇▃▂▁▁▂▂▃
wandb:          val_mse █▃▄▃▇▃▂▁▁▂▂▃
wandb:           val_r2 ▁▆▅▆▂▆▇██▇▇▆
wandb:         val_rmse █▄▅▃▇▄▂▁▂▂▂▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03722
wandb:     best_val_mse 0.03887
wandb:      best_val_r2 -0.16007
wandb:    best_val_rmse 0.19714
wandb: early_stop_epoch 12
wandb:            epoch 12
wandb:   final_test_mse 0.04023
wandb:    final_test_r2 -0.15237
wandb:  final_test_rmse 0.20059
wandb:  final_train_mse 0.02975
wandb:   final_train_r2 -0.01856
wandb: final_train_rmse 0.17248
wandb:    final_val_mse 0.03887
wandb:     final_val_r2 -0.16007
wandb:   final_val_rmse 0.19714
wandb:    learning_rate 0.0001
wandb:       train_loss 0.054
wandb:       train_time 24.63331
wandb:         val_loss 0.04962
wandb:          val_mse 0.05311
wandb:           val_r2 -0.58518
wandb:         val_rmse 0.23045
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_221322-rf44o3rz
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_221322-rf44o3rz/logs
Experiment probe_layer2_avg_max_depth_control2_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_avg_max_depth_control3_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_max_depth_control3_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:14:35,337][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer2/ko
experiment_name: probe_layer2_avg_max_depth_control3_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:14:35,337][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:14:35,337][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 22:14:35,337][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:14:35,337][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:14:35,342][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:14:35,342][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 22:14:35,342][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:14:38,578][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:14:40,837][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:14:40,837][src.data.datasets][INFO] - Loading 'control_avg_max_depth_seed3' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:14:40,990][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_max_depth_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:22:13 2025).
[2025-05-07 22:14:41,081][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_max_depth_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:22:13 2025).
[2025-05-07 22:14:41,333][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:14:41,340][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:14:41,340][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:14:41,342][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:14:41,427][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:14:41,510][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:14:41,541][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:14:41,544][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:14:41,544][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:14:41,545][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:14:41,615][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:14:41,691][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:14:41,705][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:14:41,707][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:14:41,707][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:14:41,708][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:14:41,709][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:14:41,709][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 22:14:41,709][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 22:14:41,709][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 22:14:41,710][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:14:41,710][src.data.datasets][INFO] -   Mean: 0.2234, Std: 0.1709
[2025-05-07 22:14:41,710][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:14:41,710][src.data.datasets][INFO] - Sample label: 0.10000000149011612
[2025-05-07 22:14:41,710][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:14:41,710][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 22:14:41,711][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 22:14:41,711][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 22:14:41,711][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8890
[2025-05-07 22:14:41,711][src.data.datasets][INFO] -   Mean: 0.2502, Std: 0.1830
[2025-05-07 22:14:41,711][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:14:41,711][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 22:14:41,711][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:14:41,711][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 22:14:41,711][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 22:14:41,711][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 22:14:41,712][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8890
[2025-05-07 22:14:41,712][src.data.datasets][INFO] -   Mean: 0.2705, Std: 0.1869
[2025-05-07 22:14:41,712][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:14:41,712][src.data.datasets][INFO] - Sample label: 0.4440000057220459
[2025-05-07 22:14:41,712][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:14:41,712][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:14:41,713][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:14:41,713][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 22:14:41,714][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:14:48,894][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:14:48,895][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:14:48,895][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:14:48,896][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:14:48,898][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:14:48,899][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:14:48,899][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:14:48,899][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:14:48,899][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:14:48,900][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:14:48,900][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.3744Epoch 1/15: [=                             ] 2/47 batches, loss: 0.5752Epoch 1/15: [=                             ] 3/47 batches, loss: 0.5483Epoch 1/15: [==                            ] 4/47 batches, loss: 0.5281Epoch 1/15: [===                           ] 5/47 batches, loss: 0.5065Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4556Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4436Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4512Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4555Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4618Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4494Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4415Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4291Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4322Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.4191Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4257Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.4152Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4363Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4306Epoch 1/15: [============                  ] 20/47 batches, loss: 0.4180Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4212Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4200Epoch 1/15: [==============                ] 23/47 batches, loss: 0.4092Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3990Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3951Epoch 1/15: [================              ] 26/47 batches, loss: 0.3908Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3876Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3794Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3848Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3791Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3701Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3649Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3621Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3644Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3589Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3578Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3519Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3496Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3451Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3445Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3439Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3406Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3387Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3363Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3365Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3311Epoch 1/15: [==============================] 47/47 batches, loss: 0.3340
[2025-05-07 22:14:55,900][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3340
[2025-05-07 22:14:56,198][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0947, Metrics: {'mse': 0.10115250945091248, 'rmse': 0.31804482302171255, 'r2': -2.019254446029663}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2555Epoch 2/15: [=                             ] 2/47 batches, loss: 0.1939Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2565Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2232Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2201Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2342Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2314Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2144Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2136Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2048Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2036Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2032Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2077Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2098Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2054Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2070Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2004Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2123Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2087Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2038Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2020Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2020Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2009Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2014Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2021Epoch 2/15: [================              ] 26/47 batches, loss: 0.2008Epoch 2/15: [=================             ] 27/47 batches, loss: 0.1982Epoch 2/15: [=================             ] 28/47 batches, loss: 0.1952Epoch 2/15: [==================            ] 29/47 batches, loss: 0.1933Epoch 2/15: [===================           ] 30/47 batches, loss: 0.1906Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1904Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1879Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1903Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1914Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1884Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1871Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1898Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1871Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1849Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1843Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1829Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1812Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1788Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1767Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1783Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1765Epoch 2/15: [==============================] 47/47 batches, loss: 0.1759
[2025-05-07 22:14:58,090][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1759
[2025-05-07 22:14:58,356][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0427, Metrics: {'mse': 0.04459346830844879, 'rmse': 0.21117165602525542, 'r2': -0.3310497999191284}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1654Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1226Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1040Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1088Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1242Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1422Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1377Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1324Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1479Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1446Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1441Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1487Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1518Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1463Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1451Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1455Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1511Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1495Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1451Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1418Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1435Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1405Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1409Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1424Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1382Epoch 3/15: [================              ] 26/47 batches, loss: 0.1369Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1363Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1340Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1322Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1314Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1331Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1324Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1321Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1311Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1324Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1315Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1308Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1292Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1268Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1268Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1270Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1276Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1273Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1300Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1312Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1308Epoch 3/15: [==============================] 47/47 batches, loss: 0.1311
[2025-05-07 22:15:00,300][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1311
[2025-05-07 22:15:00,579][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0545, Metrics: {'mse': 0.05830424278974533, 'rmse': 0.24146271511300732, 'r2': -0.7402963638305664}
[2025-05-07 22:15:00,580][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1422Epoch 4/15: [=                             ] 2/47 batches, loss: 0.0999Epoch 4/15: [=                             ] 3/47 batches, loss: 0.0841Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1003Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1101Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1062Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1086Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1092Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1104Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1186Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1274Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1271Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1243Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1219Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1201Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1179Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1166Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1156Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1148Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1203Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1229Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1215Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1212Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1207Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1230Epoch 4/15: [================              ] 26/47 batches, loss: 0.1206Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1213Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1222Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1215Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1197Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1196Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1198Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1183Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1195Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1197Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1192Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1224Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1221Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1212Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1213Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1215Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1220Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1199Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1193Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1185Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1180Epoch 4/15: [==============================] 47/47 batches, loss: 0.1177
[2025-05-07 22:15:02,042][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1177
[2025-05-07 22:15:02,482][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0478, Metrics: {'mse': 0.05110708251595497, 'rmse': 0.22606875616934546, 'r2': -0.5254716873168945}
[2025-05-07 22:15:02,482][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.1563Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1402Epoch 5/15: [=                             ] 3/47 batches, loss: 0.1255Epoch 5/15: [==                            ] 4/47 batches, loss: 0.1051Epoch 5/15: [===                           ] 5/47 batches, loss: 0.1007Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0981Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0992Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0975Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0966Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0966Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0967Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0948Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0933Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0920Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0904Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0904Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0883Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0888Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0895Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0896Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0935Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0927Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0916Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0912Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0930Epoch 5/15: [================              ] 26/47 batches, loss: 0.0922Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0912Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0904Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0904Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0924Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0920Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0923Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0928Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0914Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0917Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0918Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0903Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0914Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0910Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0913Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0905Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0899Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0915Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0915Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0915Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0911Epoch 5/15: [==============================] 47/47 batches, loss: 0.0911
[2025-05-07 22:15:04,037][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0911
[2025-05-07 22:15:04,350][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0507, Metrics: {'mse': 0.054538026452064514, 'rmse': 0.23353378010914078, 'r2': -0.6278802156448364}
[2025-05-07 22:15:04,351][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.0748Epoch 6/15: [=                             ] 2/47 batches, loss: 0.0841Epoch 6/15: [=                             ] 3/47 batches, loss: 0.0879Epoch 6/15: [==                            ] 4/47 batches, loss: 0.0814Epoch 6/15: [===                           ] 5/47 batches, loss: 0.0991Epoch 6/15: [===                           ] 6/47 batches, loss: 0.0975Epoch 6/15: [====                          ] 7/47 batches, loss: 0.0957Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.1038Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.0975Epoch 6/15: [======                        ] 10/47 batches, loss: 0.0991Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.0953Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.1062Epoch 6/15: [========                      ] 13/47 batches, loss: 0.1023Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0976Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0951Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.0970Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.0978Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.0984Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0949Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0930Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0905Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0908Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0913Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0897Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0890Epoch 6/15: [================              ] 26/47 batches, loss: 0.0880Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0887Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0880Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0892Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0882Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0900Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0877Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0881Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0895Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0896Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0879Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0868Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0859Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0860Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0854Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0872Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0885Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0882Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0881Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0879Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0868Epoch 6/15: [==============================] 47/47 batches, loss: 0.0865
[2025-05-07 22:15:05,885][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0865
[2025-05-07 22:15:06,179][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0486, Metrics: {'mse': 0.051926389336586, 'rmse': 0.22787362580295684, 'r2': -0.5499267578125}
[2025-05-07 22:15:06,180][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:15:06,180][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-07 22:15:06,180][src.training.lm_trainer][INFO] - Training completed in 12.52 seconds
[2025-05-07 22:15:06,180][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:15:08,464][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03358876705169678, 'rmse': 0.1832723848584308, 'r2': -0.15000700950622559}
[2025-05-07 22:15:08,464][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.04459346830844879, 'rmse': 0.21117165602525542, 'r2': -0.3310497999191284}
[2025-05-07 22:15:08,465][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.050504714250564575, 'rmse': 0.2247325393674992, 'r2': -0.4465065002441406}
[2025-05-07 22:15:10,126][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer2/ko/ko/model.pt
[2025-05-07 22:15:10,128][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▇▆▆▆
wandb:       train_loss █▄▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▃▂▂▂
wandb:          val_mse █▁▃▂▂▂
wandb:           val_r2 ▁█▆▇▇▇
wandb:         val_rmse █▁▃▂▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04271
wandb:     best_val_mse 0.04459
wandb:      best_val_r2 -0.33105
wandb:    best_val_rmse 0.21117
wandb: early_stop_epoch 6
wandb:            epoch 6
wandb:   final_test_mse 0.0505
wandb:    final_test_r2 -0.44651
wandb:  final_test_rmse 0.22473
wandb:  final_train_mse 0.03359
wandb:   final_train_r2 -0.15001
wandb: final_train_rmse 0.18327
wandb:    final_val_mse 0.04459
wandb:     final_val_r2 -0.33105
wandb:   final_val_rmse 0.21117
wandb:    learning_rate 0.0001
wandb:       train_loss 0.08648
wandb:       train_time 12.51848
wandb:         val_loss 0.04857
wandb:          val_mse 0.05193
wandb:           val_r2 -0.54993
wandb:         val_rmse 0.22787
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_221435-hrbgmiyh
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_221435-hrbgmiyh/logs
Experiment probe_layer2_avg_max_depth_control3_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_avg_subordinate_chain_len_control1_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_subordinate_chain_len_control1_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_subordinate_chain_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:15:40,598][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer2/ko
experiment_name: probe_layer2_avg_subordinate_chain_len_control1_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:15:40,598][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:15:40,598][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 22:15:40,598][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:15:40,598][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:15:40,603][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:15:40,603][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 22:15:40,603][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:15:44,056][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:15:46,356][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:15:46,357][src.data.datasets][INFO] - Loading 'control_avg_subordinate_chain_len_seed1' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:15:46,545][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:23:31 2025).
[2025-05-07 22:15:46,619][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:23:31 2025).
[2025-05-07 22:15:46,897][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:15:46,904][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:15:46,904][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:15:46,907][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:15:46,999][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:15:47,074][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:15:47,099][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:15:47,100][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:15:47,100][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:15:47,102][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:15:47,231][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:15:47,305][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:15:47,336][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:15:47,338][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:15:47,338][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:15:47,340][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:15:47,341][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:15:47,341][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 22:15:47,341][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 22:15:47,342][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 22:15:47,342][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:15:47,342][src.data.datasets][INFO] -   Mean: 0.1845, Std: 0.2535
[2025-05-07 22:15:47,342][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:15:47,342][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 22:15:47,342][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:15:47,342][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 22:15:47,342][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 22:15:47,343][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 22:15:47,343][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:15:47,343][src.data.datasets][INFO] -   Mean: 0.2673, Std: 0.2561
[2025-05-07 22:15:47,343][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:15:47,343][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 22:15:47,343][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:15:47,343][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 22:15:47,343][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 22:15:47,343][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 22:15:47,343][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6670
[2025-05-07 22:15:47,344][src.data.datasets][INFO] -   Mean: 0.2435, Std: 0.2248
[2025-05-07 22:15:47,344][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:15:47,344][src.data.datasets][INFO] - Sample label: 0.5
[2025-05-07 22:15:47,344][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:15:47,344][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:15:47,344][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:15:47,345][__main__][INFO] - Using model type: lm_probe for submetric avg_subordinate_chain_len
[2025-05-07 22:15:47,345][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:15:54,465][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:15:54,467][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:15:54,467][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:15:54,467][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:15:54,470][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:15:54,470][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:15:54,470][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:15:54,470][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:15:54,470][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:15:54,471][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:15:54,471][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.3935Epoch 1/15: [=                             ] 2/47 batches, loss: 0.4869Epoch 1/15: [=                             ] 3/47 batches, loss: 0.4751Epoch 1/15: [==                            ] 4/47 batches, loss: 0.4488Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4428Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4082Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4166Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4317Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4518Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4454Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4352Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4454Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4402Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4313Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.4295Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4322Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.4234Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4294Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4205Epoch 1/15: [============                  ] 20/47 batches, loss: 0.4131Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4282Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4224Epoch 1/15: [==============                ] 23/47 batches, loss: 0.4174Epoch 1/15: [===============               ] 24/47 batches, loss: 0.4090Epoch 1/15: [===============               ] 25/47 batches, loss: 0.4054Epoch 1/15: [================              ] 26/47 batches, loss: 0.4057Epoch 1/15: [=================             ] 27/47 batches, loss: 0.4083Epoch 1/15: [=================             ] 28/47 batches, loss: 0.4015Epoch 1/15: [==================            ] 29/47 batches, loss: 0.4036Epoch 1/15: [===================           ] 30/47 batches, loss: 0.4022Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3937Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3889Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3885Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3857Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3815Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3777Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3739Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3705Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3677Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3698Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3698Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3672Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3644Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3645Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3656Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3604Epoch 1/15: [==============================] 47/47 batches, loss: 0.3687
[2025-05-07 22:16:02,613][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3687
[2025-05-07 22:16:02,965][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1474, Metrics: {'mse': 0.1501029133796692, 'rmse': 0.38743117244185343, 'r2': -1.2891323566436768}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.4249Epoch 2/15: [=                             ] 2/47 batches, loss: 0.3207Epoch 2/15: [=                             ] 3/47 batches, loss: 0.3400Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2883Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2930Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2952Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2784Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2762Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2630Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2607Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2712Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2642Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2663Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2596Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2595Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2569Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2475Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2494Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2455Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2377Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2384Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2373Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2360Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2337Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2319Epoch 2/15: [================              ] 26/47 batches, loss: 0.2297Epoch 2/15: [=================             ] 27/47 batches, loss: 0.2246Epoch 2/15: [=================             ] 28/47 batches, loss: 0.2219Epoch 2/15: [==================            ] 29/47 batches, loss: 0.2196Epoch 2/15: [===================           ] 30/47 batches, loss: 0.2145Epoch 2/15: [===================           ] 31/47 batches, loss: 0.2155Epoch 2/15: [====================          ] 32/47 batches, loss: 0.2147Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.2168Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.2160Epoch 2/15: [======================        ] 35/47 batches, loss: 0.2127Epoch 2/15: [======================        ] 36/47 batches, loss: 0.2094Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.2114Epoch 2/15: [========================      ] 38/47 batches, loss: 0.2114Epoch 2/15: [========================      ] 39/47 batches, loss: 0.2102Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.2103Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.2084Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.2070Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.2051Epoch 2/15: [============================  ] 44/47 batches, loss: 0.2041Epoch 2/15: [============================  ] 45/47 batches, loss: 0.2020Epoch 2/15: [============================= ] 46/47 batches, loss: 0.2008Epoch 2/15: [==============================] 47/47 batches, loss: 0.1973
[2025-05-07 22:16:05,046][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1973
[2025-05-07 22:16:05,380][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1013, Metrics: {'mse': 0.10211782902479172, 'rmse': 0.31955880370409406, 'r2': -0.5573397874832153}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.2197Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1731Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1706Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1679Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1854Epoch 3/15: [===                           ] 6/47 batches, loss: 0.2057Epoch 3/15: [====                          ] 7/47 batches, loss: 0.2019Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1883Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1977Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1922Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1943Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1966Epoch 3/15: [========                      ] 13/47 batches, loss: 0.2019Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1949Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1901Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1939Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1984Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.2001Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1944Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1907Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1914Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1906Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1902Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1909Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1876Epoch 3/15: [================              ] 26/47 batches, loss: 0.1860Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1864Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1847Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1819Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1793Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1790Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1765Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1773Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1775Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1792Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1801Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1796Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1789Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1771Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1770Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1755Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1747Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1756Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1768Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1771Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1759Epoch 3/15: [==============================] 47/47 batches, loss: 0.1768
[2025-05-07 22:16:07,288][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1768
[2025-05-07 22:16:07,557][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1277, Metrics: {'mse': 0.12957903742790222, 'rmse': 0.3599708841391234, 'r2': -0.9761347770690918}
[2025-05-07 22:16:07,558][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1797Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1384Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1456Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1451Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1349Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1408Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1427Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1612Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1641Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1629Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1777Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1682Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1675Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1609Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1628Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1596Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1566Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1583Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1566Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1606Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1579Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1608Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1599Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1600Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1647Epoch 4/15: [================              ] 26/47 batches, loss: 0.1627Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1635Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1633Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1660Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1661Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1654Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1680Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1648Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1655Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1647Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1640Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1646Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1640Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1622Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1612Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1603Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1601Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1598Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1608Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1594Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1606Epoch 4/15: [==============================] 47/47 batches, loss: 0.1610
[2025-05-07 22:16:09,018][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1610
[2025-05-07 22:16:09,286][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1005, Metrics: {'mse': 0.10181556642055511, 'rmse': 0.3190855158426266, 'r2': -0.5527302026748657}
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.1332Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1470Epoch 5/15: [=                             ] 3/47 batches, loss: 0.1563Epoch 5/15: [==                            ] 4/47 batches, loss: 0.1406Epoch 5/15: [===                           ] 5/47 batches, loss: 0.1205Epoch 5/15: [===                           ] 6/47 batches, loss: 0.1178Epoch 5/15: [====                          ] 7/47 batches, loss: 0.1164Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.1187Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.1283Epoch 5/15: [======                        ] 10/47 batches, loss: 0.1217Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.1204Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.1222Epoch 5/15: [========                      ] 13/47 batches, loss: 0.1186Epoch 5/15: [========                      ] 14/47 batches, loss: 0.1185Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.1177Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.1182Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.1148Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.1165Epoch 5/15: [============                  ] 19/47 batches, loss: 0.1149Epoch 5/15: [============                  ] 20/47 batches, loss: 0.1187Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.1196Epoch 5/15: [==============                ] 22/47 batches, loss: 0.1243Epoch 5/15: [==============                ] 23/47 batches, loss: 0.1256Epoch 5/15: [===============               ] 24/47 batches, loss: 0.1246Epoch 5/15: [===============               ] 25/47 batches, loss: 0.1244Epoch 5/15: [================              ] 26/47 batches, loss: 0.1246Epoch 5/15: [=================             ] 27/47 batches, loss: 0.1279Epoch 5/15: [=================             ] 28/47 batches, loss: 0.1275Epoch 5/15: [==================            ] 29/47 batches, loss: 0.1289Epoch 5/15: [===================           ] 30/47 batches, loss: 0.1329Epoch 5/15: [===================           ] 31/47 batches, loss: 0.1311Epoch 5/15: [====================          ] 32/47 batches, loss: 0.1313Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.1337Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.1331Epoch 5/15: [======================        ] 35/47 batches, loss: 0.1355Epoch 5/15: [======================        ] 36/47 batches, loss: 0.1355Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.1351Epoch 5/15: [========================      ] 38/47 batches, loss: 0.1347Epoch 5/15: [========================      ] 39/47 batches, loss: 0.1333Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.1343Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.1347Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.1348Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.1339Epoch 5/15: [============================  ] 44/47 batches, loss: 0.1326Epoch 5/15: [============================  ] 45/47 batches, loss: 0.1315Epoch 5/15: [============================= ] 46/47 batches, loss: 0.1311Epoch 5/15: [==============================] 47/47 batches, loss: 0.1308
[2025-05-07 22:16:11,224][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1308
[2025-05-07 22:16:11,522][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1048, Metrics: {'mse': 0.10629837960004807, 'rmse': 0.3260343227331258, 'r2': -0.6210949420928955}
[2025-05-07 22:16:11,523][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.1652Epoch 6/15: [=                             ] 2/47 batches, loss: 0.1275Epoch 6/15: [=                             ] 3/47 batches, loss: 0.1417Epoch 6/15: [==                            ] 4/47 batches, loss: 0.1389Epoch 6/15: [===                           ] 5/47 batches, loss: 0.1413Epoch 6/15: [===                           ] 6/47 batches, loss: 0.1361Epoch 6/15: [====                          ] 7/47 batches, loss: 0.1319Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.1312Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.1269Epoch 6/15: [======                        ] 10/47 batches, loss: 0.1344Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.1344Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.1357Epoch 6/15: [========                      ] 13/47 batches, loss: 0.1317Epoch 6/15: [========                      ] 14/47 batches, loss: 0.1267Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.1257Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.1277Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.1267Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.1264Epoch 6/15: [============                  ] 19/47 batches, loss: 0.1225Epoch 6/15: [============                  ] 20/47 batches, loss: 0.1202Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.1191Epoch 6/15: [==============                ] 22/47 batches, loss: 0.1200Epoch 6/15: [==============                ] 23/47 batches, loss: 0.1188Epoch 6/15: [===============               ] 24/47 batches, loss: 0.1193Epoch 6/15: [===============               ] 25/47 batches, loss: 0.1170Epoch 6/15: [================              ] 26/47 batches, loss: 0.1149Epoch 6/15: [=================             ] 27/47 batches, loss: 0.1142Epoch 6/15: [=================             ] 28/47 batches, loss: 0.1161Epoch 6/15: [==================            ] 29/47 batches, loss: 0.1204Epoch 6/15: [===================           ] 30/47 batches, loss: 0.1204Epoch 6/15: [===================           ] 31/47 batches, loss: 0.1220Epoch 6/15: [====================          ] 32/47 batches, loss: 0.1207Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.1240Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.1227Epoch 6/15: [======================        ] 35/47 batches, loss: 0.1222Epoch 6/15: [======================        ] 36/47 batches, loss: 0.1218Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.1232Epoch 6/15: [========================      ] 38/47 batches, loss: 0.1226Epoch 6/15: [========================      ] 39/47 batches, loss: 0.1232Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.1231Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.1235Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.1235Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.1240Epoch 6/15: [============================  ] 44/47 batches, loss: 0.1241Epoch 6/15: [============================  ] 45/47 batches, loss: 0.1251Epoch 6/15: [============================= ] 46/47 batches, loss: 0.1249Epoch 6/15: [==============================] 47/47 batches, loss: 0.1238
[2025-05-07 22:16:13,012][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1238
[2025-05-07 22:16:13,355][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0870, Metrics: {'mse': 0.08767322450876236, 'rmse': 0.29609664724336604, 'r2': -0.3370535373687744}
Epoch 7/15: [Epoch 7/15: [                              ] 1/47 batches, loss: 0.1627Epoch 7/15: [=                             ] 2/47 batches, loss: 0.1503Epoch 7/15: [=                             ] 3/47 batches, loss: 0.1365Epoch 7/15: [==                            ] 4/47 batches, loss: 0.1220Epoch 7/15: [===                           ] 5/47 batches, loss: 0.1197Epoch 7/15: [===                           ] 6/47 batches, loss: 0.1145Epoch 7/15: [====                          ] 7/47 batches, loss: 0.1261Epoch 7/15: [=====                         ] 8/47 batches, loss: 0.1229Epoch 7/15: [=====                         ] 9/47 batches, loss: 0.1199Epoch 7/15: [======                        ] 10/47 batches, loss: 0.1164Epoch 7/15: [=======                       ] 11/47 batches, loss: 0.1137Epoch 7/15: [=======                       ] 12/47 batches, loss: 0.1154Epoch 7/15: [========                      ] 13/47 batches, loss: 0.1156Epoch 7/15: [========                      ] 14/47 batches, loss: 0.1165Epoch 7/15: [=========                     ] 15/47 batches, loss: 0.1175Epoch 7/15: [==========                    ] 16/47 batches, loss: 0.1214Epoch 7/15: [==========                    ] 17/47 batches, loss: 0.1177Epoch 7/15: [===========                   ] 18/47 batches, loss: 0.1192Epoch 7/15: [============                  ] 19/47 batches, loss: 0.1196Epoch 7/15: [============                  ] 20/47 batches, loss: 0.1172Epoch 7/15: [=============                 ] 21/47 batches, loss: 0.1186Epoch 7/15: [==============                ] 22/47 batches, loss: 0.1183Epoch 7/15: [==============                ] 23/47 batches, loss: 0.1173Epoch 7/15: [===============               ] 24/47 batches, loss: 0.1160Epoch 7/15: [===============               ] 25/47 batches, loss: 0.1159Epoch 7/15: [================              ] 26/47 batches, loss: 0.1185Epoch 7/15: [=================             ] 27/47 batches, loss: 0.1170Epoch 7/15: [=================             ] 28/47 batches, loss: 0.1152Epoch 7/15: [==================            ] 29/47 batches, loss: 0.1142Epoch 7/15: [===================           ] 30/47 batches, loss: 0.1164Epoch 7/15: [===================           ] 31/47 batches, loss: 0.1161Epoch 7/15: [====================          ] 32/47 batches, loss: 0.1158Epoch 7/15: [=====================         ] 33/47 batches, loss: 0.1153Epoch 7/15: [=====================         ] 34/47 batches, loss: 0.1152Epoch 7/15: [======================        ] 35/47 batches, loss: 0.1143Epoch 7/15: [======================        ] 36/47 batches, loss: 0.1155Epoch 7/15: [=======================       ] 37/47 batches, loss: 0.1154Epoch 7/15: [========================      ] 38/47 batches, loss: 0.1139Epoch 7/15: [========================      ] 39/47 batches, loss: 0.1138Epoch 7/15: [=========================     ] 40/47 batches, loss: 0.1139Epoch 7/15: [==========================    ] 41/47 batches, loss: 0.1136Epoch 7/15: [==========================    ] 42/47 batches, loss: 0.1128Epoch 7/15: [===========================   ] 43/47 batches, loss: 0.1128Epoch 7/15: [============================  ] 44/47 batches, loss: 0.1125Epoch 7/15: [============================  ] 45/47 batches, loss: 0.1135Epoch 7/15: [============================= ] 46/47 batches, loss: 0.1119Epoch 7/15: [==============================] 47/47 batches, loss: 0.1138
[2025-05-07 22:16:15,196][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.1138
[2025-05-07 22:16:15,485][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1019, Metrics: {'mse': 0.10318783670663834, 'rmse': 0.32122863618712194, 'r2': -0.5736578702926636}
[2025-05-07 22:16:15,486][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/47 batches, loss: 0.0698Epoch 8/15: [=                             ] 2/47 batches, loss: 0.0793Epoch 8/15: [=                             ] 3/47 batches, loss: 0.0960Epoch 8/15: [==                            ] 4/47 batches, loss: 0.0996Epoch 8/15: [===                           ] 5/47 batches, loss: 0.1102Epoch 8/15: [===                           ] 6/47 batches, loss: 0.1057Epoch 8/15: [====                          ] 7/47 batches, loss: 0.1127Epoch 8/15: [=====                         ] 8/47 batches, loss: 0.1120Epoch 8/15: [=====                         ] 9/47 batches, loss: 0.1061Epoch 8/15: [======                        ] 10/47 batches, loss: 0.1095Epoch 8/15: [=======                       ] 11/47 batches, loss: 0.1085Epoch 8/15: [=======                       ] 12/47 batches, loss: 0.1063Epoch 8/15: [========                      ] 13/47 batches, loss: 0.1039Epoch 8/15: [========                      ] 14/47 batches, loss: 0.1011Epoch 8/15: [=========                     ] 15/47 batches, loss: 0.1014Epoch 8/15: [==========                    ] 16/47 batches, loss: 0.0969Epoch 8/15: [==========                    ] 17/47 batches, loss: 0.1001Epoch 8/15: [===========                   ] 18/47 batches, loss: 0.0989Epoch 8/15: [============                  ] 19/47 batches, loss: 0.0982Epoch 8/15: [============                  ] 20/47 batches, loss: 0.0965Epoch 8/15: [=============                 ] 21/47 batches, loss: 0.0961Epoch 8/15: [==============                ] 22/47 batches, loss: 0.0959Epoch 8/15: [==============                ] 23/47 batches, loss: 0.0940Epoch 8/15: [===============               ] 24/47 batches, loss: 0.0955Epoch 8/15: [===============               ] 25/47 batches, loss: 0.0971Epoch 8/15: [================              ] 26/47 batches, loss: 0.0972Epoch 8/15: [=================             ] 27/47 batches, loss: 0.0966Epoch 8/15: [=================             ] 28/47 batches, loss: 0.0977Epoch 8/15: [==================            ] 29/47 batches, loss: 0.0976Epoch 8/15: [===================           ] 30/47 batches, loss: 0.0986Epoch 8/15: [===================           ] 31/47 batches, loss: 0.0979Epoch 8/15: [====================          ] 32/47 batches, loss: 0.0978Epoch 8/15: [=====================         ] 33/47 batches, loss: 0.0977Epoch 8/15: [=====================         ] 34/47 batches, loss: 0.0968Epoch 8/15: [======================        ] 35/47 batches, loss: 0.0978Epoch 8/15: [======================        ] 36/47 batches, loss: 0.0977Epoch 8/15: [=======================       ] 37/47 batches, loss: 0.0973Epoch 8/15: [========================      ] 38/47 batches, loss: 0.0970Epoch 8/15: [========================      ] 39/47 batches, loss: 0.0957Epoch 8/15: [=========================     ] 40/47 batches, loss: 0.0954Epoch 8/15: [==========================    ] 41/47 batches, loss: 0.0949Epoch 8/15: [==========================    ] 42/47 batches, loss: 0.0950Epoch 8/15: [===========================   ] 43/47 batches, loss: 0.0947Epoch 8/15: [============================  ] 44/47 batches, loss: 0.0947Epoch 8/15: [============================  ] 45/47 batches, loss: 0.0939Epoch 8/15: [============================= ] 46/47 batches, loss: 0.0937Epoch 8/15: [==============================] 47/47 batches, loss: 0.0947
[2025-05-07 22:16:16,972][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0947
[2025-05-07 22:16:17,262][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0942, Metrics: {'mse': 0.09532430768013, 'rmse': 0.30874634844825294, 'r2': -0.4537358283996582}
[2025-05-07 22:16:17,263][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/47 batches, loss: 0.0850Epoch 9/15: [=                             ] 2/47 batches, loss: 0.0822Epoch 9/15: [=                             ] 3/47 batches, loss: 0.0856Epoch 9/15: [==                            ] 4/47 batches, loss: 0.0995Epoch 9/15: [===                           ] 5/47 batches, loss: 0.1061Epoch 9/15: [===                           ] 6/47 batches, loss: 0.1108Epoch 9/15: [====                          ] 7/47 batches, loss: 0.1088Epoch 9/15: [=====                         ] 8/47 batches, loss: 0.1071Epoch 9/15: [=====                         ] 9/47 batches, loss: 0.1062Epoch 9/15: [======                        ] 10/47 batches, loss: 0.1066Epoch 9/15: [=======                       ] 11/47 batches, loss: 0.1018Epoch 9/15: [=======                       ] 12/47 batches, loss: 0.1016Epoch 9/15: [========                      ] 13/47 batches, loss: 0.1017Epoch 9/15: [========                      ] 14/47 batches, loss: 0.0998Epoch 9/15: [=========                     ] 15/47 batches, loss: 0.0995Epoch 9/15: [==========                    ] 16/47 batches, loss: 0.0975Epoch 9/15: [==========                    ] 17/47 batches, loss: 0.1012Epoch 9/15: [===========                   ] 18/47 batches, loss: 0.0992Epoch 9/15: [============                  ] 19/47 batches, loss: 0.0993Epoch 9/15: [============                  ] 20/47 batches, loss: 0.0990Epoch 9/15: [=============                 ] 21/47 batches, loss: 0.0999Epoch 9/15: [==============                ] 22/47 batches, loss: 0.0996Epoch 9/15: [==============                ] 23/47 batches, loss: 0.0991Epoch 9/15: [===============               ] 24/47 batches, loss: 0.0981Epoch 9/15: [===============               ] 25/47 batches, loss: 0.0983Epoch 9/15: [================              ] 26/47 batches, loss: 0.0983Epoch 9/15: [=================             ] 27/47 batches, loss: 0.1009Epoch 9/15: [=================             ] 28/47 batches, loss: 0.1008Epoch 9/15: [==================            ] 29/47 batches, loss: 0.0998Epoch 9/15: [===================           ] 30/47 batches, loss: 0.1014Epoch 9/15: [===================           ] 31/47 batches, loss: 0.1007Epoch 9/15: [====================          ] 32/47 batches, loss: 0.1018Epoch 9/15: [=====================         ] 33/47 batches, loss: 0.1020Epoch 9/15: [=====================         ] 34/47 batches, loss: 0.1031Epoch 9/15: [======================        ] 35/47 batches, loss: 0.1037Epoch 9/15: [======================        ] 36/47 batches, loss: 0.1028Epoch 9/15: [=======================       ] 37/47 batches, loss: 0.1038Epoch 9/15: [========================      ] 38/47 batches, loss: 0.1039Epoch 9/15: [========================      ] 39/47 batches, loss: 0.1039Epoch 9/15: [=========================     ] 40/47 batches, loss: 0.1043Epoch 9/15: [==========================    ] 41/47 batches, loss: 0.1046Epoch 9/15: [==========================    ] 42/47 batches, loss: 0.1055Epoch 9/15: [===========================   ] 43/47 batches, loss: 0.1066Epoch 9/15: [============================  ] 44/47 batches, loss: 0.1059Epoch 9/15: [============================  ] 45/47 batches, loss: 0.1053Epoch 9/15: [============================= ] 46/47 batches, loss: 0.1059Epoch 9/15: [==============================] 47/47 batches, loss: 0.1054
[2025-05-07 22:16:18,736][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.1054
[2025-05-07 22:16:18,999][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0857, Metrics: {'mse': 0.08633845299482346, 'rmse': 0.29383405690086956, 'r2': -0.31669771671295166}
Epoch 10/15: [Epoch 10/15: [                              ] 1/47 batches, loss: 0.1052Epoch 10/15: [=                             ] 2/47 batches, loss: 0.0825Epoch 10/15: [=                             ] 3/47 batches, loss: 0.0850Epoch 10/15: [==                            ] 4/47 batches, loss: 0.0872Epoch 10/15: [===                           ] 5/47 batches, loss: 0.0969Epoch 10/15: [===                           ] 6/47 batches, loss: 0.0937Epoch 10/15: [====                          ] 7/47 batches, loss: 0.0930Epoch 10/15: [=====                         ] 8/47 batches, loss: 0.0941Epoch 10/15: [=====                         ] 9/47 batches, loss: 0.1013Epoch 10/15: [======                        ] 10/47 batches, loss: 0.1053Epoch 10/15: [=======                       ] 11/47 batches, loss: 0.1030Epoch 10/15: [=======                       ] 12/47 batches, loss: 0.1015Epoch 10/15: [========                      ] 13/47 batches, loss: 0.1009Epoch 10/15: [========                      ] 14/47 batches, loss: 0.1012Epoch 10/15: [=========                     ] 15/47 batches, loss: 0.0995Epoch 10/15: [==========                    ] 16/47 batches, loss: 0.0971Epoch 10/15: [==========                    ] 17/47 batches, loss: 0.0962Epoch 10/15: [===========                   ] 18/47 batches, loss: 0.0977Epoch 10/15: [============                  ] 19/47 batches, loss: 0.1013Epoch 10/15: [============                  ] 20/47 batches, loss: 0.1046Epoch 10/15: [=============                 ] 21/47 batches, loss: 0.1082Epoch 10/15: [==============                ] 22/47 batches, loss: 0.1071Epoch 10/15: [==============                ] 23/47 batches, loss: 0.1075Epoch 10/15: [===============               ] 24/47 batches, loss: 0.1075Epoch 10/15: [===============               ] 25/47 batches, loss: 0.1080Epoch 10/15: [================              ] 26/47 batches, loss: 0.1054Epoch 10/15: [=================             ] 27/47 batches, loss: 0.1046Epoch 10/15: [=================             ] 28/47 batches, loss: 0.1035Epoch 10/15: [==================            ] 29/47 batches, loss: 0.1035Epoch 10/15: [===================           ] 30/47 batches, loss: 0.1029Epoch 10/15: [===================           ] 31/47 batches, loss: 0.1029Epoch 10/15: [====================          ] 32/47 batches, loss: 0.1022Epoch 10/15: [=====================         ] 33/47 batches, loss: 0.1009Epoch 10/15: [=====================         ] 34/47 batches, loss: 0.1003Epoch 10/15: [======================        ] 35/47 batches, loss: 0.0995Epoch 10/15: [======================        ] 36/47 batches, loss: 0.0992Epoch 10/15: [=======================       ] 37/47 batches, loss: 0.0984Epoch 10/15: [========================      ] 38/47 batches, loss: 0.0999Epoch 10/15: [========================      ] 39/47 batches, loss: 0.0988Epoch 10/15: [=========================     ] 40/47 batches, loss: 0.0991Epoch 10/15: [==========================    ] 41/47 batches, loss: 0.0988Epoch 10/15: [==========================    ] 42/47 batches, loss: 0.0994Epoch 10/15: [===========================   ] 43/47 batches, loss: 0.0988Epoch 10/15: [============================  ] 44/47 batches, loss: 0.1003Epoch 10/15: [============================  ] 45/47 batches, loss: 0.1034Epoch 10/15: [============================= ] 46/47 batches, loss: 0.1031Epoch 10/15: [==============================] 47/47 batches, loss: 0.1023
[2025-05-07 22:16:20,910][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.1023
[2025-05-07 22:16:21,177][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0931, Metrics: {'mse': 0.09404081851243973, 'rmse': 0.30666075476402216, 'r2': -0.4341620206832886}
[2025-05-07 22:16:21,177][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/47 batches, loss: 0.1118Epoch 11/15: [=                             ] 2/47 batches, loss: 0.1120Epoch 11/15: [=                             ] 3/47 batches, loss: 0.1178Epoch 11/15: [==                            ] 4/47 batches, loss: 0.1295Epoch 11/15: [===                           ] 5/47 batches, loss: 0.1177Epoch 11/15: [===                           ] 6/47 batches, loss: 0.1217Epoch 11/15: [====                          ] 7/47 batches, loss: 0.1229Epoch 11/15: [=====                         ] 8/47 batches, loss: 0.1240Epoch 11/15: [=====                         ] 9/47 batches, loss: 0.1232Epoch 11/15: [======                        ] 10/47 batches, loss: 0.1234Epoch 11/15: [=======                       ] 11/47 batches, loss: 0.1183Epoch 11/15: [=======                       ] 12/47 batches, loss: 0.1153Epoch 11/15: [========                      ] 13/47 batches, loss: 0.1172Epoch 11/15: [========                      ] 14/47 batches, loss: 0.1143Epoch 11/15: [=========                     ] 15/47 batches, loss: 0.1114Epoch 11/15: [==========                    ] 16/47 batches, loss: 0.1115Epoch 11/15: [==========                    ] 17/47 batches, loss: 0.1090Epoch 11/15: [===========                   ] 18/47 batches, loss: 0.1095Epoch 11/15: [============                  ] 19/47 batches, loss: 0.1115Epoch 11/15: [============                  ] 20/47 batches, loss: 0.1130Epoch 11/15: [=============                 ] 21/47 batches, loss: 0.1103Epoch 11/15: [==============                ] 22/47 batches, loss: 0.1099Epoch 11/15: [==============                ] 23/47 batches, loss: 0.1084Epoch 11/15: [===============               ] 24/47 batches, loss: 0.1067Epoch 11/15: [===============               ] 25/47 batches, loss: 0.1050Epoch 11/15: [================              ] 26/47 batches, loss: 0.1041Epoch 11/15: [=================             ] 27/47 batches, loss: 0.1033Epoch 11/15: [=================             ] 28/47 batches, loss: 0.1023Epoch 11/15: [==================            ] 29/47 batches, loss: 0.1017Epoch 11/15: [===================           ] 30/47 batches, loss: 0.1022Epoch 11/15: [===================           ] 31/47 batches, loss: 0.1016Epoch 11/15: [====================          ] 32/47 batches, loss: 0.1008Epoch 11/15: [=====================         ] 33/47 batches, loss: 0.0997Epoch 11/15: [=====================         ] 34/47 batches, loss: 0.1000Epoch 11/15: [======================        ] 35/47 batches, loss: 0.0995Epoch 11/15: [======================        ] 36/47 batches, loss: 0.1002Epoch 11/15: [=======================       ] 37/47 batches, loss: 0.0991Epoch 11/15: [========================      ] 38/47 batches, loss: 0.0983Epoch 11/15: [========================      ] 39/47 batches, loss: 0.0988Epoch 11/15: [=========================     ] 40/47 batches, loss: 0.0978Epoch 11/15: [==========================    ] 41/47 batches, loss: 0.0996Epoch 11/15: [==========================    ] 42/47 batches, loss: 0.1001Epoch 11/15: [===========================   ] 43/47 batches, loss: 0.1000Epoch 11/15: [============================  ] 44/47 batches, loss: 0.0999Epoch 11/15: [============================  ] 45/47 batches, loss: 0.0999Epoch 11/15: [============================= ] 46/47 batches, loss: 0.0996Epoch 11/15: [==============================] 47/47 batches, loss: 0.1004
[2025-05-07 22:16:22,651][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.1004
[2025-05-07 22:16:22,977][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0838, Metrics: {'mse': 0.08469005674123764, 'rmse': 0.2910155609950053, 'r2': -0.2915588617324829}
Epoch 12/15: [Epoch 12/15: [                              ] 1/47 batches, loss: 0.1166Epoch 12/15: [=                             ] 2/47 batches, loss: 0.0916Epoch 12/15: [=                             ] 3/47 batches, loss: 0.0949Epoch 12/15: [==                            ] 4/47 batches, loss: 0.0806Epoch 12/15: [===                           ] 5/47 batches, loss: 0.0865Epoch 12/15: [===                           ] 6/47 batches, loss: 0.0900Epoch 12/15: [====                          ] 7/47 batches, loss: 0.0879Epoch 12/15: [=====                         ] 8/47 batches, loss: 0.0842Epoch 12/15: [=====                         ] 9/47 batches, loss: 0.0833Epoch 12/15: [======                        ] 10/47 batches, loss: 0.0811Epoch 12/15: [=======                       ] 11/47 batches, loss: 0.0845Epoch 12/15: [=======                       ] 12/47 batches, loss: 0.0847Epoch 12/15: [========                      ] 13/47 batches, loss: 0.0964Epoch 12/15: [========                      ] 14/47 batches, loss: 0.0980Epoch 12/15: [=========                     ] 15/47 batches, loss: 0.0941Epoch 12/15: [==========                    ] 16/47 batches, loss: 0.0967Epoch 12/15: [==========                    ] 17/47 batches, loss: 0.0944Epoch 12/15: [===========                   ] 18/47 batches, loss: 0.0930Epoch 12/15: [============                  ] 19/47 batches, loss: 0.0909Epoch 12/15: [============                  ] 20/47 batches, loss: 0.0936Epoch 12/15: [=============                 ] 21/47 batches, loss: 0.0934Epoch 12/15: [==============                ] 22/47 batches, loss: 0.0919Epoch 12/15: [==============                ] 23/47 batches, loss: 0.0930Epoch 12/15: [===============               ] 24/47 batches, loss: 0.0925Epoch 12/15: [===============               ] 25/47 batches, loss: 0.0938Epoch 12/15: [================              ] 26/47 batches, loss: 0.0932Epoch 12/15: [=================             ] 27/47 batches, loss: 0.0928Epoch 12/15: [=================             ] 28/47 batches, loss: 0.0921Epoch 12/15: [==================            ] 29/47 batches, loss: 0.0920Epoch 12/15: [===================           ] 30/47 batches, loss: 0.0925Epoch 12/15: [===================           ] 31/47 batches, loss: 0.0909Epoch 12/15: [====================          ] 32/47 batches, loss: 0.0899Epoch 12/15: [=====================         ] 33/47 batches, loss: 0.0917Epoch 12/15: [=====================         ] 34/47 batches, loss: 0.0928Epoch 12/15: [======================        ] 35/47 batches, loss: 0.0924Epoch 12/15: [======================        ] 36/47 batches, loss: 0.0930Epoch 12/15: [=======================       ] 37/47 batches, loss: 0.0927Epoch 12/15: [========================      ] 38/47 batches, loss: 0.0928Epoch 12/15: [========================      ] 39/47 batches, loss: 0.0917Epoch 12/15: [=========================     ] 40/47 batches, loss: 0.0912Epoch 12/15: [==========================    ] 41/47 batches, loss: 0.0904Epoch 12/15: [==========================    ] 42/47 batches, loss: 0.0900Epoch 12/15: [===========================   ] 43/47 batches, loss: 0.0910Epoch 12/15: [============================  ] 44/47 batches, loss: 0.0913Epoch 12/15: [============================  ] 45/47 batches, loss: 0.0919Epoch 12/15: [============================= ] 46/47 batches, loss: 0.0915Epoch 12/15: [==============================] 47/47 batches, loss: 0.0909
[2025-05-07 22:16:24,849][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0909
[2025-05-07 22:16:25,116][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0907, Metrics: {'mse': 0.0918421521782875, 'rmse': 0.30305470162709486, 'r2': -0.4006314277648926}
[2025-05-07 22:16:25,116][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/47 batches, loss: 0.1545Epoch 13/15: [=                             ] 2/47 batches, loss: 0.1126Epoch 13/15: [=                             ] 3/47 batches, loss: 0.1265Epoch 13/15: [==                            ] 4/47 batches, loss: 0.1154Epoch 13/15: [===                           ] 5/47 batches, loss: 0.1117Epoch 13/15: [===                           ] 6/47 batches, loss: 0.1030Epoch 13/15: [====                          ] 7/47 batches, loss: 0.1083Epoch 13/15: [=====                         ] 8/47 batches, loss: 0.1068Epoch 13/15: [=====                         ] 9/47 batches, loss: 0.1037Epoch 13/15: [======                        ] 10/47 batches, loss: 0.1024Epoch 13/15: [=======                       ] 11/47 batches, loss: 0.0999Epoch 13/15: [=======                       ] 12/47 batches, loss: 0.1003Epoch 13/15: [========                      ] 13/47 batches, loss: 0.1012Epoch 13/15: [========                      ] 14/47 batches, loss: 0.0971Epoch 13/15: [=========                     ] 15/47 batches, loss: 0.0966Epoch 13/15: [==========                    ] 16/47 batches, loss: 0.0980Epoch 13/15: [==========                    ] 17/47 batches, loss: 0.0978Epoch 13/15: [===========                   ] 18/47 batches, loss: 0.1012Epoch 13/15: [============                  ] 19/47 batches, loss: 0.0992Epoch 13/15: [============                  ] 20/47 batches, loss: 0.0984Epoch 13/15: [=============                 ] 21/47 batches, loss: 0.0975Epoch 13/15: [==============                ] 22/47 batches, loss: 0.0980Epoch 13/15: [==============                ] 23/47 batches, loss: 0.0994Epoch 13/15: [===============               ] 24/47 batches, loss: 0.1013Epoch 13/15: [===============               ] 25/47 batches, loss: 0.1008Epoch 13/15: [================              ] 26/47 batches, loss: 0.0991Epoch 13/15: [=================             ] 27/47 batches, loss: 0.0977Epoch 13/15: [=================             ] 28/47 batches, loss: 0.0975Epoch 13/15: [==================            ] 29/47 batches, loss: 0.0975Epoch 13/15: [===================           ] 30/47 batches, loss: 0.0966Epoch 13/15: [===================           ] 31/47 batches, loss: 0.0956Epoch 13/15: [====================          ] 32/47 batches, loss: 0.0946Epoch 13/15: [=====================         ] 33/47 batches, loss: 0.0935Epoch 13/15: [=====================         ] 34/47 batches, loss: 0.0924Epoch 13/15: [======================        ] 35/47 batches, loss: 0.0937Epoch 13/15: [======================        ] 36/47 batches, loss: 0.0931Epoch 13/15: [=======================       ] 37/47 batches, loss: 0.0922Epoch 13/15: [========================      ] 38/47 batches, loss: 0.0919Epoch 13/15: [========================      ] 39/47 batches, loss: 0.0914Epoch 13/15: [=========================     ] 40/47 batches, loss: 0.0923Epoch 13/15: [==========================    ] 41/47 batches, loss: 0.0919Epoch 13/15: [==========================    ] 42/47 batches, loss: 0.0921Epoch 13/15: [===========================   ] 43/47 batches, loss: 0.0911Epoch 13/15: [============================  ] 44/47 batches, loss: 0.0907Epoch 13/15: [============================  ] 45/47 batches, loss: 0.0915Epoch 13/15: [============================= ] 46/47 batches, loss: 0.0911Epoch 13/15: [==============================] 47/47 batches, loss: 0.0908
[2025-05-07 22:16:26,614][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0908
[2025-05-07 22:16:26,939][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0983, Metrics: {'mse': 0.09933403134346008, 'rmse': 0.31517301810824494, 'r2': -0.5148857831954956}
[2025-05-07 22:16:26,940][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/47 batches, loss: 0.0970Epoch 14/15: [=                             ] 2/47 batches, loss: 0.0988Epoch 14/15: [=                             ] 3/47 batches, loss: 0.0958Epoch 14/15: [==                            ] 4/47 batches, loss: 0.1032Epoch 14/15: [===                           ] 5/47 batches, loss: 0.1069Epoch 14/15: [===                           ] 6/47 batches, loss: 0.0981Epoch 14/15: [====                          ] 7/47 batches, loss: 0.0940Epoch 14/15: [=====                         ] 8/47 batches, loss: 0.0906Epoch 14/15: [=====                         ] 9/47 batches, loss: 0.0952Epoch 14/15: [======                        ] 10/47 batches, loss: 0.0976Epoch 14/15: [=======                       ] 11/47 batches, loss: 0.0966Epoch 14/15: [=======                       ] 12/47 batches, loss: 0.0930Epoch 14/15: [========                      ] 13/47 batches, loss: 0.0912Epoch 14/15: [========                      ] 14/47 batches, loss: 0.0922Epoch 14/15: [=========                     ] 15/47 batches, loss: 0.0932Epoch 14/15: [==========                    ] 16/47 batches, loss: 0.0968Epoch 14/15: [==========                    ] 17/47 batches, loss: 0.0939Epoch 14/15: [===========                   ] 18/47 batches, loss: 0.0924Epoch 14/15: [============                  ] 19/47 batches, loss: 0.0921Epoch 14/15: [============                  ] 20/47 batches, loss: 0.0938Epoch 14/15: [=============                 ] 21/47 batches, loss: 0.0935Epoch 14/15: [==============                ] 22/47 batches, loss: 0.0915Epoch 14/15: [==============                ] 23/47 batches, loss: 0.0896Epoch 14/15: [===============               ] 24/47 batches, loss: 0.0898Epoch 14/15: [===============               ] 25/47 batches, loss: 0.0913Epoch 14/15: [================              ] 26/47 batches, loss: 0.0919Epoch 14/15: [=================             ] 27/47 batches, loss: 0.0910Epoch 14/15: [=================             ] 28/47 batches, loss: 0.0917Epoch 14/15: [==================            ] 29/47 batches, loss: 0.0912Epoch 14/15: [===================           ] 30/47 batches, loss: 0.0903Epoch 14/15: [===================           ] 31/47 batches, loss: 0.0894Epoch 14/15: [====================          ] 32/47 batches, loss: 0.0895Epoch 14/15: [=====================         ] 33/47 batches, loss: 0.0881Epoch 14/15: [=====================         ] 34/47 batches, loss: 0.0875Epoch 14/15: [======================        ] 35/47 batches, loss: 0.0872Epoch 14/15: [======================        ] 36/47 batches, loss: 0.0886Epoch 14/15: [=======================       ] 37/47 batches, loss: 0.0888Epoch 14/15: [========================      ] 38/47 batches, loss: 0.0885Epoch 14/15: [========================      ] 39/47 batches, loss: 0.0892Epoch 14/15: [=========================     ] 40/47 batches, loss: 0.0897Epoch 14/15: [==========================    ] 41/47 batches, loss: 0.0901Epoch 14/15: [==========================    ] 42/47 batches, loss: 0.0893Epoch 14/15: [===========================   ] 43/47 batches, loss: 0.0886Epoch 14/15: [============================  ] 44/47 batches, loss: 0.0895Epoch 14/15: [============================  ] 45/47 batches, loss: 0.0888Epoch 14/15: [============================= ] 46/47 batches, loss: 0.0881Epoch 14/15: [==============================] 47/47 batches, loss: 0.0869
[2025-05-07 22:16:28,402][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0869
[2025-05-07 22:16:28,683][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0785, Metrics: {'mse': 0.07904119789600372, 'rmse': 0.28114266466689775, 'r2': -0.20541143417358398}
Epoch 15/15: [Epoch 15/15: [                              ] 1/47 batches, loss: 0.0641Epoch 15/15: [=                             ] 2/47 batches, loss: 0.0875Epoch 15/15: [=                             ] 3/47 batches, loss: 0.0860Epoch 15/15: [==                            ] 4/47 batches, loss: 0.0811Epoch 15/15: [===                           ] 5/47 batches, loss: 0.0769Epoch 15/15: [===                           ] 6/47 batches, loss: 0.0835Epoch 15/15: [====                          ] 7/47 batches, loss: 0.0845Epoch 15/15: [=====                         ] 8/47 batches, loss: 0.0882Epoch 15/15: [=====                         ] 9/47 batches, loss: 0.0909Epoch 15/15: [======                        ] 10/47 batches, loss: 0.0872Epoch 15/15: [=======                       ] 11/47 batches, loss: 0.0865Epoch 15/15: [=======                       ] 12/47 batches, loss: 0.0886Epoch 15/15: [========                      ] 13/47 batches, loss: 0.0891Epoch 15/15: [========                      ] 14/47 batches, loss: 0.0860Epoch 15/15: [=========                     ] 15/47 batches, loss: 0.0839Epoch 15/15: [==========                    ] 16/47 batches, loss: 0.0830Epoch 15/15: [==========                    ] 17/47 batches, loss: 0.0841Epoch 15/15: [===========                   ] 18/47 batches, loss: 0.0835Epoch 15/15: [============                  ] 19/47 batches, loss: 0.0853Epoch 15/15: [============                  ] 20/47 batches, loss: 0.0840Epoch 15/15: [=============                 ] 21/47 batches, loss: 0.0861Epoch 15/15: [==============                ] 22/47 batches, loss: 0.0882Epoch 15/15: [==============                ] 23/47 batches, loss: 0.0872Epoch 15/15: [===============               ] 24/47 batches, loss: 0.0871Epoch 15/15: [===============               ] 25/47 batches, loss: 0.0868Epoch 15/15: [================              ] 26/47 batches, loss: 0.0877Epoch 15/15: [=================             ] 27/47 batches, loss: 0.0866Epoch 15/15: [=================             ] 28/47 batches, loss: 0.0849Epoch 15/15: [==================            ] 29/47 batches, loss: 0.0845Epoch 15/15: [===================           ] 30/47 batches, loss: 0.0837Epoch 15/15: [===================           ] 31/47 batches, loss: 0.0841Epoch 15/15: [====================          ] 32/47 batches, loss: 0.0832Epoch 15/15: [=====================         ] 33/47 batches, loss: 0.0833Epoch 15/15: [=====================         ] 34/47 batches, loss: 0.0839Epoch 15/15: [======================        ] 35/47 batches, loss: 0.0852Epoch 15/15: [======================        ] 36/47 batches, loss: 0.0845Epoch 15/15: [=======================       ] 37/47 batches, loss: 0.0859Epoch 15/15: [========================      ] 38/47 batches, loss: 0.0880Epoch 15/15: [========================      ] 39/47 batches, loss: 0.0874Epoch 15/15: [=========================     ] 40/47 batches, loss: 0.0883Epoch 15/15: [==========================    ] 41/47 batches, loss: 0.0879Epoch 15/15: [==========================    ] 42/47 batches, loss: 0.0879Epoch 15/15: [===========================   ] 43/47 batches, loss: 0.0872Epoch 15/15: [============================  ] 44/47 batches, loss: 0.0870Epoch 15/15: [============================  ] 45/47 batches, loss: 0.0869Epoch 15/15: [============================= ] 46/47 batches, loss: 0.0871Epoch 15/15: [==============================] 47/47 batches, loss: 0.0873
[2025-05-07 22:16:30,539][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0873
[2025-05-07 22:16:30,812][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0758, Metrics: {'mse': 0.07630222290754318, 'rmse': 0.2762285700421721, 'r2': -0.16364097595214844}
[2025-05-07 22:16:31,210][src.training.lm_trainer][INFO] - Training completed in 31.01 seconds
[2025-05-07 22:16:31,210][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:16:33,432][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.06332240998744965, 'rmse': 0.251639444418894, 'r2': 0.014879345893859863}
[2025-05-07 22:16:33,432][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07630222290754318, 'rmse': 0.2762285700421721, 'r2': -0.16364097595214844}
[2025-05-07 22:16:33,432][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.05699862539768219, 'rmse': 0.23874384892114434, 'r2': -0.12800681591033936}
[2025-05-07 22:16:35,449][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer2/ko/ko/model.pt
[2025-05-07 22:16:35,450][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▃▂▂▂▁▁
wandb:     best_val_mse █▃▃▂▂▂▁▁
wandb:      best_val_r2 ▁▆▆▇▇▇██
wandb:    best_val_rmse █▄▄▂▂▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▅▃▅▄▆▅▅▆▅▆▅▅▆
wandb:       train_loss █▄▃▃▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▆▃▄▂▄▃▂▃▂▂▃▁▁
wandb:          val_mse █▃▆▃▄▂▄▃▂▃▂▂▃▁▁
wandb:           val_r2 ▁▆▃▆▅▇▅▆▇▆▇▇▆██
wandb:         val_rmse █▄▆▄▄▂▄▃▂▃▂▃▃▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07577
wandb:     best_val_mse 0.0763
wandb:      best_val_r2 -0.16364
wandb:    best_val_rmse 0.27623
wandb:            epoch 15
wandb:   final_test_mse 0.057
wandb:    final_test_r2 -0.12801
wandb:  final_test_rmse 0.23874
wandb:  final_train_mse 0.06332
wandb:   final_train_r2 0.01488
wandb: final_train_rmse 0.25164
wandb:    final_val_mse 0.0763
wandb:     final_val_r2 -0.16364
wandb:   final_val_rmse 0.27623
wandb:    learning_rate 0.0001
wandb:       train_loss 0.08731
wandb:       train_time 31.01123
wandb:         val_loss 0.07577
wandb:          val_mse 0.0763
wandb:           val_r2 -0.16364
wandb:         val_rmse 0.27623
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_221540-rqpqfgoa
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_221540-rqpqfgoa/logs
Experiment probe_layer2_avg_subordinate_chain_len_control1_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_avg_subordinate_chain_len_control2_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_subordinate_chain_len_control2_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=avg_subordinate_chain_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:17:08,239][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer2/ko
experiment_name: probe_layer2_avg_subordinate_chain_len_control2_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:17:08,239][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:17:08,239][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 22:17:08,239][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:17:08,240][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:17:08,286][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:17:08,286][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 22:17:08,286][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:17:12,319][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:17:14,781][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:17:14,782][src.data.datasets][INFO] - Loading 'control_avg_subordinate_chain_len_seed2' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:17:14,919][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:25:20 2025).
[2025-05-07 22:17:15,015][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:25:20 2025).
[2025-05-07 22:17:15,309][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:17:15,316][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:17:15,317][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:17:15,319][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:17:15,380][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:17:15,459][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:17:15,479][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:17:15,481][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:17:15,481][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:17:15,482][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:17:15,581][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:17:15,636][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:17:15,673][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:17:15,675][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:17:15,675][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:17:15,676][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:17:15,677][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:17:15,677][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 22:17:15,677][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 22:17:15,677][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 22:17:15,677][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:17:15,677][src.data.datasets][INFO] -   Mean: 0.1845, Std: 0.2535
[2025-05-07 22:17:15,678][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:17:15,678][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 22:17:15,678][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:17:15,678][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 22:17:15,678][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 22:17:15,678][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 22:17:15,678][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:17:15,678][src.data.datasets][INFO] -   Mean: 0.2673, Std: 0.2561
[2025-05-07 22:17:15,678][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:17:15,678][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 22:17:15,679][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:17:15,679][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 22:17:15,679][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 22:17:15,679][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 22:17:15,679][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6670
[2025-05-07 22:17:15,679][src.data.datasets][INFO] -   Mean: 0.2435, Std: 0.2248
[2025-05-07 22:17:15,679][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:17:15,679][src.data.datasets][INFO] - Sample label: 0.5
[2025-05-07 22:17:15,679][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:17:15,679][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:17:15,680][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:17:15,680][__main__][INFO] - Using model type: lm_probe for submetric avg_subordinate_chain_len
[2025-05-07 22:17:15,680][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:17:22,544][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:17:22,545][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:17:22,545][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:17:22,545][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:17:22,548][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:17:22,549][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:17:22,549][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:17:22,549][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:17:22,549][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:17:22,550][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:17:22,550][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.3718Epoch 1/15: [=                             ] 2/47 batches, loss: 0.5015Epoch 1/15: [=                             ] 3/47 batches, loss: 0.4755Epoch 1/15: [==                            ] 4/47 batches, loss: 0.4798Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4661Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4200Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4056Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4205Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4428Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4332Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4251Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4224Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4099Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4088Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.3938Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4175Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.4069Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4216Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4133Epoch 1/15: [============                  ] 20/47 batches, loss: 0.4076Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4108Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4130Epoch 1/15: [==============                ] 23/47 batches, loss: 0.3997Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3889Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3882Epoch 1/15: [================              ] 26/47 batches, loss: 0.3826Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3837Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3769Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3795Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3730Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3665Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3606Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3572Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3621Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3583Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3563Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3519Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3505Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3496Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3520Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3512Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3474Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3472Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3451Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3466Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3416Epoch 1/15: [==============================] 47/47 batches, loss: 0.3436
[2025-05-07 22:17:27,884][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3436
[2025-05-07 22:17:28,127][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1674, Metrics: {'mse': 0.1704503893852234, 'rmse': 0.41285637864180247, 'r2': -1.599440097808838}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.3100Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2264Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2521Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2235Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2589Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2492Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2543Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2523Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2556Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2512Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2555Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2552Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2510Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2512Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2492Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2469Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2430Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2476Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2465Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2438Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2392Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2394Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2408Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2454Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2494Epoch 2/15: [================              ] 26/47 batches, loss: 0.2466Epoch 2/15: [=================             ] 27/47 batches, loss: 0.2442Epoch 2/15: [=================             ] 28/47 batches, loss: 0.2435Epoch 2/15: [==================            ] 29/47 batches, loss: 0.2391Epoch 2/15: [===================           ] 30/47 batches, loss: 0.2390Epoch 2/15: [===================           ] 31/47 batches, loss: 0.2401Epoch 2/15: [====================          ] 32/47 batches, loss: 0.2380Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.2353Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.2342Epoch 2/15: [======================        ] 35/47 batches, loss: 0.2308Epoch 2/15: [======================        ] 36/47 batches, loss: 0.2289Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.2293Epoch 2/15: [========================      ] 38/47 batches, loss: 0.2257Epoch 2/15: [========================      ] 39/47 batches, loss: 0.2248Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.2229Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.2235Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.2217Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.2189Epoch 2/15: [============================  ] 44/47 batches, loss: 0.2179Epoch 2/15: [============================  ] 45/47 batches, loss: 0.2183Epoch 2/15: [============================= ] 46/47 batches, loss: 0.2179Epoch 2/15: [==============================] 47/47 batches, loss: 0.2169
[2025-05-07 22:17:30,000][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.2169
[2025-05-07 22:17:30,259][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1091, Metrics: {'mse': 0.11028750240802765, 'rmse': 0.3320956223861249, 'r2': -0.6819307804107666}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.3453Epoch 3/15: [=                             ] 2/47 batches, loss: 0.2378Epoch 3/15: [=                             ] 3/47 batches, loss: 0.2248Epoch 3/15: [==                            ] 4/47 batches, loss: 0.2127Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1964Epoch 3/15: [===                           ] 6/47 batches, loss: 0.2102Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1961Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1846Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1958Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1917Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1888Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1935Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1883Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1840Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1830Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1809Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1846Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1859Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1841Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1822Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1857Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1832Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1864Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1863Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1822Epoch 3/15: [================              ] 26/47 batches, loss: 0.1805Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1814Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1777Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1752Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1757Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1757Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1747Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1752Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1755Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1782Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1764Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1746Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1723Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1704Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1700Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1703Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1699Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1697Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1725Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1720Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1705Epoch 3/15: [==============================] 47/47 batches, loss: 0.1677
[2025-05-07 22:17:32,156][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1677
[2025-05-07 22:17:32,460][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1288, Metrics: {'mse': 0.1308215707540512, 'rmse': 0.3616926468067207, 'r2': -0.9950839281082153}
[2025-05-07 22:17:32,460][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.2394Epoch 4/15: [=                             ] 2/47 batches, loss: 0.2077Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1682Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1548Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1618Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1579Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1568Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1616Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1635Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1694Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1735Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1778Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1749Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1711Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1693Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1645Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1691Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1677Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1680Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1762Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1757Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1742Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1741Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1717Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1748Epoch 4/15: [================              ] 26/47 batches, loss: 0.1748Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1765Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1750Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1763Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1734Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1739Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1703Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1693Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1673Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1683Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1679Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1671Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1645Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1631Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1617Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1640Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1644Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1631Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1623Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1621Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1622Epoch 4/15: [==============================] 47/47 batches, loss: 0.1600
[2025-05-07 22:17:33,914][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1600
[2025-05-07 22:17:34,180][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0943, Metrics: {'mse': 0.0956389382481575, 'rmse': 0.3092554579116713, 'r2': -0.45853400230407715}
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.1442Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1657Epoch 5/15: [=                             ] 3/47 batches, loss: 0.1532Epoch 5/15: [==                            ] 4/47 batches, loss: 0.1367Epoch 5/15: [===                           ] 5/47 batches, loss: 0.1365Epoch 5/15: [===                           ] 6/47 batches, loss: 0.1345Epoch 5/15: [====                          ] 7/47 batches, loss: 0.1382Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.1331Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.1342Epoch 5/15: [======                        ] 10/47 batches, loss: 0.1326Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.1313Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.1306Epoch 5/15: [========                      ] 13/47 batches, loss: 0.1263Epoch 5/15: [========                      ] 14/47 batches, loss: 0.1248Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.1251Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.1242Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.1214Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.1213Epoch 5/15: [============                  ] 19/47 batches, loss: 0.1212Epoch 5/15: [============                  ] 20/47 batches, loss: 0.1213Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.1206Epoch 5/15: [==============                ] 22/47 batches, loss: 0.1216Epoch 5/15: [==============                ] 23/47 batches, loss: 0.1226Epoch 5/15: [===============               ] 24/47 batches, loss: 0.1268Epoch 5/15: [===============               ] 25/47 batches, loss: 0.1272Epoch 5/15: [================              ] 26/47 batches, loss: 0.1275Epoch 5/15: [=================             ] 27/47 batches, loss: 0.1246Epoch 5/15: [=================             ] 28/47 batches, loss: 0.1241Epoch 5/15: [==================            ] 29/47 batches, loss: 0.1237Epoch 5/15: [===================           ] 30/47 batches, loss: 0.1264Epoch 5/15: [===================           ] 31/47 batches, loss: 0.1258Epoch 5/15: [====================          ] 32/47 batches, loss: 0.1253Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.1242Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.1241Epoch 5/15: [======================        ] 35/47 batches, loss: 0.1244Epoch 5/15: [======================        ] 36/47 batches, loss: 0.1249Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.1245Epoch 5/15: [========================      ] 38/47 batches, loss: 0.1244Epoch 5/15: [========================      ] 39/47 batches, loss: 0.1257Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.1252Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.1254Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.1244Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.1240Epoch 5/15: [============================  ] 44/47 batches, loss: 0.1240Epoch 5/15: [============================  ] 45/47 batches, loss: 0.1250Epoch 5/15: [============================= ] 46/47 batches, loss: 0.1245Epoch 5/15: [==============================] 47/47 batches, loss: 0.1222
[2025-05-07 22:17:36,108][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1222
[2025-05-07 22:17:36,458][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1358, Metrics: {'mse': 0.13830703496932983, 'rmse': 0.37189653799051403, 'r2': -1.1092405319213867}
[2025-05-07 22:17:36,458][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.1358Epoch 6/15: [=                             ] 2/47 batches, loss: 0.1309Epoch 6/15: [=                             ] 3/47 batches, loss: 0.1173Epoch 6/15: [==                            ] 4/47 batches, loss: 0.1294Epoch 6/15: [===                           ] 5/47 batches, loss: 0.1430Epoch 6/15: [===                           ] 6/47 batches, loss: 0.1366Epoch 6/15: [====                          ] 7/47 batches, loss: 0.1325Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.1329Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.1320Epoch 6/15: [======                        ] 10/47 batches, loss: 0.1301Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.1286Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.1333Epoch 6/15: [========                      ] 13/47 batches, loss: 0.1297Epoch 6/15: [========                      ] 14/47 batches, loss: 0.1260Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.1259Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.1239Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.1246Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.1254Epoch 6/15: [============                  ] 19/47 batches, loss: 0.1233Epoch 6/15: [============                  ] 20/47 batches, loss: 0.1237Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.1211Epoch 6/15: [==============                ] 22/47 batches, loss: 0.1225Epoch 6/15: [==============                ] 23/47 batches, loss: 0.1231Epoch 6/15: [===============               ] 24/47 batches, loss: 0.1217Epoch 6/15: [===============               ] 25/47 batches, loss: 0.1205Epoch 6/15: [================              ] 26/47 batches, loss: 0.1204Epoch 6/15: [=================             ] 27/47 batches, loss: 0.1211Epoch 6/15: [=================             ] 28/47 batches, loss: 0.1208Epoch 6/15: [==================            ] 29/47 batches, loss: 0.1212Epoch 6/15: [===================           ] 30/47 batches, loss: 0.1212Epoch 6/15: [===================           ] 31/47 batches, loss: 0.1217Epoch 6/15: [====================          ] 32/47 batches, loss: 0.1224Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.1219Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.1206Epoch 6/15: [======================        ] 35/47 batches, loss: 0.1206Epoch 6/15: [======================        ] 36/47 batches, loss: 0.1203Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.1201Epoch 6/15: [========================      ] 38/47 batches, loss: 0.1202Epoch 6/15: [========================      ] 39/47 batches, loss: 0.1205Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.1203Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.1198Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.1214Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.1223Epoch 6/15: [============================  ] 44/47 batches, loss: 0.1221Epoch 6/15: [============================  ] 45/47 batches, loss: 0.1223Epoch 6/15: [============================= ] 46/47 batches, loss: 0.1220Epoch 6/15: [==============================] 47/47 batches, loss: 0.1199
[2025-05-07 22:17:38,040][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1199
[2025-05-07 22:17:38,307][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.1121, Metrics: {'mse': 0.11396116763353348, 'rmse': 0.3375813496529888, 'r2': -0.7379556894302368}
[2025-05-07 22:17:38,308][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/47 batches, loss: 0.1132Epoch 7/15: [=                             ] 2/47 batches, loss: 0.1180Epoch 7/15: [=                             ] 3/47 batches, loss: 0.1028Epoch 7/15: [==                            ] 4/47 batches, loss: 0.1000Epoch 7/15: [===                           ] 5/47 batches, loss: 0.1044Epoch 7/15: [===                           ] 6/47 batches, loss: 0.1080Epoch 7/15: [====                          ] 7/47 batches, loss: 0.1011Epoch 7/15: [=====                         ] 8/47 batches, loss: 0.0994Epoch 7/15: [=====                         ] 9/47 batches, loss: 0.0989Epoch 7/15: [======                        ] 10/47 batches, loss: 0.1051Epoch 7/15: [=======                       ] 11/47 batches, loss: 0.1021Epoch 7/15: [=======                       ] 12/47 batches, loss: 0.1015Epoch 7/15: [========                      ] 13/47 batches, loss: 0.0988Epoch 7/15: [========                      ] 14/47 batches, loss: 0.0975Epoch 7/15: [=========                     ] 15/47 batches, loss: 0.0993Epoch 7/15: [==========                    ] 16/47 batches, loss: 0.0972Epoch 7/15: [==========                    ] 17/47 batches, loss: 0.0947Epoch 7/15: [===========                   ] 18/47 batches, loss: 0.0941Epoch 7/15: [============                  ] 19/47 batches, loss: 0.1001Epoch 7/15: [============                  ] 20/47 batches, loss: 0.0995Epoch 7/15: [=============                 ] 21/47 batches, loss: 0.0973Epoch 7/15: [==============                ] 22/47 batches, loss: 0.0986Epoch 7/15: [==============                ] 23/47 batches, loss: 0.0979Epoch 7/15: [===============               ] 24/47 batches, loss: 0.0988Epoch 7/15: [===============               ] 25/47 batches, loss: 0.0983Epoch 7/15: [================              ] 26/47 batches, loss: 0.1011Epoch 7/15: [=================             ] 27/47 batches, loss: 0.1035Epoch 7/15: [=================             ] 28/47 batches, loss: 0.1024Epoch 7/15: [==================            ] 29/47 batches, loss: 0.1017Epoch 7/15: [===================           ] 30/47 batches, loss: 0.1021Epoch 7/15: [===================           ] 31/47 batches, loss: 0.1032Epoch 7/15: [====================          ] 32/47 batches, loss: 0.1038Epoch 7/15: [=====================         ] 33/47 batches, loss: 0.1026Epoch 7/15: [=====================         ] 34/47 batches, loss: 0.1040Epoch 7/15: [======================        ] 35/47 batches, loss: 0.1048Epoch 7/15: [======================        ] 36/47 batches, loss: 0.1044Epoch 7/15: [=======================       ] 37/47 batches, loss: 0.1068Epoch 7/15: [========================      ] 38/47 batches, loss: 0.1078Epoch 7/15: [========================      ] 39/47 batches, loss: 0.1079Epoch 7/15: [=========================     ] 40/47 batches, loss: 0.1074Epoch 7/15: [==========================    ] 41/47 batches, loss: 0.1060Epoch 7/15: [==========================    ] 42/47 batches, loss: 0.1072Epoch 7/15: [===========================   ] 43/47 batches, loss: 0.1071Epoch 7/15: [============================  ] 44/47 batches, loss: 0.1064Epoch 7/15: [============================  ] 45/47 batches, loss: 0.1070Epoch 7/15: [============================= ] 46/47 batches, loss: 0.1069Epoch 7/15: [==============================] 47/47 batches, loss: 0.1053
[2025-05-07 22:17:39,802][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.1053
[2025-05-07 22:17:40,088][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0991, Metrics: {'mse': 0.10056117177009583, 'rmse': 0.31711381516751336, 'r2': -0.5336002111434937}
[2025-05-07 22:17:40,089][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/47 batches, loss: 0.0551Epoch 8/15: [=                             ] 2/47 batches, loss: 0.0677Epoch 8/15: [=                             ] 3/47 batches, loss: 0.0946Epoch 8/15: [==                            ] 4/47 batches, loss: 0.1246Epoch 8/15: [===                           ] 5/47 batches, loss: 0.1293Epoch 8/15: [===                           ] 6/47 batches, loss: 0.1252Epoch 8/15: [====                          ] 7/47 batches, loss: 0.1303Epoch 8/15: [=====                         ] 8/47 batches, loss: 0.1247Epoch 8/15: [=====                         ] 9/47 batches, loss: 0.1201Epoch 8/15: [======                        ] 10/47 batches, loss: 0.1210Epoch 8/15: [=======                       ] 11/47 batches, loss: 0.1197Epoch 8/15: [=======                       ] 12/47 batches, loss: 0.1146Epoch 8/15: [========                      ] 13/47 batches, loss: 0.1096Epoch 8/15: [========                      ] 14/47 batches, loss: 0.1154Epoch 8/15: [=========                     ] 15/47 batches, loss: 0.1168Epoch 8/15: [==========                    ] 16/47 batches, loss: 0.1129Epoch 8/15: [==========                    ] 17/47 batches, loss: 0.1150Epoch 8/15: [===========                   ] 18/47 batches, loss: 0.1165Epoch 8/15: [============                  ] 19/47 batches, loss: 0.1139Epoch 8/15: [============                  ] 20/47 batches, loss: 0.1130Epoch 8/15: [=============                 ] 21/47 batches, loss: 0.1131Epoch 8/15: [==============                ] 22/47 batches, loss: 0.1140Epoch 8/15: [==============                ] 23/47 batches, loss: 0.1149Epoch 8/15: [===============               ] 24/47 batches, loss: 0.1127Epoch 8/15: [===============               ] 25/47 batches, loss: 0.1114Epoch 8/15: [================              ] 26/47 batches, loss: 0.1123Epoch 8/15: [=================             ] 27/47 batches, loss: 0.1104Epoch 8/15: [=================             ] 28/47 batches, loss: 0.1092Epoch 8/15: [==================            ] 29/47 batches, loss: 0.1081Epoch 8/15: [===================           ] 30/47 batches, loss: 0.1100Epoch 8/15: [===================           ] 31/47 batches, loss: 0.1092Epoch 8/15: [====================          ] 32/47 batches, loss: 0.1086Epoch 8/15: [=====================         ] 33/47 batches, loss: 0.1087Epoch 8/15: [=====================         ] 34/47 batches, loss: 0.1088Epoch 8/15: [======================        ] 35/47 batches, loss: 0.1096Epoch 8/15: [======================        ] 36/47 batches, loss: 0.1092Epoch 8/15: [=======================       ] 37/47 batches, loss: 0.1089Epoch 8/15: [========================      ] 38/47 batches, loss: 0.1104Epoch 8/15: [========================      ] 39/47 batches, loss: 0.1110Epoch 8/15: [=========================     ] 40/47 batches, loss: 0.1107Epoch 8/15: [==========================    ] 41/47 batches, loss: 0.1101Epoch 8/15: [==========================    ] 42/47 batches, loss: 0.1097Epoch 8/15: [===========================   ] 43/47 batches, loss: 0.1100Epoch 8/15: [============================  ] 44/47 batches, loss: 0.1097Epoch 8/15: [============================  ] 45/47 batches, loss: 0.1093Epoch 8/15: [============================= ] 46/47 batches, loss: 0.1090Epoch 8/15: [==============================] 47/47 batches, loss: 0.1091
[2025-05-07 22:17:41,584][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.1091
[2025-05-07 22:17:41,850][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0737, Metrics: {'mse': 0.07441499829292297, 'rmse': 0.27279112575911074, 'r2': -0.13485991954803467}
Epoch 9/15: [Epoch 9/15: [                              ] 1/47 batches, loss: 0.1415Epoch 9/15: [=                             ] 2/47 batches, loss: 0.1083Epoch 9/15: [=                             ] 3/47 batches, loss: 0.1157Epoch 9/15: [==                            ] 4/47 batches, loss: 0.1033Epoch 9/15: [===                           ] 5/47 batches, loss: 0.1001Epoch 9/15: [===                           ] 6/47 batches, loss: 0.0954Epoch 9/15: [====                          ] 7/47 batches, loss: 0.0988Epoch 9/15: [=====                         ] 8/47 batches, loss: 0.1086Epoch 9/15: [=====                         ] 9/47 batches, loss: 0.1109Epoch 9/15: [======                        ] 10/47 batches, loss: 0.1164Epoch 9/15: [=======                       ] 11/47 batches, loss: 0.1157Epoch 9/15: [=======                       ] 12/47 batches, loss: 0.1126Epoch 9/15: [========                      ] 13/47 batches, loss: 0.1093Epoch 9/15: [========                      ] 14/47 batches, loss: 0.1108Epoch 9/15: [=========                     ] 15/47 batches, loss: 0.1115Epoch 9/15: [==========                    ] 16/47 batches, loss: 0.1085Epoch 9/15: [==========                    ] 17/47 batches, loss: 0.1103Epoch 9/15: [===========                   ] 18/47 batches, loss: 0.1092Epoch 9/15: [============                  ] 19/47 batches, loss: 0.1069Epoch 9/15: [============                  ] 20/47 batches, loss: 0.1046Epoch 9/15: [=============                 ] 21/47 batches, loss: 0.1040Epoch 9/15: [==============                ] 22/47 batches, loss: 0.1043Epoch 9/15: [==============                ] 23/47 batches, loss: 0.1023Epoch 9/15: [===============               ] 24/47 batches, loss: 0.1005Epoch 9/15: [===============               ] 25/47 batches, loss: 0.1007Epoch 9/15: [================              ] 26/47 batches, loss: 0.1005Epoch 9/15: [=================             ] 27/47 batches, loss: 0.1010Epoch 9/15: [=================             ] 28/47 batches, loss: 0.1033Epoch 9/15: [==================            ] 29/47 batches, loss: 0.1030Epoch 9/15: [===================           ] 30/47 batches, loss: 0.1013Epoch 9/15: [===================           ] 31/47 batches, loss: 0.1007Epoch 9/15: [====================          ] 32/47 batches, loss: 0.1000Epoch 9/15: [=====================         ] 33/47 batches, loss: 0.0994Epoch 9/15: [=====================         ] 34/47 batches, loss: 0.0987Epoch 9/15: [======================        ] 35/47 batches, loss: 0.0975Epoch 9/15: [======================        ] 36/47 batches, loss: 0.0973Epoch 9/15: [=======================       ] 37/47 batches, loss: 0.0969Epoch 9/15: [========================      ] 38/47 batches, loss: 0.0985Epoch 9/15: [========================      ] 39/47 batches, loss: 0.0970Epoch 9/15: [=========================     ] 40/47 batches, loss: 0.0982Epoch 9/15: [==========================    ] 41/47 batches, loss: 0.0986Epoch 9/15: [==========================    ] 42/47 batches, loss: 0.0989Epoch 9/15: [===========================   ] 43/47 batches, loss: 0.0988Epoch 9/15: [============================  ] 44/47 batches, loss: 0.0982Epoch 9/15: [============================  ] 45/47 batches, loss: 0.0980Epoch 9/15: [============================= ] 46/47 batches, loss: 0.0976Epoch 9/15: [==============================] 47/47 batches, loss: 0.0968
[2025-05-07 22:17:43,754][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0968
[2025-05-07 22:17:44,100][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0841, Metrics: {'mse': 0.08517235517501831, 'rmse': 0.2918430317396979, 'r2': -0.29891419410705566}
[2025-05-07 22:17:44,101][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/47 batches, loss: 0.1060Epoch 10/15: [=                             ] 2/47 batches, loss: 0.1114Epoch 10/15: [=                             ] 3/47 batches, loss: 0.0973Epoch 10/15: [==                            ] 4/47 batches, loss: 0.0960Epoch 10/15: [===                           ] 5/47 batches, loss: 0.0975Epoch 10/15: [===                           ] 6/47 batches, loss: 0.0929Epoch 10/15: [====                          ] 7/47 batches, loss: 0.0968Epoch 10/15: [=====                         ] 8/47 batches, loss: 0.1015Epoch 10/15: [=====                         ] 9/47 batches, loss: 0.1019Epoch 10/15: [======                        ] 10/47 batches, loss: 0.1006Epoch 10/15: [=======                       ] 11/47 batches, loss: 0.1047Epoch 10/15: [=======                       ] 12/47 batches, loss: 0.1092Epoch 10/15: [========                      ] 13/47 batches, loss: 0.1079Epoch 10/15: [========                      ] 14/47 batches, loss: 0.1053Epoch 10/15: [=========                     ] 15/47 batches, loss: 0.1041Epoch 10/15: [==========                    ] 16/47 batches, loss: 0.1039Epoch 10/15: [==========                    ] 17/47 batches, loss: 0.1040Epoch 10/15: [===========                   ] 18/47 batches, loss: 0.1046Epoch 10/15: [============                  ] 19/47 batches, loss: 0.1099Epoch 10/15: [============                  ] 20/47 batches, loss: 0.1127Epoch 10/15: [=============                 ] 21/47 batches, loss: 0.1123Epoch 10/15: [==============                ] 22/47 batches, loss: 0.1109Epoch 10/15: [==============                ] 23/47 batches, loss: 0.1094Epoch 10/15: [===============               ] 24/47 batches, loss: 0.1090Epoch 10/15: [===============               ] 25/47 batches, loss: 0.1084Epoch 10/15: [================              ] 26/47 batches, loss: 0.1090Epoch 10/15: [=================             ] 27/47 batches, loss: 0.1079Epoch 10/15: [=================             ] 28/47 batches, loss: 0.1071Epoch 10/15: [==================            ] 29/47 batches, loss: 0.1067Epoch 10/15: [===================           ] 30/47 batches, loss: 0.1066Epoch 10/15: [===================           ] 31/47 batches, loss: 0.1054Epoch 10/15: [====================          ] 32/47 batches, loss: 0.1044Epoch 10/15: [=====================         ] 33/47 batches, loss: 0.1034Epoch 10/15: [=====================         ] 34/47 batches, loss: 0.1030Epoch 10/15: [======================        ] 35/47 batches, loss: 0.1035Epoch 10/15: [======================        ] 36/47 batches, loss: 0.1033Epoch 10/15: [=======================       ] 37/47 batches, loss: 0.1032Epoch 10/15: [========================      ] 38/47 batches, loss: 0.1031Epoch 10/15: [========================      ] 39/47 batches, loss: 0.1019Epoch 10/15: [=========================     ] 40/47 batches, loss: 0.1009Epoch 10/15: [==========================    ] 41/47 batches, loss: 0.0995Epoch 10/15: [==========================    ] 42/47 batches, loss: 0.0989Epoch 10/15: [===========================   ] 43/47 batches, loss: 0.0985Epoch 10/15: [============================  ] 44/47 batches, loss: 0.0987Epoch 10/15: [============================  ] 45/47 batches, loss: 0.0989Epoch 10/15: [============================= ] 46/47 batches, loss: 0.0992Epoch 10/15: [==============================] 47/47 batches, loss: 0.0992
[2025-05-07 22:17:45,586][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0992
[2025-05-07 22:17:45,868][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0852, Metrics: {'mse': 0.0859869197010994, 'rmse': 0.29323526339971356, 'r2': -0.3113366365432739}
[2025-05-07 22:17:45,869][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/47 batches, loss: 0.1071Epoch 11/15: [=                             ] 2/47 batches, loss: 0.1076Epoch 11/15: [=                             ] 3/47 batches, loss: 0.1009Epoch 11/15: [==                            ] 4/47 batches, loss: 0.0936Epoch 11/15: [===                           ] 5/47 batches, loss: 0.0992Epoch 11/15: [===                           ] 6/47 batches, loss: 0.0924Epoch 11/15: [====                          ] 7/47 batches, loss: 0.0902Epoch 11/15: [=====                         ] 8/47 batches, loss: 0.0889Epoch 11/15: [=====                         ] 9/47 batches, loss: 0.0901Epoch 11/15: [======                        ] 10/47 batches, loss: 0.0884Epoch 11/15: [=======                       ] 11/47 batches, loss: 0.0909Epoch 11/15: [=======                       ] 12/47 batches, loss: 0.0888Epoch 11/15: [========                      ] 13/47 batches, loss: 0.0951Epoch 11/15: [========                      ] 14/47 batches, loss: 0.0929Epoch 11/15: [=========                     ] 15/47 batches, loss: 0.0942Epoch 11/15: [==========                    ] 16/47 batches, loss: 0.0926Epoch 11/15: [==========                    ] 17/47 batches, loss: 0.0926Epoch 11/15: [===========                   ] 18/47 batches, loss: 0.0919Epoch 11/15: [============                  ] 19/47 batches, loss: 0.0888Epoch 11/15: [============                  ] 20/47 batches, loss: 0.0884Epoch 11/15: [=============                 ] 21/47 batches, loss: 0.0873Epoch 11/15: [==============                ] 22/47 batches, loss: 0.0888Epoch 11/15: [==============                ] 23/47 batches, loss: 0.0882Epoch 11/15: [===============               ] 24/47 batches, loss: 0.0886Epoch 11/15: [===============               ] 25/47 batches, loss: 0.0867Epoch 11/15: [================              ] 26/47 batches, loss: 0.0908Epoch 11/15: [=================             ] 27/47 batches, loss: 0.0910Epoch 11/15: [=================             ] 28/47 batches, loss: 0.0910Epoch 11/15: [==================            ] 29/47 batches, loss: 0.0909Epoch 11/15: [===================           ] 30/47 batches, loss: 0.0899Epoch 11/15: [===================           ] 31/47 batches, loss: 0.0905Epoch 11/15: [====================          ] 32/47 batches, loss: 0.0901Epoch 11/15: [=====================         ] 33/47 batches, loss: 0.0895Epoch 11/15: [=====================         ] 34/47 batches, loss: 0.0893Epoch 11/15: [======================        ] 35/47 batches, loss: 0.0913Epoch 11/15: [======================        ] 36/47 batches, loss: 0.0909Epoch 11/15: [=======================       ] 37/47 batches, loss: 0.0925Epoch 11/15: [========================      ] 38/47 batches, loss: 0.0925Epoch 11/15: [========================      ] 39/47 batches, loss: 0.0930Epoch 11/15: [=========================     ] 40/47 batches, loss: 0.0934Epoch 11/15: [==========================    ] 41/47 batches, loss: 0.0929Epoch 11/15: [==========================    ] 42/47 batches, loss: 0.0923Epoch 11/15: [===========================   ] 43/47 batches, loss: 0.0919Epoch 11/15: [============================  ] 44/47 batches, loss: 0.0923Epoch 11/15: [============================  ] 45/47 batches, loss: 0.0916Epoch 11/15: [============================= ] 46/47 batches, loss: 0.0916Epoch 11/15: [==============================] 47/47 batches, loss: 0.0912
[2025-05-07 22:17:47,447][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0912
[2025-05-07 22:17:47,722][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0811, Metrics: {'mse': 0.08193351328372955, 'rmse': 0.2862403068816996, 'r2': -0.2495204210281372}
[2025-05-07 22:17:47,723][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/47 batches, loss: 0.0630Epoch 12/15: [=                             ] 2/47 batches, loss: 0.0999Epoch 12/15: [=                             ] 3/47 batches, loss: 0.0856Epoch 12/15: [==                            ] 4/47 batches, loss: 0.1003Epoch 12/15: [===                           ] 5/47 batches, loss: 0.0987Epoch 12/15: [===                           ] 6/47 batches, loss: 0.1055Epoch 12/15: [====                          ] 7/47 batches, loss: 0.1030Epoch 12/15: [=====                         ] 8/47 batches, loss: 0.1010Epoch 12/15: [=====                         ] 9/47 batches, loss: 0.1049Epoch 12/15: [======                        ] 10/47 batches, loss: 0.1016Epoch 12/15: [=======                       ] 11/47 batches, loss: 0.1014Epoch 12/15: [=======                       ] 12/47 batches, loss: 0.0992Epoch 12/15: [========                      ] 13/47 batches, loss: 0.1032Epoch 12/15: [========                      ] 14/47 batches, loss: 0.1015Epoch 12/15: [=========                     ] 15/47 batches, loss: 0.1013Epoch 12/15: [==========                    ] 16/47 batches, loss: 0.1043Epoch 12/15: [==========                    ] 17/47 batches, loss: 0.1024Epoch 12/15: [===========                   ] 18/47 batches, loss: 0.0997Epoch 12/15: [============                  ] 19/47 batches, loss: 0.1003Epoch 12/15: [============                  ] 20/47 batches, loss: 0.1004Epoch 12/15: [=============                 ] 21/47 batches, loss: 0.0990Epoch 12/15: [==============                ] 22/47 batches, loss: 0.0973Epoch 12/15: [==============                ] 23/47 batches, loss: 0.0963Epoch 12/15: [===============               ] 24/47 batches, loss: 0.0992Epoch 12/15: [===============               ] 25/47 batches, loss: 0.1004Epoch 12/15: [================              ] 26/47 batches, loss: 0.0983Epoch 12/15: [=================             ] 27/47 batches, loss: 0.0966Epoch 12/15: [=================             ] 28/47 batches, loss: 0.0966Epoch 12/15: [==================            ] 29/47 batches, loss: 0.0956Epoch 12/15: [===================           ] 30/47 batches, loss: 0.0941Epoch 12/15: [===================           ] 31/47 batches, loss: 0.0938Epoch 12/15: [====================          ] 32/47 batches, loss: 0.0936Epoch 12/15: [=====================         ] 33/47 batches, loss: 0.0932Epoch 12/15: [=====================         ] 34/47 batches, loss: 0.0952Epoch 12/15: [======================        ] 35/47 batches, loss: 0.0962Epoch 12/15: [======================        ] 36/47 batches, loss: 0.0963Epoch 12/15: [=======================       ] 37/47 batches, loss: 0.0959Epoch 12/15: [========================      ] 38/47 batches, loss: 0.0955Epoch 12/15: [========================      ] 39/47 batches, loss: 0.0948Epoch 12/15: [=========================     ] 40/47 batches, loss: 0.0939Epoch 12/15: [==========================    ] 41/47 batches, loss: 0.0944Epoch 12/15: [==========================    ] 42/47 batches, loss: 0.0944Epoch 12/15: [===========================   ] 43/47 batches, loss: 0.0946Epoch 12/15: [============================  ] 44/47 batches, loss: 0.0940Epoch 12/15: [============================  ] 45/47 batches, loss: 0.0931Epoch 12/15: [============================= ] 46/47 batches, loss: 0.0929Epoch 12/15: [==============================] 47/47 batches, loss: 0.0912
[2025-05-07 22:17:49,279][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0912
[2025-05-07 22:17:49,552][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.1135, Metrics: {'mse': 0.11517747491598129, 'rmse': 0.33937807076471704, 'r2': -0.7565048933029175}
[2025-05-07 22:17:49,552][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:17:49,552][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 12
[2025-05-07 22:17:49,553][src.training.lm_trainer][INFO] - Training completed in 23.97 seconds
[2025-05-07 22:17:49,553][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:17:51,894][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.06577181071043015, 'rmse': 0.2564601542353707, 'r2': -0.023226618766784668}
[2025-05-07 22:17:51,895][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07441499829292297, 'rmse': 0.27279112575911074, 'r2': -0.13485991954803467}
[2025-05-07 22:17:51,895][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.05522657185792923, 'rmse': 0.23500334435477557, 'r2': -0.09293782711029053}
[2025-05-07 22:17:53,581][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer2/ko/ko/model.pt
[2025-05-07 22:17:53,582][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▃▁
wandb:     best_val_mse █▄▃▁
wandb:      best_val_r2 ▁▅▆█
wandb:    best_val_rmse █▄▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▅▄▆▃▅▆▇▆▆▆
wandb:       train_loss █▄▃▃▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▅▃▆▄▃▁▂▂▂▄
wandb:          val_mse █▄▅▃▆▄▃▁▂▂▂▄
wandb:           val_r2 ▁▅▄▆▃▅▆█▇▇▇▅
wandb:         val_rmse █▄▅▃▆▄▃▁▂▂▂▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07369
wandb:     best_val_mse 0.07441
wandb:      best_val_r2 -0.13486
wandb:    best_val_rmse 0.27279
wandb: early_stop_epoch 12
wandb:            epoch 12
wandb:   final_test_mse 0.05523
wandb:    final_test_r2 -0.09294
wandb:  final_test_rmse 0.235
wandb:  final_train_mse 0.06577
wandb:   final_train_r2 -0.02323
wandb: final_train_rmse 0.25646
wandb:    final_val_mse 0.07441
wandb:     final_val_r2 -0.13486
wandb:   final_val_rmse 0.27279
wandb:    learning_rate 0.0001
wandb:       train_loss 0.09121
wandb:       train_time 23.97194
wandb:         val_loss 0.1135
wandb:          val_mse 0.11518
wandb:           val_r2 -0.7565
wandb:         val_rmse 0.33938
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_221708-yo0r0q3d
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_221708-yo0r0q3d/logs
Experiment probe_layer2_avg_subordinate_chain_len_control2_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_avg_subordinate_chain_len_control3_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_subordinate_chain_len_control3_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control3/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=avg_subordinate_chain_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:18:22,733][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control3/layer2/ko
experiment_name: probe_layer2_avg_subordinate_chain_len_control3_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:18:22,733][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:18:22,733][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 22:18:22,733][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:18:22,733][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:18:22,738][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:18:22,738][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 22:18:22,738][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:18:27,494][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:18:29,868][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:18:29,868][src.data.datasets][INFO] - Loading 'control_avg_subordinate_chain_len_seed3' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:18:30,079][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:27:11 2025).
[2025-05-07 22:18:30,174][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:27:11 2025).
[2025-05-07 22:18:30,452][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:18:30,459][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:18:30,459][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:18:30,461][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:18:30,543][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:18:30,608][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:18:30,641][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:18:30,642][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:18:30,642][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:18:30,645][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:18:30,700][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:18:30,775][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:18:30,815][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:18:30,817][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:18:30,817][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:18:30,819][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:18:30,820][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:18:30,820][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 22:18:30,820][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 22:18:30,820][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 22:18:30,820][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:18:30,820][src.data.datasets][INFO] -   Mean: 0.1845, Std: 0.2535
[2025-05-07 22:18:30,821][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:18:30,821][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 22:18:30,821][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:18:30,821][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 22:18:30,821][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 22:18:30,821][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 22:18:30,821][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:18:30,821][src.data.datasets][INFO] -   Mean: 0.2673, Std: 0.2561
[2025-05-07 22:18:30,821][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:18:30,821][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 22:18:30,822][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:18:30,822][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 22:18:30,822][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 22:18:30,822][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 22:18:30,822][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6670
[2025-05-07 22:18:30,822][src.data.datasets][INFO] -   Mean: 0.2435, Std: 0.2248
[2025-05-07 22:18:30,822][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:18:30,822][src.data.datasets][INFO] - Sample label: 0.5
[2025-05-07 22:18:30,822][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:18:30,822][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:18:30,823][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:18:30,823][__main__][INFO] - Using model type: lm_probe for submetric avg_subordinate_chain_len
[2025-05-07 22:18:30,823][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:18:37,650][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:18:37,651][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:18:37,651][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:18:37,651][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:18:37,654][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:18:37,655][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:18:37,655][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:18:37,655][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:18:37,655][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:18:37,656][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:18:37,656][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.4001Epoch 1/15: [=                             ] 2/47 batches, loss: 0.5640Epoch 1/15: [=                             ] 3/47 batches, loss: 0.5375Epoch 1/15: [==                            ] 4/47 batches, loss: 0.5456Epoch 1/15: [===                           ] 5/47 batches, loss: 0.5317Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4900Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4752Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4741Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4823Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4940Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4740Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4686Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4657Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4721Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.4584Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4590Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.4529Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4692Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4647Epoch 1/15: [============                  ] 20/47 batches, loss: 0.4531Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4566Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4547Epoch 1/15: [==============                ] 23/47 batches, loss: 0.4497Epoch 1/15: [===============               ] 24/47 batches, loss: 0.4380Epoch 1/15: [===============               ] 25/47 batches, loss: 0.4324Epoch 1/15: [================              ] 26/47 batches, loss: 0.4278Epoch 1/15: [=================             ] 27/47 batches, loss: 0.4254Epoch 1/15: [=================             ] 28/47 batches, loss: 0.4162Epoch 1/15: [==================            ] 29/47 batches, loss: 0.4186Epoch 1/15: [===================           ] 30/47 batches, loss: 0.4135Epoch 1/15: [===================           ] 31/47 batches, loss: 0.4038Epoch 1/15: [====================          ] 32/47 batches, loss: 0.4002Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3973Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3972Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3909Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3916Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3869Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3821Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3771Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3772Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3779Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3748Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3702Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3679Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3674Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3628Epoch 1/15: [==============================] 47/47 batches, loss: 0.3659
[2025-05-07 22:18:43,079][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3659
[2025-05-07 22:18:43,355][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1690, Metrics: {'mse': 0.17299988865852356, 'rmse': 0.41593255301613935, 'r2': -1.6383209228515625}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.3039Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2667Epoch 2/15: [=                             ] 3/47 batches, loss: 0.3020Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2681Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2661Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2922Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2866Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2783Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2717Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2601Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2595Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2536Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2613Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2664Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2586Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2617Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2552Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2630Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2560Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2494Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2472Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2479Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2453Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2423Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2430Epoch 2/15: [================              ] 26/47 batches, loss: 0.2397Epoch 2/15: [=================             ] 27/47 batches, loss: 0.2371Epoch 2/15: [=================             ] 28/47 batches, loss: 0.2336Epoch 2/15: [==================            ] 29/47 batches, loss: 0.2306Epoch 2/15: [===================           ] 30/47 batches, loss: 0.2261Epoch 2/15: [===================           ] 31/47 batches, loss: 0.2253Epoch 2/15: [====================          ] 32/47 batches, loss: 0.2219Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.2240Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.2239Epoch 2/15: [======================        ] 35/47 batches, loss: 0.2211Epoch 2/15: [======================        ] 36/47 batches, loss: 0.2204Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.2241Epoch 2/15: [========================      ] 38/47 batches, loss: 0.2215Epoch 2/15: [========================      ] 39/47 batches, loss: 0.2185Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.2181Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.2161Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.2168Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.2144Epoch 2/15: [============================  ] 44/47 batches, loss: 0.2125Epoch 2/15: [============================  ] 45/47 batches, loss: 0.2120Epoch 2/15: [============================= ] 46/47 batches, loss: 0.2112Epoch 2/15: [==============================] 47/47 batches, loss: 0.2093
[2025-05-07 22:18:45,227][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.2093
[2025-05-07 22:18:45,552][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0981, Metrics: {'mse': 0.09956052899360657, 'rmse': 0.31553213622958687, 'r2': -0.518339991569519}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1976Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1822Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1625Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1572Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1697Epoch 3/15: [===                           ] 6/47 batches, loss: 0.2047Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1983Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1925Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1976Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1924Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1908Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1954Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1933Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1846Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1858Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1884Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1923Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1930Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1908Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1900Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1941Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1895Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1877Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1867Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1826Epoch 3/15: [================              ] 26/47 batches, loss: 0.1815Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1829Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1795Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1781Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1760Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1747Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1766Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1766Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1752Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1780Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1770Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1766Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1755Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1723Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1723Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1724Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1720Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1712Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1751Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1759Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1760Epoch 3/15: [==============================] 47/47 batches, loss: 0.1764
[2025-05-07 22:18:47,469][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1764
[2025-05-07 22:18:47,779][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1219, Metrics: {'mse': 0.1241871789097786, 'rmse': 0.3524020132033564, 'r2': -0.8939065933227539}
[2025-05-07 22:18:47,779][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1429Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1344Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1221Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1371Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1599Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1515Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1463Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1481Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1516Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1540Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1698Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1632Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1647Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1615Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1600Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1579Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1544Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1556Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1539Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1641Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1684Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1680Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1658Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1654Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1650Epoch 4/15: [================              ] 26/47 batches, loss: 0.1637Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1647Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1650Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1636Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1607Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1628Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1623Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1605Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1630Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1627Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1629Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1659Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1649Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1647Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1628Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1631Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1614Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1596Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1590Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1576Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1576Epoch 4/15: [==============================] 47/47 batches, loss: 0.1582
[2025-05-07 22:18:49,274][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1582
[2025-05-07 22:18:49,579][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1050, Metrics: {'mse': 0.10723263025283813, 'rmse': 0.32746393733178947, 'r2': -0.6353427171707153}
[2025-05-07 22:18:49,580][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.2176Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1947Epoch 5/15: [=                             ] 3/47 batches, loss: 0.1715Epoch 5/15: [==                            ] 4/47 batches, loss: 0.1523Epoch 5/15: [===                           ] 5/47 batches, loss: 0.1464Epoch 5/15: [===                           ] 6/47 batches, loss: 0.1528Epoch 5/15: [====                          ] 7/47 batches, loss: 0.1542Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.1494Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.1468Epoch 5/15: [======                        ] 10/47 batches, loss: 0.1421Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.1504Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.1478Epoch 5/15: [========                      ] 13/47 batches, loss: 0.1441Epoch 5/15: [========                      ] 14/47 batches, loss: 0.1408Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.1436Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.1415Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.1383Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.1404Epoch 5/15: [============                  ] 19/47 batches, loss: 0.1409Epoch 5/15: [============                  ] 20/47 batches, loss: 0.1397Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.1420Epoch 5/15: [==============                ] 22/47 batches, loss: 0.1398Epoch 5/15: [==============                ] 23/47 batches, loss: 0.1403Epoch 5/15: [===============               ] 24/47 batches, loss: 0.1401Epoch 5/15: [===============               ] 25/47 batches, loss: 0.1379Epoch 5/15: [================              ] 26/47 batches, loss: 0.1382Epoch 5/15: [=================             ] 27/47 batches, loss: 0.1372Epoch 5/15: [=================             ] 28/47 batches, loss: 0.1356Epoch 5/15: [==================            ] 29/47 batches, loss: 0.1349Epoch 5/15: [===================           ] 30/47 batches, loss: 0.1363Epoch 5/15: [===================           ] 31/47 batches, loss: 0.1352Epoch 5/15: [====================          ] 32/47 batches, loss: 0.1352Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.1344Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.1339Epoch 5/15: [======================        ] 35/47 batches, loss: 0.1331Epoch 5/15: [======================        ] 36/47 batches, loss: 0.1324Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.1302Epoch 5/15: [========================      ] 38/47 batches, loss: 0.1318Epoch 5/15: [========================      ] 39/47 batches, loss: 0.1319Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.1310Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.1300Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.1285Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.1293Epoch 5/15: [============================  ] 44/47 batches, loss: 0.1293Epoch 5/15: [============================  ] 45/47 batches, loss: 0.1297Epoch 5/15: [============================= ] 46/47 batches, loss: 0.1288Epoch 5/15: [==============================] 47/47 batches, loss: 0.1292
[2025-05-07 22:18:51,050][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1292
[2025-05-07 22:18:51,308][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1119, Metrics: {'mse': 0.11444412171840668, 'rmse': 0.33829590851561697, 'r2': -0.7453209161758423}
[2025-05-07 22:18:51,309][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.1095Epoch 6/15: [=                             ] 2/47 batches, loss: 0.1023Epoch 6/15: [=                             ] 3/47 batches, loss: 0.0958Epoch 6/15: [==                            ] 4/47 batches, loss: 0.1029Epoch 6/15: [===                           ] 5/47 batches, loss: 0.1374Epoch 6/15: [===                           ] 6/47 batches, loss: 0.1362Epoch 6/15: [====                          ] 7/47 batches, loss: 0.1299Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.1360Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.1287Epoch 6/15: [======                        ] 10/47 batches, loss: 0.1358Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.1301Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.1347Epoch 6/15: [========                      ] 13/47 batches, loss: 0.1284Epoch 6/15: [========                      ] 14/47 batches, loss: 0.1223Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.1256Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.1282Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.1307Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.1317Epoch 6/15: [============                  ] 19/47 batches, loss: 0.1290Epoch 6/15: [============                  ] 20/47 batches, loss: 0.1271Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.1242Epoch 6/15: [==============                ] 22/47 batches, loss: 0.1257Epoch 6/15: [==============                ] 23/47 batches, loss: 0.1281Epoch 6/15: [===============               ] 24/47 batches, loss: 0.1252Epoch 6/15: [===============               ] 25/47 batches, loss: 0.1259Epoch 6/15: [================              ] 26/47 batches, loss: 0.1253Epoch 6/15: [=================             ] 27/47 batches, loss: 0.1256Epoch 6/15: [=================             ] 28/47 batches, loss: 0.1242Epoch 6/15: [==================            ] 29/47 batches, loss: 0.1282Epoch 6/15: [===================           ] 30/47 batches, loss: 0.1282Epoch 6/15: [===================           ] 31/47 batches, loss: 0.1293Epoch 6/15: [====================          ] 32/47 batches, loss: 0.1280Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.1300Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.1308Epoch 6/15: [======================        ] 35/47 batches, loss: 0.1302Epoch 6/15: [======================        ] 36/47 batches, loss: 0.1285Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.1282Epoch 6/15: [========================      ] 38/47 batches, loss: 0.1270Epoch 6/15: [========================      ] 39/47 batches, loss: 0.1260Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.1263Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.1263Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.1274Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.1266Epoch 6/15: [============================  ] 44/47 batches, loss: 0.1273Epoch 6/15: [============================  ] 45/47 batches, loss: 0.1280Epoch 6/15: [============================= ] 46/47 batches, loss: 0.1268Epoch 6/15: [==============================] 47/47 batches, loss: 0.1290
[2025-05-07 22:18:52,825][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1290
[2025-05-07 22:18:53,117][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.1103, Metrics: {'mse': 0.11253876239061356, 'rmse': 0.335467975208683, 'r2': -0.7162634134292603}
[2025-05-07 22:18:53,118][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:18:53,118][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-07 22:18:53,118][src.training.lm_trainer][INFO] - Training completed in 12.36 seconds
[2025-05-07 22:18:53,118][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:18:55,462][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.06677553802728653, 'rmse': 0.25840963222621277, 'r2': -0.03884172439575195}
[2025-05-07 22:18:55,462][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.09956052899360657, 'rmse': 0.31553213622958687, 'r2': -0.518339991569519}
[2025-05-07 22:18:55,462][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.07660670578479767, 'rmse': 0.27677916428950655, 'r2': -0.5160521268844604}
[2025-05-07 22:18:57,174][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control3/layer2/ko/ko/model.pt
[2025-05-07 22:18:57,176][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▆▄▅▅
wandb:       train_loss █▃▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▃▂▂▂
wandb:          val_mse █▁▃▂▂▂
wandb:           val_r2 ▁█▆▇▇▇
wandb:         val_rmse █▁▄▂▃▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.09806
wandb:     best_val_mse 0.09956
wandb:      best_val_r2 -0.51834
wandb:    best_val_rmse 0.31553
wandb: early_stop_epoch 6
wandb:            epoch 6
wandb:   final_test_mse 0.07661
wandb:    final_test_r2 -0.51605
wandb:  final_test_rmse 0.27678
wandb:  final_train_mse 0.06678
wandb:   final_train_r2 -0.03884
wandb: final_train_rmse 0.25841
wandb:    final_val_mse 0.09956
wandb:     final_val_r2 -0.51834
wandb:   final_val_rmse 0.31553
wandb:    learning_rate 0.0001
wandb:       train_loss 0.12899
wandb:       train_time 12.36285
wandb:         val_loss 0.11025
wandb:          val_mse 0.11254
wandb:           val_r2 -0.71626
wandb:         val_rmse 0.33547
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_221822-vh4sp3he
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_221822-vh4sp3he/logs
Experiment probe_layer2_avg_subordinate_chain_len_control3_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control3/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_avg_verb_edges_control1_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_verb_edges_control1_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control1/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_verb_edges"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:19:24,003][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control1/layer2/ko
experiment_name: probe_layer2_avg_verb_edges_control1_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:19:24,003][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:19:24,003][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 22:19:24,003][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:19:24,003][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:19:24,007][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:19:24,007][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 22:19:24,007][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:19:27,172][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:19:29,502][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:19:29,502][src.data.datasets][INFO] - Loading 'control_avg_verb_edges_seed1' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:19:29,652][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_verb_edges_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:29:02 2025).
[2025-05-07 22:19:29,805][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_verb_edges_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:29:02 2025).
[2025-05-07 22:19:30,142][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:19:30,147][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:19:30,148][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:19:30,151][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:19:30,395][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:19:30,536][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:19:30,556][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:19:30,558][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:19:30,558][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:19:30,560][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:19:30,635][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:19:30,728][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:19:30,744][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:19:30,746][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:19:30,746][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:19:30,748][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:19:30,749][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:19:30,749][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 22:19:30,749][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 22:19:30,749][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 22:19:30,749][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:19:30,749][src.data.datasets][INFO] -   Mean: 0.3982, Std: 0.1920
[2025-05-07 22:19:30,749][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:19:30,749][src.data.datasets][INFO] - Sample label: 0.4000000059604645
[2025-05-07 22:19:30,750][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:19:30,750][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 22:19:30,750][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 22:19:30,750][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 22:19:30,750][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-05-07 22:19:30,750][src.data.datasets][INFO] -   Mean: 0.3198, Std: 0.1567
[2025-05-07 22:19:30,750][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:19:30,750][src.data.datasets][INFO] - Sample label: 0.27799999713897705
[2025-05-07 22:19:30,750][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:19:30,751][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 22:19:30,751][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 22:19:30,751][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 22:19:30,751][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-05-07 22:19:30,751][src.data.datasets][INFO] -   Mean: 0.3201, Std: 0.1863
[2025-05-07 22:19:30,751][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:19:30,751][src.data.datasets][INFO] - Sample label: 0.25
[2025-05-07 22:19:30,751][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:19:30,751][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:19:30,752][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:19:30,752][__main__][INFO] - Using model type: lm_probe for submetric avg_verb_edges
[2025-05-07 22:19:30,752][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:19:37,039][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:19:37,040][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:19:37,040][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:19:37,040][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:19:37,043][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:19:37,043][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:19:37,043][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:19:37,044][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:19:37,044][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:19:37,045][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:19:37,045][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.3676Epoch 1/15: [=                             ] 2/47 batches, loss: 0.5223Epoch 1/15: [=                             ] 3/47 batches, loss: 0.5110Epoch 1/15: [==                            ] 4/47 batches, loss: 0.4768Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4627Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4231Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4067Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4237Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4344Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4447Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4363Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4357Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4180Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4180Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.4013Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4112Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.4058Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4163Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4074Epoch 1/15: [============                  ] 20/47 batches, loss: 0.3998Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4082Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4085Epoch 1/15: [==============                ] 23/47 batches, loss: 0.3960Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3861Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3843Epoch 1/15: [================              ] 26/47 batches, loss: 0.3808Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3816Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3739Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3771Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3743Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3679Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3648Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3645Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3653Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3610Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3600Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3543Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3542Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3512Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3515Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3494Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3475Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3454Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3432Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3419Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3361Epoch 1/15: [==============================] 47/47 batches, loss: 0.3409
[2025-05-07 22:19:44,754][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3409
[2025-05-07 22:19:45,061][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0406, Metrics: {'mse': 0.04223708063364029, 'rmse': 0.20551661887458222, 'r2': -0.7191578149795532}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2836Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2171Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2506Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2135Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2342Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2512Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2384Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2396Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2400Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2357Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2357Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2327Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2324Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2358Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2439Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2387Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2356Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2439Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2429Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2376Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2320Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2317Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2262Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2258Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2271Epoch 2/15: [================              ] 26/47 batches, loss: 0.2255Epoch 2/15: [=================             ] 27/47 batches, loss: 0.2233Epoch 2/15: [=================             ] 28/47 batches, loss: 0.2238Epoch 2/15: [==================            ] 29/47 batches, loss: 0.2200Epoch 2/15: [===================           ] 30/47 batches, loss: 0.2183Epoch 2/15: [===================           ] 31/47 batches, loss: 0.2168Epoch 2/15: [====================          ] 32/47 batches, loss: 0.2187Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.2215Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.2233Epoch 2/15: [======================        ] 35/47 batches, loss: 0.2191Epoch 2/15: [======================        ] 36/47 batches, loss: 0.2174Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.2195Epoch 2/15: [========================      ] 38/47 batches, loss: 0.2156Epoch 2/15: [========================      ] 39/47 batches, loss: 0.2145Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.2136Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.2121Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.2102Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.2067Epoch 2/15: [============================  ] 44/47 batches, loss: 0.2051Epoch 2/15: [============================  ] 45/47 batches, loss: 0.2026Epoch 2/15: [============================= ] 46/47 batches, loss: 0.2005Epoch 2/15: [==============================] 47/47 batches, loss: 0.2028
[2025-05-07 22:19:46,990][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.2028
[2025-05-07 22:19:47,301][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0296, Metrics: {'mse': 0.029674066230654716, 'rmse': 0.17226162146762325, 'r2': -0.2078108787536621}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.2618Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1803Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1565Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1403Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1514Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1694Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1627Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1577Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1598Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1587Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1574Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1604Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1609Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1536Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1538Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1531Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1582Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1584Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1542Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1533Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1572Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1547Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1529Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1551Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1540Epoch 3/15: [================              ] 26/47 batches, loss: 0.1541Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1544Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1530Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1520Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1502Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1514Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1506Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1509Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1498Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1522Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1513Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1493Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1495Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1482Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1476Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1460Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1479Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1473Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1481Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1515Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1512Epoch 3/15: [==============================] 47/47 batches, loss: 0.1550
[2025-05-07 22:19:49,212][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1550
[2025-05-07 22:19:49,492][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0285, Metrics: {'mse': 0.028709743171930313, 'rmse': 0.1694394970835617, 'r2': -0.16856038570404053}
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1921Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1597Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1231Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1399Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1444Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1393Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1427Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1440Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1353Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1334Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1393Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1373Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1335Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1291Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1276Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1231Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1242Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1237Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1243Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1289Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1295Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1284Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1287Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1287Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1322Epoch 4/15: [================              ] 26/47 batches, loss: 0.1303Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1295Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1317Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1317Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1294Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1333Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1346Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1324Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1334Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1353Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1341Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1352Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1339Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1333Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1317Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1320Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1310Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1285Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1288Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1282Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1278Epoch 4/15: [==============================] 47/47 batches, loss: 0.1291
[2025-05-07 22:19:51,378][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1291
[2025-05-07 22:19:51,719][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0267, Metrics: {'mse': 0.027050405740737915, 'rmse': 0.16447007551751752, 'r2': -0.10102105140686035}
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.1014Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1531Epoch 5/15: [=                             ] 3/47 batches, loss: 0.1324Epoch 5/15: [==                            ] 4/47 batches, loss: 0.1144Epoch 5/15: [===                           ] 5/47 batches, loss: 0.1034Epoch 5/15: [===                           ] 6/47 batches, loss: 0.1046Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0981Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0952Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0943Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0927Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0914Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0951Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0951Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0936Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0960Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0973Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0964Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0961Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0942Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0948Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0970Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0978Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0963Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0980Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0979Epoch 5/15: [================              ] 26/47 batches, loss: 0.0956Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0960Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0965Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0949Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0945Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0962Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0972Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0960Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0950Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0992Epoch 5/15: [======================        ] 36/47 batches, loss: 0.1002Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0994Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0994Epoch 5/15: [========================      ] 39/47 batches, loss: 0.1008Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.1011Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.1015Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.1018Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.1038Epoch 5/15: [============================  ] 44/47 batches, loss: 0.1028Epoch 5/15: [============================  ] 45/47 batches, loss: 0.1055Epoch 5/15: [============================= ] 46/47 batches, loss: 0.1047Epoch 5/15: [==============================] 47/47 batches, loss: 0.1032
[2025-05-07 22:19:53,570][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1032
[2025-05-07 22:19:53,839][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0271, Metrics: {'mse': 0.02737431973218918, 'rmse': 0.1654518653028402, 'r2': -0.1142052412033081}
[2025-05-07 22:19:53,840][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.1671Epoch 6/15: [=                             ] 2/47 batches, loss: 0.1300Epoch 6/15: [=                             ] 3/47 batches, loss: 0.1629Epoch 6/15: [==                            ] 4/47 batches, loss: 0.1350Epoch 6/15: [===                           ] 5/47 batches, loss: 0.1402Epoch 6/15: [===                           ] 6/47 batches, loss: 0.1403Epoch 6/15: [====                          ] 7/47 batches, loss: 0.1331Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.1311Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.1271Epoch 6/15: [======                        ] 10/47 batches, loss: 0.1243Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.1292Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.1339Epoch 6/15: [========                      ] 13/47 batches, loss: 0.1289Epoch 6/15: [========                      ] 14/47 batches, loss: 0.1232Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.1186Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.1190Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.1181Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.1189Epoch 6/15: [============                  ] 19/47 batches, loss: 0.1147Epoch 6/15: [============                  ] 20/47 batches, loss: 0.1131Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.1108Epoch 6/15: [==============                ] 22/47 batches, loss: 0.1085Epoch 6/15: [==============                ] 23/47 batches, loss: 0.1093Epoch 6/15: [===============               ] 24/47 batches, loss: 0.1084Epoch 6/15: [===============               ] 25/47 batches, loss: 0.1066Epoch 6/15: [================              ] 26/47 batches, loss: 0.1057Epoch 6/15: [=================             ] 27/47 batches, loss: 0.1044Epoch 6/15: [=================             ] 28/47 batches, loss: 0.1053Epoch 6/15: [==================            ] 29/47 batches, loss: 0.1039Epoch 6/15: [===================           ] 30/47 batches, loss: 0.1030Epoch 6/15: [===================           ] 31/47 batches, loss: 0.1021Epoch 6/15: [====================          ] 32/47 batches, loss: 0.1008Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.1010Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.1011Epoch 6/15: [======================        ] 35/47 batches, loss: 0.1026Epoch 6/15: [======================        ] 36/47 batches, loss: 0.1008Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.1011Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0997Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0990Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0983Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0972Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0975Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0978Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0973Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0965Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0960Epoch 6/15: [==============================] 47/47 batches, loss: 0.0956
[2025-05-07 22:19:55,305][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0956
[2025-05-07 22:19:55,577][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0305, Metrics: {'mse': 0.030823562294244766, 'rmse': 0.17556640422998007, 'r2': -0.2545982599258423}
[2025-05-07 22:19:55,578][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/47 batches, loss: 0.0470Epoch 7/15: [=                             ] 2/47 batches, loss: 0.0638Epoch 7/15: [=                             ] 3/47 batches, loss: 0.0750Epoch 7/15: [==                            ] 4/47 batches, loss: 0.0888Epoch 7/15: [===                           ] 5/47 batches, loss: 0.0912Epoch 7/15: [===                           ] 6/47 batches, loss: 0.0934Epoch 7/15: [====                          ] 7/47 batches, loss: 0.0877Epoch 7/15: [=====                         ] 8/47 batches, loss: 0.0842Epoch 7/15: [=====                         ] 9/47 batches, loss: 0.0958Epoch 7/15: [======                        ] 10/47 batches, loss: 0.1021Epoch 7/15: [=======                       ] 11/47 batches, loss: 0.0983Epoch 7/15: [=======                       ] 12/47 batches, loss: 0.1013Epoch 7/15: [========                      ] 13/47 batches, loss: 0.0983Epoch 7/15: [========                      ] 14/47 batches, loss: 0.0974Epoch 7/15: [=========                     ] 15/47 batches, loss: 0.0972Epoch 7/15: [==========                    ] 16/47 batches, loss: 0.0965Epoch 7/15: [==========                    ] 17/47 batches, loss: 0.0967Epoch 7/15: [===========                   ] 18/47 batches, loss: 0.0983Epoch 7/15: [============                  ] 19/47 batches, loss: 0.1002Epoch 7/15: [============                  ] 20/47 batches, loss: 0.0980Epoch 7/15: [=============                 ] 21/47 batches, loss: 0.0948Epoch 7/15: [==============                ] 22/47 batches, loss: 0.0935Epoch 7/15: [==============                ] 23/47 batches, loss: 0.0947Epoch 7/15: [===============               ] 24/47 batches, loss: 0.0940Epoch 7/15: [===============               ] 25/47 batches, loss: 0.0943Epoch 7/15: [================              ] 26/47 batches, loss: 0.0947Epoch 7/15: [=================             ] 27/47 batches, loss: 0.0948Epoch 7/15: [=================             ] 28/47 batches, loss: 0.0949Epoch 7/15: [==================            ] 29/47 batches, loss: 0.0946Epoch 7/15: [===================           ] 30/47 batches, loss: 0.0945Epoch 7/15: [===================           ] 31/47 batches, loss: 0.0931Epoch 7/15: [====================          ] 32/47 batches, loss: 0.0930Epoch 7/15: [=====================         ] 33/47 batches, loss: 0.0926Epoch 7/15: [=====================         ] 34/47 batches, loss: 0.0921Epoch 7/15: [======================        ] 35/47 batches, loss: 0.0912Epoch 7/15: [======================        ] 36/47 batches, loss: 0.0924Epoch 7/15: [=======================       ] 37/47 batches, loss: 0.0913Epoch 7/15: [========================      ] 38/47 batches, loss: 0.0901Epoch 7/15: [========================      ] 39/47 batches, loss: 0.0907Epoch 7/15: [=========================     ] 40/47 batches, loss: 0.0905Epoch 7/15: [==========================    ] 41/47 batches, loss: 0.0889Epoch 7/15: [==========================    ] 42/47 batches, loss: 0.0899Epoch 7/15: [===========================   ] 43/47 batches, loss: 0.0892Epoch 7/15: [============================  ] 44/47 batches, loss: 0.0883Epoch 7/15: [============================  ] 45/47 batches, loss: 0.0877Epoch 7/15: [============================= ] 46/47 batches, loss: 0.0874Epoch 7/15: [==============================] 47/47 batches, loss: 0.0876
[2025-05-07 22:19:57,056][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0876
[2025-05-07 22:19:57,383][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0269, Metrics: {'mse': 0.02702399343252182, 'rmse': 0.16438976072895117, 'r2': -0.09994614124298096}
[2025-05-07 22:19:57,384][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/47 batches, loss: 0.0670Epoch 8/15: [=                             ] 2/47 batches, loss: 0.0574Epoch 8/15: [=                             ] 3/47 batches, loss: 0.0706Epoch 8/15: [==                            ] 4/47 batches, loss: 0.0832Epoch 8/15: [===                           ] 5/47 batches, loss: 0.0915Epoch 8/15: [===                           ] 6/47 batches, loss: 0.0877Epoch 8/15: [====                          ] 7/47 batches, loss: 0.0872Epoch 8/15: [=====                         ] 8/47 batches, loss: 0.0844Epoch 8/15: [=====                         ] 9/47 batches, loss: 0.0796Epoch 8/15: [======                        ] 10/47 batches, loss: 0.0808Epoch 8/15: [=======                       ] 11/47 batches, loss: 0.0812Epoch 8/15: [=======                       ] 12/47 batches, loss: 0.0787Epoch 8/15: [========                      ] 13/47 batches, loss: 0.0764Epoch 8/15: [========                      ] 14/47 batches, loss: 0.0757Epoch 8/15: [=========                     ] 15/47 batches, loss: 0.0802Epoch 8/15: [==========                    ] 16/47 batches, loss: 0.0799Epoch 8/15: [==========                    ] 17/47 batches, loss: 0.0801Epoch 8/15: [===========                   ] 18/47 batches, loss: 0.0793Epoch 8/15: [============                  ] 19/47 batches, loss: 0.0780Epoch 8/15: [============                  ] 20/47 batches, loss: 0.0782Epoch 8/15: [=============                 ] 21/47 batches, loss: 0.0792Epoch 8/15: [==============                ] 22/47 batches, loss: 0.0786Epoch 8/15: [==============                ] 23/47 batches, loss: 0.0775Epoch 8/15: [===============               ] 24/47 batches, loss: 0.0789Epoch 8/15: [===============               ] 25/47 batches, loss: 0.0772Epoch 8/15: [================              ] 26/47 batches, loss: 0.0782Epoch 8/15: [=================             ] 27/47 batches, loss: 0.0770Epoch 8/15: [=================             ] 28/47 batches, loss: 0.0764Epoch 8/15: [==================            ] 29/47 batches, loss: 0.0786Epoch 8/15: [===================           ] 30/47 batches, loss: 0.0771Epoch 8/15: [===================           ] 31/47 batches, loss: 0.0769Epoch 8/15: [====================          ] 32/47 batches, loss: 0.0775Epoch 8/15: [=====================         ] 33/47 batches, loss: 0.0780Epoch 8/15: [=====================         ] 34/47 batches, loss: 0.0781Epoch 8/15: [======================        ] 35/47 batches, loss: 0.0780Epoch 8/15: [======================        ] 36/47 batches, loss: 0.0787Epoch 8/15: [=======================       ] 37/47 batches, loss: 0.0779Epoch 8/15: [========================      ] 38/47 batches, loss: 0.0791Epoch 8/15: [========================      ] 39/47 batches, loss: 0.0792Epoch 8/15: [=========================     ] 40/47 batches, loss: 0.0791Epoch 8/15: [==========================    ] 41/47 batches, loss: 0.0787Epoch 8/15: [==========================    ] 42/47 batches, loss: 0.0775Epoch 8/15: [===========================   ] 43/47 batches, loss: 0.0775Epoch 8/15: [============================  ] 44/47 batches, loss: 0.0771Epoch 8/15: [============================  ] 45/47 batches, loss: 0.0772Epoch 8/15: [============================= ] 46/47 batches, loss: 0.0775Epoch 8/15: [==============================] 47/47 batches, loss: 0.0764
[2025-05-07 22:19:58,968][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0764
[2025-05-07 22:19:59,351][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0333, Metrics: {'mse': 0.032831866294145584, 'rmse': 0.18119565749251715, 'r2': -0.3363415002822876}
[2025-05-07 22:19:59,352][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:19:59,352][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-07 22:19:59,352][src.training.lm_trainer][INFO] - Training completed in 16.87 seconds
[2025-05-07 22:19:59,352][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:20:01,529][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.04669605568051338, 'rmse': 0.21609270159011243, 'r2': -0.2662721872329712}
[2025-05-07 22:20:01,529][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.027050405740737915, 'rmse': 0.16447007551751752, 'r2': -0.10102105140686035}
[2025-05-07 22:20:01,529][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03547263890504837, 'rmse': 0.18834181401125022, 'r2': -0.0224379301071167}
[2025-05-07 22:20:03,340][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control1/layer2/ko/ko/model.pt
[2025-05-07 22:20:03,341][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▂▁
wandb:     best_val_mse █▂▂▁
wandb:      best_val_r2 ▁▇▇█
wandb:    best_val_rmse █▂▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▄▅▅▅▄▅
wandb:       train_loss █▄▃▂▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▂▂▁▁▃▁▄
wandb:          val_mse █▂▂▁▁▃▁▄
wandb:           val_r2 ▁▇▇██▆█▅
wandb:         val_rmse █▂▂▁▁▃▁▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02673
wandb:     best_val_mse 0.02705
wandb:      best_val_r2 -0.10102
wandb:    best_val_rmse 0.16447
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.03547
wandb:    final_test_r2 -0.02244
wandb:  final_test_rmse 0.18834
wandb:  final_train_mse 0.0467
wandb:   final_train_r2 -0.26627
wandb: final_train_rmse 0.21609
wandb:    final_val_mse 0.02705
wandb:     final_val_r2 -0.10102
wandb:   final_val_rmse 0.16447
wandb:    learning_rate 0.0001
wandb:       train_loss 0.07636
wandb:       train_time 16.86645
wandb:         val_loss 0.03333
wandb:          val_mse 0.03283
wandb:           val_r2 -0.33634
wandb:         val_rmse 0.1812
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_221924-agbmuejw
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_221924-agbmuejw/logs
Experiment probe_layer2_avg_verb_edges_control1_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control1/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_avg_verb_edges_control2_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_verb_edges_control2_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control2/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=avg_verb_edges"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:20:34,195][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control2/layer2/ko
experiment_name: probe_layer2_avg_verb_edges_control2_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:20:34,195][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:20:34,195][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 22:20:34,195][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:20:34,195][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:20:34,200][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:20:34,200][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 22:20:34,200][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:20:37,778][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:20:40,283][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:20:40,283][src.data.datasets][INFO] - Loading 'control_avg_verb_edges_seed2' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:20:40,645][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_verb_edges_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:30:51 2025).
[2025-05-07 22:20:40,805][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_verb_edges_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:30:51 2025).
[2025-05-07 22:20:41,143][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:20:41,150][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:20:41,150][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:20:41,152][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:20:41,230][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:20:41,303][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:20:41,322][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:20:41,323][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:20:41,324][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:20:41,325][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:20:41,474][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:20:41,558][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:20:41,606][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:20:41,608][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:20:41,608][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:20:41,609][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:20:41,610][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:20:41,610][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 22:20:41,610][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 22:20:41,610][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 22:20:41,610][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:20:41,610][src.data.datasets][INFO] -   Mean: 0.3982, Std: 0.1920
[2025-05-07 22:20:41,611][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:20:41,611][src.data.datasets][INFO] - Sample label: 0.4000000059604645
[2025-05-07 22:20:41,611][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:20:41,611][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 22:20:41,611][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 22:20:41,611][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 22:20:41,611][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-05-07 22:20:41,611][src.data.datasets][INFO] -   Mean: 0.3198, Std: 0.1567
[2025-05-07 22:20:41,611][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:20:41,612][src.data.datasets][INFO] - Sample label: 0.27799999713897705
[2025-05-07 22:20:41,612][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:20:41,612][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 22:20:41,612][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 22:20:41,612][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 22:20:41,612][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-05-07 22:20:41,612][src.data.datasets][INFO] -   Mean: 0.3201, Std: 0.1863
[2025-05-07 22:20:41,612][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:20:41,612][src.data.datasets][INFO] - Sample label: 0.25
[2025-05-07 22:20:41,612][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:20:41,612][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:20:41,613][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:20:41,613][__main__][INFO] - Using model type: lm_probe for submetric avg_verb_edges
[2025-05-07 22:20:41,613][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:20:49,027][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:20:49,027][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:20:49,028][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:20:49,028][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:20:49,031][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:20:49,031][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:20:49,031][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:20:49,031][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:20:49,031][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:20:49,032][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:20:49,032][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.4158Epoch 1/15: [=                             ] 2/47 batches, loss: 0.4909Epoch 1/15: [=                             ] 3/47 batches, loss: 0.4539Epoch 1/15: [==                            ] 4/47 batches, loss: 0.4681Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4563Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4224Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4079Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4266Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4347Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4273Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4088Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4035Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4001Epoch 1/15: [========                      ] 14/47 batches, loss: 0.3914Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.3836Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.3932Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.3873Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4026Epoch 1/15: [============                  ] 19/47 batches, loss: 0.3970Epoch 1/15: [============                  ] 20/47 batches, loss: 0.3935Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4013Epoch 1/15: [==============                ] 22/47 batches, loss: 0.3996Epoch 1/15: [==============                ] 23/47 batches, loss: 0.3901Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3806Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3765Epoch 1/15: [================              ] 26/47 batches, loss: 0.3727Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3704Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3631Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3686Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3628Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3580Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3524Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3485Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3475Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3432Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3392Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3375Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3350Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3329Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3354Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3338Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3319Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3297Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3291Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3286Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3235Epoch 1/15: [==============================] 47/47 batches, loss: 0.3250
[2025-05-07 22:20:55,110][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3250
[2025-05-07 22:20:55,399][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0394, Metrics: {'mse': 0.040860846638679504, 'rmse': 0.20214066052795887, 'r2': -0.6631414890289307}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.3535Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2977Epoch 2/15: [=                             ] 3/47 batches, loss: 0.3013Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2446Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2701Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2748Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2697Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2601Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2632Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2480Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2516Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2453Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2390Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2386Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2332Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2358Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2379Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2445Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2391Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2345Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2345Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2330Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2306Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2305Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2288Epoch 2/15: [================              ] 26/47 batches, loss: 0.2252Epoch 2/15: [=================             ] 27/47 batches, loss: 0.2209Epoch 2/15: [=================             ] 28/47 batches, loss: 0.2189Epoch 2/15: [==================            ] 29/47 batches, loss: 0.2153Epoch 2/15: [===================           ] 30/47 batches, loss: 0.2133Epoch 2/15: [===================           ] 31/47 batches, loss: 0.2117Epoch 2/15: [====================          ] 32/47 batches, loss: 0.2107Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.2160Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.2157Epoch 2/15: [======================        ] 35/47 batches, loss: 0.2117Epoch 2/15: [======================        ] 36/47 batches, loss: 0.2097Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.2095Epoch 2/15: [========================      ] 38/47 batches, loss: 0.2058Epoch 2/15: [========================      ] 39/47 batches, loss: 0.2045Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.2037Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.2020Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.2001Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1976Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1947Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1926Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1900Epoch 2/15: [==============================] 47/47 batches, loss: 0.1895
[2025-05-07 22:20:57,313][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1895
[2025-05-07 22:20:57,588][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0339, Metrics: {'mse': 0.03392232954502106, 'rmse': 0.18418015513355682, 'r2': -0.3807259798049927}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1616Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1465Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1451Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1338Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1452Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1493Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1515Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1490Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1562Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1525Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1511Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1541Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1535Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1480Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1535Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1536Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1555Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1556Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1514Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1496Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1495Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1470Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1475Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1501Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1488Epoch 3/15: [================              ] 26/47 batches, loss: 0.1495Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1480Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1455Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1431Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1430Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1439Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1437Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1453Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1448Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1455Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1446Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1446Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1435Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1418Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1418Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1411Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1406Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1412Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1455Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1457Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1454Epoch 3/15: [==============================] 47/47 batches, loss: 0.1440
[2025-05-07 22:20:59,680][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1440
[2025-05-07 22:21:00,067][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0298, Metrics: {'mse': 0.03007678873836994, 'rmse': 0.17342660908398672, 'r2': -0.22420263290405273}
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1450Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1635Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1387Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1367Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1344Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1286Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1289Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1302Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1260Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1281Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1306Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1260Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1253Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1242Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1214Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1196Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1181Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1170Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1195Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1239Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1226Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1222Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1214Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1214Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1200Epoch 4/15: [================              ] 26/47 batches, loss: 0.1211Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1213Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1236Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1270Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1254Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1272Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1287Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1264Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1255Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1243Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1232Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1241Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1229Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1231Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1223Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1235Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1237Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1226Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1232Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1235Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1231Epoch 4/15: [==============================] 47/47 batches, loss: 0.1246
[2025-05-07 22:21:01,884][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1246
[2025-05-07 22:21:02,213][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0302, Metrics: {'mse': 0.030405210331082344, 'rmse': 0.17437089875057232, 'r2': -0.23757028579711914}
[2025-05-07 22:21:02,213][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.1185Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1302Epoch 5/15: [=                             ] 3/47 batches, loss: 0.1230Epoch 5/15: [==                            ] 4/47 batches, loss: 0.1055Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0972Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0998Epoch 5/15: [====                          ] 7/47 batches, loss: 0.1005Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0994Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0999Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0954Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0980Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.1004Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0975Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0951Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0931Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0931Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0931Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0946Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0933Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0961Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.1005Epoch 5/15: [==============                ] 22/47 batches, loss: 0.1008Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0986Epoch 5/15: [===============               ] 24/47 batches, loss: 0.1003Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0987Epoch 5/15: [================              ] 26/47 batches, loss: 0.0979Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0971Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0949Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0939Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0944Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0953Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0959Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0957Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0969Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0975Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0971Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0959Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0990Epoch 5/15: [========================      ] 39/47 batches, loss: 0.1003Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.1036Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.1034Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.1034Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.1039Epoch 5/15: [============================  ] 44/47 batches, loss: 0.1038Epoch 5/15: [============================  ] 45/47 batches, loss: 0.1044Epoch 5/15: [============================= ] 46/47 batches, loss: 0.1035Epoch 5/15: [==============================] 47/47 batches, loss: 0.1019
[2025-05-07 22:21:03,722][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1019
[2025-05-07 22:21:04,142][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0288, Metrics: {'mse': 0.029079608619213104, 'rmse': 0.1705274424226585, 'r2': -0.18361496925354004}
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.0963Epoch 6/15: [=                             ] 2/47 batches, loss: 0.0648Epoch 6/15: [=                             ] 3/47 batches, loss: 0.0997Epoch 6/15: [==                            ] 4/47 batches, loss: 0.1032Epoch 6/15: [===                           ] 5/47 batches, loss: 0.1078Epoch 6/15: [===                           ] 6/47 batches, loss: 0.1136Epoch 6/15: [====                          ] 7/47 batches, loss: 0.1130Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.1135Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.1083Epoch 6/15: [======                        ] 10/47 batches, loss: 0.1065Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.1150Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.1182Epoch 6/15: [========                      ] 13/47 batches, loss: 0.1161Epoch 6/15: [========                      ] 14/47 batches, loss: 0.1123Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.1104Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.1112Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.1136Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.1123Epoch 6/15: [============                  ] 19/47 batches, loss: 0.1090Epoch 6/15: [============                  ] 20/47 batches, loss: 0.1067Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.1038Epoch 6/15: [==============                ] 22/47 batches, loss: 0.1031Epoch 6/15: [==============                ] 23/47 batches, loss: 0.1027Epoch 6/15: [===============               ] 24/47 batches, loss: 0.1020Epoch 6/15: [===============               ] 25/47 batches, loss: 0.1009Epoch 6/15: [================              ] 26/47 batches, loss: 0.1005Epoch 6/15: [=================             ] 27/47 batches, loss: 0.1008Epoch 6/15: [=================             ] 28/47 batches, loss: 0.1014Epoch 6/15: [==================            ] 29/47 batches, loss: 0.1026Epoch 6/15: [===================           ] 30/47 batches, loss: 0.1034Epoch 6/15: [===================           ] 31/47 batches, loss: 0.1035Epoch 6/15: [====================          ] 32/47 batches, loss: 0.1024Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.1026Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.1029Epoch 6/15: [======================        ] 35/47 batches, loss: 0.1024Epoch 6/15: [======================        ] 36/47 batches, loss: 0.1010Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.1031Epoch 6/15: [========================      ] 38/47 batches, loss: 0.1026Epoch 6/15: [========================      ] 39/47 batches, loss: 0.1034Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.1022Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.1017Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.1023Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.1019Epoch 6/15: [============================  ] 44/47 batches, loss: 0.1024Epoch 6/15: [============================  ] 45/47 batches, loss: 0.1019Epoch 6/15: [============================= ] 46/47 batches, loss: 0.1029Epoch 6/15: [==============================] 47/47 batches, loss: 0.1042
[2025-05-07 22:21:06,003][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1042
[2025-05-07 22:21:06,271][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0297, Metrics: {'mse': 0.029890278354287148, 'rmse': 0.172888051508157, 'r2': -0.21661126613616943}
[2025-05-07 22:21:06,271][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/47 batches, loss: 0.0944Epoch 7/15: [=                             ] 2/47 batches, loss: 0.0918Epoch 7/15: [=                             ] 3/47 batches, loss: 0.0948Epoch 7/15: [==                            ] 4/47 batches, loss: 0.0966Epoch 7/15: [===                           ] 5/47 batches, loss: 0.0985Epoch 7/15: [===                           ] 6/47 batches, loss: 0.0953Epoch 7/15: [====                          ] 7/47 batches, loss: 0.0930Epoch 7/15: [=====                         ] 8/47 batches, loss: 0.0963Epoch 7/15: [=====                         ] 9/47 batches, loss: 0.0996Epoch 7/15: [======                        ] 10/47 batches, loss: 0.0976Epoch 7/15: [=======                       ] 11/47 batches, loss: 0.0928Epoch 7/15: [=======                       ] 12/47 batches, loss: 0.0902Epoch 7/15: [========                      ] 13/47 batches, loss: 0.0884Epoch 7/15: [========                      ] 14/47 batches, loss: 0.0863Epoch 7/15: [=========                     ] 15/47 batches, loss: 0.0870Epoch 7/15: [==========                    ] 16/47 batches, loss: 0.0866Epoch 7/15: [==========                    ] 17/47 batches, loss: 0.0849Epoch 7/15: [===========                   ] 18/47 batches, loss: 0.0865Epoch 7/15: [============                  ] 19/47 batches, loss: 0.0859Epoch 7/15: [============                  ] 20/47 batches, loss: 0.0846Epoch 7/15: [=============                 ] 21/47 batches, loss: 0.0826Epoch 7/15: [==============                ] 22/47 batches, loss: 0.0824Epoch 7/15: [==============                ] 23/47 batches, loss: 0.0838Epoch 7/15: [===============               ] 24/47 batches, loss: 0.0839Epoch 7/15: [===============               ] 25/47 batches, loss: 0.0844Epoch 7/15: [================              ] 26/47 batches, loss: 0.0831Epoch 7/15: [=================             ] 27/47 batches, loss: 0.0818Epoch 7/15: [=================             ] 28/47 batches, loss: 0.0827Epoch 7/15: [==================            ] 29/47 batches, loss: 0.0843Epoch 7/15: [===================           ] 30/47 batches, loss: 0.0849Epoch 7/15: [===================           ] 31/47 batches, loss: 0.0844Epoch 7/15: [====================          ] 32/47 batches, loss: 0.0833Epoch 7/15: [=====================         ] 33/47 batches, loss: 0.0825Epoch 7/15: [=====================         ] 34/47 batches, loss: 0.0820Epoch 7/15: [======================        ] 35/47 batches, loss: 0.0825Epoch 7/15: [======================        ] 36/47 batches, loss: 0.0832Epoch 7/15: [=======================       ] 37/47 batches, loss: 0.0835Epoch 7/15: [========================      ] 38/47 batches, loss: 0.0833Epoch 7/15: [========================      ] 39/47 batches, loss: 0.0833Epoch 7/15: [=========================     ] 40/47 batches, loss: 0.0834Epoch 7/15: [==========================    ] 41/47 batches, loss: 0.0825Epoch 7/15: [==========================    ] 42/47 batches, loss: 0.0826Epoch 7/15: [===========================   ] 43/47 batches, loss: 0.0833Epoch 7/15: [============================  ] 44/47 batches, loss: 0.0829Epoch 7/15: [============================  ] 45/47 batches, loss: 0.0826Epoch 7/15: [============================= ] 46/47 batches, loss: 0.0822Epoch 7/15: [==============================] 47/47 batches, loss: 0.0845
[2025-05-07 22:21:07,851][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0845
[2025-05-07 22:21:08,160][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0278, Metrics: {'mse': 0.02766227349638939, 'rmse': 0.16631979285818446, 'r2': -0.12592566013336182}
Epoch 8/15: [Epoch 8/15: [                              ] 1/47 batches, loss: 0.0331Epoch 8/15: [=                             ] 2/47 batches, loss: 0.0490Epoch 8/15: [=                             ] 3/47 batches, loss: 0.0712Epoch 8/15: [==                            ] 4/47 batches, loss: 0.0728Epoch 8/15: [===                           ] 5/47 batches, loss: 0.0750Epoch 8/15: [===                           ] 6/47 batches, loss: 0.0739Epoch 8/15: [====                          ] 7/47 batches, loss: 0.0768Epoch 8/15: [=====                         ] 8/47 batches, loss: 0.0765Epoch 8/15: [=====                         ] 9/47 batches, loss: 0.0732Epoch 8/15: [======                        ] 10/47 batches, loss: 0.0737Epoch 8/15: [=======                       ] 11/47 batches, loss: 0.0728Epoch 8/15: [=======                       ] 12/47 batches, loss: 0.0797Epoch 8/15: [========                      ] 13/47 batches, loss: 0.0770Epoch 8/15: [========                      ] 14/47 batches, loss: 0.0766Epoch 8/15: [=========                     ] 15/47 batches, loss: 0.0778Epoch 8/15: [==========                    ] 16/47 batches, loss: 0.0764Epoch 8/15: [==========                    ] 17/47 batches, loss: 0.0755Epoch 8/15: [===========                   ] 18/47 batches, loss: 0.0747Epoch 8/15: [============                  ] 19/47 batches, loss: 0.0741Epoch 8/15: [============                  ] 20/47 batches, loss: 0.0757Epoch 8/15: [=============                 ] 21/47 batches, loss: 0.0753Epoch 8/15: [==============                ] 22/47 batches, loss: 0.0754Epoch 8/15: [==============                ] 23/47 batches, loss: 0.0752Epoch 8/15: [===============               ] 24/47 batches, loss: 0.0755Epoch 8/15: [===============               ] 25/47 batches, loss: 0.0756Epoch 8/15: [================              ] 26/47 batches, loss: 0.0753Epoch 8/15: [=================             ] 27/47 batches, loss: 0.0749Epoch 8/15: [=================             ] 28/47 batches, loss: 0.0753Epoch 8/15: [==================            ] 29/47 batches, loss: 0.0747Epoch 8/15: [===================           ] 30/47 batches, loss: 0.0747Epoch 8/15: [===================           ] 31/47 batches, loss: 0.0736Epoch 8/15: [====================          ] 32/47 batches, loss: 0.0732Epoch 8/15: [=====================         ] 33/47 batches, loss: 0.0764Epoch 8/15: [=====================         ] 34/47 batches, loss: 0.0767Epoch 8/15: [======================        ] 35/47 batches, loss: 0.0762Epoch 8/15: [======================        ] 36/47 batches, loss: 0.0751Epoch 8/15: [=======================       ] 37/47 batches, loss: 0.0769Epoch 8/15: [========================      ] 38/47 batches, loss: 0.0764Epoch 8/15: [========================      ] 39/47 batches, loss: 0.0761Epoch 8/15: [=========================     ] 40/47 batches, loss: 0.0755Epoch 8/15: [==========================    ] 41/47 batches, loss: 0.0752Epoch 8/15: [==========================    ] 42/47 batches, loss: 0.0756Epoch 8/15: [===========================   ] 43/47 batches, loss: 0.0757Epoch 8/15: [============================  ] 44/47 batches, loss: 0.0758Epoch 8/15: [============================  ] 45/47 batches, loss: 0.0756Epoch 8/15: [============================= ] 46/47 batches, loss: 0.0756Epoch 8/15: [==============================] 47/47 batches, loss: 0.0752
[2025-05-07 22:21:10,039][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0752
[2025-05-07 22:21:10,390][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0293, Metrics: {'mse': 0.029004020616412163, 'rmse': 0.1703056681863882, 'r2': -0.18053829669952393}
[2025-05-07 22:21:10,390][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/47 batches, loss: 0.1149Epoch 9/15: [=                             ] 2/47 batches, loss: 0.0881Epoch 9/15: [=                             ] 3/47 batches, loss: 0.0865Epoch 9/15: [==                            ] 4/47 batches, loss: 0.0901Epoch 9/15: [===                           ] 5/47 batches, loss: 0.0802Epoch 9/15: [===                           ] 6/47 batches, loss: 0.0878Epoch 9/15: [====                          ] 7/47 batches, loss: 0.0851Epoch 9/15: [=====                         ] 8/47 batches, loss: 0.0934Epoch 9/15: [=====                         ] 9/47 batches, loss: 0.0892Epoch 9/15: [======                        ] 10/47 batches, loss: 0.0887Epoch 9/15: [=======                       ] 11/47 batches, loss: 0.0838Epoch 9/15: [=======                       ] 12/47 batches, loss: 0.0813Epoch 9/15: [========                      ] 13/47 batches, loss: 0.0791Epoch 9/15: [========                      ] 14/47 batches, loss: 0.0810Epoch 9/15: [=========                     ] 15/47 batches, loss: 0.0808Epoch 9/15: [==========                    ] 16/47 batches, loss: 0.0798Epoch 9/15: [==========                    ] 17/47 batches, loss: 0.0775Epoch 9/15: [===========                   ] 18/47 batches, loss: 0.0759Epoch 9/15: [============                  ] 19/47 batches, loss: 0.0764Epoch 9/15: [============                  ] 20/47 batches, loss: 0.0761Epoch 9/15: [=============                 ] 21/47 batches, loss: 0.0756Epoch 9/15: [==============                ] 22/47 batches, loss: 0.0773Epoch 9/15: [==============                ] 23/47 batches, loss: 0.0755Epoch 9/15: [===============               ] 24/47 batches, loss: 0.0759Epoch 9/15: [===============               ] 25/47 batches, loss: 0.0773Epoch 9/15: [================              ] 26/47 batches, loss: 0.0792Epoch 9/15: [=================             ] 27/47 batches, loss: 0.0786Epoch 9/15: [=================             ] 28/47 batches, loss: 0.0773Epoch 9/15: [==================            ] 29/47 batches, loss: 0.0789Epoch 9/15: [===================           ] 30/47 batches, loss: 0.0776Epoch 9/15: [===================           ] 31/47 batches, loss: 0.0763Epoch 9/15: [====================          ] 32/47 batches, loss: 0.0773Epoch 9/15: [=====================         ] 33/47 batches, loss: 0.0772Epoch 9/15: [=====================         ] 34/47 batches, loss: 0.0764Epoch 9/15: [======================        ] 35/47 batches, loss: 0.0775Epoch 9/15: [======================        ] 36/47 batches, loss: 0.0784Epoch 9/15: [=======================       ] 37/47 batches, loss: 0.0794Epoch 9/15: [========================      ] 38/47 batches, loss: 0.0788Epoch 9/15: [========================      ] 39/47 batches, loss: 0.0792Epoch 9/15: [=========================     ] 40/47 batches, loss: 0.0789Epoch 9/15: [==========================    ] 41/47 batches, loss: 0.0783Epoch 9/15: [==========================    ] 42/47 batches, loss: 0.0782Epoch 9/15: [===========================   ] 43/47 batches, loss: 0.0773Epoch 9/15: [============================  ] 44/47 batches, loss: 0.0771Epoch 9/15: [============================  ] 45/47 batches, loss: 0.0777Epoch 9/15: [============================= ] 46/47 batches, loss: 0.0772Epoch 9/15: [==============================] 47/47 batches, loss: 0.0773
[2025-05-07 22:21:11,989][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0773
[2025-05-07 22:21:12,265][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0272, Metrics: {'mse': 0.026912687346339226, 'rmse': 0.1640508681669781, 'r2': -0.09541559219360352}
Epoch 10/15: [Epoch 10/15: [                              ] 1/47 batches, loss: 0.0562Epoch 10/15: [=                             ] 2/47 batches, loss: 0.0699Epoch 10/15: [=                             ] 3/47 batches, loss: 0.0766Epoch 10/15: [==                            ] 4/47 batches, loss: 0.0877Epoch 10/15: [===                           ] 5/47 batches, loss: 0.0916Epoch 10/15: [===                           ] 6/47 batches, loss: 0.0889Epoch 10/15: [====                          ] 7/47 batches, loss: 0.0829Epoch 10/15: [=====                         ] 8/47 batches, loss: 0.0814Epoch 10/15: [=====                         ] 9/47 batches, loss: 0.0792Epoch 10/15: [======                        ] 10/47 batches, loss: 0.0782Epoch 10/15: [=======                       ] 11/47 batches, loss: 0.0777Epoch 10/15: [=======                       ] 12/47 batches, loss: 0.0777Epoch 10/15: [========                      ] 13/47 batches, loss: 0.0775Epoch 10/15: [========                      ] 14/47 batches, loss: 0.0768Epoch 10/15: [=========                     ] 15/47 batches, loss: 0.0755Epoch 10/15: [==========                    ] 16/47 batches, loss: 0.0743Epoch 10/15: [==========                    ] 17/47 batches, loss: 0.0755Epoch 10/15: [===========                   ] 18/47 batches, loss: 0.0746Epoch 10/15: [============                  ] 19/47 batches, loss: 0.0759Epoch 10/15: [============                  ] 20/47 batches, loss: 0.0749Epoch 10/15: [=============                 ] 21/47 batches, loss: 0.0764Epoch 10/15: [==============                ] 22/47 batches, loss: 0.0761Epoch 10/15: [==============                ] 23/47 batches, loss: 0.0743Epoch 10/15: [===============               ] 24/47 batches, loss: 0.0753Epoch 10/15: [===============               ] 25/47 batches, loss: 0.0761Epoch 10/15: [================              ] 26/47 batches, loss: 0.0749Epoch 10/15: [=================             ] 27/47 batches, loss: 0.0747Epoch 10/15: [=================             ] 28/47 batches, loss: 0.0760Epoch 10/15: [==================            ] 29/47 batches, loss: 0.0766Epoch 10/15: [===================           ] 30/47 batches, loss: 0.0756Epoch 10/15: [===================           ] 31/47 batches, loss: 0.0739Epoch 10/15: [====================          ] 32/47 batches, loss: 0.0743Epoch 10/15: [=====================         ] 33/47 batches, loss: 0.0743Epoch 10/15: [=====================         ] 34/47 batches, loss: 0.0740Epoch 10/15: [======================        ] 35/47 batches, loss: 0.0741Epoch 10/15: [======================        ] 36/47 batches, loss: 0.0731Epoch 10/15: [=======================       ] 37/47 batches, loss: 0.0735Epoch 10/15: [========================      ] 38/47 batches, loss: 0.0738Epoch 10/15: [========================      ] 39/47 batches, loss: 0.0736Epoch 10/15: [=========================     ] 40/47 batches, loss: 0.0735Epoch 10/15: [==========================    ] 41/47 batches, loss: 0.0726Epoch 10/15: [==========================    ] 42/47 batches, loss: 0.0720Epoch 10/15: [===========================   ] 43/47 batches, loss: 0.0719Epoch 10/15: [============================  ] 44/47 batches, loss: 0.0720Epoch 10/15: [============================  ] 45/47 batches, loss: 0.0717Epoch 10/15: [============================= ] 46/47 batches, loss: 0.0711Epoch 10/15: [==============================] 47/47 batches, loss: 0.0706
[2025-05-07 22:21:14,207][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0706
[2025-05-07 22:21:14,504][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0270, Metrics: {'mse': 0.026744332164525986, 'rmse': 0.1635369443414117, 'r2': -0.08856308460235596}
Epoch 11/15: [Epoch 11/15: [                              ] 1/47 batches, loss: 0.0579Epoch 11/15: [=                             ] 2/47 batches, loss: 0.0632Epoch 11/15: [=                             ] 3/47 batches, loss: 0.0679Epoch 11/15: [==                            ] 4/47 batches, loss: 0.0733Epoch 11/15: [===                           ] 5/47 batches, loss: 0.0699Epoch 11/15: [===                           ] 6/47 batches, loss: 0.0651Epoch 11/15: [====                          ] 7/47 batches, loss: 0.0631Epoch 11/15: [=====                         ] 8/47 batches, loss: 0.0628Epoch 11/15: [=====                         ] 9/47 batches, loss: 0.0636Epoch 11/15: [======                        ] 10/47 batches, loss: 0.0661Epoch 11/15: [=======                       ] 11/47 batches, loss: 0.0631Epoch 11/15: [=======                       ] 12/47 batches, loss: 0.0624Epoch 11/15: [========                      ] 13/47 batches, loss: 0.0626Epoch 11/15: [========                      ] 14/47 batches, loss: 0.0600Epoch 11/15: [=========                     ] 15/47 batches, loss: 0.0594Epoch 11/15: [==========                    ] 16/47 batches, loss: 0.0609Epoch 11/15: [==========                    ] 17/47 batches, loss: 0.0594Epoch 11/15: [===========                   ] 18/47 batches, loss: 0.0615Epoch 11/15: [============                  ] 19/47 batches, loss: 0.0619Epoch 11/15: [============                  ] 20/47 batches, loss: 0.0631Epoch 11/15: [=============                 ] 21/47 batches, loss: 0.0623Epoch 11/15: [==============                ] 22/47 batches, loss: 0.0627Epoch 11/15: [==============                ] 23/47 batches, loss: 0.0627Epoch 11/15: [===============               ] 24/47 batches, loss: 0.0623Epoch 11/15: [===============               ] 25/47 batches, loss: 0.0616Epoch 11/15: [================              ] 26/47 batches, loss: 0.0619Epoch 11/15: [=================             ] 27/47 batches, loss: 0.0619Epoch 11/15: [=================             ] 28/47 batches, loss: 0.0614Epoch 11/15: [==================            ] 29/47 batches, loss: 0.0613Epoch 11/15: [===================           ] 30/47 batches, loss: 0.0619Epoch 11/15: [===================           ] 31/47 batches, loss: 0.0612Epoch 11/15: [====================          ] 32/47 batches, loss: 0.0613Epoch 11/15: [=====================         ] 33/47 batches, loss: 0.0619Epoch 11/15: [=====================         ] 34/47 batches, loss: 0.0610Epoch 11/15: [======================        ] 35/47 batches, loss: 0.0621Epoch 11/15: [======================        ] 36/47 batches, loss: 0.0634Epoch 11/15: [=======================       ] 37/47 batches, loss: 0.0651Epoch 11/15: [========================      ] 38/47 batches, loss: 0.0644Epoch 11/15: [========================      ] 39/47 batches, loss: 0.0649Epoch 11/15: [=========================     ] 40/47 batches, loss: 0.0654Epoch 11/15: [==========================    ] 41/47 batches, loss: 0.0657Epoch 11/15: [==========================    ] 42/47 batches, loss: 0.0660Epoch 11/15: [===========================   ] 43/47 batches, loss: 0.0655Epoch 11/15: [============================  ] 44/47 batches, loss: 0.0660Epoch 11/15: [============================  ] 45/47 batches, loss: 0.0654Epoch 11/15: [============================= ] 46/47 batches, loss: 0.0658Epoch 11/15: [==============================] 47/47 batches, loss: 0.0693
[2025-05-07 22:21:16,379][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0693
[2025-05-07 22:21:16,689][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0283, Metrics: {'mse': 0.02795870043337345, 'rmse': 0.16720855370875454, 'r2': -0.1379910707473755}
[2025-05-07 22:21:16,690][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/47 batches, loss: 0.0667Epoch 12/15: [=                             ] 2/47 batches, loss: 0.0539Epoch 12/15: [=                             ] 3/47 batches, loss: 0.0483Epoch 12/15: [==                            ] 4/47 batches, loss: 0.0453Epoch 12/15: [===                           ] 5/47 batches, loss: 0.0461Epoch 12/15: [===                           ] 6/47 batches, loss: 0.0538Epoch 12/15: [====                          ] 7/47 batches, loss: 0.0531Epoch 12/15: [=====                         ] 8/47 batches, loss: 0.0686Epoch 12/15: [=====                         ] 9/47 batches, loss: 0.0663Epoch 12/15: [======                        ] 10/47 batches, loss: 0.0643Epoch 12/15: [=======                       ] 11/47 batches, loss: 0.0654Epoch 12/15: [=======                       ] 12/47 batches, loss: 0.0643Epoch 12/15: [========                      ] 13/47 batches, loss: 0.0645Epoch 12/15: [========                      ] 14/47 batches, loss: 0.0640Epoch 12/15: [=========                     ] 15/47 batches, loss: 0.0660Epoch 12/15: [==========                    ] 16/47 batches, loss: 0.0676Epoch 12/15: [==========                    ] 17/47 batches, loss: 0.0660Epoch 12/15: [===========                   ] 18/47 batches, loss: 0.0644Epoch 12/15: [============                  ] 19/47 batches, loss: 0.0644Epoch 12/15: [============                  ] 20/47 batches, loss: 0.0642Epoch 12/15: [=============                 ] 21/47 batches, loss: 0.0624Epoch 12/15: [==============                ] 22/47 batches, loss: 0.0622Epoch 12/15: [==============                ] 23/47 batches, loss: 0.0617Epoch 12/15: [===============               ] 24/47 batches, loss: 0.0615Epoch 12/15: [===============               ] 25/47 batches, loss: 0.0613Epoch 12/15: [================              ] 26/47 batches, loss: 0.0609Epoch 12/15: [=================             ] 27/47 batches, loss: 0.0602Epoch 12/15: [=================             ] 28/47 batches, loss: 0.0597Epoch 12/15: [==================            ] 29/47 batches, loss: 0.0601Epoch 12/15: [===================           ] 30/47 batches, loss: 0.0596Epoch 12/15: [===================           ] 31/47 batches, loss: 0.0593Epoch 12/15: [====================          ] 32/47 batches, loss: 0.0591Epoch 12/15: [=====================         ] 33/47 batches, loss: 0.0593Epoch 12/15: [=====================         ] 34/47 batches, loss: 0.0594Epoch 12/15: [======================        ] 35/47 batches, loss: 0.0593Epoch 12/15: [======================        ] 36/47 batches, loss: 0.0593Epoch 12/15: [=======================       ] 37/47 batches, loss: 0.0595Epoch 12/15: [========================      ] 38/47 batches, loss: 0.0598Epoch 12/15: [========================      ] 39/47 batches, loss: 0.0595Epoch 12/15: [=========================     ] 40/47 batches, loss: 0.0597Epoch 12/15: [==========================    ] 41/47 batches, loss: 0.0600Epoch 12/15: [==========================    ] 42/47 batches, loss: 0.0598Epoch 12/15: [===========================   ] 43/47 batches, loss: 0.0596Epoch 12/15: [============================  ] 44/47 batches, loss: 0.0598Epoch 12/15: [============================  ] 45/47 batches, loss: 0.0602Epoch 12/15: [============================= ] 46/47 batches, loss: 0.0601Epoch 12/15: [==============================] 47/47 batches, loss: 0.0620
[2025-05-07 22:21:18,185][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0620
[2025-05-07 22:21:18,501][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0274, Metrics: {'mse': 0.0271135326474905, 'rmse': 0.16466187369118118, 'r2': -0.10359048843383789}
[2025-05-07 22:21:18,502][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/47 batches, loss: 0.0925Epoch 13/15: [=                             ] 2/47 batches, loss: 0.0664Epoch 13/15: [=                             ] 3/47 batches, loss: 0.0633Epoch 13/15: [==                            ] 4/47 batches, loss: 0.0562Epoch 13/15: [===                           ] 5/47 batches, loss: 0.0503Epoch 13/15: [===                           ] 6/47 batches, loss: 0.0552Epoch 13/15: [====                          ] 7/47 batches, loss: 0.0539Epoch 13/15: [=====                         ] 8/47 batches, loss: 0.0520Epoch 13/15: [=====                         ] 9/47 batches, loss: 0.0493Epoch 13/15: [======                        ] 10/47 batches, loss: 0.0484Epoch 13/15: [=======                       ] 11/47 batches, loss: 0.0485Epoch 13/15: [=======                       ] 12/47 batches, loss: 0.0502Epoch 13/15: [========                      ] 13/47 batches, loss: 0.0523Epoch 13/15: [========                      ] 14/47 batches, loss: 0.0532Epoch 13/15: [=========                     ] 15/47 batches, loss: 0.0542Epoch 13/15: [==========                    ] 16/47 batches, loss: 0.0550Epoch 13/15: [==========                    ] 17/47 batches, loss: 0.0568Epoch 13/15: [===========                   ] 18/47 batches, loss: 0.0592Epoch 13/15: [============                  ] 19/47 batches, loss: 0.0596Epoch 13/15: [============                  ] 20/47 batches, loss: 0.0587Epoch 13/15: [=============                 ] 21/47 batches, loss: 0.0597Epoch 13/15: [==============                ] 22/47 batches, loss: 0.0585Epoch 13/15: [==============                ] 23/47 batches, loss: 0.0590Epoch 13/15: [===============               ] 24/47 batches, loss: 0.0585Epoch 13/15: [===============               ] 25/47 batches, loss: 0.0626Epoch 13/15: [================              ] 26/47 batches, loss: 0.0624Epoch 13/15: [=================             ] 27/47 batches, loss: 0.0614Epoch 13/15: [=================             ] 28/47 batches, loss: 0.0614Epoch 13/15: [==================            ] 29/47 batches, loss: 0.0627Epoch 13/15: [===================           ] 30/47 batches, loss: 0.0630Epoch 13/15: [===================           ] 31/47 batches, loss: 0.0625Epoch 13/15: [====================          ] 32/47 batches, loss: 0.0628Epoch 13/15: [=====================         ] 33/47 batches, loss: 0.0623Epoch 13/15: [=====================         ] 34/47 batches, loss: 0.0623Epoch 13/15: [======================        ] 35/47 batches, loss: 0.0623Epoch 13/15: [======================        ] 36/47 batches, loss: 0.0615Epoch 13/15: [=======================       ] 37/47 batches, loss: 0.0611Epoch 13/15: [========================      ] 38/47 batches, loss: 0.0619Epoch 13/15: [========================      ] 39/47 batches, loss: 0.0611Epoch 13/15: [=========================     ] 40/47 batches, loss: 0.0605Epoch 13/15: [==========================    ] 41/47 batches, loss: 0.0609Epoch 13/15: [==========================    ] 42/47 batches, loss: 0.0601Epoch 13/15: [===========================   ] 43/47 batches, loss: 0.0596Epoch 13/15: [============================  ] 44/47 batches, loss: 0.0596Epoch 13/15: [============================  ] 45/47 batches, loss: 0.0594Epoch 13/15: [============================= ] 46/47 batches, loss: 0.0597Epoch 13/15: [==============================] 47/47 batches, loss: 0.0601
[2025-05-07 22:21:19,978][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0601
[2025-05-07 22:21:20,271][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0274, Metrics: {'mse': 0.027386128902435303, 'rmse': 0.16548754908583094, 'r2': -0.11468589305877686}
[2025-05-07 22:21:20,272][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/47 batches, loss: 0.0412Epoch 14/15: [=                             ] 2/47 batches, loss: 0.0457Epoch 14/15: [=                             ] 3/47 batches, loss: 0.0564Epoch 14/15: [==                            ] 4/47 batches, loss: 0.0542Epoch 14/15: [===                           ] 5/47 batches, loss: 0.0502Epoch 14/15: [===                           ] 6/47 batches, loss: 0.0513Epoch 14/15: [====                          ] 7/47 batches, loss: 0.0568Epoch 14/15: [=====                         ] 8/47 batches, loss: 0.0532Epoch 14/15: [=====                         ] 9/47 batches, loss: 0.0537Epoch 14/15: [======                        ] 10/47 batches, loss: 0.0537Epoch 14/15: [=======                       ] 11/47 batches, loss: 0.0534Epoch 14/15: [=======                       ] 12/47 batches, loss: 0.0549Epoch 14/15: [========                      ] 13/47 batches, loss: 0.0559Epoch 14/15: [========                      ] 14/47 batches, loss: 0.0570Epoch 14/15: [=========                     ] 15/47 batches, loss: 0.0582Epoch 14/15: [==========                    ] 16/47 batches, loss: 0.0568Epoch 14/15: [==========                    ] 17/47 batches, loss: 0.0558Epoch 14/15: [===========                   ] 18/47 batches, loss: 0.0574Epoch 14/15: [============                  ] 19/47 batches, loss: 0.0563Epoch 14/15: [============                  ] 20/47 batches, loss: 0.0560Epoch 14/15: [=============                 ] 21/47 batches, loss: 0.0566Epoch 14/15: [==============                ] 22/47 batches, loss: 0.0564Epoch 14/15: [==============                ] 23/47 batches, loss: 0.0556Epoch 14/15: [===============               ] 24/47 batches, loss: 0.0548Epoch 14/15: [===============               ] 25/47 batches, loss: 0.0551Epoch 14/15: [================              ] 26/47 batches, loss: 0.0554Epoch 14/15: [=================             ] 27/47 batches, loss: 0.0544Epoch 14/15: [=================             ] 28/47 batches, loss: 0.0568Epoch 14/15: [==================            ] 29/47 batches, loss: 0.0576Epoch 14/15: [===================           ] 30/47 batches, loss: 0.0587Epoch 14/15: [===================           ] 31/47 batches, loss: 0.0580Epoch 14/15: [====================          ] 32/47 batches, loss: 0.0576Epoch 14/15: [=====================         ] 33/47 batches, loss: 0.0582Epoch 14/15: [=====================         ] 34/47 batches, loss: 0.0581Epoch 14/15: [======================        ] 35/47 batches, loss: 0.0587Epoch 14/15: [======================        ] 36/47 batches, loss: 0.0585Epoch 14/15: [=======================       ] 37/47 batches, loss: 0.0583Epoch 14/15: [========================      ] 38/47 batches, loss: 0.0584Epoch 14/15: [========================      ] 39/47 batches, loss: 0.0590Epoch 14/15: [=========================     ] 40/47 batches, loss: 0.0583Epoch 14/15: [==========================    ] 41/47 batches, loss: 0.0587Epoch 14/15: [==========================    ] 42/47 batches, loss: 0.0583Epoch 14/15: [===========================   ] 43/47 batches, loss: 0.0582Epoch 14/15: [============================  ] 44/47 batches, loss: 0.0581Epoch 14/15: [============================  ] 45/47 batches, loss: 0.0580Epoch 14/15: [============================= ] 46/47 batches, loss: 0.0589Epoch 14/15: [==============================] 47/47 batches, loss: 0.0580
[2025-05-07 22:21:21,752][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0580
[2025-05-07 22:21:22,128][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0265, Metrics: {'mse': 0.026259012520313263, 'rmse': 0.16204632831481638, 'r2': -0.06880927085876465}
Epoch 15/15: [Epoch 15/15: [                              ] 1/47 batches, loss: 0.0297Epoch 15/15: [=                             ] 2/47 batches, loss: 0.0563Epoch 15/15: [=                             ] 3/47 batches, loss: 0.0493Epoch 15/15: [==                            ] 4/47 batches, loss: 0.0455Epoch 15/15: [===                           ] 5/47 batches, loss: 0.0436Epoch 15/15: [===                           ] 6/47 batches, loss: 0.0483Epoch 15/15: [====                          ] 7/47 batches, loss: 0.0505Epoch 15/15: [=====                         ] 8/47 batches, loss: 0.0501Epoch 15/15: [=====                         ] 9/47 batches, loss: 0.0563Epoch 15/15: [======                        ] 10/47 batches, loss: 0.0556Epoch 15/15: [=======                       ] 11/47 batches, loss: 0.0571Epoch 15/15: [=======                       ] 12/47 batches, loss: 0.0604Epoch 15/15: [========                      ] 13/47 batches, loss: 0.0601Epoch 15/15: [========                      ] 14/47 batches, loss: 0.0600Epoch 15/15: [=========                     ] 15/47 batches, loss: 0.0624Epoch 15/15: [==========                    ] 16/47 batches, loss: 0.0634Epoch 15/15: [==========                    ] 17/47 batches, loss: 0.0622Epoch 15/15: [===========                   ] 18/47 batches, loss: 0.0609Epoch 15/15: [============                  ] 19/47 batches, loss: 0.0627Epoch 15/15: [============                  ] 20/47 batches, loss: 0.0628Epoch 15/15: [=============                 ] 21/47 batches, loss: 0.0632Epoch 15/15: [==============                ] 22/47 batches, loss: 0.0638Epoch 15/15: [==============                ] 23/47 batches, loss: 0.0635Epoch 15/15: [===============               ] 24/47 batches, loss: 0.0635Epoch 15/15: [===============               ] 25/47 batches, loss: 0.0634Epoch 15/15: [================              ] 26/47 batches, loss: 0.0632Epoch 15/15: [=================             ] 27/47 batches, loss: 0.0625Epoch 15/15: [=================             ] 28/47 batches, loss: 0.0615Epoch 15/15: [==================            ] 29/47 batches, loss: 0.0614Epoch 15/15: [===================           ] 30/47 batches, loss: 0.0615Epoch 15/15: [===================           ] 31/47 batches, loss: 0.0614Epoch 15/15: [====================          ] 32/47 batches, loss: 0.0612Epoch 15/15: [=====================         ] 33/47 batches, loss: 0.0607Epoch 15/15: [=====================         ] 34/47 batches, loss: 0.0602Epoch 15/15: [======================        ] 35/47 batches, loss: 0.0630Epoch 15/15: [======================        ] 36/47 batches, loss: 0.0625Epoch 15/15: [=======================       ] 37/47 batches, loss: 0.0624Epoch 15/15: [========================      ] 38/47 batches, loss: 0.0620Epoch 15/15: [========================      ] 39/47 batches, loss: 0.0614Epoch 15/15: [=========================     ] 40/47 batches, loss: 0.0613Epoch 15/15: [==========================    ] 41/47 batches, loss: 0.0603Epoch 15/15: [==========================    ] 42/47 batches, loss: 0.0600Epoch 15/15: [===========================   ] 43/47 batches, loss: 0.0592Epoch 15/15: [============================  ] 44/47 batches, loss: 0.0590Epoch 15/15: [============================  ] 45/47 batches, loss: 0.0593Epoch 15/15: [============================= ] 46/47 batches, loss: 0.0587Epoch 15/15: [==============================] 47/47 batches, loss: 0.0575
[2025-05-07 22:21:24,012][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0575
[2025-05-07 22:21:24,410][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0269, Metrics: {'mse': 0.02664017304778099, 'rmse': 0.16321817621754323, 'r2': -0.08432364463806152}
[2025-05-07 22:21:24,410][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-07 22:21:24,411][src.training.lm_trainer][INFO] - Training completed in 31.72 seconds
[2025-05-07 22:21:24,411][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:21:26,750][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.0412147082388401, 'rmse': 0.20301405921472557, 'r2': -0.11763262748718262}
[2025-05-07 22:21:26,750][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.026259012520313263, 'rmse': 0.16204632831481638, 'r2': -0.06880927085876465}
[2025-05-07 22:21:26,750][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03484378382563591, 'rmse': 0.18666489714361378, 'r2': -0.004312276840209961}
[2025-05-07 22:21:28,874][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control2/layer2/ko/ko/model.pt
[2025-05-07 22:21:28,875][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▂▂▁▁▁
wandb:     best_val_mse █▅▃▂▂▁▁▁
wandb:      best_val_r2 ▁▄▆▇▇███
wandb:    best_val_rmse █▅▃▂▂▁▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▄▄▄▄▅▄▅▅▄▅▅▅
wandb:       train_loss █▄▃▃▂▂▂▁▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▃▃▂▃▂▃▁▁▂▂▂▁▁
wandb:          val_mse █▅▃▃▂▃▂▂▁▁▂▁▂▁▁
wandb:           val_r2 ▁▄▆▆▇▆▇▇██▇█▇██
wandb:         val_rmse █▅▃▃▂▃▂▂▁▁▂▁▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02645
wandb:     best_val_mse 0.02626
wandb:      best_val_r2 -0.06881
wandb:    best_val_rmse 0.16205
wandb:            epoch 15
wandb:   final_test_mse 0.03484
wandb:    final_test_r2 -0.00431
wandb:  final_test_rmse 0.18666
wandb:  final_train_mse 0.04121
wandb:   final_train_r2 -0.11763
wandb: final_train_rmse 0.20301
wandb:    final_val_mse 0.02626
wandb:     final_val_r2 -0.06881
wandb:   final_val_rmse 0.16205
wandb:    learning_rate 0.0001
wandb:       train_loss 0.05752
wandb:       train_time 31.72295
wandb:         val_loss 0.02693
wandb:          val_mse 0.02664
wandb:           val_r2 -0.08432
wandb:         val_rmse 0.16322
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_222034-buxf9bxx
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_222034-buxf9bxx/logs
Experiment probe_layer2_avg_verb_edges_control2_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control2/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_avg_verb_edges_control3_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_verb_edges_control3_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control3/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=avg_verb_edges"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:21:58,402][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control3/layer2/ko
experiment_name: probe_layer2_avg_verb_edges_control3_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:21:58,402][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:21:58,402][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 22:21:58,402][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:21:58,402][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:21:58,406][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:21:58,406][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 22:21:58,407][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:22:01,379][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:22:03,827][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:22:03,828][src.data.datasets][INFO] - Loading 'control_avg_verb_edges_seed3' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:22:03,944][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_verb_edges_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:32:16 2025).
[2025-05-07 22:22:04,056][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_verb_edges_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:32:16 2025).
[2025-05-07 22:22:04,304][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:22:04,310][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:22:04,310][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:22:04,312][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:22:04,439][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:22:04,487][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:22:04,535][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:22:04,537][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:22:04,537][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:22:04,538][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:22:04,627][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:22:04,710][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:22:04,746][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:22:04,747][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:22:04,747][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:22:04,748][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:22:04,749][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:22:04,749][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 22:22:04,749][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 22:22:04,749][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 22:22:04,749][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:22:04,749][src.data.datasets][INFO] -   Mean: 0.3982, Std: 0.1920
[2025-05-07 22:22:04,749][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:22:04,750][src.data.datasets][INFO] - Sample label: 0.4000000059604645
[2025-05-07 22:22:04,750][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:22:04,750][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 22:22:04,750][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 22:22:04,750][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 22:22:04,750][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-05-07 22:22:04,750][src.data.datasets][INFO] -   Mean: 0.3198, Std: 0.1567
[2025-05-07 22:22:04,750][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:22:04,750][src.data.datasets][INFO] - Sample label: 0.27799999713897705
[2025-05-07 22:22:04,750][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:22:04,751][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 22:22:04,751][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 22:22:04,751][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 22:22:04,751][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-05-07 22:22:04,751][src.data.datasets][INFO] -   Mean: 0.3201, Std: 0.1863
[2025-05-07 22:22:04,751][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:22:04,751][src.data.datasets][INFO] - Sample label: 0.25
[2025-05-07 22:22:04,751][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:22:04,751][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:22:04,752][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:22:04,752][__main__][INFO] - Using model type: lm_probe for submetric avg_verb_edges
[2025-05-07 22:22:04,752][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:22:12,031][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:22:12,032][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:22:12,032][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:22:12,032][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:22:12,035][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:22:12,035][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:22:12,035][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:22:12,036][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:22:12,036][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:22:12,037][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:22:12,037][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.4088Epoch 1/15: [=                             ] 2/47 batches, loss: 0.5221Epoch 1/15: [=                             ] 3/47 batches, loss: 0.4935Epoch 1/15: [==                            ] 4/47 batches, loss: 0.4784Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4609Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4119Epoch 1/15: [====                          ] 7/47 batches, loss: 0.3902Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4029Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4259Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4352Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4249Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4154Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4047Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4044Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.3900Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.3951Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.3873Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4054Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4031Epoch 1/15: [============                  ] 20/47 batches, loss: 0.3965Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4038Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4020Epoch 1/15: [==============                ] 23/47 batches, loss: 0.3903Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3798Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3781Epoch 1/15: [================              ] 26/47 batches, loss: 0.3732Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3740Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3657Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3747Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3684Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3610Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3554Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3526Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3549Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3500Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3482Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3453Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3433Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3408Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3427Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3403Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3396Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3389Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3361Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3362Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3311Epoch 1/15: [==============================] 47/47 batches, loss: 0.3303
[2025-05-07 22:22:18,488][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3303
[2025-05-07 22:22:18,822][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0403, Metrics: {'mse': 0.041802432388067245, 'rmse': 0.20445643151553644, 'r2': -0.7014665603637695}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2399Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2387Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2498Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2097Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2384Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2499Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2336Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2232Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2384Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2329Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2304Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2303Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2270Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2239Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2242Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2195Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2160Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2232Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2222Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2237Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2197Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2194Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2156Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2158Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2145Epoch 2/15: [================              ] 26/47 batches, loss: 0.2123Epoch 2/15: [=================             ] 27/47 batches, loss: 0.2088Epoch 2/15: [=================             ] 28/47 batches, loss: 0.2063Epoch 2/15: [==================            ] 29/47 batches, loss: 0.2041Epoch 2/15: [===================           ] 30/47 batches, loss: 0.2042Epoch 2/15: [===================           ] 31/47 batches, loss: 0.2049Epoch 2/15: [====================          ] 32/47 batches, loss: 0.2043Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.2043Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.2042Epoch 2/15: [======================        ] 35/47 batches, loss: 0.2016Epoch 2/15: [======================        ] 36/47 batches, loss: 0.2001Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.2005Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1966Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1953Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1955Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1942Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1909Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1886Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1865Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1872Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1862Epoch 2/15: [==============================] 47/47 batches, loss: 0.1831
[2025-05-07 22:22:20,677][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1831
[2025-05-07 22:22:20,960][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0336, Metrics: {'mse': 0.033426932990550995, 'rmse': 0.18283033936015924, 'r2': -0.36056220531463623}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.2205Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1709Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1581Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1446Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1461Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1656Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1584Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1538Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1572Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1558Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1532Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1525Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1548Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1500Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1484Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1457Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1460Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1431Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1415Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1406Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1402Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1397Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1395Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1431Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1404Epoch 3/15: [================              ] 26/47 batches, loss: 0.1407Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1405Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1387Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1371Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1356Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1377Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1361Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1358Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1367Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1368Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1359Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1352Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1345Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1334Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1335Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1322Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1318Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1322Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1336Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1351Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1358Epoch 3/15: [==============================] 47/47 batches, loss: 0.1376
[2025-05-07 22:22:22,874][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1376
[2025-05-07 22:22:23,146][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0300, Metrics: {'mse': 0.030211342498660088, 'rmse': 0.17381410327893443, 'r2': -0.22967934608459473}
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1557Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1117Epoch 4/15: [=                             ] 3/47 batches, loss: 0.0967Epoch 4/15: [==                            ] 4/47 batches, loss: 0.0896Epoch 4/15: [===                           ] 5/47 batches, loss: 0.0907Epoch 4/15: [===                           ] 6/47 batches, loss: 0.0920Epoch 4/15: [====                          ] 7/47 batches, loss: 0.0945Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1032Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1023Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1117Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1204Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1223Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1227Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1215Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1212Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1225Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1234Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1203Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1221Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1269Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1262Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1270Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1262Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1275Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1274Epoch 4/15: [================              ] 26/47 batches, loss: 0.1267Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1246Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1255Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1260Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1257Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1265Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1266Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1259Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1279Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1276Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1274Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1291Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1287Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1272Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1255Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1248Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1252Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1245Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1256Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1262Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1265Epoch 4/15: [==============================] 47/47 batches, loss: 0.1283
[2025-05-07 22:22:24,978][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1283
[2025-05-07 22:22:25,280][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0323, Metrics: {'mse': 0.032150182873010635, 'rmse': 0.1793047207214875, 'r2': -0.3085951805114746}
[2025-05-07 22:22:25,281][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.0859Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1150Epoch 5/15: [=                             ] 3/47 batches, loss: 0.1044Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0971Epoch 5/15: [===                           ] 5/47 batches, loss: 0.1170Epoch 5/15: [===                           ] 6/47 batches, loss: 0.1160Epoch 5/15: [====                          ] 7/47 batches, loss: 0.1166Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.1170Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.1243Epoch 5/15: [======                        ] 10/47 batches, loss: 0.1178Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.1185Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.1192Epoch 5/15: [========                      ] 13/47 batches, loss: 0.1145Epoch 5/15: [========                      ] 14/47 batches, loss: 0.1129Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.1129Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.1131Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.1101Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.1086Epoch 5/15: [============                  ] 19/47 batches, loss: 0.1076Epoch 5/15: [============                  ] 20/47 batches, loss: 0.1075Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.1051Epoch 5/15: [==============                ] 22/47 batches, loss: 0.1061Epoch 5/15: [==============                ] 23/47 batches, loss: 0.1062Epoch 5/15: [===============               ] 24/47 batches, loss: 0.1057Epoch 5/15: [===============               ] 25/47 batches, loss: 0.1051Epoch 5/15: [================              ] 26/47 batches, loss: 0.1039Epoch 5/15: [=================             ] 27/47 batches, loss: 0.1024Epoch 5/15: [=================             ] 28/47 batches, loss: 0.1004Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0990Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0996Epoch 5/15: [===================           ] 31/47 batches, loss: 0.1004Epoch 5/15: [====================          ] 32/47 batches, loss: 0.1000Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.1000Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0997Epoch 5/15: [======================        ] 35/47 batches, loss: 0.1010Epoch 5/15: [======================        ] 36/47 batches, loss: 0.1009Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0999Epoch 5/15: [========================      ] 38/47 batches, loss: 0.1005Epoch 5/15: [========================      ] 39/47 batches, loss: 0.1017Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.1010Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.1015Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.1011Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.1007Epoch 5/15: [============================  ] 44/47 batches, loss: 0.1003Epoch 5/15: [============================  ] 45/47 batches, loss: 0.1012Epoch 5/15: [============================= ] 46/47 batches, loss: 0.1007Epoch 5/15: [==============================] 47/47 batches, loss: 0.0993
[2025-05-07 22:22:26,789][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0993
[2025-05-07 22:22:27,105][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0296, Metrics: {'mse': 0.029955247417092323, 'rmse': 0.17307584296224682, 'r2': -0.21925568580627441}
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.1473Epoch 6/15: [=                             ] 2/47 batches, loss: 0.1436Epoch 6/15: [=                             ] 3/47 batches, loss: 0.1499Epoch 6/15: [==                            ] 4/47 batches, loss: 0.1388Epoch 6/15: [===                           ] 5/47 batches, loss: 0.1307Epoch 6/15: [===                           ] 6/47 batches, loss: 0.1180Epoch 6/15: [====                          ] 7/47 batches, loss: 0.1126Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.1206Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.1191Epoch 6/15: [======                        ] 10/47 batches, loss: 0.1188Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.1174Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.1213Epoch 6/15: [========                      ] 13/47 batches, loss: 0.1173Epoch 6/15: [========                      ] 14/47 batches, loss: 0.1156Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.1104Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.1109Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.1081Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.1122Epoch 6/15: [============                  ] 19/47 batches, loss: 0.1091Epoch 6/15: [============                  ] 20/47 batches, loss: 0.1093Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.1079Epoch 6/15: [==============                ] 22/47 batches, loss: 0.1051Epoch 6/15: [==============                ] 23/47 batches, loss: 0.1037Epoch 6/15: [===============               ] 24/47 batches, loss: 0.1018Epoch 6/15: [===============               ] 25/47 batches, loss: 0.1017Epoch 6/15: [================              ] 26/47 batches, loss: 0.1018Epoch 6/15: [=================             ] 27/47 batches, loss: 0.1002Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0992Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0996Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0985Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0984Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0972Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0979Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0989Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0986Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0980Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0972Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0971Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0974Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0968Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0965Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0960Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0949Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0958Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0956Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0961Epoch 6/15: [==============================] 47/47 batches, loss: 0.0959
[2025-05-07 22:22:28,995][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0959
[2025-05-07 22:22:29,257][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0331, Metrics: {'mse': 0.033321186900138855, 'rmse': 0.1825409184269074, 'r2': -0.3562581539154053}
[2025-05-07 22:22:29,258][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/47 batches, loss: 0.0917Epoch 7/15: [=                             ] 2/47 batches, loss: 0.0819Epoch 7/15: [=                             ] 3/47 batches, loss: 0.0895Epoch 7/15: [==                            ] 4/47 batches, loss: 0.0889Epoch 7/15: [===                           ] 5/47 batches, loss: 0.0995Epoch 7/15: [===                           ] 6/47 batches, loss: 0.1015Epoch 7/15: [====                          ] 7/47 batches, loss: 0.0956Epoch 7/15: [=====                         ] 8/47 batches, loss: 0.0955Epoch 7/15: [=====                         ] 9/47 batches, loss: 0.0923Epoch 7/15: [======                        ] 10/47 batches, loss: 0.0895Epoch 7/15: [=======                       ] 11/47 batches, loss: 0.0866Epoch 7/15: [=======                       ] 12/47 batches, loss: 0.0846Epoch 7/15: [========                      ] 13/47 batches, loss: 0.0815Epoch 7/15: [========                      ] 14/47 batches, loss: 0.0788Epoch 7/15: [=========                     ] 15/47 batches, loss: 0.0810Epoch 7/15: [==========                    ] 16/47 batches, loss: 0.0812Epoch 7/15: [==========                    ] 17/47 batches, loss: 0.0797Epoch 7/15: [===========                   ] 18/47 batches, loss: 0.0798Epoch 7/15: [============                  ] 19/47 batches, loss: 0.0833Epoch 7/15: [============                  ] 20/47 batches, loss: 0.0816Epoch 7/15: [=============                 ] 21/47 batches, loss: 0.0817Epoch 7/15: [==============                ] 22/47 batches, loss: 0.0833Epoch 7/15: [==============                ] 23/47 batches, loss: 0.0845Epoch 7/15: [===============               ] 24/47 batches, loss: 0.0829Epoch 7/15: [===============               ] 25/47 batches, loss: 0.0842Epoch 7/15: [================              ] 26/47 batches, loss: 0.0840Epoch 7/15: [=================             ] 27/47 batches, loss: 0.0841Epoch 7/15: [=================             ] 28/47 batches, loss: 0.0829Epoch 7/15: [==================            ] 29/47 batches, loss: 0.0823Epoch 7/15: [===================           ] 30/47 batches, loss: 0.0840Epoch 7/15: [===================           ] 31/47 batches, loss: 0.0844Epoch 7/15: [====================          ] 32/47 batches, loss: 0.0836Epoch 7/15: [=====================         ] 33/47 batches, loss: 0.0823Epoch 7/15: [=====================         ] 34/47 batches, loss: 0.0831Epoch 7/15: [======================        ] 35/47 batches, loss: 0.0828Epoch 7/15: [======================        ] 36/47 batches, loss: 0.0833Epoch 7/15: [=======================       ] 37/47 batches, loss: 0.0822Epoch 7/15: [========================      ] 38/47 batches, loss: 0.0832Epoch 7/15: [========================      ] 39/47 batches, loss: 0.0839Epoch 7/15: [=========================     ] 40/47 batches, loss: 0.0841Epoch 7/15: [==========================    ] 41/47 batches, loss: 0.0828Epoch 7/15: [==========================    ] 42/47 batches, loss: 0.0854Epoch 7/15: [===========================   ] 43/47 batches, loss: 0.0844Epoch 7/15: [============================  ] 44/47 batches, loss: 0.0835Epoch 7/15: [============================  ] 45/47 batches, loss: 0.0832Epoch 7/15: [============================= ] 46/47 batches, loss: 0.0826Epoch 7/15: [==============================] 47/47 batches, loss: 0.0836
[2025-05-07 22:22:30,798][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0836
[2025-05-07 22:22:31,091][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0275, Metrics: {'mse': 0.0272621251642704, 'rmse': 0.16511246217130432, 'r2': -0.10963857173919678}
Epoch 8/15: [Epoch 8/15: [                              ] 1/47 batches, loss: 0.0732Epoch 8/15: [=                             ] 2/47 batches, loss: 0.0684Epoch 8/15: [=                             ] 3/47 batches, loss: 0.0751Epoch 8/15: [==                            ] 4/47 batches, loss: 0.0847Epoch 8/15: [===                           ] 5/47 batches, loss: 0.0889Epoch 8/15: [===                           ] 6/47 batches, loss: 0.0823Epoch 8/15: [====                          ] 7/47 batches, loss: 0.0798Epoch 8/15: [=====                         ] 8/47 batches, loss: 0.0793Epoch 8/15: [=====                         ] 9/47 batches, loss: 0.0767Epoch 8/15: [======                        ] 10/47 batches, loss: 0.0744Epoch 8/15: [=======                       ] 11/47 batches, loss: 0.0745Epoch 8/15: [=======                       ] 12/47 batches, loss: 0.0812Epoch 8/15: [========                      ] 13/47 batches, loss: 0.0786Epoch 8/15: [========                      ] 14/47 batches, loss: 0.0774Epoch 8/15: [=========                     ] 15/47 batches, loss: 0.0781Epoch 8/15: [==========                    ] 16/47 batches, loss: 0.0778Epoch 8/15: [==========                    ] 17/47 batches, loss: 0.0770Epoch 8/15: [===========                   ] 18/47 batches, loss: 0.0752Epoch 8/15: [============                  ] 19/47 batches, loss: 0.0760Epoch 8/15: [============                  ] 20/47 batches, loss: 0.0735Epoch 8/15: [=============                 ] 21/47 batches, loss: 0.0758Epoch 8/15: [==============                ] 22/47 batches, loss: 0.0739Epoch 8/15: [==============                ] 23/47 batches, loss: 0.0736Epoch 8/15: [===============               ] 24/47 batches, loss: 0.0727Epoch 8/15: [===============               ] 25/47 batches, loss: 0.0724Epoch 8/15: [================              ] 26/47 batches, loss: 0.0716Epoch 8/15: [=================             ] 27/47 batches, loss: 0.0710Epoch 8/15: [=================             ] 28/47 batches, loss: 0.0728Epoch 8/15: [==================            ] 29/47 batches, loss: 0.0726Epoch 8/15: [===================           ] 30/47 batches, loss: 0.0719Epoch 8/15: [===================           ] 31/47 batches, loss: 0.0716Epoch 8/15: [====================          ] 32/47 batches, loss: 0.0729Epoch 8/15: [=====================         ] 33/47 batches, loss: 0.0731Epoch 8/15: [=====================         ] 34/47 batches, loss: 0.0727Epoch 8/15: [======================        ] 35/47 batches, loss: 0.0732Epoch 8/15: [======================        ] 36/47 batches, loss: 0.0726Epoch 8/15: [=======================       ] 37/47 batches, loss: 0.0722Epoch 8/15: [========================      ] 38/47 batches, loss: 0.0727Epoch 8/15: [========================      ] 39/47 batches, loss: 0.0729Epoch 8/15: [=========================     ] 40/47 batches, loss: 0.0725Epoch 8/15: [==========================    ] 41/47 batches, loss: 0.0725Epoch 8/15: [==========================    ] 42/47 batches, loss: 0.0718Epoch 8/15: [===========================   ] 43/47 batches, loss: 0.0716Epoch 8/15: [============================  ] 44/47 batches, loss: 0.0721Epoch 8/15: [============================  ] 45/47 batches, loss: 0.0721Epoch 8/15: [============================= ] 46/47 batches, loss: 0.0723Epoch 8/15: [==============================] 47/47 batches, loss: 0.0730
[2025-05-07 22:22:32,929][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0730
[2025-05-07 22:22:33,268][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0296, Metrics: {'mse': 0.029125072062015533, 'rmse': 0.17066069278546694, 'r2': -0.18546533584594727}
[2025-05-07 22:22:33,269][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/47 batches, loss: 0.0771Epoch 9/15: [=                             ] 2/47 batches, loss: 0.0667Epoch 9/15: [=                             ] 3/47 batches, loss: 0.0710Epoch 9/15: [==                            ] 4/47 batches, loss: 0.0647Epoch 9/15: [===                           ] 5/47 batches, loss: 0.0636Epoch 9/15: [===                           ] 6/47 batches, loss: 0.0642Epoch 9/15: [====                          ] 7/47 batches, loss: 0.0693Epoch 9/15: [=====                         ] 8/47 batches, loss: 0.0771Epoch 9/15: [=====                         ] 9/47 batches, loss: 0.0740Epoch 9/15: [======                        ] 10/47 batches, loss: 0.0727Epoch 9/15: [=======                       ] 11/47 batches, loss: 0.0718Epoch 9/15: [=======                       ] 12/47 batches, loss: 0.0723Epoch 9/15: [========                      ] 13/47 batches, loss: 0.0697Epoch 9/15: [========                      ] 14/47 batches, loss: 0.0721Epoch 9/15: [=========                     ] 15/47 batches, loss: 0.0708Epoch 9/15: [==========                    ] 16/47 batches, loss: 0.0706Epoch 9/15: [==========                    ] 17/47 batches, loss: 0.0695Epoch 9/15: [===========                   ] 18/47 batches, loss: 0.0686Epoch 9/15: [============                  ] 19/47 batches, loss: 0.0688Epoch 9/15: [============                  ] 20/47 batches, loss: 0.0719Epoch 9/15: [=============                 ] 21/47 batches, loss: 0.0716Epoch 9/15: [==============                ] 22/47 batches, loss: 0.0708Epoch 9/15: [==============                ] 23/47 batches, loss: 0.0710Epoch 9/15: [===============               ] 24/47 batches, loss: 0.0700Epoch 9/15: [===============               ] 25/47 batches, loss: 0.0699Epoch 9/15: [================              ] 26/47 batches, loss: 0.0714Epoch 9/15: [=================             ] 27/47 batches, loss: 0.0731Epoch 9/15: [=================             ] 28/47 batches, loss: 0.0735Epoch 9/15: [==================            ] 29/47 batches, loss: 0.0731Epoch 9/15: [===================           ] 30/47 batches, loss: 0.0727Epoch 9/15: [===================           ] 31/47 batches, loss: 0.0724Epoch 9/15: [====================          ] 32/47 batches, loss: 0.0723Epoch 9/15: [=====================         ] 33/47 batches, loss: 0.0712Epoch 9/15: [=====================         ] 34/47 batches, loss: 0.0712Epoch 9/15: [======================        ] 35/47 batches, loss: 0.0706Epoch 9/15: [======================        ] 36/47 batches, loss: 0.0695Epoch 9/15: [=======================       ] 37/47 batches, loss: 0.0717Epoch 9/15: [========================      ] 38/47 batches, loss: 0.0721Epoch 9/15: [========================      ] 39/47 batches, loss: 0.0714Epoch 9/15: [=========================     ] 40/47 batches, loss: 0.0729Epoch 9/15: [==========================    ] 41/47 batches, loss: 0.0724Epoch 9/15: [==========================    ] 42/47 batches, loss: 0.0728Epoch 9/15: [===========================   ] 43/47 batches, loss: 0.0721Epoch 9/15: [============================  ] 44/47 batches, loss: 0.0723Epoch 9/15: [============================  ] 45/47 batches, loss: 0.0728Epoch 9/15: [============================= ] 46/47 batches, loss: 0.0723Epoch 9/15: [==============================] 47/47 batches, loss: 0.0719
[2025-05-07 22:22:34,810][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0719
[2025-05-07 22:22:35,141][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0269, Metrics: {'mse': 0.0267025176435709, 'rmse': 0.1634090500663011, 'r2': -0.08686113357543945}
Epoch 10/15: [Epoch 10/15: [                              ] 1/47 batches, loss: 0.0287Epoch 10/15: [=                             ] 2/47 batches, loss: 0.0494Epoch 10/15: [=                             ] 3/47 batches, loss: 0.0559Epoch 10/15: [==                            ] 4/47 batches, loss: 0.0555Epoch 10/15: [===                           ] 5/47 batches, loss: 0.0558Epoch 10/15: [===                           ] 6/47 batches, loss: 0.0546Epoch 10/15: [====                          ] 7/47 batches, loss: 0.0599Epoch 10/15: [=====                         ] 8/47 batches, loss: 0.0587Epoch 10/15: [=====                         ] 9/47 batches, loss: 0.0667Epoch 10/15: [======                        ] 10/47 batches, loss: 0.0651Epoch 10/15: [=======                       ] 11/47 batches, loss: 0.0664Epoch 10/15: [=======                       ] 12/47 batches, loss: 0.0695Epoch 10/15: [========                      ] 13/47 batches, loss: 0.0692Epoch 10/15: [========                      ] 14/47 batches, loss: 0.0684Epoch 10/15: [=========                     ] 15/47 batches, loss: 0.0680Epoch 10/15: [==========                    ] 16/47 batches, loss: 0.0670Epoch 10/15: [==========                    ] 17/47 batches, loss: 0.0657Epoch 10/15: [===========                   ] 18/47 batches, loss: 0.0660Epoch 10/15: [============                  ] 19/47 batches, loss: 0.0654Epoch 10/15: [============                  ] 20/47 batches, loss: 0.0678Epoch 10/15: [=============                 ] 21/47 batches, loss: 0.0682Epoch 10/15: [==============                ] 22/47 batches, loss: 0.0668Epoch 10/15: [==============                ] 23/47 batches, loss: 0.0659Epoch 10/15: [===============               ] 24/47 batches, loss: 0.0672Epoch 10/15: [===============               ] 25/47 batches, loss: 0.0671Epoch 10/15: [================              ] 26/47 batches, loss: 0.0662Epoch 10/15: [=================             ] 27/47 batches, loss: 0.0651Epoch 10/15: [=================             ] 28/47 batches, loss: 0.0655Epoch 10/15: [==================            ] 29/47 batches, loss: 0.0666Epoch 10/15: [===================           ] 30/47 batches, loss: 0.0674Epoch 10/15: [===================           ] 31/47 batches, loss: 0.0669Epoch 10/15: [====================          ] 32/47 batches, loss: 0.0672Epoch 10/15: [=====================         ] 33/47 batches, loss: 0.0674Epoch 10/15: [=====================         ] 34/47 batches, loss: 0.0676Epoch 10/15: [======================        ] 35/47 batches, loss: 0.0688Epoch 10/15: [======================        ] 36/47 batches, loss: 0.0678Epoch 10/15: [=======================       ] 37/47 batches, loss: 0.0679Epoch 10/15: [========================      ] 38/47 batches, loss: 0.0701Epoch 10/15: [========================      ] 39/47 batches, loss: 0.0708Epoch 10/15: [=========================     ] 40/47 batches, loss: 0.0708Epoch 10/15: [==========================    ] 41/47 batches, loss: 0.0723Epoch 10/15: [==========================    ] 42/47 batches, loss: 0.0725Epoch 10/15: [===========================   ] 43/47 batches, loss: 0.0725Epoch 10/15: [============================  ] 44/47 batches, loss: 0.0730Epoch 10/15: [============================  ] 45/47 batches, loss: 0.0735Epoch 10/15: [============================= ] 46/47 batches, loss: 0.0734Epoch 10/15: [==============================] 47/47 batches, loss: 0.0732
[2025-05-07 22:22:37,132][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0732
[2025-05-07 22:22:37,464][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0272, Metrics: {'mse': 0.026952551677823067, 'rmse': 0.16417232311757993, 'r2': -0.0970381498336792}
[2025-05-07 22:22:37,464][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/47 batches, loss: 0.0368Epoch 11/15: [=                             ] 2/47 batches, loss: 0.0574Epoch 11/15: [=                             ] 3/47 batches, loss: 0.0671Epoch 11/15: [==                            ] 4/47 batches, loss: 0.0630Epoch 11/15: [===                           ] 5/47 batches, loss: 0.0640Epoch 11/15: [===                           ] 6/47 batches, loss: 0.0640Epoch 11/15: [====                          ] 7/47 batches, loss: 0.0611Epoch 11/15: [=====                         ] 8/47 batches, loss: 0.0608Epoch 11/15: [=====                         ] 9/47 batches, loss: 0.0621Epoch 11/15: [======                        ] 10/47 batches, loss: 0.0620Epoch 11/15: [=======                       ] 11/47 batches, loss: 0.0595Epoch 11/15: [=======                       ] 12/47 batches, loss: 0.0578Epoch 11/15: [========                      ] 13/47 batches, loss: 0.0614Epoch 11/15: [========                      ] 14/47 batches, loss: 0.0619Epoch 11/15: [=========                     ] 15/47 batches, loss: 0.0616Epoch 11/15: [==========                    ] 16/47 batches, loss: 0.0643Epoch 11/15: [==========                    ] 17/47 batches, loss: 0.0656Epoch 11/15: [===========                   ] 18/47 batches, loss: 0.0633Epoch 11/15: [============                  ] 19/47 batches, loss: 0.0629Epoch 11/15: [============                  ] 20/47 batches, loss: 0.0634Epoch 11/15: [=============                 ] 21/47 batches, loss: 0.0638Epoch 11/15: [==============                ] 22/47 batches, loss: 0.0639Epoch 11/15: [==============                ] 23/47 batches, loss: 0.0646Epoch 11/15: [===============               ] 24/47 batches, loss: 0.0653Epoch 11/15: [===============               ] 25/47 batches, loss: 0.0653Epoch 11/15: [================              ] 26/47 batches, loss: 0.0658Epoch 11/15: [=================             ] 27/47 batches, loss: 0.0676Epoch 11/15: [=================             ] 28/47 batches, loss: 0.0691Epoch 11/15: [==================            ] 29/47 batches, loss: 0.0678Epoch 11/15: [===================           ] 30/47 batches, loss: 0.0678Epoch 11/15: [===================           ] 31/47 batches, loss: 0.0680Epoch 11/15: [====================          ] 32/47 batches, loss: 0.0673Epoch 11/15: [=====================         ] 33/47 batches, loss: 0.0671Epoch 11/15: [=====================         ] 34/47 batches, loss: 0.0669Epoch 11/15: [======================        ] 35/47 batches, loss: 0.0673Epoch 11/15: [======================        ] 36/47 batches, loss: 0.0671Epoch 11/15: [=======================       ] 37/47 batches, loss: 0.0672Epoch 11/15: [========================      ] 38/47 batches, loss: 0.0669Epoch 11/15: [========================      ] 39/47 batches, loss: 0.0668Epoch 11/15: [=========================     ] 40/47 batches, loss: 0.0671Epoch 11/15: [==========================    ] 41/47 batches, loss: 0.0676Epoch 11/15: [==========================    ] 42/47 batches, loss: 0.0678Epoch 11/15: [===========================   ] 43/47 batches, loss: 0.0676Epoch 11/15: [============================  ] 44/47 batches, loss: 0.0678Epoch 11/15: [============================  ] 45/47 batches, loss: 0.0682Epoch 11/15: [============================= ] 46/47 batches, loss: 0.0674Epoch 11/15: [==============================] 47/47 batches, loss: 0.0690
[2025-05-07 22:22:39,002][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0690
[2025-05-07 22:22:39,391][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0277, Metrics: {'mse': 0.027346568182110786, 'rmse': 0.16536797810371506, 'r2': -0.1130756139755249}
[2025-05-07 22:22:39,391][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/47 batches, loss: 0.0219Epoch 12/15: [=                             ] 2/47 batches, loss: 0.0257Epoch 12/15: [=                             ] 3/47 batches, loss: 0.0294Epoch 12/15: [==                            ] 4/47 batches, loss: 0.0335Epoch 12/15: [===                           ] 5/47 batches, loss: 0.0326Epoch 12/15: [===                           ] 6/47 batches, loss: 0.0412Epoch 12/15: [====                          ] 7/47 batches, loss: 0.0394Epoch 12/15: [=====                         ] 8/47 batches, loss: 0.0467Epoch 12/15: [=====                         ] 9/47 batches, loss: 0.0470Epoch 12/15: [======                        ] 10/47 batches, loss: 0.0505Epoch 12/15: [=======                       ] 11/47 batches, loss: 0.0501Epoch 12/15: [=======                       ] 12/47 batches, loss: 0.0482Epoch 12/15: [========                      ] 13/47 batches, loss: 0.0517Epoch 12/15: [========                      ] 14/47 batches, loss: 0.0502Epoch 12/15: [=========                     ] 15/47 batches, loss: 0.0525Epoch 12/15: [==========                    ] 16/47 batches, loss: 0.0538Epoch 12/15: [==========                    ] 17/47 batches, loss: 0.0532Epoch 12/15: [===========                   ] 18/47 batches, loss: 0.0522Epoch 12/15: [============                  ] 19/47 batches, loss: 0.0527Epoch 12/15: [============                  ] 20/47 batches, loss: 0.0546Epoch 12/15: [=============                 ] 21/47 batches, loss: 0.0565Epoch 12/15: [==============                ] 22/47 batches, loss: 0.0563Epoch 12/15: [==============                ] 23/47 batches, loss: 0.0559Epoch 12/15: [===============               ] 24/47 batches, loss: 0.0557Epoch 12/15: [===============               ] 25/47 batches, loss: 0.0553Epoch 12/15: [================              ] 26/47 batches, loss: 0.0563Epoch 12/15: [=================             ] 27/47 batches, loss: 0.0577Epoch 12/15: [=================             ] 28/47 batches, loss: 0.0584Epoch 12/15: [==================            ] 29/47 batches, loss: 0.0588Epoch 12/15: [===================           ] 30/47 batches, loss: 0.0594Epoch 12/15: [===================           ] 31/47 batches, loss: 0.0595Epoch 12/15: [====================          ] 32/47 batches, loss: 0.0603Epoch 12/15: [=====================         ] 33/47 batches, loss: 0.0599Epoch 12/15: [=====================         ] 34/47 batches, loss: 0.0599Epoch 12/15: [======================        ] 35/47 batches, loss: 0.0590Epoch 12/15: [======================        ] 36/47 batches, loss: 0.0583Epoch 12/15: [=======================       ] 37/47 batches, loss: 0.0588Epoch 12/15: [========================      ] 38/47 batches, loss: 0.0591Epoch 12/15: [========================      ] 39/47 batches, loss: 0.0590Epoch 12/15: [=========================     ] 40/47 batches, loss: 0.0588Epoch 12/15: [==========================    ] 41/47 batches, loss: 0.0586Epoch 12/15: [==========================    ] 42/47 batches, loss: 0.0588Epoch 12/15: [===========================   ] 43/47 batches, loss: 0.0594Epoch 12/15: [============================  ] 44/47 batches, loss: 0.0589Epoch 12/15: [============================  ] 45/47 batches, loss: 0.0590Epoch 12/15: [============================= ] 46/47 batches, loss: 0.0591Epoch 12/15: [==============================] 47/47 batches, loss: 0.0585
[2025-05-07 22:22:40,885][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0585
[2025-05-07 22:22:41,232][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0265, Metrics: {'mse': 0.026186177507042885, 'rmse': 0.16182143710597457, 'r2': -0.06584477424621582}
Epoch 13/15: [Epoch 13/15: [                              ] 1/47 batches, loss: 0.0549Epoch 13/15: [=                             ] 2/47 batches, loss: 0.0512Epoch 13/15: [=                             ] 3/47 batches, loss: 0.0548Epoch 13/15: [==                            ] 4/47 batches, loss: 0.0602Epoch 13/15: [===                           ] 5/47 batches, loss: 0.0659Epoch 13/15: [===                           ] 6/47 batches, loss: 0.0698Epoch 13/15: [====                          ] 7/47 batches, loss: 0.0691Epoch 13/15: [=====                         ] 8/47 batches, loss: 0.0667Epoch 13/15: [=====                         ] 9/47 batches, loss: 0.0649Epoch 13/15: [======                        ] 10/47 batches, loss: 0.0656Epoch 13/15: [=======                       ] 11/47 batches, loss: 0.0673Epoch 13/15: [=======                       ] 12/47 batches, loss: 0.0664Epoch 13/15: [========                      ] 13/47 batches, loss: 0.0676Epoch 13/15: [========                      ] 14/47 batches, loss: 0.0650Epoch 13/15: [=========                     ] 15/47 batches, loss: 0.0641Epoch 13/15: [==========                    ] 16/47 batches, loss: 0.0652Epoch 13/15: [==========                    ] 17/47 batches, loss: 0.0639Epoch 13/15: [===========                   ] 18/47 batches, loss: 0.0637Epoch 13/15: [============                  ] 19/47 batches, loss: 0.0617Epoch 13/15: [============                  ] 20/47 batches, loss: 0.0608Epoch 13/15: [=============                 ] 21/47 batches, loss: 0.0616Epoch 13/15: [==============                ] 22/47 batches, loss: 0.0627Epoch 13/15: [==============                ] 23/47 batches, loss: 0.0625Epoch 13/15: [===============               ] 24/47 batches, loss: 0.0612Epoch 13/15: [===============               ] 25/47 batches, loss: 0.0608Epoch 13/15: [================              ] 26/47 batches, loss: 0.0595Epoch 13/15: [=================             ] 27/47 batches, loss: 0.0594Epoch 13/15: [=================             ] 28/47 batches, loss: 0.0590Epoch 13/15: [==================            ] 29/47 batches, loss: 0.0598Epoch 13/15: [===================           ] 30/47 batches, loss: 0.0593Epoch 13/15: [===================           ] 31/47 batches, loss: 0.0605Epoch 13/15: [====================          ] 32/47 batches, loss: 0.0599Epoch 13/15: [=====================         ] 33/47 batches, loss: 0.0602Epoch 13/15: [=====================         ] 34/47 batches, loss: 0.0608Epoch 13/15: [======================        ] 35/47 batches, loss: 0.0602Epoch 13/15: [======================        ] 36/47 batches, loss: 0.0604Epoch 13/15: [=======================       ] 37/47 batches, loss: 0.0598Epoch 13/15: [========================      ] 38/47 batches, loss: 0.0593Epoch 13/15: [========================      ] 39/47 batches, loss: 0.0587Epoch 13/15: [=========================     ] 40/47 batches, loss: 0.0586Epoch 13/15: [==========================    ] 41/47 batches, loss: 0.0587Epoch 13/15: [==========================    ] 42/47 batches, loss: 0.0590Epoch 13/15: [===========================   ] 43/47 batches, loss: 0.0587Epoch 13/15: [============================  ] 44/47 batches, loss: 0.0587Epoch 13/15: [============================  ] 45/47 batches, loss: 0.0596Epoch 13/15: [============================= ] 46/47 batches, loss: 0.0591Epoch 13/15: [==============================] 47/47 batches, loss: 0.0587
[2025-05-07 22:22:43,180][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0587
[2025-05-07 22:22:43,497][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0262, Metrics: {'mse': 0.02596648596227169, 'rmse': 0.16114119883590194, 'r2': -0.05690276622772217}
Epoch 14/15: [Epoch 14/15: [                              ] 1/47 batches, loss: 0.0483Epoch 14/15: [=                             ] 2/47 batches, loss: 0.0560Epoch 14/15: [=                             ] 3/47 batches, loss: 0.0541Epoch 14/15: [==                            ] 4/47 batches, loss: 0.0557Epoch 14/15: [===                           ] 5/47 batches, loss: 0.0570Epoch 14/15: [===                           ] 6/47 batches, loss: 0.0547Epoch 14/15: [====                          ] 7/47 batches, loss: 0.0554Epoch 14/15: [=====                         ] 8/47 batches, loss: 0.0558Epoch 14/15: [=====                         ] 9/47 batches, loss: 0.0530Epoch 14/15: [======                        ] 10/47 batches, loss: 0.0536Epoch 14/15: [=======                       ] 11/47 batches, loss: 0.0525Epoch 14/15: [=======                       ] 12/47 batches, loss: 0.0534Epoch 14/15: [========                      ] 13/47 batches, loss: 0.0574Epoch 14/15: [========                      ] 14/47 batches, loss: 0.0571Epoch 14/15: [=========                     ] 15/47 batches, loss: 0.0597Epoch 14/15: [==========                    ] 16/47 batches, loss: 0.0586Epoch 14/15: [==========                    ] 17/47 batches, loss: 0.0581Epoch 14/15: [===========                   ] 18/47 batches, loss: 0.0580Epoch 14/15: [============                  ] 19/47 batches, loss: 0.0578Epoch 14/15: [============                  ] 20/47 batches, loss: 0.0577Epoch 14/15: [=============                 ] 21/47 batches, loss: 0.0565Epoch 14/15: [==============                ] 22/47 batches, loss: 0.0559Epoch 14/15: [==============                ] 23/47 batches, loss: 0.0564Epoch 14/15: [===============               ] 24/47 batches, loss: 0.0561Epoch 14/15: [===============               ] 25/47 batches, loss: 0.0562Epoch 14/15: [================              ] 26/47 batches, loss: 0.0562Epoch 14/15: [=================             ] 27/47 batches, loss: 0.0571Epoch 14/15: [=================             ] 28/47 batches, loss: 0.0563Epoch 14/15: [==================            ] 29/47 batches, loss: 0.0572Epoch 14/15: [===================           ] 30/47 batches, loss: 0.0567Epoch 14/15: [===================           ] 31/47 batches, loss: 0.0563Epoch 14/15: [====================          ] 32/47 batches, loss: 0.0554Epoch 14/15: [=====================         ] 33/47 batches, loss: 0.0545Epoch 14/15: [=====================         ] 34/47 batches, loss: 0.0545Epoch 14/15: [======================        ] 35/47 batches, loss: 0.0559Epoch 14/15: [======================        ] 36/47 batches, loss: 0.0558Epoch 14/15: [=======================       ] 37/47 batches, loss: 0.0552Epoch 14/15: [========================      ] 38/47 batches, loss: 0.0552Epoch 14/15: [========================      ] 39/47 batches, loss: 0.0561Epoch 14/15: [=========================     ] 40/47 batches, loss: 0.0563Epoch 14/15: [==========================    ] 41/47 batches, loss: 0.0563Epoch 14/15: [==========================    ] 42/47 batches, loss: 0.0560Epoch 14/15: [===========================   ] 43/47 batches, loss: 0.0565Epoch 14/15: [============================  ] 44/47 batches, loss: 0.0568Epoch 14/15: [============================  ] 45/47 batches, loss: 0.0567Epoch 14/15: [============================= ] 46/47 batches, loss: 0.0569Epoch 14/15: [==============================] 47/47 batches, loss: 0.0563
[2025-05-07 22:22:45,435][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0563
[2025-05-07 22:22:45,755][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0291, Metrics: {'mse': 0.028717705979943275, 'rmse': 0.16946299295109618, 'r2': -0.1688845157623291}
[2025-05-07 22:22:45,755][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/47 batches, loss: 0.0290Epoch 15/15: [=                             ] 2/47 batches, loss: 0.0379Epoch 15/15: [=                             ] 3/47 batches, loss: 0.0461Epoch 15/15: [==                            ] 4/47 batches, loss: 0.0548Epoch 15/15: [===                           ] 5/47 batches, loss: 0.0542Epoch 15/15: [===                           ] 6/47 batches, loss: 0.0571Epoch 15/15: [====                          ] 7/47 batches, loss: 0.0539Epoch 15/15: [=====                         ] 8/47 batches, loss: 0.0540Epoch 15/15: [=====                         ] 9/47 batches, loss: 0.0540Epoch 15/15: [======                        ] 10/47 batches, loss: 0.0540Epoch 15/15: [=======                       ] 11/47 batches, loss: 0.0545Epoch 15/15: [=======                       ] 12/47 batches, loss: 0.0558Epoch 15/15: [========                      ] 13/47 batches, loss: 0.0569Epoch 15/15: [========                      ] 14/47 batches, loss: 0.0558Epoch 15/15: [=========                     ] 15/47 batches, loss: 0.0559Epoch 15/15: [==========                    ] 16/47 batches, loss: 0.0579Epoch 15/15: [==========                    ] 17/47 batches, loss: 0.0591Epoch 15/15: [===========                   ] 18/47 batches, loss: 0.0605Epoch 15/15: [============                  ] 19/47 batches, loss: 0.0590Epoch 15/15: [============                  ] 20/47 batches, loss: 0.0584Epoch 15/15: [=============                 ] 21/47 batches, loss: 0.0590Epoch 15/15: [==============                ] 22/47 batches, loss: 0.0636Epoch 15/15: [==============                ] 23/47 batches, loss: 0.0621Epoch 15/15: [===============               ] 24/47 batches, loss: 0.0608Epoch 15/15: [===============               ] 25/47 batches, loss: 0.0604Epoch 15/15: [================              ] 26/47 batches, loss: 0.0601Epoch 15/15: [=================             ] 27/47 batches, loss: 0.0596Epoch 15/15: [=================             ] 28/47 batches, loss: 0.0592Epoch 15/15: [==================            ] 29/47 batches, loss: 0.0598Epoch 15/15: [===================           ] 30/47 batches, loss: 0.0599Epoch 15/15: [===================           ] 31/47 batches, loss: 0.0595Epoch 15/15: [====================          ] 32/47 batches, loss: 0.0599Epoch 15/15: [=====================         ] 33/47 batches, loss: 0.0601Epoch 15/15: [=====================         ] 34/47 batches, loss: 0.0589Epoch 15/15: [======================        ] 35/47 batches, loss: 0.0599Epoch 15/15: [======================        ] 36/47 batches, loss: 0.0598Epoch 15/15: [=======================       ] 37/47 batches, loss: 0.0591Epoch 15/15: [========================      ] 38/47 batches, loss: 0.0588Epoch 15/15: [========================      ] 39/47 batches, loss: 0.0588Epoch 15/15: [=========================     ] 40/47 batches, loss: 0.0589Epoch 15/15: [==========================    ] 41/47 batches, loss: 0.0583Epoch 15/15: [==========================    ] 42/47 batches, loss: 0.0582Epoch 15/15: [===========================   ] 43/47 batches, loss: 0.0593Epoch 15/15: [============================  ] 44/47 batches, loss: 0.0600Epoch 15/15: [============================  ] 45/47 batches, loss: 0.0600Epoch 15/15: [============================= ] 46/47 batches, loss: 0.0599Epoch 15/15: [==============================] 47/47 batches, loss: 0.0592
[2025-05-07 22:22:47,263][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0592
[2025-05-07 22:22:47,642][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0272, Metrics: {'mse': 0.026890093460679054, 'rmse': 0.1639819912694045, 'r2': -0.09449601173400879}
[2025-05-07 22:22:47,643][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
[2025-05-07 22:22:47,643][src.training.lm_trainer][INFO] - Training completed in 31.44 seconds
[2025-05-07 22:22:47,643][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:22:49,907][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.04340173676609993, 'rmse': 0.20833083489032517, 'r2': -0.1769390106201172}
[2025-05-07 22:22:49,908][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.02596648596227169, 'rmse': 0.16114119883590194, 'r2': -0.05690276622772217}
[2025-05-07 22:22:49,908][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03465072065591812, 'rmse': 0.18614704041675795, 'r2': 0.0012524127960205078}
[2025-05-07 22:22:51,925][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control3/layer2/ko/ko/model.pt
[2025-05-07 22:22:51,927][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▃▂▁▁▁
wandb:     best_val_mse █▄▃▃▂▁▁▁
wandb:      best_val_r2 ▁▅▆▆▇███
wandb:    best_val_rmse █▅▃▃▂▁▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▄▄▄▃▅▄▅▅▅▅▅▄
wandb:       train_loss █▄▃▃▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▃▄▃▄▂▃▁▂▂▁▁▂▁
wandb:          val_mse █▄▃▄▃▄▂▂▁▁▂▁▁▂▁
wandb:           val_r2 ▁▅▆▅▆▅▇▇██▇██▇█
wandb:         val_rmse █▅▃▄▃▄▂▃▁▁▂▁▁▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02623
wandb:     best_val_mse 0.02597
wandb:      best_val_r2 -0.0569
wandb:    best_val_rmse 0.16114
wandb:            epoch 15
wandb:   final_test_mse 0.03465
wandb:    final_test_r2 0.00125
wandb:  final_test_rmse 0.18615
wandb:  final_train_mse 0.0434
wandb:   final_train_r2 -0.17694
wandb: final_train_rmse 0.20833
wandb:    final_val_mse 0.02597
wandb:     final_val_r2 -0.0569
wandb:   final_val_rmse 0.16114
wandb:    learning_rate 0.0001
wandb:       train_loss 0.05923
wandb:       train_time 31.43742
wandb:         val_loss 0.02722
wandb:          val_mse 0.02689
wandb:           val_r2 -0.0945
wandb:         val_rmse 0.16398
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_222158-sd5ml7sa
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_222158-sd5ml7sa/logs
Experiment probe_layer2_avg_verb_edges_control3_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control3/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_lexical_density_control1_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_lexical_density_control1_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control1/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=lexical_density"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:23:22,157][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control1/layer2/ko
experiment_name: probe_layer2_lexical_density_control1_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: lexical_density
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:23:22,157][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:23:22,157][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 22:23:22,157][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:23:22,157][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:23:22,162][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:23:22,162][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 22:23:22,162][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:23:25,316][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'lexical_density'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:23:27,591][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:23:27,592][src.data.datasets][INFO] - Loading 'control_lexical_density_seed1' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:23:27,773][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_lexical_density_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:47:48 2025).
[2025-05-07 22:23:27,846][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_lexical_density_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:47:48 2025).
[2025-05-07 22:23:28,114][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:23:28,120][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:23:28,121][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:23:28,123][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:23:28,178][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:23:28,265][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:23:28,295][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:23:28,297][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:23:28,297][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:23:28,301][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:23:28,371][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:23:28,483][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:23:28,500][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:23:28,502][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:23:28,502][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:23:28,504][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:23:28,505][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:23:28,505][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 22:23:28,505][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 22:23:28,505][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 22:23:28,505][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:23:28,505][src.data.datasets][INFO] -   Mean: 0.8900, Std: 0.1851
[2025-05-07 22:23:28,506][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:23:28,506][src.data.datasets][INFO] - Sample label: 0.625
[2025-05-07 22:23:28,506][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:23:28,506][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 22:23:28,506][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 22:23:28,506][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 22:23:28,506][src.data.datasets][INFO] -   Min: 0.2860, Max: 1.0000
[2025-05-07 22:23:28,506][src.data.datasets][INFO] -   Mean: 0.8166, Std: 0.2038
[2025-05-07 22:23:28,506][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:23:28,506][src.data.datasets][INFO] - Sample label: 0.6669999957084656
[2025-05-07 22:23:28,507][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:23:28,507][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 22:23:28,507][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 22:23:28,507][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 22:23:28,507][src.data.datasets][INFO] -   Min: 0.1000, Max: 1.0000
[2025-05-07 22:23:28,507][src.data.datasets][INFO] -   Mean: 0.6309, Std: 0.1999
[2025-05-07 22:23:28,507][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:23:28,507][src.data.datasets][INFO] - Sample label: 0.8640000224113464
[2025-05-07 22:23:28,507][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:23:28,507][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:23:28,508][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:23:28,508][__main__][INFO] - Using model type: lm_probe for submetric lexical_density
[2025-05-07 22:23:28,508][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:23:35,549][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:23:35,549][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:23:35,550][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:23:35,550][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:23:35,552][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:23:35,553][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:23:35,553][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:23:35,553][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:23:35,553][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:23:35,554][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:23:35,554][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.6283Epoch 1/15: [=                             ] 2/47 batches, loss: 0.8792Epoch 1/15: [=                             ] 3/47 batches, loss: 0.7865Epoch 1/15: [==                            ] 4/47 batches, loss: 0.7270Epoch 1/15: [===                           ] 5/47 batches, loss: 0.6819Epoch 1/15: [===                           ] 6/47 batches, loss: 0.6159Epoch 1/15: [====                          ] 7/47 batches, loss: 0.5719Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.5627Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.5687Epoch 1/15: [======                        ] 10/47 batches, loss: 0.5555Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.5288Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.5215Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4987Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4968Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.4858Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4886Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.4726Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4751Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4667Epoch 1/15: [============                  ] 20/47 batches, loss: 0.4582Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4609Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4559Epoch 1/15: [==============                ] 23/47 batches, loss: 0.4443Epoch 1/15: [===============               ] 24/47 batches, loss: 0.4335Epoch 1/15: [===============               ] 25/47 batches, loss: 0.4356Epoch 1/15: [================              ] 26/47 batches, loss: 0.4314Epoch 1/15: [=================             ] 27/47 batches, loss: 0.4273Epoch 1/15: [=================             ] 28/47 batches, loss: 0.4181Epoch 1/15: [==================            ] 29/47 batches, loss: 0.4163Epoch 1/15: [===================           ] 30/47 batches, loss: 0.4141Epoch 1/15: [===================           ] 31/47 batches, loss: 0.4040Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3939Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3925Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3905Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3845Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3811Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3773Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3733Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3692Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3705Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3708Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3686Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3694Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3664Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3669Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3604Epoch 1/15: [==============================] 47/47 batches, loss: 0.3645
[2025-05-07 22:23:41,123][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3645
[2025-05-07 22:23:41,433][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0549, Metrics: {'mse': 0.05004352703690529, 'rmse': 0.22370410599026852, 'r2': -0.20428383350372314}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.1974Epoch 2/15: [=                             ] 2/47 batches, loss: 0.1749Epoch 2/15: [=                             ] 3/47 batches, loss: 0.1909Epoch 2/15: [==                            ] 4/47 batches, loss: 0.1683Epoch 2/15: [===                           ] 5/47 batches, loss: 0.1867Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2091Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2115Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2177Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2221Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2164Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2179Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2206Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2294Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2302Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2284Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2272Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2193Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2285Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2270Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2250Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2233Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2234Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2214Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2225Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2206Epoch 2/15: [================              ] 26/47 batches, loss: 0.2190Epoch 2/15: [=================             ] 27/47 batches, loss: 0.2159Epoch 2/15: [=================             ] 28/47 batches, loss: 0.2142Epoch 2/15: [==================            ] 29/47 batches, loss: 0.2142Epoch 2/15: [===================           ] 30/47 batches, loss: 0.2160Epoch 2/15: [===================           ] 31/47 batches, loss: 0.2156Epoch 2/15: [====================          ] 32/47 batches, loss: 0.2164Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.2175Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.2181Epoch 2/15: [======================        ] 35/47 batches, loss: 0.2161Epoch 2/15: [======================        ] 36/47 batches, loss: 0.2132Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.2126Epoch 2/15: [========================      ] 38/47 batches, loss: 0.2086Epoch 2/15: [========================      ] 39/47 batches, loss: 0.2071Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.2059Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.2029Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.2011Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1988Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1965Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1963Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1951Epoch 2/15: [==============================] 47/47 batches, loss: 0.1916
[2025-05-07 22:23:43,305][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1916
[2025-05-07 22:23:43,585][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0572, Metrics: {'mse': 0.052787020802497864, 'rmse': 0.22975426177222016, 'r2': -0.27030515670776367}
[2025-05-07 22:23:43,586][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1665Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1534Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1442Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1315Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1463Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1629Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1563Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1488Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1696Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1650Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1581Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1546Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1543Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1507Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1487Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1446Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1502Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1530Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1511Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1501Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1514Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1474Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1487Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1505Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1493Epoch 3/15: [================              ] 26/47 batches, loss: 0.1474Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1491Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1474Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1467Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1452Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1485Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1491Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1493Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1473Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1468Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1460Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1447Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1454Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1437Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1464Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1459Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1448Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1437Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1464Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1457Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1442Epoch 3/15: [==============================] 47/47 batches, loss: 0.1445
[2025-05-07 22:23:45,092][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1445
[2025-05-07 22:23:45,340][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0437, Metrics: {'mse': 0.040519338101148605, 'rmse': 0.2012941581396455, 'r2': 0.024913251399993896}
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1370Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1854Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1501Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1557Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1622Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1485Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1554Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1602Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1613Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1569Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1662Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1628Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1563Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1516Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1492Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1461Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1465Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1486Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1507Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1541Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1526Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1503Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1490Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1493Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1493Epoch 4/15: [================              ] 26/47 batches, loss: 0.1485Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1486Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1495Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1496Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1488Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1520Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1520Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1502Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1501Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1490Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1480Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1483Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1474Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1450Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1436Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1437Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1436Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1417Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1410Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1406Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1393Epoch 4/15: [==============================] 47/47 batches, loss: 0.1392
[2025-05-07 22:23:47,307][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1392
[2025-05-07 22:23:47,588][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0435, Metrics: {'mse': 0.04042007401585579, 'rmse': 0.2010474422017246, 'r2': 0.02730196714401245}
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.1348Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1082Epoch 5/15: [=                             ] 3/47 batches, loss: 0.0955Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0889Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0888Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0859Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0917Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0956Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0963Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0919Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0978Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.1030Epoch 5/15: [========                      ] 13/47 batches, loss: 0.1023Epoch 5/15: [========                      ] 14/47 batches, loss: 0.1107Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.1103Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.1112Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.1073Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.1065Epoch 5/15: [============                  ] 19/47 batches, loss: 0.1059Epoch 5/15: [============                  ] 20/47 batches, loss: 0.1049Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.1079Epoch 5/15: [==============                ] 22/47 batches, loss: 0.1098Epoch 5/15: [==============                ] 23/47 batches, loss: 0.1099Epoch 5/15: [===============               ] 24/47 batches, loss: 0.1125Epoch 5/15: [===============               ] 25/47 batches, loss: 0.1109Epoch 5/15: [================              ] 26/47 batches, loss: 0.1087Epoch 5/15: [=================             ] 27/47 batches, loss: 0.1062Epoch 5/15: [=================             ] 28/47 batches, loss: 0.1044Epoch 5/15: [==================            ] 29/47 batches, loss: 0.1035Epoch 5/15: [===================           ] 30/47 batches, loss: 0.1041Epoch 5/15: [===================           ] 31/47 batches, loss: 0.1049Epoch 5/15: [====================          ] 32/47 batches, loss: 0.1042Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.1034Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.1015Epoch 5/15: [======================        ] 35/47 batches, loss: 0.1022Epoch 5/15: [======================        ] 36/47 batches, loss: 0.1027Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.1013Epoch 5/15: [========================      ] 38/47 batches, loss: 0.1009Epoch 5/15: [========================      ] 39/47 batches, loss: 0.1030Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.1020Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.1008Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.1009Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.1040Epoch 5/15: [============================  ] 44/47 batches, loss: 0.1039Epoch 5/15: [============================  ] 45/47 batches, loss: 0.1046Epoch 5/15: [============================= ] 46/47 batches, loss: 0.1045Epoch 5/15: [==============================] 47/47 batches, loss: 0.1038
[2025-05-07 22:23:49,461][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1038
[2025-05-07 22:23:49,813][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0447, Metrics: {'mse': 0.042110517621040344, 'rmse': 0.20520847356052416, 'r2': -0.013378024101257324}
[2025-05-07 22:23:49,814][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.0368Epoch 6/15: [=                             ] 2/47 batches, loss: 0.0594Epoch 6/15: [=                             ] 3/47 batches, loss: 0.1109Epoch 6/15: [==                            ] 4/47 batches, loss: 0.1171Epoch 6/15: [===                           ] 5/47 batches, loss: 0.1175Epoch 6/15: [===                           ] 6/47 batches, loss: 0.1081Epoch 6/15: [====                          ] 7/47 batches, loss: 0.1061Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.1098Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.1035Epoch 6/15: [======                        ] 10/47 batches, loss: 0.1036Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.1073Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.1077Epoch 6/15: [========                      ] 13/47 batches, loss: 0.1025Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0992Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0990Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.1023Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.1038Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.1024Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0991Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0984Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0972Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0955Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0946Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0970Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0943Epoch 6/15: [================              ] 26/47 batches, loss: 0.0937Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0940Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0950Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0969Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0962Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0962Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0953Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0953Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0952Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0963Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0960Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0971Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0954Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0952Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0946Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0989Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0999Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0994Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0992Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0993Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0990Epoch 6/15: [==============================] 47/47 batches, loss: 0.0983
[2025-05-07 22:23:51,310][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0983
[2025-05-07 22:23:51,696][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0422, Metrics: {'mse': 0.03942282497882843, 'rmse': 0.1985518193792956, 'r2': 0.0513005256652832}
Epoch 7/15: [Epoch 7/15: [                              ] 1/47 batches, loss: 0.0936Epoch 7/15: [=                             ] 2/47 batches, loss: 0.0959Epoch 7/15: [=                             ] 3/47 batches, loss: 0.0769Epoch 7/15: [==                            ] 4/47 batches, loss: 0.0751Epoch 7/15: [===                           ] 5/47 batches, loss: 0.0888Epoch 7/15: [===                           ] 6/47 batches, loss: 0.0975Epoch 7/15: [====                          ] 7/47 batches, loss: 0.0915Epoch 7/15: [=====                         ] 8/47 batches, loss: 0.0930Epoch 7/15: [=====                         ] 9/47 batches, loss: 0.0878Epoch 7/15: [======                        ] 10/47 batches, loss: 0.0911Epoch 7/15: [=======                       ] 11/47 batches, loss: 0.0863Epoch 7/15: [=======                       ] 12/47 batches, loss: 0.0917Epoch 7/15: [========                      ] 13/47 batches, loss: 0.0901Epoch 7/15: [========                      ] 14/47 batches, loss: 0.0869Epoch 7/15: [=========                     ] 15/47 batches, loss: 0.0863Epoch 7/15: [==========                    ] 16/47 batches, loss: 0.0875Epoch 7/15: [==========                    ] 17/47 batches, loss: 0.0874Epoch 7/15: [===========                   ] 18/47 batches, loss: 0.0901Epoch 7/15: [============                  ] 19/47 batches, loss: 0.0937Epoch 7/15: [============                  ] 20/47 batches, loss: 0.0927Epoch 7/15: [=============                 ] 21/47 batches, loss: 0.0918Epoch 7/15: [==============                ] 22/47 batches, loss: 0.0911Epoch 7/15: [==============                ] 23/47 batches, loss: 0.0907Epoch 7/15: [===============               ] 24/47 batches, loss: 0.0899Epoch 7/15: [===============               ] 25/47 batches, loss: 0.0908Epoch 7/15: [================              ] 26/47 batches, loss: 0.0909Epoch 7/15: [=================             ] 27/47 batches, loss: 0.0889Epoch 7/15: [=================             ] 28/47 batches, loss: 0.0898Epoch 7/15: [==================            ] 29/47 batches, loss: 0.0916Epoch 7/15: [===================           ] 30/47 batches, loss: 0.0919Epoch 7/15: [===================           ] 31/47 batches, loss: 0.0909Epoch 7/15: [====================          ] 32/47 batches, loss: 0.0908Epoch 7/15: [=====================         ] 33/47 batches, loss: 0.0896Epoch 7/15: [=====================         ] 34/47 batches, loss: 0.0908Epoch 7/15: [======================        ] 35/47 batches, loss: 0.0907Epoch 7/15: [======================        ] 36/47 batches, loss: 0.0913Epoch 7/15: [=======================       ] 37/47 batches, loss: 0.0907Epoch 7/15: [========================      ] 38/47 batches, loss: 0.0900Epoch 7/15: [========================      ] 39/47 batches, loss: 0.0917Epoch 7/15: [=========================     ] 40/47 batches, loss: 0.0913Epoch 7/15: [==========================    ] 41/47 batches, loss: 0.0915Epoch 7/15: [==========================    ] 42/47 batches, loss: 0.0916Epoch 7/15: [===========================   ] 43/47 batches, loss: 0.0914Epoch 7/15: [============================  ] 44/47 batches, loss: 0.0908Epoch 7/15: [============================  ] 45/47 batches, loss: 0.0899Epoch 7/15: [============================= ] 46/47 batches, loss: 0.0891Epoch 7/15: [==============================] 47/47 batches, loss: 0.0897
[2025-05-07 22:23:53,538][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0897
[2025-05-07 22:23:53,806][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0419, Metrics: {'mse': 0.03928754851222038, 'rmse': 0.19821086880446387, 'r2': 0.05455595254898071}
Epoch 8/15: [Epoch 8/15: [                              ] 1/47 batches, loss: 0.0349Epoch 8/15: [=                             ] 2/47 batches, loss: 0.0480Epoch 8/15: [=                             ] 3/47 batches, loss: 0.0682Epoch 8/15: [==                            ] 4/47 batches, loss: 0.0723Epoch 8/15: [===                           ] 5/47 batches, loss: 0.0707Epoch 8/15: [===                           ] 6/47 batches, loss: 0.0671Epoch 8/15: [====                          ] 7/47 batches, loss: 0.0723Epoch 8/15: [=====                         ] 8/47 batches, loss: 0.0690Epoch 8/15: [=====                         ] 9/47 batches, loss: 0.0726Epoch 8/15: [======                        ] 10/47 batches, loss: 0.0704Epoch 8/15: [=======                       ] 11/47 batches, loss: 0.0705Epoch 8/15: [=======                       ] 12/47 batches, loss: 0.0729Epoch 8/15: [========                      ] 13/47 batches, loss: 0.0712Epoch 8/15: [========                      ] 14/47 batches, loss: 0.0697Epoch 8/15: [=========                     ] 15/47 batches, loss: 0.0677Epoch 8/15: [==========                    ] 16/47 batches, loss: 0.0670Epoch 8/15: [==========                    ] 17/47 batches, loss: 0.0669Epoch 8/15: [===========                   ] 18/47 batches, loss: 0.0657Epoch 8/15: [============                  ] 19/47 batches, loss: 0.0662Epoch 8/15: [============                  ] 20/47 batches, loss: 0.0657Epoch 8/15: [=============                 ] 21/47 batches, loss: 0.0664Epoch 8/15: [==============                ] 22/47 batches, loss: 0.0660Epoch 8/15: [==============                ] 23/47 batches, loss: 0.0658Epoch 8/15: [===============               ] 24/47 batches, loss: 0.0656Epoch 8/15: [===============               ] 25/47 batches, loss: 0.0653Epoch 8/15: [================              ] 26/47 batches, loss: 0.0650Epoch 8/15: [=================             ] 27/47 batches, loss: 0.0687Epoch 8/15: [=================             ] 28/47 batches, loss: 0.0699Epoch 8/15: [==================            ] 29/47 batches, loss: 0.0707Epoch 8/15: [===================           ] 30/47 batches, loss: 0.0704Epoch 8/15: [===================           ] 31/47 batches, loss: 0.0721Epoch 8/15: [====================          ] 32/47 batches, loss: 0.0727Epoch 8/15: [=====================         ] 33/47 batches, loss: 0.0734Epoch 8/15: [=====================         ] 34/47 batches, loss: 0.0747Epoch 8/15: [======================        ] 35/47 batches, loss: 0.0743Epoch 8/15: [======================        ] 36/47 batches, loss: 0.0744Epoch 8/15: [=======================       ] 37/47 batches, loss: 0.0742Epoch 8/15: [========================      ] 38/47 batches, loss: 0.0740Epoch 8/15: [========================      ] 39/47 batches, loss: 0.0742Epoch 8/15: [=========================     ] 40/47 batches, loss: 0.0743Epoch 8/15: [==========================    ] 41/47 batches, loss: 0.0741Epoch 8/15: [==========================    ] 42/47 batches, loss: 0.0729Epoch 8/15: [===========================   ] 43/47 batches, loss: 0.0733Epoch 8/15: [============================  ] 44/47 batches, loss: 0.0729Epoch 8/15: [============================  ] 45/47 batches, loss: 0.0726Epoch 8/15: [============================= ] 46/47 batches, loss: 0.0724Epoch 8/15: [==============================] 47/47 batches, loss: 0.0709
[2025-05-07 22:23:55,700][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0709
[2025-05-07 22:23:55,959][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0425, Metrics: {'mse': 0.039755310863256454, 'rmse': 0.1993873387736956, 'r2': 0.043299317359924316}
[2025-05-07 22:23:55,959][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/47 batches, loss: 0.0992Epoch 9/15: [=                             ] 2/47 batches, loss: 0.0714Epoch 9/15: [=                             ] 3/47 batches, loss: 0.0779Epoch 9/15: [==                            ] 4/47 batches, loss: 0.0772Epoch 9/15: [===                           ] 5/47 batches, loss: 0.0692Epoch 9/15: [===                           ] 6/47 batches, loss: 0.0720Epoch 9/15: [====                          ] 7/47 batches, loss: 0.0714Epoch 9/15: [=====                         ] 8/47 batches, loss: 0.0744Epoch 9/15: [=====                         ] 9/47 batches, loss: 0.0730Epoch 9/15: [======                        ] 10/47 batches, loss: 0.0720Epoch 9/15: [=======                       ] 11/47 batches, loss: 0.0730Epoch 9/15: [=======                       ] 12/47 batches, loss: 0.0748Epoch 9/15: [========                      ] 13/47 batches, loss: 0.0729Epoch 9/15: [========                      ] 14/47 batches, loss: 0.0755Epoch 9/15: [=========                     ] 15/47 batches, loss: 0.0767Epoch 9/15: [==========                    ] 16/47 batches, loss: 0.0773Epoch 9/15: [==========                    ] 17/47 batches, loss: 0.0774Epoch 9/15: [===========                   ] 18/47 batches, loss: 0.0763Epoch 9/15: [============                  ] 19/47 batches, loss: 0.0763Epoch 9/15: [============                  ] 20/47 batches, loss: 0.0754Epoch 9/15: [=============                 ] 21/47 batches, loss: 0.0749Epoch 9/15: [==============                ] 22/47 batches, loss: 0.0743Epoch 9/15: [==============                ] 23/47 batches, loss: 0.0732Epoch 9/15: [===============               ] 24/47 batches, loss: 0.0725Epoch 9/15: [===============               ] 25/47 batches, loss: 0.0745Epoch 9/15: [================              ] 26/47 batches, loss: 0.0759Epoch 9/15: [=================             ] 27/47 batches, loss: 0.0755Epoch 9/15: [=================             ] 28/47 batches, loss: 0.0758Epoch 9/15: [==================            ] 29/47 batches, loss: 0.0755Epoch 9/15: [===================           ] 30/47 batches, loss: 0.0749Epoch 9/15: [===================           ] 31/47 batches, loss: 0.0764Epoch 9/15: [====================          ] 32/47 batches, loss: 0.0758Epoch 9/15: [=====================         ] 33/47 batches, loss: 0.0751Epoch 9/15: [=====================         ] 34/47 batches, loss: 0.0752Epoch 9/15: [======================        ] 35/47 batches, loss: 0.0753Epoch 9/15: [======================        ] 36/47 batches, loss: 0.0756Epoch 9/15: [=======================       ] 37/47 batches, loss: 0.0776Epoch 9/15: [========================      ] 38/47 batches, loss: 0.0775Epoch 9/15: [========================      ] 39/47 batches, loss: 0.0771Epoch 9/15: [=========================     ] 40/47 batches, loss: 0.0764Epoch 9/15: [==========================    ] 41/47 batches, loss: 0.0759Epoch 9/15: [==========================    ] 42/47 batches, loss: 0.0755Epoch 9/15: [===========================   ] 43/47 batches, loss: 0.0744Epoch 9/15: [============================  ] 44/47 batches, loss: 0.0754Epoch 9/15: [============================  ] 45/47 batches, loss: 0.0758Epoch 9/15: [============================= ] 46/47 batches, loss: 0.0756Epoch 9/15: [==============================] 47/47 batches, loss: 0.0760
[2025-05-07 22:23:57,422][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0760
[2025-05-07 22:23:57,718][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0429, Metrics: {'mse': 0.04063930734992027, 'rmse': 0.2015919327500986, 'r2': 0.0220261812210083}
[2025-05-07 22:23:57,719][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/47 batches, loss: 0.0883Epoch 10/15: [=                             ] 2/47 batches, loss: 0.0662Epoch 10/15: [=                             ] 3/47 batches, loss: 0.0595Epoch 10/15: [==                            ] 4/47 batches, loss: 0.0754Epoch 10/15: [===                           ] 5/47 batches, loss: 0.0683Epoch 10/15: [===                           ] 6/47 batches, loss: 0.0661Epoch 10/15: [====                          ] 7/47 batches, loss: 0.0640Epoch 10/15: [=====                         ] 8/47 batches, loss: 0.0663Epoch 10/15: [=====                         ] 9/47 batches, loss: 0.0673Epoch 10/15: [======                        ] 10/47 batches, loss: 0.0712Epoch 10/15: [=======                       ] 11/47 batches, loss: 0.0737Epoch 10/15: [=======                       ] 12/47 batches, loss: 0.0764Epoch 10/15: [========                      ] 13/47 batches, loss: 0.0811Epoch 10/15: [========                      ] 14/47 batches, loss: 0.0784Epoch 10/15: [=========                     ] 15/47 batches, loss: 0.0766Epoch 10/15: [==========                    ] 16/47 batches, loss: 0.0742Epoch 10/15: [==========                    ] 17/47 batches, loss: 0.0748Epoch 10/15: [===========                   ] 18/47 batches, loss: 0.0727Epoch 10/15: [============                  ] 19/47 batches, loss: 0.0762Epoch 10/15: [============                  ] 20/47 batches, loss: 0.0772Epoch 10/15: [=============                 ] 21/47 batches, loss: 0.0750Epoch 10/15: [==============                ] 22/47 batches, loss: 0.0762Epoch 10/15: [==============                ] 23/47 batches, loss: 0.0760Epoch 10/15: [===============               ] 24/47 batches, loss: 0.0766Epoch 10/15: [===============               ] 25/47 batches, loss: 0.0772Epoch 10/15: [================              ] 26/47 batches, loss: 0.0757Epoch 10/15: [=================             ] 27/47 batches, loss: 0.0752Epoch 10/15: [=================             ] 28/47 batches, loss: 0.0744Epoch 10/15: [==================            ] 29/47 batches, loss: 0.0746Epoch 10/15: [===================           ] 30/47 batches, loss: 0.0738Epoch 10/15: [===================           ] 31/47 batches, loss: 0.0725Epoch 10/15: [====================          ] 32/47 batches, loss: 0.0719Epoch 10/15: [=====================         ] 33/47 batches, loss: 0.0711Epoch 10/15: [=====================         ] 34/47 batches, loss: 0.0704Epoch 10/15: [======================        ] 35/47 batches, loss: 0.0710Epoch 10/15: [======================        ] 36/47 batches, loss: 0.0707Epoch 10/15: [=======================       ] 37/47 batches, loss: 0.0714Epoch 10/15: [========================      ] 38/47 batches, loss: 0.0724Epoch 10/15: [========================      ] 39/47 batches, loss: 0.0719Epoch 10/15: [=========================     ] 40/47 batches, loss: 0.0716Epoch 10/15: [==========================    ] 41/47 batches, loss: 0.0712Epoch 10/15: [==========================    ] 42/47 batches, loss: 0.0713Epoch 10/15: [===========================   ] 43/47 batches, loss: 0.0708Epoch 10/15: [============================  ] 44/47 batches, loss: 0.0707Epoch 10/15: [============================  ] 45/47 batches, loss: 0.0703Epoch 10/15: [============================= ] 46/47 batches, loss: 0.0700Epoch 10/15: [==============================] 47/47 batches, loss: 0.0702
[2025-05-07 22:23:59,199][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0702
[2025-05-07 22:23:59,509][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0420, Metrics: {'mse': 0.03932388126850128, 'rmse': 0.1983024994005403, 'r2': 0.05368149280548096}
[2025-05-07 22:23:59,510][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/47 batches, loss: 0.0254Epoch 11/15: [=                             ] 2/47 batches, loss: 0.0402Epoch 11/15: [=                             ] 3/47 batches, loss: 0.0605Epoch 11/15: [==                            ] 4/47 batches, loss: 0.0635Epoch 11/15: [===                           ] 5/47 batches, loss: 0.0630Epoch 11/15: [===                           ] 6/47 batches, loss: 0.0577Epoch 11/15: [====                          ] 7/47 batches, loss: 0.0562Epoch 11/15: [=====                         ] 8/47 batches, loss: 0.0607Epoch 11/15: [=====                         ] 9/47 batches, loss: 0.0614Epoch 11/15: [======                        ] 10/47 batches, loss: 0.0601Epoch 11/15: [=======                       ] 11/47 batches, loss: 0.0566Epoch 11/15: [=======                       ] 12/47 batches, loss: 0.0550Epoch 11/15: [========                      ] 13/47 batches, loss: 0.0584Epoch 11/15: [========                      ] 14/47 batches, loss: 0.0589Epoch 11/15: [=========                     ] 15/47 batches, loss: 0.0601Epoch 11/15: [==========                    ] 16/47 batches, loss: 0.0602Epoch 11/15: [==========                    ] 17/47 batches, loss: 0.0607Epoch 11/15: [===========                   ] 18/47 batches, loss: 0.0627Epoch 11/15: [============                  ] 19/47 batches, loss: 0.0639Epoch 11/15: [============                  ] 20/47 batches, loss: 0.0655Epoch 11/15: [=============                 ] 21/47 batches, loss: 0.0657Epoch 11/15: [==============                ] 22/47 batches, loss: 0.0652Epoch 11/15: [==============                ] 23/47 batches, loss: 0.0638Epoch 11/15: [===============               ] 24/47 batches, loss: 0.0631Epoch 11/15: [===============               ] 25/47 batches, loss: 0.0638Epoch 11/15: [================              ] 26/47 batches, loss: 0.0643Epoch 11/15: [=================             ] 27/47 batches, loss: 0.0651Epoch 11/15: [=================             ] 28/47 batches, loss: 0.0651Epoch 11/15: [==================            ] 29/47 batches, loss: 0.0648Epoch 11/15: [===================           ] 30/47 batches, loss: 0.0632Epoch 11/15: [===================           ] 31/47 batches, loss: 0.0649Epoch 11/15: [====================          ] 32/47 batches, loss: 0.0639Epoch 11/15: [=====================         ] 33/47 batches, loss: 0.0633Epoch 11/15: [=====================         ] 34/47 batches, loss: 0.0628Epoch 11/15: [======================        ] 35/47 batches, loss: 0.0634Epoch 11/15: [======================        ] 36/47 batches, loss: 0.0635Epoch 11/15: [=======================       ] 37/47 batches, loss: 0.0642Epoch 11/15: [========================      ] 38/47 batches, loss: 0.0649Epoch 11/15: [========================      ] 39/47 batches, loss: 0.0646Epoch 11/15: [=========================     ] 40/47 batches, loss: 0.0645Epoch 11/15: [==========================    ] 41/47 batches, loss: 0.0644Epoch 11/15: [==========================    ] 42/47 batches, loss: 0.0639Epoch 11/15: [===========================   ] 43/47 batches, loss: 0.0629Epoch 11/15: [============================  ] 44/47 batches, loss: 0.0625Epoch 11/15: [============================  ] 45/47 batches, loss: 0.0621Epoch 11/15: [============================= ] 46/47 batches, loss: 0.0624Epoch 11/15: [==============================] 47/47 batches, loss: 0.0640
[2025-05-07 22:24:01,064][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0640
[2025-05-07 22:24:01,369][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0422, Metrics: {'mse': 0.03981245309114456, 'rmse': 0.19953058184434927, 'r2': 0.041924238204956055}
[2025-05-07 22:24:01,370][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:24:01,370][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 11
[2025-05-07 22:24:01,370][src.training.lm_trainer][INFO] - Training completed in 22.60 seconds
[2025-05-07 22:24:01,370][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:24:03,569][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.039339207112789154, 'rmse': 0.19834113822600988, 'r2': -0.14872753620147705}
[2025-05-07 22:24:03,570][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03928754851222038, 'rmse': 0.19821086880446387, 'r2': 0.05455595254898071}
[2025-05-07 22:24:03,570][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06724859774112701, 'rmse': 0.2593233459238235, 'r2': -0.6822861433029175}
[2025-05-07 22:24:05,465][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control1/layer2/ko/ko/model.pt
[2025-05-07 22:24:05,466][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▂▁▁
wandb:     best_val_mse █▂▂▁▁
wandb:      best_val_r2 ▁▇▇██
wandb:    best_val_rmse █▂▂▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▄▄▄▄▄▄▄▄
wandb:       train_loss █▄▃▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▂▂▂▁▁▁▁▁▁
wandb:          val_mse ▇█▂▂▂▁▁▁▂▁▁
wandb:           val_r2 ▂▁▇▇▇███▇██
wandb:         val_rmse ▇█▂▂▃▁▁▁▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04192
wandb:     best_val_mse 0.03929
wandb:      best_val_r2 0.05456
wandb:    best_val_rmse 0.19821
wandb: early_stop_epoch 11
wandb:            epoch 11
wandb:   final_test_mse 0.06725
wandb:    final_test_r2 -0.68229
wandb:  final_test_rmse 0.25932
wandb:  final_train_mse 0.03934
wandb:   final_train_r2 -0.14873
wandb: final_train_rmse 0.19834
wandb:    final_val_mse 0.03929
wandb:     final_val_r2 0.05456
wandb:   final_val_rmse 0.19821
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06402
wandb:       train_time 22.60008
wandb:         val_loss 0.04215
wandb:          val_mse 0.03981
wandb:           val_r2 0.04192
wandb:         val_rmse 0.19953
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_222322-szu2edqn
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_222322-szu2edqn/logs
Experiment probe_layer2_lexical_density_control1_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control1/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_lexical_density_control2_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_lexical_density_control2_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control2/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=lexical_density"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:24:33,494][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control2/layer2/ko
experiment_name: probe_layer2_lexical_density_control2_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: lexical_density
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:24:33,494][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:24:33,494][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 22:24:33,494][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:24:33,494][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:24:33,499][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:24:33,499][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 22:24:33,499][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:24:36,739][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'lexical_density'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:24:39,073][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:24:39,074][src.data.datasets][INFO] - Loading 'control_lexical_density_seed2' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:24:39,239][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_lexical_density_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:48:37 2025).
[2025-05-07 22:24:39,302][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_lexical_density_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:48:37 2025).
[2025-05-07 22:24:39,596][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:24:39,602][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:24:39,603][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:24:39,606][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:24:39,712][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:24:39,817][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:24:39,834][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:24:39,836][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:24:39,836][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:24:39,838][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:24:39,897][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:24:39,980][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:24:40,009][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:24:40,010][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:24:40,011][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:24:40,012][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:24:40,012][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:24:40,013][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 22:24:40,013][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 22:24:40,013][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 22:24:40,013][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:24:40,013][src.data.datasets][INFO] -   Mean: 0.8900, Std: 0.1851
[2025-05-07 22:24:40,013][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:24:40,013][src.data.datasets][INFO] - Sample label: 1.0
[2025-05-07 22:24:40,013][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:24:40,014][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 22:24:40,014][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 22:24:40,014][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 22:24:40,014][src.data.datasets][INFO] -   Min: 0.2860, Max: 1.0000
[2025-05-07 22:24:40,014][src.data.datasets][INFO] -   Mean: 0.8166, Std: 0.2038
[2025-05-07 22:24:40,014][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:24:40,014][src.data.datasets][INFO] - Sample label: 0.6669999957084656
[2025-05-07 22:24:40,014][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:24:40,014][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 22:24:40,014][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 22:24:40,015][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 22:24:40,015][src.data.datasets][INFO] -   Min: 0.1000, Max: 1.0000
[2025-05-07 22:24:40,015][src.data.datasets][INFO] -   Mean: 0.6309, Std: 0.1999
[2025-05-07 22:24:40,015][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:24:40,015][src.data.datasets][INFO] - Sample label: 0.8640000224113464
[2025-05-07 22:24:40,015][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:24:40,015][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:24:40,015][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:24:40,016][__main__][INFO] - Using model type: lm_probe for submetric lexical_density
[2025-05-07 22:24:40,016][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:24:47,523][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:24:47,524][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:24:47,524][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:24:47,524][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:24:47,527][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:24:47,527][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:24:47,527][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:24:47,527][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:24:47,528][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:24:47,528][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:24:47,528][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.5789Epoch 1/15: [=                             ] 2/47 batches, loss: 0.7721Epoch 1/15: [=                             ] 3/47 batches, loss: 0.7015Epoch 1/15: [==                            ] 4/47 batches, loss: 0.6532Epoch 1/15: [===                           ] 5/47 batches, loss: 0.5899Epoch 1/15: [===                           ] 6/47 batches, loss: 0.5191Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4875Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4860Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4837Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4820Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4733Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4647Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4511Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4508Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.4464Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4455Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.4367Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4473Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4432Epoch 1/15: [============                  ] 20/47 batches, loss: 0.4351Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4400Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4373Epoch 1/15: [==============                ] 23/47 batches, loss: 0.4259Epoch 1/15: [===============               ] 24/47 batches, loss: 0.4162Epoch 1/15: [===============               ] 25/47 batches, loss: 0.4176Epoch 1/15: [================              ] 26/47 batches, loss: 0.4124Epoch 1/15: [=================             ] 27/47 batches, loss: 0.4076Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3988Epoch 1/15: [==================            ] 29/47 batches, loss: 0.4032Epoch 1/15: [===================           ] 30/47 batches, loss: 0.4000Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3937Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3866Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3830Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3828Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3777Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3747Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3702Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3686Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3662Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3677Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3658Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3613Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3593Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3563Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3581Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3526Epoch 1/15: [==============================] 47/47 batches, loss: 0.3582
[2025-05-07 22:24:54,528][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3582
[2025-05-07 22:24:54,867][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0555, Metrics: {'mse': 0.05068189650774002, 'rmse': 0.2251264011788489, 'r2': -0.21964597702026367}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.3326Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2821Epoch 2/15: [=                             ] 3/47 batches, loss: 0.3224Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2696Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2763Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2838Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2763Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2615Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2601Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2597Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2628Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2576Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2656Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2634Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2585Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2548Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2466Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2541Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2469Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2403Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2353Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2326Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2285Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2270Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2258Epoch 2/15: [================              ] 26/47 batches, loss: 0.2211Epoch 2/15: [=================             ] 27/47 batches, loss: 0.2197Epoch 2/15: [=================             ] 28/47 batches, loss: 0.2187Epoch 2/15: [==================            ] 29/47 batches, loss: 0.2164Epoch 2/15: [===================           ] 30/47 batches, loss: 0.2152Epoch 2/15: [===================           ] 31/47 batches, loss: 0.2177Epoch 2/15: [====================          ] 32/47 batches, loss: 0.2164Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.2177Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.2196Epoch 2/15: [======================        ] 35/47 batches, loss: 0.2178Epoch 2/15: [======================        ] 36/47 batches, loss: 0.2151Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.2166Epoch 2/15: [========================      ] 38/47 batches, loss: 0.2128Epoch 2/15: [========================      ] 39/47 batches, loss: 0.2121Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.2111Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.2085Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.2061Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.2035Epoch 2/15: [============================  ] 44/47 batches, loss: 0.2016Epoch 2/15: [============================  ] 45/47 batches, loss: 0.2001Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1986Epoch 2/15: [==============================] 47/47 batches, loss: 0.1957
[2025-05-07 22:24:56,807][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1957
[2025-05-07 22:24:57,065][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0512, Metrics: {'mse': 0.047153301537036896, 'rmse': 0.2171481096787096, 'r2': -0.13473129272460938}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.2410Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1903Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1910Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1603Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1523Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1552Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1553Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1545Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1516Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1481Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1443Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1516Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1569Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1493Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1499Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1522Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1516Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1543Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1552Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1539Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1539Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1494Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1498Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1529Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1503Epoch 3/15: [================              ] 26/47 batches, loss: 0.1506Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1529Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1502Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1510Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1512Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1524Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1507Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1510Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1485Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1487Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1507Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1502Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1485Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1474Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1457Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1442Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1428Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1429Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1444Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1451Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1444Epoch 3/15: [==============================] 47/47 batches, loss: 0.1453
[2025-05-07 22:24:59,146][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1453
[2025-05-07 22:24:59,438][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0451, Metrics: {'mse': 0.04236819967627525, 'rmse': 0.2058353703236527, 'r2': -0.019579172134399414}
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1082Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1001Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1091Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1173Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1268Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1329Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1398Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1494Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1501Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1476Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1512Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1474Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1418Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1351Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1343Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1329Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1331Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1333Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1354Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1392Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1375Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1370Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1359Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1351Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1383Epoch 4/15: [================              ] 26/47 batches, loss: 0.1423Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1418Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1419Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1428Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1398Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1417Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1412Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1388Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1402Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1452Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1434Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1415Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1406Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1396Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1385Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1384Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1385Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1365Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1353Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1355Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1353Epoch 4/15: [==============================] 47/47 batches, loss: 0.1347
[2025-05-07 22:25:01,275][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1347
[2025-05-07 22:25:01,588][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0441, Metrics: {'mse': 0.041197359561920166, 'rmse': 0.20297132694526132, 'r2': 0.008596837520599365}
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.0757Epoch 5/15: [=                             ] 2/47 batches, loss: 0.0870Epoch 5/15: [=                             ] 3/47 batches, loss: 0.0800Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0802Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0793Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0801Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0861Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0867Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0901Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0838Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0877Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0948Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0922Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0928Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0956Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.1039Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.1008Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.1003Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0982Epoch 5/15: [============                  ] 20/47 batches, loss: 0.1018Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.1031Epoch 5/15: [==============                ] 22/47 batches, loss: 0.1013Epoch 5/15: [==============                ] 23/47 batches, loss: 0.1018Epoch 5/15: [===============               ] 24/47 batches, loss: 0.1035Epoch 5/15: [===============               ] 25/47 batches, loss: 0.1014Epoch 5/15: [================              ] 26/47 batches, loss: 0.1008Epoch 5/15: [=================             ] 27/47 batches, loss: 0.1004Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0985Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0969Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0968Epoch 5/15: [===================           ] 31/47 batches, loss: 0.1011Epoch 5/15: [====================          ] 32/47 batches, loss: 0.1029Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.1017Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.1007Epoch 5/15: [======================        ] 35/47 batches, loss: 0.1038Epoch 5/15: [======================        ] 36/47 batches, loss: 0.1039Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.1026Epoch 5/15: [========================      ] 38/47 batches, loss: 0.1035Epoch 5/15: [========================      ] 39/47 batches, loss: 0.1036Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.1043Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.1032Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.1019Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.1016Epoch 5/15: [============================  ] 44/47 batches, loss: 0.1027Epoch 5/15: [============================  ] 45/47 batches, loss: 0.1038Epoch 5/15: [============================= ] 46/47 batches, loss: 0.1037Epoch 5/15: [==============================] 47/47 batches, loss: 0.1047
[2025-05-07 22:25:03,544][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1047
[2025-05-07 22:25:03,901][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0437, Metrics: {'mse': 0.041131649166345596, 'rmse': 0.20280939121832006, 'r2': 0.010178148746490479}
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.0392Epoch 6/15: [=                             ] 2/47 batches, loss: 0.0742Epoch 6/15: [=                             ] 3/47 batches, loss: 0.1027Epoch 6/15: [==                            ] 4/47 batches, loss: 0.0911Epoch 6/15: [===                           ] 5/47 batches, loss: 0.1172Epoch 6/15: [===                           ] 6/47 batches, loss: 0.1114Epoch 6/15: [====                          ] 7/47 batches, loss: 0.1094Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.1097Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.1041Epoch 6/15: [======                        ] 10/47 batches, loss: 0.1043Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.1059Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.1119Epoch 6/15: [========                      ] 13/47 batches, loss: 0.1064Epoch 6/15: [========                      ] 14/47 batches, loss: 0.1054Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.1031Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.1092Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.1105Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.1120Epoch 6/15: [============                  ] 19/47 batches, loss: 0.1114Epoch 6/15: [============                  ] 20/47 batches, loss: 0.1111Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.1093Epoch 6/15: [==============                ] 22/47 batches, loss: 0.1066Epoch 6/15: [==============                ] 23/47 batches, loss: 0.1041Epoch 6/15: [===============               ] 24/47 batches, loss: 0.1017Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0996Epoch 6/15: [================              ] 26/47 batches, loss: 0.0994Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0979Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0980Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0989Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0982Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0964Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0955Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0953Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0954Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0960Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0958Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0954Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0954Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0954Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0954Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0952Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0957Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0973Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0970Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0970Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0975Epoch 6/15: [==============================] 47/47 batches, loss: 0.0982
[2025-05-07 22:25:05,755][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0982
[2025-05-07 22:25:06,098][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0427, Metrics: {'mse': 0.04023426026105881, 'rmse': 0.20058479568765628, 'r2': 0.031773507595062256}
Epoch 7/15: [Epoch 7/15: [                              ] 1/47 batches, loss: 0.0528Epoch 7/15: [=                             ] 2/47 batches, loss: 0.0764Epoch 7/15: [=                             ] 3/47 batches, loss: 0.0656Epoch 7/15: [==                            ] 4/47 batches, loss: 0.0684Epoch 7/15: [===                           ] 5/47 batches, loss: 0.0820Epoch 7/15: [===                           ] 6/47 batches, loss: 0.0856Epoch 7/15: [====                          ] 7/47 batches, loss: 0.0809Epoch 7/15: [=====                         ] 8/47 batches, loss: 0.0803Epoch 7/15: [=====                         ] 9/47 batches, loss: 0.0834Epoch 7/15: [======                        ] 10/47 batches, loss: 0.0907Epoch 7/15: [=======                       ] 11/47 batches, loss: 0.0899Epoch 7/15: [=======                       ] 12/47 batches, loss: 0.0914Epoch 7/15: [========                      ] 13/47 batches, loss: 0.0916Epoch 7/15: [========                      ] 14/47 batches, loss: 0.0890Epoch 7/15: [=========                     ] 15/47 batches, loss: 0.0907Epoch 7/15: [==========                    ] 16/47 batches, loss: 0.0927Epoch 7/15: [==========                    ] 17/47 batches, loss: 0.0950Epoch 7/15: [===========                   ] 18/47 batches, loss: 0.0943Epoch 7/15: [============                  ] 19/47 batches, loss: 0.0924Epoch 7/15: [============                  ] 20/47 batches, loss: 0.0901Epoch 7/15: [=============                 ] 21/47 batches, loss: 0.0898Epoch 7/15: [==============                ] 22/47 batches, loss: 0.0893Epoch 7/15: [==============                ] 23/47 batches, loss: 0.0919Epoch 7/15: [===============               ] 24/47 batches, loss: 0.0903Epoch 7/15: [===============               ] 25/47 batches, loss: 0.0901Epoch 7/15: [================              ] 26/47 batches, loss: 0.0902Epoch 7/15: [=================             ] 27/47 batches, loss: 0.0896Epoch 7/15: [=================             ] 28/47 batches, loss: 0.0905Epoch 7/15: [==================            ] 29/47 batches, loss: 0.0905Epoch 7/15: [===================           ] 30/47 batches, loss: 0.0908Epoch 7/15: [===================           ] 31/47 batches, loss: 0.0930Epoch 7/15: [====================          ] 32/47 batches, loss: 0.0912Epoch 7/15: [=====================         ] 33/47 batches, loss: 0.0892Epoch 7/15: [=====================         ] 34/47 batches, loss: 0.0903Epoch 7/15: [======================        ] 35/47 batches, loss: 0.0908Epoch 7/15: [======================        ] 36/47 batches, loss: 0.0918Epoch 7/15: [=======================       ] 37/47 batches, loss: 0.0907Epoch 7/15: [========================      ] 38/47 batches, loss: 0.0891Epoch 7/15: [========================      ] 39/47 batches, loss: 0.0889Epoch 7/15: [=========================     ] 40/47 batches, loss: 0.0880Epoch 7/15: [==========================    ] 41/47 batches, loss: 0.0876Epoch 7/15: [==========================    ] 42/47 batches, loss: 0.0883Epoch 7/15: [===========================   ] 43/47 batches, loss: 0.0888Epoch 7/15: [============================  ] 44/47 batches, loss: 0.0882Epoch 7/15: [============================  ] 45/47 batches, loss: 0.0870Epoch 7/15: [============================= ] 46/47 batches, loss: 0.0865Epoch 7/15: [==============================] 47/47 batches, loss: 0.0887
[2025-05-07 22:25:08,099][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0887
[2025-05-07 22:25:08,386][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0423, Metrics: {'mse': 0.039840392768383026, 'rmse': 0.19960058308628015, 'r2': 0.04125183820724487}
Epoch 8/15: [Epoch 8/15: [                              ] 1/47 batches, loss: 0.0410Epoch 8/15: [=                             ] 2/47 batches, loss: 0.0524Epoch 8/15: [=                             ] 3/47 batches, loss: 0.0704Epoch 8/15: [==                            ] 4/47 batches, loss: 0.0733Epoch 8/15: [===                           ] 5/47 batches, loss: 0.0720Epoch 8/15: [===                           ] 6/47 batches, loss: 0.0692Epoch 8/15: [====                          ] 7/47 batches, loss: 0.0753Epoch 8/15: [=====                         ] 8/47 batches, loss: 0.0714Epoch 8/15: [=====                         ] 9/47 batches, loss: 0.0658Epoch 8/15: [======                        ] 10/47 batches, loss: 0.0627Epoch 8/15: [=======                       ] 11/47 batches, loss: 0.0625Epoch 8/15: [=======                       ] 12/47 batches, loss: 0.0636Epoch 8/15: [========                      ] 13/47 batches, loss: 0.0634Epoch 8/15: [========                      ] 14/47 batches, loss: 0.0627Epoch 8/15: [=========                     ] 15/47 batches, loss: 0.0621Epoch 8/15: [==========                    ] 16/47 batches, loss: 0.0624Epoch 8/15: [==========                    ] 17/47 batches, loss: 0.0626Epoch 8/15: [===========                   ] 18/47 batches, loss: 0.0622Epoch 8/15: [============                  ] 19/47 batches, loss: 0.0635Epoch 8/15: [============                  ] 20/47 batches, loss: 0.0628Epoch 8/15: [=============                 ] 21/47 batches, loss: 0.0630Epoch 8/15: [==============                ] 22/47 batches, loss: 0.0615Epoch 8/15: [==============                ] 23/47 batches, loss: 0.0604Epoch 8/15: [===============               ] 24/47 batches, loss: 0.0604Epoch 8/15: [===============               ] 25/47 batches, loss: 0.0615Epoch 8/15: [================              ] 26/47 batches, loss: 0.0607Epoch 8/15: [=================             ] 27/47 batches, loss: 0.0607Epoch 8/15: [=================             ] 28/47 batches, loss: 0.0610Epoch 8/15: [==================            ] 29/47 batches, loss: 0.0607Epoch 8/15: [===================           ] 30/47 batches, loss: 0.0610Epoch 8/15: [===================           ] 31/47 batches, loss: 0.0615Epoch 8/15: [====================          ] 32/47 batches, loss: 0.0619Epoch 8/15: [=====================         ] 33/47 batches, loss: 0.0629Epoch 8/15: [=====================         ] 34/47 batches, loss: 0.0640Epoch 8/15: [======================        ] 35/47 batches, loss: 0.0649Epoch 8/15: [======================        ] 36/47 batches, loss: 0.0651Epoch 8/15: [=======================       ] 37/47 batches, loss: 0.0648Epoch 8/15: [========================      ] 38/47 batches, loss: 0.0648Epoch 8/15: [========================      ] 39/47 batches, loss: 0.0641Epoch 8/15: [=========================     ] 40/47 batches, loss: 0.0637Epoch 8/15: [==========================    ] 41/47 batches, loss: 0.0640Epoch 8/15: [==========================    ] 42/47 batches, loss: 0.0641Epoch 8/15: [===========================   ] 43/47 batches, loss: 0.0659Epoch 8/15: [============================  ] 44/47 batches, loss: 0.0652Epoch 8/15: [============================  ] 45/47 batches, loss: 0.0655Epoch 8/15: [============================= ] 46/47 batches, loss: 0.0664Epoch 8/15: [==============================] 47/47 batches, loss: 0.0654
[2025-05-07 22:25:10,291][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0654
[2025-05-07 22:25:10,549][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0419, Metrics: {'mse': 0.039372559636831284, 'rmse': 0.1984251990973709, 'r2': 0.05251014232635498}
Epoch 9/15: [Epoch 9/15: [                              ] 1/47 batches, loss: 0.0738Epoch 9/15: [=                             ] 2/47 batches, loss: 0.0696Epoch 9/15: [=                             ] 3/47 batches, loss: 0.0571Epoch 9/15: [==                            ] 4/47 batches, loss: 0.0604Epoch 9/15: [===                           ] 5/47 batches, loss: 0.0573Epoch 9/15: [===                           ] 6/47 batches, loss: 0.0633Epoch 9/15: [====                          ] 7/47 batches, loss: 0.0594Epoch 9/15: [=====                         ] 8/47 batches, loss: 0.0667Epoch 9/15: [=====                         ] 9/47 batches, loss: 0.0659Epoch 9/15: [======                        ] 10/47 batches, loss: 0.0664Epoch 9/15: [=======                       ] 11/47 batches, loss: 0.0690Epoch 9/15: [=======                       ] 12/47 batches, loss: 0.0697Epoch 9/15: [========                      ] 13/47 batches, loss: 0.0673Epoch 9/15: [========                      ] 14/47 batches, loss: 0.0711Epoch 9/15: [=========                     ] 15/47 batches, loss: 0.0709Epoch 9/15: [==========                    ] 16/47 batches, loss: 0.0709Epoch 9/15: [==========                    ] 17/47 batches, loss: 0.0705Epoch 9/15: [===========                   ] 18/47 batches, loss: 0.0680Epoch 9/15: [============                  ] 19/47 batches, loss: 0.0668Epoch 9/15: [============                  ] 20/47 batches, loss: 0.0678Epoch 9/15: [=============                 ] 21/47 batches, loss: 0.0674Epoch 9/15: [==============                ] 22/47 batches, loss: 0.0673Epoch 9/15: [==============                ] 23/47 batches, loss: 0.0683Epoch 9/15: [===============               ] 24/47 batches, loss: 0.0679Epoch 9/15: [===============               ] 25/47 batches, loss: 0.0684Epoch 9/15: [================              ] 26/47 batches, loss: 0.0697Epoch 9/15: [=================             ] 27/47 batches, loss: 0.0722Epoch 9/15: [=================             ] 28/47 batches, loss: 0.0713Epoch 9/15: [==================            ] 29/47 batches, loss: 0.0719Epoch 9/15: [===================           ] 30/47 batches, loss: 0.0706Epoch 9/15: [===================           ] 31/47 batches, loss: 0.0697Epoch 9/15: [====================          ] 32/47 batches, loss: 0.0695Epoch 9/15: [=====================         ] 33/47 batches, loss: 0.0690Epoch 9/15: [=====================         ] 34/47 batches, loss: 0.0691Epoch 9/15: [======================        ] 35/47 batches, loss: 0.0684Epoch 9/15: [======================        ] 36/47 batches, loss: 0.0688Epoch 9/15: [=======================       ] 37/47 batches, loss: 0.0708Epoch 9/15: [========================      ] 38/47 batches, loss: 0.0708Epoch 9/15: [========================      ] 39/47 batches, loss: 0.0710Epoch 9/15: [=========================     ] 40/47 batches, loss: 0.0703Epoch 9/15: [==========================    ] 41/47 batches, loss: 0.0705Epoch 9/15: [==========================    ] 42/47 batches, loss: 0.0712Epoch 9/15: [===========================   ] 43/47 batches, loss: 0.0705Epoch 9/15: [============================  ] 44/47 batches, loss: 0.0711Epoch 9/15: [============================  ] 45/47 batches, loss: 0.0719Epoch 9/15: [============================= ] 46/47 batches, loss: 0.0712Epoch 9/15: [==============================] 47/47 batches, loss: 0.0706
[2025-05-07 22:25:12,565][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0706
[2025-05-07 22:25:12,895][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0416, Metrics: {'mse': 0.03921208903193474, 'rmse': 0.19802042579475163, 'r2': 0.05637180805206299}
Epoch 10/15: [Epoch 10/15: [                              ] 1/47 batches, loss: 0.0401Epoch 10/15: [=                             ] 2/47 batches, loss: 0.0604Epoch 10/15: [=                             ] 3/47 batches, loss: 0.0636Epoch 10/15: [==                            ] 4/47 batches, loss: 0.0636Epoch 10/15: [===                           ] 5/47 batches, loss: 0.0767Epoch 10/15: [===                           ] 6/47 batches, loss: 0.0719Epoch 10/15: [====                          ] 7/47 batches, loss: 0.0661Epoch 10/15: [=====                         ] 8/47 batches, loss: 0.0623Epoch 10/15: [=====                         ] 9/47 batches, loss: 0.0646Epoch 10/15: [======                        ] 10/47 batches, loss: 0.0682Epoch 10/15: [=======                       ] 11/47 batches, loss: 0.0724Epoch 10/15: [=======                       ] 12/47 batches, loss: 0.0751Epoch 10/15: [========                      ] 13/47 batches, loss: 0.0793Epoch 10/15: [========                      ] 14/47 batches, loss: 0.0798Epoch 10/15: [=========                     ] 15/47 batches, loss: 0.0769Epoch 10/15: [==========                    ] 16/47 batches, loss: 0.0745Epoch 10/15: [==========                    ] 17/47 batches, loss: 0.0751Epoch 10/15: [===========                   ] 18/47 batches, loss: 0.0752Epoch 10/15: [============                  ] 19/47 batches, loss: 0.0783Epoch 10/15: [============                  ] 20/47 batches, loss: 0.0812Epoch 10/15: [=============                 ] 21/47 batches, loss: 0.0798Epoch 10/15: [==============                ] 22/47 batches, loss: 0.0788Epoch 10/15: [==============                ] 23/47 batches, loss: 0.0777Epoch 10/15: [===============               ] 24/47 batches, loss: 0.0811Epoch 10/15: [===============               ] 25/47 batches, loss: 0.0804Epoch 10/15: [================              ] 26/47 batches, loss: 0.0797Epoch 10/15: [=================             ] 27/47 batches, loss: 0.0791Epoch 10/15: [=================             ] 28/47 batches, loss: 0.0796Epoch 10/15: [==================            ] 29/47 batches, loss: 0.0787Epoch 10/15: [===================           ] 30/47 batches, loss: 0.0778Epoch 10/15: [===================           ] 31/47 batches, loss: 0.0770Epoch 10/15: [====================          ] 32/47 batches, loss: 0.0767Epoch 10/15: [=====================         ] 33/47 batches, loss: 0.0760Epoch 10/15: [=====================         ] 34/47 batches, loss: 0.0752Epoch 10/15: [======================        ] 35/47 batches, loss: 0.0746Epoch 10/15: [======================        ] 36/47 batches, loss: 0.0741Epoch 10/15: [=======================       ] 37/47 batches, loss: 0.0734Epoch 10/15: [========================      ] 38/47 batches, loss: 0.0741Epoch 10/15: [========================      ] 39/47 batches, loss: 0.0744Epoch 10/15: [=========================     ] 40/47 batches, loss: 0.0736Epoch 10/15: [==========================    ] 41/47 batches, loss: 0.0727Epoch 10/15: [==========================    ] 42/47 batches, loss: 0.0724Epoch 10/15: [===========================   ] 43/47 batches, loss: 0.0727Epoch 10/15: [============================  ] 44/47 batches, loss: 0.0730Epoch 10/15: [============================  ] 45/47 batches, loss: 0.0724Epoch 10/15: [============================= ] 46/47 batches, loss: 0.0725Epoch 10/15: [==============================] 47/47 batches, loss: 0.0712
[2025-05-07 22:25:14,796][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0712
[2025-05-07 22:25:15,085][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0426, Metrics: {'mse': 0.03987590968608856, 'rmse': 0.19968953324120062, 'r2': 0.04039710760116577}
[2025-05-07 22:25:15,085][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/47 batches, loss: 0.0461Epoch 11/15: [=                             ] 2/47 batches, loss: 0.0425Epoch 11/15: [=                             ] 3/47 batches, loss: 0.0443Epoch 11/15: [==                            ] 4/47 batches, loss: 0.0554Epoch 11/15: [===                           ] 5/47 batches, loss: 0.0602Epoch 11/15: [===                           ] 6/47 batches, loss: 0.0588Epoch 11/15: [====                          ] 7/47 batches, loss: 0.0612Epoch 11/15: [=====                         ] 8/47 batches, loss: 0.0659Epoch 11/15: [=====                         ] 9/47 batches, loss: 0.0688Epoch 11/15: [======                        ] 10/47 batches, loss: 0.0716Epoch 11/15: [=======                       ] 11/47 batches, loss: 0.0698Epoch 11/15: [=======                       ] 12/47 batches, loss: 0.0689Epoch 11/15: [========                      ] 13/47 batches, loss: 0.0701Epoch 11/15: [========                      ] 14/47 batches, loss: 0.0723Epoch 11/15: [=========                     ] 15/47 batches, loss: 0.0708Epoch 11/15: [==========                    ] 16/47 batches, loss: 0.0696Epoch 11/15: [==========                    ] 17/47 batches, loss: 0.0706Epoch 11/15: [===========                   ] 18/47 batches, loss: 0.0689Epoch 11/15: [============                  ] 19/47 batches, loss: 0.0707Epoch 11/15: [============                  ] 20/47 batches, loss: 0.0714Epoch 11/15: [=============                 ] 21/47 batches, loss: 0.0710Epoch 11/15: [==============                ] 22/47 batches, loss: 0.0702Epoch 11/15: [==============                ] 23/47 batches, loss: 0.0704Epoch 11/15: [===============               ] 24/47 batches, loss: 0.0698Epoch 11/15: [===============               ] 25/47 batches, loss: 0.0682Epoch 11/15: [================              ] 26/47 batches, loss: 0.0726Epoch 11/15: [=================             ] 27/47 batches, loss: 0.0714Epoch 11/15: [=================             ] 28/47 batches, loss: 0.0713Epoch 11/15: [==================            ] 29/47 batches, loss: 0.0727Epoch 11/15: [===================           ] 30/47 batches, loss: 0.0712Epoch 11/15: [===================           ] 31/47 batches, loss: 0.0697Epoch 11/15: [====================          ] 32/47 batches, loss: 0.0711Epoch 11/15: [=====================         ] 33/47 batches, loss: 0.0703Epoch 11/15: [=====================         ] 34/47 batches, loss: 0.0713Epoch 11/15: [======================        ] 35/47 batches, loss: 0.0713Epoch 11/15: [======================        ] 36/47 batches, loss: 0.0704Epoch 11/15: [=======================       ] 37/47 batches, loss: 0.0701Epoch 11/15: [========================      ] 38/47 batches, loss: 0.0702Epoch 11/15: [========================      ] 39/47 batches, loss: 0.0698Epoch 11/15: [=========================     ] 40/47 batches, loss: 0.0709Epoch 11/15: [==========================    ] 41/47 batches, loss: 0.0711Epoch 11/15: [==========================    ] 42/47 batches, loss: 0.0709Epoch 11/15: [===========================   ] 43/47 batches, loss: 0.0703Epoch 11/15: [============================  ] 44/47 batches, loss: 0.0706Epoch 11/15: [============================  ] 45/47 batches, loss: 0.0695Epoch 11/15: [============================= ] 46/47 batches, loss: 0.0689Epoch 11/15: [==============================] 47/47 batches, loss: 0.0709
[2025-05-07 22:25:16,583][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0709
[2025-05-07 22:25:16,930][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0422, Metrics: {'mse': 0.03962237387895584, 'rmse': 0.1990536959690923, 'r2': 0.04649841785430908}
[2025-05-07 22:25:16,931][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/47 batches, loss: 0.0411Epoch 12/15: [=                             ] 2/47 batches, loss: 0.0517Epoch 12/15: [=                             ] 3/47 batches, loss: 0.0491Epoch 12/15: [==                            ] 4/47 batches, loss: 0.0607Epoch 12/15: [===                           ] 5/47 batches, loss: 0.0508Epoch 12/15: [===                           ] 6/47 batches, loss: 0.0534Epoch 12/15: [====                          ] 7/47 batches, loss: 0.0598Epoch 12/15: [=====                         ] 8/47 batches, loss: 0.0621Epoch 12/15: [=====                         ] 9/47 batches, loss: 0.0616Epoch 12/15: [======                        ] 10/47 batches, loss: 0.0605Epoch 12/15: [=======                       ] 11/47 batches, loss: 0.0598Epoch 12/15: [=======                       ] 12/47 batches, loss: 0.0597Epoch 12/15: [========                      ] 13/47 batches, loss: 0.0636Epoch 12/15: [========                      ] 14/47 batches, loss: 0.0644Epoch 12/15: [=========                     ] 15/47 batches, loss: 0.0675Epoch 12/15: [==========                    ] 16/47 batches, loss: 0.0671Epoch 12/15: [==========                    ] 17/47 batches, loss: 0.0666Epoch 12/15: [===========                   ] 18/47 batches, loss: 0.0678Epoch 12/15: [============                  ] 19/47 batches, loss: 0.0662Epoch 12/15: [============                  ] 20/47 batches, loss: 0.0667Epoch 12/15: [=============                 ] 21/47 batches, loss: 0.0664Epoch 12/15: [==============                ] 22/47 batches, loss: 0.0657Epoch 12/15: [==============                ] 23/47 batches, loss: 0.0654Epoch 12/15: [===============               ] 24/47 batches, loss: 0.0679Epoch 12/15: [===============               ] 25/47 batches, loss: 0.0690Epoch 12/15: [================              ] 26/47 batches, loss: 0.0678Epoch 12/15: [=================             ] 27/47 batches, loss: 0.0680Epoch 12/15: [=================             ] 28/47 batches, loss: 0.0672Epoch 12/15: [==================            ] 29/47 batches, loss: 0.0668Epoch 12/15: [===================           ] 30/47 batches, loss: 0.0678Epoch 12/15: [===================           ] 31/47 batches, loss: 0.0671Epoch 12/15: [====================          ] 32/47 batches, loss: 0.0667Epoch 12/15: [=====================         ] 33/47 batches, loss: 0.0658Epoch 12/15: [=====================         ] 34/47 batches, loss: 0.0665Epoch 12/15: [======================        ] 35/47 batches, loss: 0.0658Epoch 12/15: [======================        ] 36/47 batches, loss: 0.0650Epoch 12/15: [=======================       ] 37/47 batches, loss: 0.0652Epoch 12/15: [========================      ] 38/47 batches, loss: 0.0663Epoch 12/15: [========================      ] 39/47 batches, loss: 0.0651Epoch 12/15: [=========================     ] 40/47 batches, loss: 0.0650Epoch 12/15: [==========================    ] 41/47 batches, loss: 0.0650Epoch 12/15: [==========================    ] 42/47 batches, loss: 0.0642Epoch 12/15: [===========================   ] 43/47 batches, loss: 0.0642Epoch 12/15: [============================  ] 44/47 batches, loss: 0.0633Epoch 12/15: [============================  ] 45/47 batches, loss: 0.0637Epoch 12/15: [============================= ] 46/47 batches, loss: 0.0652Epoch 12/15: [==============================] 47/47 batches, loss: 0.0646
[2025-05-07 22:25:18,428][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0646
[2025-05-07 22:25:18,764][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0434, Metrics: {'mse': 0.04127354919910431, 'rmse': 0.20315892596463564, 'r2': 0.00676339864730835}
[2025-05-07 22:25:18,764][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/47 batches, loss: 0.0394Epoch 13/15: [=                             ] 2/47 batches, loss: 0.0710Epoch 13/15: [=                             ] 3/47 batches, loss: 0.0615Epoch 13/15: [==                            ] 4/47 batches, loss: 0.0711Epoch 13/15: [===                           ] 5/47 batches, loss: 0.0687Epoch 13/15: [===                           ] 6/47 batches, loss: 0.0667Epoch 13/15: [====                          ] 7/47 batches, loss: 0.0684Epoch 13/15: [=====                         ] 8/47 batches, loss: 0.0686Epoch 13/15: [=====                         ] 9/47 batches, loss: 0.0741Epoch 13/15: [======                        ] 10/47 batches, loss: 0.0710Epoch 13/15: [=======                       ] 11/47 batches, loss: 0.0692Epoch 13/15: [=======                       ] 12/47 batches, loss: 0.0705Epoch 13/15: [========                      ] 13/47 batches, loss: 0.0709Epoch 13/15: [========                      ] 14/47 batches, loss: 0.0705Epoch 13/15: [=========                     ] 15/47 batches, loss: 0.0693Epoch 13/15: [==========                    ] 16/47 batches, loss: 0.0669Epoch 13/15: [==========                    ] 17/47 batches, loss: 0.0674Epoch 13/15: [===========                   ] 18/47 batches, loss: 0.0667Epoch 13/15: [============                  ] 19/47 batches, loss: 0.0650Epoch 13/15: [============                  ] 20/47 batches, loss: 0.0638Epoch 13/15: [=============                 ] 21/47 batches, loss: 0.0664Epoch 13/15: [==============                ] 22/47 batches, loss: 0.0657Epoch 13/15: [==============                ] 23/47 batches, loss: 0.0666Epoch 13/15: [===============               ] 24/47 batches, loss: 0.0657Epoch 13/15: [===============               ] 25/47 batches, loss: 0.0663Epoch 13/15: [================              ] 26/47 batches, loss: 0.0674Epoch 13/15: [=================             ] 27/47 batches, loss: 0.0686Epoch 13/15: [=================             ] 28/47 batches, loss: 0.0682Epoch 13/15: [==================            ] 29/47 batches, loss: 0.0696Epoch 13/15: [===================           ] 30/47 batches, loss: 0.0688Epoch 13/15: [===================           ] 31/47 batches, loss: 0.0685Epoch 13/15: [====================          ] 32/47 batches, loss: 0.0682Epoch 13/15: [=====================         ] 33/47 batches, loss: 0.0704Epoch 13/15: [=====================         ] 34/47 batches, loss: 0.0696Epoch 13/15: [======================        ] 35/47 batches, loss: 0.0688Epoch 13/15: [======================        ] 36/47 batches, loss: 0.0683Epoch 13/15: [=======================       ] 37/47 batches, loss: 0.0678Epoch 13/15: [========================      ] 38/47 batches, loss: 0.0672Epoch 13/15: [========================      ] 39/47 batches, loss: 0.0672Epoch 13/15: [=========================     ] 40/47 batches, loss: 0.0664Epoch 13/15: [==========================    ] 41/47 batches, loss: 0.0653Epoch 13/15: [==========================    ] 42/47 batches, loss: 0.0657Epoch 13/15: [===========================   ] 43/47 batches, loss: 0.0647Epoch 13/15: [============================  ] 44/47 batches, loss: 0.0646Epoch 13/15: [============================  ] 45/47 batches, loss: 0.0642Epoch 13/15: [============================= ] 46/47 batches, loss: 0.0649Epoch 13/15: [==============================] 47/47 batches, loss: 0.0636
[2025-05-07 22:25:20,303][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0636
[2025-05-07 22:25:20,648][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0430, Metrics: {'mse': 0.04069288447499275, 'rmse': 0.2017247740734706, 'r2': 0.02073681354522705}
[2025-05-07 22:25:20,648][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:25:20,648][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 13
[2025-05-07 22:25:20,649][src.training.lm_trainer][INFO] - Training completed in 28.37 seconds
[2025-05-07 22:25:20,649][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:25:22,995][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.0414419062435627, 'rmse': 0.20357285242281864, 'r2': -0.21012747287750244}
[2025-05-07 22:25:22,996][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03921208903193474, 'rmse': 0.19802042579475163, 'r2': 0.05637180805206299}
[2025-05-07 22:25:22,996][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06349789351224899, 'rmse': 0.2519878836615939, 'r2': -0.588458776473999}
[2025-05-07 22:25:25,057][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control2/layer2/ko/ko/model.pt
[2025-05-07 22:25:25,058][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▃▂▂▂▁▁▁
wandb:     best_val_mse █▆▃▂▂▂▁▁▁
wandb:      best_val_r2 ▁▃▆▇▇▇███
wandb:    best_val_rmse █▆▃▂▂▂▁▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▃▄▄▄▄▄▄▄▄▄
wandb:       train_loss █▄▃▃▂▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▃▂▂▂▁▁▁▂▁▂▂
wandb:          val_mse █▆▃▂▂▂▁▁▁▁▁▂▂
wandb:           val_r2 ▁▃▆▇▇▇█████▇▇
wandb:         val_rmse █▆▃▂▂▂▁▁▁▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04156
wandb:     best_val_mse 0.03921
wandb:      best_val_r2 0.05637
wandb:    best_val_rmse 0.19802
wandb: early_stop_epoch 13
wandb:            epoch 13
wandb:   final_test_mse 0.0635
wandb:    final_test_r2 -0.58846
wandb:  final_test_rmse 0.25199
wandb:  final_train_mse 0.04144
wandb:   final_train_r2 -0.21013
wandb: final_train_rmse 0.20357
wandb:    final_val_mse 0.03921
wandb:     final_val_r2 0.05637
wandb:   final_val_rmse 0.19802
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06363
wandb:       train_time 28.36827
wandb:         val_loss 0.04295
wandb:          val_mse 0.04069
wandb:           val_r2 0.02074
wandb:         val_rmse 0.20172
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_222433-h5t0h2vz
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_222433-h5t0h2vz/logs
Experiment probe_layer2_lexical_density_control2_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control2/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_lexical_density_control3_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_lexical_density_control3_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control3/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=lexical_density"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:25:49,889][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control3/layer2/ko
experiment_name: probe_layer2_lexical_density_control3_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: lexical_density
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:25:49,889][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:25:49,889][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 22:25:49,890][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:25:49,890][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:25:49,894][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:25:49,894][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 22:25:49,894][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:25:52,722][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'lexical_density'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:25:55,223][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:25:55,223][src.data.datasets][INFO] - Loading 'control_lexical_density_seed3' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:25:55,316][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_lexical_density_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:49:28 2025).
[2025-05-07 22:25:55,370][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_lexical_density_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:49:28 2025).
[2025-05-07 22:25:55,527][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:25:55,533][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:25:55,534][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:25:55,544][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:25:55,610][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:25:55,718][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:25:55,743][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:25:55,745][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:25:55,745][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:25:55,746][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:25:55,800][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:25:55,872][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:25:55,907][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:25:55,909][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:25:55,909][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:25:55,910][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:25:55,910][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:25:55,910][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 22:25:55,910][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 22:25:55,910][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 22:25:55,910][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:25:55,911][src.data.datasets][INFO] -   Mean: 0.8900, Std: 0.1851
[2025-05-07 22:25:55,911][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:25:55,911][src.data.datasets][INFO] - Sample label: 1.0
[2025-05-07 22:25:55,911][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:25:55,911][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 22:25:55,911][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 22:25:55,911][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 22:25:55,911][src.data.datasets][INFO] -   Min: 0.2860, Max: 1.0000
[2025-05-07 22:25:55,912][src.data.datasets][INFO] -   Mean: 0.8166, Std: 0.2038
[2025-05-07 22:25:55,912][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:25:55,912][src.data.datasets][INFO] - Sample label: 0.6669999957084656
[2025-05-07 22:25:55,912][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:25:55,912][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 22:25:55,912][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 22:25:55,912][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 22:25:55,912][src.data.datasets][INFO] -   Min: 0.1000, Max: 1.0000
[2025-05-07 22:25:55,912][src.data.datasets][INFO] -   Mean: 0.6309, Std: 0.1999
[2025-05-07 22:25:55,912][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:25:55,912][src.data.datasets][INFO] - Sample label: 0.8640000224113464
[2025-05-07 22:25:55,913][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:25:55,913][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:25:55,913][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:25:55,913][__main__][INFO] - Using model type: lm_probe for submetric lexical_density
[2025-05-07 22:25:55,913][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:26:02,657][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:26:02,658][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:26:02,658][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:26:02,658][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:26:02,661][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:26:02,661][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:26:02,661][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:26:02,662][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:26:02,662][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:26:02,663][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:26:02,663][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.7397Epoch 1/15: [=                             ] 2/47 batches, loss: 0.8176Epoch 1/15: [=                             ] 3/47 batches, loss: 0.6945Epoch 1/15: [==                            ] 4/47 batches, loss: 0.6401Epoch 1/15: [===                           ] 5/47 batches, loss: 0.6382Epoch 1/15: [===                           ] 6/47 batches, loss: 0.5744Epoch 1/15: [====                          ] 7/47 batches, loss: 0.5494Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.5470Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.5484Epoch 1/15: [======                        ] 10/47 batches, loss: 0.5293Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.5113Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.5061Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4947Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4935Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.4810Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4820Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.4676Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4815Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4756Epoch 1/15: [============                  ] 20/47 batches, loss: 0.4668Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4667Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4612Epoch 1/15: [==============                ] 23/47 batches, loss: 0.4457Epoch 1/15: [===============               ] 24/47 batches, loss: 0.4376Epoch 1/15: [===============               ] 25/47 batches, loss: 0.4372Epoch 1/15: [================              ] 26/47 batches, loss: 0.4339Epoch 1/15: [=================             ] 27/47 batches, loss: 0.4331Epoch 1/15: [=================             ] 28/47 batches, loss: 0.4239Epoch 1/15: [==================            ] 29/47 batches, loss: 0.4228Epoch 1/15: [===================           ] 30/47 batches, loss: 0.4188Epoch 1/15: [===================           ] 31/47 batches, loss: 0.4116Epoch 1/15: [====================          ] 32/47 batches, loss: 0.4038Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.4024Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.4002Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3961Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3929Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3864Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3845Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3794Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3794Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3775Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3740Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3737Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3709Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3718Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3657Epoch 1/15: [==============================] 47/47 batches, loss: 0.3685
[2025-05-07 22:26:08,023][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3685
[2025-05-07 22:26:08,319][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0668, Metrics: {'mse': 0.060743920505046844, 'rmse': 0.24646281769274417, 'r2': -0.46178579330444336}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2657Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2018Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2123Epoch 2/15: [==                            ] 4/47 batches, loss: 0.1932Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2170Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2398Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2274Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2339Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2344Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2364Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2408Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2410Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2410Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2394Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2416Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2409Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2369Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2486Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2445Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2391Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2376Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2352Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2331Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2317Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2297Epoch 2/15: [================              ] 26/47 batches, loss: 0.2275Epoch 2/15: [=================             ] 27/47 batches, loss: 0.2232Epoch 2/15: [=================             ] 28/47 batches, loss: 0.2203Epoch 2/15: [==================            ] 29/47 batches, loss: 0.2189Epoch 2/15: [===================           ] 30/47 batches, loss: 0.2189Epoch 2/15: [===================           ] 31/47 batches, loss: 0.2192Epoch 2/15: [====================          ] 32/47 batches, loss: 0.2194Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.2212Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.2204Epoch 2/15: [======================        ] 35/47 batches, loss: 0.2163Epoch 2/15: [======================        ] 36/47 batches, loss: 0.2166Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.2171Epoch 2/15: [========================      ] 38/47 batches, loss: 0.2123Epoch 2/15: [========================      ] 39/47 batches, loss: 0.2107Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.2081Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.2070Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.2049Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.2019Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1995Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1983Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1962Epoch 2/15: [==============================] 47/47 batches, loss: 0.1949
[2025-05-07 22:26:10,191][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1949
[2025-05-07 22:26:10,523][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0535, Metrics: {'mse': 0.04899546504020691, 'rmse': 0.22134919254473667, 'r2': -0.17906248569488525}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1594Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1316Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1343Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1158Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1234Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1430Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1419Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1397Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1455Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1377Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1371Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1388Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1431Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1395Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1393Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1386Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1398Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1458Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1441Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1440Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1449Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1412Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1425Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1439Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1428Epoch 3/15: [================              ] 26/47 batches, loss: 0.1431Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1460Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1444Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1421Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1427Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1454Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1439Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1448Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1431Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1427Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1430Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1428Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1414Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1394Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1393Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1376Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1376Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1366Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1371Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1393Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1383Epoch 3/15: [==============================] 47/47 batches, loss: 0.1446
[2025-05-07 22:26:12,459][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1446
[2025-05-07 22:26:12,728][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0450, Metrics: {'mse': 0.0414358451962471, 'rmse': 0.20355796519971184, 'r2': 0.0028577446937561035}
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1765Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1457Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1143Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1137Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1229Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1224Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1329Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1396Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1391Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1378Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1439Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1385Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1413Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1375Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1364Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1327Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1320Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1281Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1312Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1347Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1354Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1361Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1383Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1374Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1365Epoch 4/15: [================              ] 26/47 batches, loss: 0.1354Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1343Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1350Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1364Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1345Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1388Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1380Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1388Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1409Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1448Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1441Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1439Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1430Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1407Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1398Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1409Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1408Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1384Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1388Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1379Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1371Epoch 4/15: [==============================] 47/47 batches, loss: 0.1354
[2025-05-07 22:26:14,609][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1354
[2025-05-07 22:26:14,914][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0435, Metrics: {'mse': 0.040806107223033905, 'rmse': 0.20200521583125994, 'r2': 0.018012166023254395}
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.0907Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1128Epoch 5/15: [=                             ] 3/47 batches, loss: 0.0943Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0899Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0889Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0920Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0961Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.1010Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.1008Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0965Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.1009Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.1099Epoch 5/15: [========                      ] 13/47 batches, loss: 0.1086Epoch 5/15: [========                      ] 14/47 batches, loss: 0.1068Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.1067Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.1063Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.1064Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.1061Epoch 5/15: [============                  ] 19/47 batches, loss: 0.1063Epoch 5/15: [============                  ] 20/47 batches, loss: 0.1077Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.1095Epoch 5/15: [==============                ] 22/47 batches, loss: 0.1093Epoch 5/15: [==============                ] 23/47 batches, loss: 0.1085Epoch 5/15: [===============               ] 24/47 batches, loss: 0.1119Epoch 5/15: [===============               ] 25/47 batches, loss: 0.1097Epoch 5/15: [================              ] 26/47 batches, loss: 0.1069Epoch 5/15: [=================             ] 27/47 batches, loss: 0.1073Epoch 5/15: [=================             ] 28/47 batches, loss: 0.1059Epoch 5/15: [==================            ] 29/47 batches, loss: 0.1050Epoch 5/15: [===================           ] 30/47 batches, loss: 0.1036Epoch 5/15: [===================           ] 31/47 batches, loss: 0.1045Epoch 5/15: [====================          ] 32/47 batches, loss: 0.1036Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.1021Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.1012Epoch 5/15: [======================        ] 35/47 batches, loss: 0.1044Epoch 5/15: [======================        ] 36/47 batches, loss: 0.1029Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.1012Epoch 5/15: [========================      ] 38/47 batches, loss: 0.1038Epoch 5/15: [========================      ] 39/47 batches, loss: 0.1063Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.1064Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.1058Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.1045Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.1046Epoch 5/15: [============================  ] 44/47 batches, loss: 0.1052Epoch 5/15: [============================  ] 45/47 batches, loss: 0.1060Epoch 5/15: [============================= ] 46/47 batches, loss: 0.1056Epoch 5/15: [==============================] 47/47 batches, loss: 0.1068
[2025-05-07 22:26:16,784][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1068
[2025-05-07 22:26:17,057][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0427, Metrics: {'mse': 0.04010998457670212, 'rmse': 0.20027477269167507, 'r2': 0.03476417064666748}
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.0522Epoch 6/15: [=                             ] 2/47 batches, loss: 0.0663Epoch 6/15: [=                             ] 3/47 batches, loss: 0.0973Epoch 6/15: [==                            ] 4/47 batches, loss: 0.0951Epoch 6/15: [===                           ] 5/47 batches, loss: 0.0961Epoch 6/15: [===                           ] 6/47 batches, loss: 0.0942Epoch 6/15: [====                          ] 7/47 batches, loss: 0.0925Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.0989Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.0942Epoch 6/15: [======                        ] 10/47 batches, loss: 0.1096Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.1188Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.1197Epoch 6/15: [========                      ] 13/47 batches, loss: 0.1161Epoch 6/15: [========                      ] 14/47 batches, loss: 0.1137Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.1127Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.1160Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.1143Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.1128Epoch 6/15: [============                  ] 19/47 batches, loss: 0.1095Epoch 6/15: [============                  ] 20/47 batches, loss: 0.1074Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.1045Epoch 6/15: [==============                ] 22/47 batches, loss: 0.1043Epoch 6/15: [==============                ] 23/47 batches, loss: 0.1006Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0983Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0975Epoch 6/15: [================              ] 26/47 batches, loss: 0.0970Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0961Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0955Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0955Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0979Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0979Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0979Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0984Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0981Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0989Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0985Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0996Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0980Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0971Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0974Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0968Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0967Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0958Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0949Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0946Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0948Epoch 6/15: [==============================] 47/47 batches, loss: 0.0949
[2025-05-07 22:26:18,936][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0949
[2025-05-07 22:26:19,237][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0422, Metrics: {'mse': 0.03974798694252968, 'rmse': 0.19936897186505648, 'r2': 0.043475568294525146}
Epoch 7/15: [Epoch 7/15: [                              ] 1/47 batches, loss: 0.1105Epoch 7/15: [=                             ] 2/47 batches, loss: 0.1141Epoch 7/15: [=                             ] 3/47 batches, loss: 0.1090Epoch 7/15: [==                            ] 4/47 batches, loss: 0.0917Epoch 7/15: [===                           ] 5/47 batches, loss: 0.0928Epoch 7/15: [===                           ] 6/47 batches, loss: 0.0878Epoch 7/15: [====                          ] 7/47 batches, loss: 0.0920Epoch 7/15: [=====                         ] 8/47 batches, loss: 0.0918Epoch 7/15: [=====                         ] 9/47 batches, loss: 0.0890Epoch 7/15: [======                        ] 10/47 batches, loss: 0.0940Epoch 7/15: [=======                       ] 11/47 batches, loss: 0.0915Epoch 7/15: [=======                       ] 12/47 batches, loss: 0.0911Epoch 7/15: [========                      ] 13/47 batches, loss: 0.0928Epoch 7/15: [========                      ] 14/47 batches, loss: 0.0894Epoch 7/15: [=========                     ] 15/47 batches, loss: 0.0874Epoch 7/15: [==========                    ] 16/47 batches, loss: 0.0888Epoch 7/15: [==========                    ] 17/47 batches, loss: 0.0873Epoch 7/15: [===========                   ] 18/47 batches, loss: 0.0887Epoch 7/15: [============                  ] 19/47 batches, loss: 0.0892Epoch 7/15: [============                  ] 20/47 batches, loss: 0.0882Epoch 7/15: [=============                 ] 21/47 batches, loss: 0.0886Epoch 7/15: [==============                ] 22/47 batches, loss: 0.0874Epoch 7/15: [==============                ] 23/47 batches, loss: 0.0881Epoch 7/15: [===============               ] 24/47 batches, loss: 0.0881Epoch 7/15: [===============               ] 25/47 batches, loss: 0.0900Epoch 7/15: [================              ] 26/47 batches, loss: 0.0883Epoch 7/15: [=================             ] 27/47 batches, loss: 0.0881Epoch 7/15: [=================             ] 28/47 batches, loss: 0.0890Epoch 7/15: [==================            ] 29/47 batches, loss: 0.0906Epoch 7/15: [===================           ] 30/47 batches, loss: 0.0908Epoch 7/15: [===================           ] 31/47 batches, loss: 0.0898Epoch 7/15: [====================          ] 32/47 batches, loss: 0.0896Epoch 7/15: [=====================         ] 33/47 batches, loss: 0.0886Epoch 7/15: [=====================         ] 34/47 batches, loss: 0.0883Epoch 7/15: [======================        ] 35/47 batches, loss: 0.0873Epoch 7/15: [======================        ] 36/47 batches, loss: 0.0867Epoch 7/15: [=======================       ] 37/47 batches, loss: 0.0884Epoch 7/15: [========================      ] 38/47 batches, loss: 0.0875Epoch 7/15: [========================      ] 39/47 batches, loss: 0.0887Epoch 7/15: [=========================     ] 40/47 batches, loss: 0.0889Epoch 7/15: [==========================    ] 41/47 batches, loss: 0.0887Epoch 7/15: [==========================    ] 42/47 batches, loss: 0.0883Epoch 7/15: [===========================   ] 43/47 batches, loss: 0.0881Epoch 7/15: [============================  ] 44/47 batches, loss: 0.0875Epoch 7/15: [============================  ] 45/47 batches, loss: 0.0865Epoch 7/15: [============================= ] 46/47 batches, loss: 0.0871Epoch 7/15: [==============================] 47/47 batches, loss: 0.0875
[2025-05-07 22:26:21,133][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0875
[2025-05-07 22:26:21,427][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0424, Metrics: {'mse': 0.04005853831768036, 'rmse': 0.20014629229061517, 'r2': 0.03600221872329712}
[2025-05-07 22:26:21,428][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/47 batches, loss: 0.0567Epoch 8/15: [=                             ] 2/47 batches, loss: 0.0749Epoch 8/15: [=                             ] 3/47 batches, loss: 0.0673Epoch 8/15: [==                            ] 4/47 batches, loss: 0.0794Epoch 8/15: [===                           ] 5/47 batches, loss: 0.0812Epoch 8/15: [===                           ] 6/47 batches, loss: 0.0766Epoch 8/15: [====                          ] 7/47 batches, loss: 0.0778Epoch 8/15: [=====                         ] 8/47 batches, loss: 0.0744Epoch 8/15: [=====                         ] 9/47 batches, loss: 0.0719Epoch 8/15: [======                        ] 10/47 batches, loss: 0.0715Epoch 8/15: [=======                       ] 11/47 batches, loss: 0.0738Epoch 8/15: [=======                       ] 12/47 batches, loss: 0.0736Epoch 8/15: [========                      ] 13/47 batches, loss: 0.0731Epoch 8/15: [========                      ] 14/47 batches, loss: 0.0701Epoch 8/15: [=========                     ] 15/47 batches, loss: 0.0709Epoch 8/15: [==========                    ] 16/47 batches, loss: 0.0731Epoch 8/15: [==========                    ] 17/47 batches, loss: 0.0719Epoch 8/15: [===========                   ] 18/47 batches, loss: 0.0706Epoch 8/15: [============                  ] 19/47 batches, loss: 0.0716Epoch 8/15: [============                  ] 20/47 batches, loss: 0.0711Epoch 8/15: [=============                 ] 21/47 batches, loss: 0.0707Epoch 8/15: [==============                ] 22/47 batches, loss: 0.0699Epoch 8/15: [==============                ] 23/47 batches, loss: 0.0723Epoch 8/15: [===============               ] 24/47 batches, loss: 0.0736Epoch 8/15: [===============               ] 25/47 batches, loss: 0.0738Epoch 8/15: [================              ] 26/47 batches, loss: 0.0740Epoch 8/15: [=================             ] 27/47 batches, loss: 0.0741Epoch 8/15: [=================             ] 28/47 batches, loss: 0.0732Epoch 8/15: [==================            ] 29/47 batches, loss: 0.0727Epoch 8/15: [===================           ] 30/47 batches, loss: 0.0710Epoch 8/15: [===================           ] 31/47 batches, loss: 0.0715Epoch 8/15: [====================          ] 32/47 batches, loss: 0.0715Epoch 8/15: [=====================         ] 33/47 batches, loss: 0.0713Epoch 8/15: [=====================         ] 34/47 batches, loss: 0.0707Epoch 8/15: [======================        ] 35/47 batches, loss: 0.0707Epoch 8/15: [======================        ] 36/47 batches, loss: 0.0712Epoch 8/15: [=======================       ] 37/47 batches, loss: 0.0724Epoch 8/15: [========================      ] 38/47 batches, loss: 0.0720Epoch 8/15: [========================      ] 39/47 batches, loss: 0.0719Epoch 8/15: [=========================     ] 40/47 batches, loss: 0.0729Epoch 8/15: [==========================    ] 41/47 batches, loss: 0.0751Epoch 8/15: [==========================    ] 42/47 batches, loss: 0.0747Epoch 8/15: [===========================   ] 43/47 batches, loss: 0.0743Epoch 8/15: [============================  ] 44/47 batches, loss: 0.0742Epoch 8/15: [============================  ] 45/47 batches, loss: 0.0745Epoch 8/15: [============================= ] 46/47 batches, loss: 0.0743Epoch 8/15: [==============================] 47/47 batches, loss: 0.0730
[2025-05-07 22:26:22,967][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0730
[2025-05-07 22:26:23,258][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0427, Metrics: {'mse': 0.039961881935596466, 'rmse': 0.19990468212524803, 'r2': 0.03832828998565674}
[2025-05-07 22:26:23,259][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/47 batches, loss: 0.0857Epoch 9/15: [=                             ] 2/47 batches, loss: 0.0760Epoch 9/15: [=                             ] 3/47 batches, loss: 0.0737Epoch 9/15: [==                            ] 4/47 batches, loss: 0.0710Epoch 9/15: [===                           ] 5/47 batches, loss: 0.0759Epoch 9/15: [===                           ] 6/47 batches, loss: 0.0823Epoch 9/15: [====                          ] 7/47 batches, loss: 0.0795Epoch 9/15: [=====                         ] 8/47 batches, loss: 0.0885Epoch 9/15: [=====                         ] 9/47 batches, loss: 0.0859Epoch 9/15: [======                        ] 10/47 batches, loss: 0.0821Epoch 9/15: [=======                       ] 11/47 batches, loss: 0.0812Epoch 9/15: [=======                       ] 12/47 batches, loss: 0.0775Epoch 9/15: [========                      ] 13/47 batches, loss: 0.0734Epoch 9/15: [========                      ] 14/47 batches, loss: 0.0776Epoch 9/15: [=========                     ] 15/47 batches, loss: 0.0779Epoch 9/15: [==========                    ] 16/47 batches, loss: 0.0761Epoch 9/15: [==========                    ] 17/47 batches, loss: 0.0777Epoch 9/15: [===========                   ] 18/47 batches, loss: 0.0771Epoch 9/15: [============                  ] 19/47 batches, loss: 0.0774Epoch 9/15: [============                  ] 20/47 batches, loss: 0.0777Epoch 9/15: [=============                 ] 21/47 batches, loss: 0.0781Epoch 9/15: [==============                ] 22/47 batches, loss: 0.0770Epoch 9/15: [==============                ] 23/47 batches, loss: 0.0750Epoch 9/15: [===============               ] 24/47 batches, loss: 0.0745Epoch 9/15: [===============               ] 25/47 batches, loss: 0.0759Epoch 9/15: [================              ] 26/47 batches, loss: 0.0752Epoch 9/15: [=================             ] 27/47 batches, loss: 0.0746Epoch 9/15: [=================             ] 28/47 batches, loss: 0.0764Epoch 9/15: [==================            ] 29/47 batches, loss: 0.0769Epoch 9/15: [===================           ] 30/47 batches, loss: 0.0755Epoch 9/15: [===================           ] 31/47 batches, loss: 0.0746Epoch 9/15: [====================          ] 32/47 batches, loss: 0.0737Epoch 9/15: [=====================         ] 33/47 batches, loss: 0.0759Epoch 9/15: [=====================         ] 34/47 batches, loss: 0.0756Epoch 9/15: [======================        ] 35/47 batches, loss: 0.0756Epoch 9/15: [======================        ] 36/47 batches, loss: 0.0746Epoch 9/15: [=======================       ] 37/47 batches, loss: 0.0764Epoch 9/15: [========================      ] 38/47 batches, loss: 0.0757Epoch 9/15: [========================      ] 39/47 batches, loss: 0.0753Epoch 9/15: [=========================     ] 40/47 batches, loss: 0.0747Epoch 9/15: [==========================    ] 41/47 batches, loss: 0.0739Epoch 9/15: [==========================    ] 42/47 batches, loss: 0.0737Epoch 9/15: [===========================   ] 43/47 batches, loss: 0.0736Epoch 9/15: [============================  ] 44/47 batches, loss: 0.0747Epoch 9/15: [============================  ] 45/47 batches, loss: 0.0747Epoch 9/15: [============================= ] 46/47 batches, loss: 0.0751Epoch 9/15: [==============================] 47/47 batches, loss: 0.0756
[2025-05-07 22:26:24,785][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0756
[2025-05-07 22:26:25,098][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0422, Metrics: {'mse': 0.039666395634412766, 'rmse': 0.19916424286104362, 'r2': 0.045439064502716064}
Epoch 10/15: [Epoch 10/15: [                              ] 1/47 batches, loss: 0.1030Epoch 10/15: [=                             ] 2/47 batches, loss: 0.0895Epoch 10/15: [=                             ] 3/47 batches, loss: 0.0884Epoch 10/15: [==                            ] 4/47 batches, loss: 0.0791Epoch 10/15: [===                           ] 5/47 batches, loss: 0.0784Epoch 10/15: [===                           ] 6/47 batches, loss: 0.0743Epoch 10/15: [====                          ] 7/47 batches, loss: 0.0693Epoch 10/15: [=====                         ] 8/47 batches, loss: 0.0696Epoch 10/15: [=====                         ] 9/47 batches, loss: 0.0680Epoch 10/15: [======                        ] 10/47 batches, loss: 0.0716Epoch 10/15: [=======                       ] 11/47 batches, loss: 0.0757Epoch 10/15: [=======                       ] 12/47 batches, loss: 0.0729Epoch 10/15: [========                      ] 13/47 batches, loss: 0.0717Epoch 10/15: [========                      ] 14/47 batches, loss: 0.0769Epoch 10/15: [=========                     ] 15/47 batches, loss: 0.0789Epoch 10/15: [==========                    ] 16/47 batches, loss: 0.0759Epoch 10/15: [==========                    ] 17/47 batches, loss: 0.0746Epoch 10/15: [===========                   ] 18/47 batches, loss: 0.0737Epoch 10/15: [============                  ] 19/47 batches, loss: 0.0731Epoch 10/15: [============                  ] 20/47 batches, loss: 0.0750Epoch 10/15: [=============                 ] 21/47 batches, loss: 0.0754Epoch 10/15: [==============                ] 22/47 batches, loss: 0.0733Epoch 10/15: [==============                ] 23/47 batches, loss: 0.0711Epoch 10/15: [===============               ] 24/47 batches, loss: 0.0715Epoch 10/15: [===============               ] 25/47 batches, loss: 0.0711Epoch 10/15: [================              ] 26/47 batches, loss: 0.0698Epoch 10/15: [=================             ] 27/47 batches, loss: 0.0694Epoch 10/15: [=================             ] 28/47 batches, loss: 0.0694Epoch 10/15: [==================            ] 29/47 batches, loss: 0.0684Epoch 10/15: [===================           ] 30/47 batches, loss: 0.0689Epoch 10/15: [===================           ] 31/47 batches, loss: 0.0676Epoch 10/15: [====================          ] 32/47 batches, loss: 0.0688Epoch 10/15: [=====================         ] 33/47 batches, loss: 0.0687Epoch 10/15: [=====================         ] 34/47 batches, loss: 0.0676Epoch 10/15: [======================        ] 35/47 batches, loss: 0.0673Epoch 10/15: [======================        ] 36/47 batches, loss: 0.0677Epoch 10/15: [=======================       ] 37/47 batches, loss: 0.0680Epoch 10/15: [========================      ] 38/47 batches, loss: 0.0691Epoch 10/15: [========================      ] 39/47 batches, loss: 0.0688Epoch 10/15: [=========================     ] 40/47 batches, loss: 0.0705Epoch 10/15: [==========================    ] 41/47 batches, loss: 0.0697Epoch 10/15: [==========================    ] 42/47 batches, loss: 0.0706Epoch 10/15: [===========================   ] 43/47 batches, loss: 0.0702Epoch 10/15: [============================  ] 44/47 batches, loss: 0.0704Epoch 10/15: [============================  ] 45/47 batches, loss: 0.0703Epoch 10/15: [============================= ] 46/47 batches, loss: 0.0711Epoch 10/15: [==============================] 47/47 batches, loss: 0.0698
[2025-05-07 22:26:27,038][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0698
[2025-05-07 22:26:27,324][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0424, Metrics: {'mse': 0.0399479866027832, 'rmse': 0.19986992420767863, 'r2': 0.038662612438201904}
[2025-05-07 22:26:27,324][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/47 batches, loss: 0.1083Epoch 11/15: [=                             ] 2/47 batches, loss: 0.0760Epoch 11/15: [=                             ] 3/47 batches, loss: 0.0894Epoch 11/15: [==                            ] 4/47 batches, loss: 0.0928Epoch 11/15: [===                           ] 5/47 batches, loss: 0.0862Epoch 11/15: [===                           ] 6/47 batches, loss: 0.0818Epoch 11/15: [====                          ] 7/47 batches, loss: 0.0796Epoch 11/15: [=====                         ] 8/47 batches, loss: 0.0755Epoch 11/15: [=====                         ] 9/47 batches, loss: 0.0756Epoch 11/15: [======                        ] 10/47 batches, loss: 0.0768Epoch 11/15: [=======                       ] 11/47 batches, loss: 0.0750Epoch 11/15: [=======                       ] 12/47 batches, loss: 0.0721Epoch 11/15: [========                      ] 13/47 batches, loss: 0.0721Epoch 11/15: [========                      ] 14/47 batches, loss: 0.0693Epoch 11/15: [=========                     ] 15/47 batches, loss: 0.0696Epoch 11/15: [==========                    ] 16/47 batches, loss: 0.0688Epoch 11/15: [==========                    ] 17/47 batches, loss: 0.0666Epoch 11/15: [===========                   ] 18/47 batches, loss: 0.0662Epoch 11/15: [============                  ] 19/47 batches, loss: 0.0676Epoch 11/15: [============                  ] 20/47 batches, loss: 0.0727Epoch 11/15: [=============                 ] 21/47 batches, loss: 0.0726Epoch 11/15: [==============                ] 22/47 batches, loss: 0.0750Epoch 11/15: [==============                ] 23/47 batches, loss: 0.0747Epoch 11/15: [===============               ] 24/47 batches, loss: 0.0740Epoch 11/15: [===============               ] 25/47 batches, loss: 0.0723Epoch 11/15: [================              ] 26/47 batches, loss: 0.0716Epoch 11/15: [=================             ] 27/47 batches, loss: 0.0705Epoch 11/15: [=================             ] 28/47 batches, loss: 0.0701Epoch 11/15: [==================            ] 29/47 batches, loss: 0.0699Epoch 11/15: [===================           ] 30/47 batches, loss: 0.0699Epoch 11/15: [===================           ] 31/47 batches, loss: 0.0688Epoch 11/15: [====================          ] 32/47 batches, loss: 0.0690Epoch 11/15: [=====================         ] 33/47 batches, loss: 0.0678Epoch 11/15: [=====================         ] 34/47 batches, loss: 0.0688Epoch 11/15: [======================        ] 35/47 batches, loss: 0.0699Epoch 11/15: [======================        ] 36/47 batches, loss: 0.0698Epoch 11/15: [=======================       ] 37/47 batches, loss: 0.0697Epoch 11/15: [========================      ] 38/47 batches, loss: 0.0692Epoch 11/15: [========================      ] 39/47 batches, loss: 0.0688Epoch 11/15: [=========================     ] 40/47 batches, loss: 0.0685Epoch 11/15: [==========================    ] 41/47 batches, loss: 0.0687Epoch 11/15: [==========================    ] 42/47 batches, loss: 0.0683Epoch 11/15: [===========================   ] 43/47 batches, loss: 0.0679Epoch 11/15: [============================  ] 44/47 batches, loss: 0.0682Epoch 11/15: [============================  ] 45/47 batches, loss: 0.0684Epoch 11/15: [============================= ] 46/47 batches, loss: 0.0681Epoch 11/15: [==============================] 47/47 batches, loss: 0.0738
[2025-05-07 22:26:28,829][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0738
[2025-05-07 22:26:29,113][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0420, Metrics: {'mse': 0.03954143077135086, 'rmse': 0.19885027224359253, 'r2': 0.04844629764556885}
Epoch 12/15: [Epoch 12/15: [                              ] 1/47 batches, loss: 0.0426Epoch 12/15: [=                             ] 2/47 batches, loss: 0.0398Epoch 12/15: [=                             ] 3/47 batches, loss: 0.0427Epoch 12/15: [==                            ] 4/47 batches, loss: 0.0451Epoch 12/15: [===                           ] 5/47 batches, loss: 0.0444Epoch 12/15: [===                           ] 6/47 batches, loss: 0.0540Epoch 12/15: [====                          ] 7/47 batches, loss: 0.0510Epoch 12/15: [=====                         ] 8/47 batches, loss: 0.0547Epoch 12/15: [=====                         ] 9/47 batches, loss: 0.0552Epoch 12/15: [======                        ] 10/47 batches, loss: 0.0525Epoch 12/15: [=======                       ] 11/47 batches, loss: 0.0536Epoch 12/15: [=======                       ] 12/47 batches, loss: 0.0517Epoch 12/15: [========                      ] 13/47 batches, loss: 0.0575Epoch 12/15: [========                      ] 14/47 batches, loss: 0.0584Epoch 12/15: [=========                     ] 15/47 batches, loss: 0.0582Epoch 12/15: [==========                    ] 16/47 batches, loss: 0.0606Epoch 12/15: [==========                    ] 17/47 batches, loss: 0.0620Epoch 12/15: [===========                   ] 18/47 batches, loss: 0.0611Epoch 12/15: [============                  ] 19/47 batches, loss: 0.0606Epoch 12/15: [============                  ] 20/47 batches, loss: 0.0611Epoch 12/15: [=============                 ] 21/47 batches, loss: 0.0633Epoch 12/15: [==============                ] 22/47 batches, loss: 0.0632Epoch 12/15: [==============                ] 23/47 batches, loss: 0.0623Epoch 12/15: [===============               ] 24/47 batches, loss: 0.0633Epoch 12/15: [===============               ] 25/47 batches, loss: 0.0631Epoch 12/15: [================              ] 26/47 batches, loss: 0.0628Epoch 12/15: [=================             ] 27/47 batches, loss: 0.0631Epoch 12/15: [=================             ] 28/47 batches, loss: 0.0699Epoch 12/15: [==================            ] 29/47 batches, loss: 0.0697Epoch 12/15: [===================           ] 30/47 batches, loss: 0.0700Epoch 12/15: [===================           ] 31/47 batches, loss: 0.0707Epoch 12/15: [====================          ] 32/47 batches, loss: 0.0715Epoch 12/15: [=====================         ] 33/47 batches, loss: 0.0707Epoch 12/15: [=====================         ] 34/47 batches, loss: 0.0705Epoch 12/15: [======================        ] 35/47 batches, loss: 0.0701Epoch 12/15: [======================        ] 36/47 batches, loss: 0.0704Epoch 12/15: [=======================       ] 37/47 batches, loss: 0.0699Epoch 12/15: [========================      ] 38/47 batches, loss: 0.0692Epoch 12/15: [========================      ] 39/47 batches, loss: 0.0688Epoch 12/15: [=========================     ] 40/47 batches, loss: 0.0686Epoch 12/15: [==========================    ] 41/47 batches, loss: 0.0682Epoch 12/15: [==========================    ] 42/47 batches, loss: 0.0686Epoch 12/15: [===========================   ] 43/47 batches, loss: 0.0686Epoch 12/15: [============================  ] 44/47 batches, loss: 0.0679Epoch 12/15: [============================  ] 45/47 batches, loss: 0.0682Epoch 12/15: [============================= ] 46/47 batches, loss: 0.0681Epoch 12/15: [==============================] 47/47 batches, loss: 0.0668
[2025-05-07 22:26:31,105][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0668
[2025-05-07 22:26:31,400][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0430, Metrics: {'mse': 0.04068562015891075, 'rmse': 0.20170676775683744, 'r2': 0.020911693572998047}
[2025-05-07 22:26:31,401][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/47 batches, loss: 0.0234Epoch 13/15: [=                             ] 2/47 batches, loss: 0.0432Epoch 13/15: [=                             ] 3/47 batches, loss: 0.0504Epoch 13/15: [==                            ] 4/47 batches, loss: 0.0563Epoch 13/15: [===                           ] 5/47 batches, loss: 0.0582Epoch 13/15: [===                           ] 6/47 batches, loss: 0.0589Epoch 13/15: [====                          ] 7/47 batches, loss: 0.0666Epoch 13/15: [=====                         ] 8/47 batches, loss: 0.0714Epoch 13/15: [=====                         ] 9/47 batches, loss: 0.0684Epoch 13/15: [======                        ] 10/47 batches, loss: 0.0635Epoch 13/15: [=======                       ] 11/47 batches, loss: 0.0609Epoch 13/15: [=======                       ] 12/47 batches, loss: 0.0607Epoch 13/15: [========                      ] 13/47 batches, loss: 0.0592Epoch 13/15: [========                      ] 14/47 batches, loss: 0.0611Epoch 13/15: [=========                     ] 15/47 batches, loss: 0.0612Epoch 13/15: [==========                    ] 16/47 batches, loss: 0.0621Epoch 13/15: [==========                    ] 17/47 batches, loss: 0.0612Epoch 13/15: [===========                   ] 18/47 batches, loss: 0.0600Epoch 13/15: [============                  ] 19/47 batches, loss: 0.0593Epoch 13/15: [============                  ] 20/47 batches, loss: 0.0584Epoch 13/15: [=============                 ] 21/47 batches, loss: 0.0608Epoch 13/15: [==============                ] 22/47 batches, loss: 0.0595Epoch 13/15: [==============                ] 23/47 batches, loss: 0.0604Epoch 13/15: [===============               ] 24/47 batches, loss: 0.0597Epoch 13/15: [===============               ] 25/47 batches, loss: 0.0607Epoch 13/15: [================              ] 26/47 batches, loss: 0.0603Epoch 13/15: [=================             ] 27/47 batches, loss: 0.0591Epoch 13/15: [=================             ] 28/47 batches, loss: 0.0591Epoch 13/15: [==================            ] 29/47 batches, loss: 0.0596Epoch 13/15: [===================           ] 30/47 batches, loss: 0.0608Epoch 13/15: [===================           ] 31/47 batches, loss: 0.0606Epoch 13/15: [====================          ] 32/47 batches, loss: 0.0603Epoch 13/15: [=====================         ] 33/47 batches, loss: 0.0616Epoch 13/15: [=====================         ] 34/47 batches, loss: 0.0608Epoch 13/15: [======================        ] 35/47 batches, loss: 0.0607Epoch 13/15: [======================        ] 36/47 batches, loss: 0.0607Epoch 13/15: [=======================       ] 37/47 batches, loss: 0.0608Epoch 13/15: [========================      ] 38/47 batches, loss: 0.0612Epoch 13/15: [========================      ] 39/47 batches, loss: 0.0605Epoch 13/15: [=========================     ] 40/47 batches, loss: 0.0599Epoch 13/15: [==========================    ] 41/47 batches, loss: 0.0600Epoch 13/15: [==========================    ] 42/47 batches, loss: 0.0596Epoch 13/15: [===========================   ] 43/47 batches, loss: 0.0593Epoch 13/15: [============================  ] 44/47 batches, loss: 0.0592Epoch 13/15: [============================  ] 45/47 batches, loss: 0.0594Epoch 13/15: [============================= ] 46/47 batches, loss: 0.0587Epoch 13/15: [==============================] 47/47 batches, loss: 0.0579
[2025-05-07 22:26:32,922][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0579
[2025-05-07 22:26:33,251][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0422, Metrics: {'mse': 0.03971642255783081, 'rmse': 0.19928979541820702, 'r2': 0.044235169887542725}
[2025-05-07 22:26:33,252][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/47 batches, loss: 0.0443Epoch 14/15: [=                             ] 2/47 batches, loss: 0.0573Epoch 14/15: [=                             ] 3/47 batches, loss: 0.0525Epoch 14/15: [==                            ] 4/47 batches, loss: 0.0523Epoch 14/15: [===                           ] 5/47 batches, loss: 0.0569Epoch 14/15: [===                           ] 6/47 batches, loss: 0.0546Epoch 14/15: [====                          ] 7/47 batches, loss: 0.0602Epoch 14/15: [=====                         ] 8/47 batches, loss: 0.0593Epoch 14/15: [=====                         ] 9/47 batches, loss: 0.0593Epoch 14/15: [======                        ] 10/47 batches, loss: 0.0579Epoch 14/15: [=======                       ] 11/47 batches, loss: 0.0580Epoch 14/15: [=======                       ] 12/47 batches, loss: 0.0578Epoch 14/15: [========                      ] 13/47 batches, loss: 0.0584Epoch 14/15: [========                      ] 14/47 batches, loss: 0.0612Epoch 14/15: [=========                     ] 15/47 batches, loss: 0.0597Epoch 14/15: [==========                    ] 16/47 batches, loss: 0.0592Epoch 14/15: [==========                    ] 17/47 batches, loss: 0.0572Epoch 14/15: [===========                   ] 18/47 batches, loss: 0.0567Epoch 14/15: [============                  ] 19/47 batches, loss: 0.0561Epoch 14/15: [============                  ] 20/47 batches, loss: 0.0556Epoch 14/15: [=============                 ] 21/47 batches, loss: 0.0570Epoch 14/15: [==============                ] 22/47 batches, loss: 0.0573Epoch 14/15: [==============                ] 23/47 batches, loss: 0.0572Epoch 14/15: [===============               ] 24/47 batches, loss: 0.0559Epoch 14/15: [===============               ] 25/47 batches, loss: 0.0561Epoch 14/15: [================              ] 26/47 batches, loss: 0.0593Epoch 14/15: [=================             ] 27/47 batches, loss: 0.0587Epoch 14/15: [=================             ] 28/47 batches, loss: 0.0584Epoch 14/15: [==================            ] 29/47 batches, loss: 0.0577Epoch 14/15: [===================           ] 30/47 batches, loss: 0.0574Epoch 14/15: [===================           ] 31/47 batches, loss: 0.0575Epoch 14/15: [====================          ] 32/47 batches, loss: 0.0568Epoch 14/15: [=====================         ] 33/47 batches, loss: 0.0589Epoch 14/15: [=====================         ] 34/47 batches, loss: 0.0584Epoch 14/15: [======================        ] 35/47 batches, loss: 0.0577Epoch 14/15: [======================        ] 36/47 batches, loss: 0.0575Epoch 14/15: [=======================       ] 37/47 batches, loss: 0.0575Epoch 14/15: [========================      ] 38/47 batches, loss: 0.0573Epoch 14/15: [========================      ] 39/47 batches, loss: 0.0573Epoch 14/15: [=========================     ] 40/47 batches, loss: 0.0580Epoch 14/15: [==========================    ] 41/47 batches, loss: 0.0579Epoch 14/15: [==========================    ] 42/47 batches, loss: 0.0580Epoch 14/15: [===========================   ] 43/47 batches, loss: 0.0580Epoch 14/15: [============================  ] 44/47 batches, loss: 0.0578Epoch 14/15: [============================  ] 45/47 batches, loss: 0.0578Epoch 14/15: [============================= ] 46/47 batches, loss: 0.0576Epoch 14/15: [==============================] 47/47 batches, loss: 0.0574
[2025-05-07 22:26:34,734][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0574
[2025-05-07 22:26:35,072][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0433, Metrics: {'mse': 0.04096688702702522, 'rmse': 0.20240278413852222, 'r2': 0.014143049716949463}
[2025-05-07 22:26:35,073][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/47 batches, loss: 0.0462Epoch 15/15: [=                             ] 2/47 batches, loss: 0.0408Epoch 15/15: [=                             ] 3/47 batches, loss: 0.0364Epoch 15/15: [==                            ] 4/47 batches, loss: 0.0449Epoch 15/15: [===                           ] 5/47 batches, loss: 0.0444Epoch 15/15: [===                           ] 6/47 batches, loss: 0.0508Epoch 15/15: [====                          ] 7/47 batches, loss: 0.0575Epoch 15/15: [=====                         ] 8/47 batches, loss: 0.0613Epoch 15/15: [=====                         ] 9/47 batches, loss: 0.0584Epoch 15/15: [======                        ] 10/47 batches, loss: 0.0609Epoch 15/15: [=======                       ] 11/47 batches, loss: 0.0600Epoch 15/15: [=======                       ] 12/47 batches, loss: 0.0580Epoch 15/15: [========                      ] 13/47 batches, loss: 0.0565Epoch 15/15: [========                      ] 14/47 batches, loss: 0.0544Epoch 15/15: [=========                     ] 15/47 batches, loss: 0.0521Epoch 15/15: [==========                    ] 16/47 batches, loss: 0.0511Epoch 15/15: [==========                    ] 17/47 batches, loss: 0.0521Epoch 15/15: [===========                   ] 18/47 batches, loss: 0.0514Epoch 15/15: [============                  ] 19/47 batches, loss: 0.0530Epoch 15/15: [============                  ] 20/47 batches, loss: 0.0536Epoch 15/15: [=============                 ] 21/47 batches, loss: 0.0528Epoch 15/15: [==============                ] 22/47 batches, loss: 0.0534Epoch 15/15: [==============                ] 23/47 batches, loss: 0.0527Epoch 15/15: [===============               ] 24/47 batches, loss: 0.0512Epoch 15/15: [===============               ] 25/47 batches, loss: 0.0506Epoch 15/15: [================              ] 26/47 batches, loss: 0.0504Epoch 15/15: [=================             ] 27/47 batches, loss: 0.0500Epoch 15/15: [=================             ] 28/47 batches, loss: 0.0503Epoch 15/15: [==================            ] 29/47 batches, loss: 0.0509Epoch 15/15: [===================           ] 30/47 batches, loss: 0.0504Epoch 15/15: [===================           ] 31/47 batches, loss: 0.0502Epoch 15/15: [====================          ] 32/47 batches, loss: 0.0511Epoch 15/15: [=====================         ] 33/47 batches, loss: 0.0513Epoch 15/15: [=====================         ] 34/47 batches, loss: 0.0517Epoch 15/15: [======================        ] 35/47 batches, loss: 0.0525Epoch 15/15: [======================        ] 36/47 batches, loss: 0.0522Epoch 15/15: [=======================       ] 37/47 batches, loss: 0.0527Epoch 15/15: [========================      ] 38/47 batches, loss: 0.0529Epoch 15/15: [========================      ] 39/47 batches, loss: 0.0524Epoch 15/15: [=========================     ] 40/47 batches, loss: 0.0527Epoch 15/15: [==========================    ] 41/47 batches, loss: 0.0539Epoch 15/15: [==========================    ] 42/47 batches, loss: 0.0542Epoch 15/15: [===========================   ] 43/47 batches, loss: 0.0537Epoch 15/15: [============================  ] 44/47 batches, loss: 0.0534Epoch 15/15: [============================  ] 45/47 batches, loss: 0.0546Epoch 15/15: [============================= ] 46/47 batches, loss: 0.0541Epoch 15/15: [==============================] 47/47 batches, loss: 0.0548
[2025-05-07 22:26:36,582][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0548
[2025-05-07 22:26:36,881][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0435, Metrics: {'mse': 0.040860969573259354, 'rmse': 0.20214096460950054, 'r2': 0.016691923141479492}
[2025-05-07 22:26:36,881][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:26:36,881][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 15
[2025-05-07 22:26:36,882][src.training.lm_trainer][INFO] - Training completed in 31.10 seconds
[2025-05-07 22:26:36,882][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:26:39,228][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.04185769334435463, 'rmse': 0.20459152803660915, 'r2': -0.22226881980895996}
[2025-05-07 22:26:39,228][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03954143077135086, 'rmse': 0.19885027224359253, 'r2': 0.04844629764556885}
[2025-05-07 22:26:39,228][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06307695806026459, 'rmse': 0.2511512652969612, 'r2': -0.5779287815093994}
[2025-05-07 22:26:41,287][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control3/layer2/ko/ko/model.pt
[2025-05-07 22:26:41,288][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁▁▁▁▁
wandb:     best_val_mse █▄▂▁▁▁▁▁
wandb:      best_val_r2 ▁▅▇█████
wandb:    best_val_rmse █▄▂▁▁▁▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▄▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       train_loss █▄▃▃▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          val_mse █▄▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:           val_r2 ▁▅▇████████████
wandb:         val_rmse █▄▂▁▁▁▁▁▁▁▁▁▁▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04202
wandb:     best_val_mse 0.03954
wandb:      best_val_r2 0.04845
wandb:    best_val_rmse 0.19885
wandb: early_stop_epoch 15
wandb:            epoch 15
wandb:   final_test_mse 0.06308
wandb:    final_test_r2 -0.57793
wandb:  final_test_rmse 0.25115
wandb:  final_train_mse 0.04186
wandb:   final_train_r2 -0.22227
wandb: final_train_rmse 0.20459
wandb:    final_val_mse 0.03954
wandb:     final_val_r2 0.04845
wandb:   final_val_rmse 0.19885
wandb:    learning_rate 0.0001
wandb:       train_loss 0.05476
wandb:       train_time 31.10069
wandb:         val_loss 0.04355
wandb:          val_mse 0.04086
wandb:           val_r2 0.01669
wandb:         val_rmse 0.20214
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_222549-3y22k0fa
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_222549-3y22k0fa/logs
Experiment probe_layer2_lexical_density_control3_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control3/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_n_tokens_control1_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_n_tokens_control1_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control1/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=n_tokens"
slurmstepd: error: *** JOB 64467094 ON k28i22 CANCELLED AT 2025-05-07T22:26:53 DUE TO TIME LIMIT ***
