SLURM_JOB_ID: 64464653
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: qtype_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Wed May  7 17:36:50 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main probing experiments (non-control)...
=======================
PROBING LAYER 2
=======================
Experiment probe_layer2_complexity_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/fi/fi/results.json for layer 2
Running control probing experiments...
=======================
PROBING LAYER 2 (CONTROL EXPERIMENTS)
=======================
Experiment probe_layer2_complexity_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_complexity_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_complexity_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/fi/fi/results.json for layer 2
Running submetric probing experiments...
=======================
PROBING LAYER 2 (SUBMETRIC EXPERIMENTS)
=======================
Running experiment: probe_layer2_n_tokens_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_n_tokens_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/layer2/fi"         "wandb.mode=offline" "experiment.submetric=n_tokens"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 17:37:52,269][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/layer2/fi
experiment_name: probe_layer2_n_tokens_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: n_tokens
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 17:37:52,269][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 17:37:52,269][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 17:37:52,269][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 17:37:52,269][__main__][INFO] - Determined Task Type: regression
[2025-05-07 17:37:52,274][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 17:37:52,274][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 17:37:52,274][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 17:37:58,568][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'n_tokens'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 17:38:00,991][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 17:38:00,991][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:38:01,246][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:38:01,365][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:38:01,557][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 17:38:01,567][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:38:01,568][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 17:38:01,569][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:38:01,681][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:38:01,759][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:38:01,774][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 17:38:01,775][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:38:01,775][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 17:38:01,777][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:38:01,855][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:38:01,903][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:38:01,928][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 17:38:01,930][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:38:01,930][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 17:38:01,931][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 17:38:01,932][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:38:01,932][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 17:38:01,932][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 17:38:01,932][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 17:38:01,932][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:38:01,932][src.data.datasets][INFO] -   Mean: 0.1375, Std: 0.1211
[2025-05-07 17:38:01,932][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 17:38:01,933][src.data.datasets][INFO] - Sample label: 0.23100000619888306
[2025-05-07 17:38:01,933][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:38:01,933][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 17:38:01,933][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 17:38:01,933][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 17:38:01,933][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6150
[2025-05-07 17:38:01,933][src.data.datasets][INFO] -   Mean: 0.1498, Std: 0.1357
[2025-05-07 17:38:01,933][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 17:38:01,933][src.data.datasets][INFO] - Sample label: 0.5860000252723694
[2025-05-07 17:38:01,934][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:38:01,934][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 17:38:01,934][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 17:38:01,934][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 17:38:01,934][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7930
[2025-05-07 17:38:01,934][src.data.datasets][INFO] -   Mean: 0.1144, Std: 0.1176
[2025-05-07 17:38:01,934][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 17:38:01,934][src.data.datasets][INFO] - Sample label: 0.04500000178813934
[2025-05-07 17:38:01,934][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 17:38:01,934][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 17:38:01,935][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 17:38:01,935][__main__][INFO] - Using model type: lm_probe for submetric n_tokens
[2025-05-07 17:38:01,935][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 17:38:10,152][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 17:38:10,153][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 17:38:10,153][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 17:38:10,153][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 17:38:10,156][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 17:38:10,157][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 17:38:10,157][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 17:38:10,157][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 17:38:10,157][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 17:38:10,158][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 17:38:10,158][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5954Epoch 1/15: [                              ] 2/75 batches, loss: 0.5968Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4852Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4526Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4350Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4036Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3931Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4332Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4367Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4189Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4145Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4071Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3902Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3982Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3928Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4089Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3980Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3971Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3978Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3949Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4047Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4063Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4016Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3906Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3859Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3783Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3715Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3666Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3657Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3655Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3601Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3552Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3512Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3500Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3455Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3460Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3413Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3396Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3384Epoch 1/15: [================              ] 40/75 batches, loss: 0.3337Epoch 1/15: [================              ] 41/75 batches, loss: 0.3296Epoch 1/15: [================              ] 42/75 batches, loss: 0.3260Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3234Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3210Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3236Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3192Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3201Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3154Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3126Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3112Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3112Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3089Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3057Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3057Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3035Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3021Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3042Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3038Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3034Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3012Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2975Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2961Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2941Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2925Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2908Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2901Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2874Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2842Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2847Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2844Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2820Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2803Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2775Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2750Epoch 1/15: [==============================] 75/75 batches, loss: 0.2729
[2025-05-07 17:38:18,096][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2729
[2025-05-07 17:38:18,415][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0324, Metrics: {'mse': 0.03224897384643555, 'rmse': 0.17957999289017568, 'r2': -0.7505507469177246}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1505Epoch 2/15: [                              ] 2/75 batches, loss: 0.1304Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1056Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1334Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1297Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1178Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1275Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1242Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1242Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1307Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1274Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1269Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1250Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1200Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1214Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1192Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1179Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1178Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1157Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1167Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1175Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1179Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1176Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1158Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1161Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1200Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1209Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1210Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1231Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1272Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1267Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1253Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1249Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1262Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1267Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1256Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1247Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1236Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1234Epoch 2/15: [================              ] 40/75 batches, loss: 0.1231Epoch 2/15: [================              ] 41/75 batches, loss: 0.1227Epoch 2/15: [================              ] 42/75 batches, loss: 0.1215Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1210Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1191Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1185Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1205Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1197Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1195Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1225Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1210Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1207Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1218Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1219Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1214Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1211Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1225Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1217Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1211Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1200Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1186Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1188Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1188Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1179Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1172Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1165Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1160Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1155Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1159Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1156Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1146Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1139Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1149Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1149Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1145Epoch 2/15: [==============================] 75/75 batches, loss: 0.1147
[2025-05-07 17:38:21,165][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1147
[2025-05-07 17:38:21,473][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0369, Metrics: {'mse': 0.036809202283620834, 'rmse': 0.1918572445429696, 'r2': -0.9980907440185547}
[2025-05-07 17:38:21,474][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1251Epoch 3/15: [                              ] 2/75 batches, loss: 0.1093Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1003Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0881Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0799Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0818Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0796Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0747Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0715Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0728Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0708Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0721Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0757Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0786Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0770Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0769Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0772Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0778Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0784Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0781Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0791Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0788Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0773Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0826Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0838Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0830Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0817Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0816Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0826Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0813Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0813Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0815Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0811Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0816Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0803Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0797Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0787Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0783Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0777Epoch 3/15: [================              ] 40/75 batches, loss: 0.0784Epoch 3/15: [================              ] 41/75 batches, loss: 0.0784Epoch 3/15: [================              ] 42/75 batches, loss: 0.0776Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0777Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0780Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0781Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0778Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0789Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0799Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0810Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0801Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0799Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0796Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0788Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0795Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0789Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0782Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0774Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0775Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0769Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0774Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0770Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0772Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0779Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0784Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0781Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0775Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0776Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0772Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0770Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0771Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0769Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0765Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0763Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0760Epoch 3/15: [==============================] 75/75 batches, loss: 0.0765
[2025-05-07 17:38:23,779][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0765
[2025-05-07 17:38:24,126][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0172, Metrics: {'mse': 0.01717139594256878, 'rmse': 0.13103967316262957, 'r2': 0.06789588928222656}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0615Epoch 4/15: [                              ] 2/75 batches, loss: 0.0640Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0541Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0630Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0609Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0638Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0656Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0652Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0657Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0621Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0654Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0652Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0617Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0600Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0641Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0656Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0680Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0678Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0673Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0662Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0659Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0675Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0675Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0676Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0696Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0701Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0699Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0691Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0691Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0681Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0680Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0677Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0668Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0675Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0673Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0668Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0660Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0665Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0655Epoch 4/15: [================              ] 40/75 batches, loss: 0.0647Epoch 4/15: [================              ] 41/75 batches, loss: 0.0648Epoch 4/15: [================              ] 42/75 batches, loss: 0.0643Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0652Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0644Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0638Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0629Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0625Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0625Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0626Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0623Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0623Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0624Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0628Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0637Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0633Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0637Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0633Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0627Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0625Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0625Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0627Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0620Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0619Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0617Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0618Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0617Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0612Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0612Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0611Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0610Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0605Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0603Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0602Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0605Epoch 4/15: [==============================] 75/75 batches, loss: 0.0606
[2025-05-07 17:38:26,893][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0606
[2025-05-07 17:38:27,201][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0149, Metrics: {'mse': 0.01484144851565361, 'rmse': 0.12182548385150625, 'r2': 0.19437098503112793}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0988Epoch 5/15: [                              ] 2/75 batches, loss: 0.0768Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0840Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0790Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0658Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0629Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0645Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0615Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0586Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0565Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0579Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0565Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0579Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0579Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0595Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0579Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0569Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0577Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0567Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0553Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0573Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0563Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0561Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0549Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0571Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0571Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0571Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0569Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0563Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0553Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0546Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0549Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0548Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0555Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0569Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0565Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0560Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0559Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0556Epoch 5/15: [================              ] 40/75 batches, loss: 0.0555Epoch 5/15: [================              ] 41/75 batches, loss: 0.0551Epoch 5/15: [================              ] 42/75 batches, loss: 0.0546Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0546Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0540Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0537Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0535Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0534Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0531Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0535Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0535Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0530Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0526Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0523Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0526Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0523Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0519Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0519Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0516Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0512Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0511Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0510Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0511Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0511Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0512Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0512Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0512Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0509Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0506Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0505Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0505Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0506Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0507Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0505Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0503Epoch 5/15: [==============================] 75/75 batches, loss: 0.0504
[2025-05-07 17:38:29,890][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0504
[2025-05-07 17:38:30,165][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0125, Metrics: {'mse': 0.012429867871105671, 'rmse': 0.11148931729589912, 'r2': 0.32527732849121094}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0202Epoch 6/15: [                              ] 2/75 batches, loss: 0.0368Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0344Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0320Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0370Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0383Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0368Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0383Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0362Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0408Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0408Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0403Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0418Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0416Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0434Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0441Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0424Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0444Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0428Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0427Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0423Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0418Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0421Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0420Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0420Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0419Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0427Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0416Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0411Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0406Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0403Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0398Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0393Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0387Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0388Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0385Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0379Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0374Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0399Epoch 6/15: [================              ] 40/75 batches, loss: 0.0400Epoch 6/15: [================              ] 41/75 batches, loss: 0.0401Epoch 6/15: [================              ] 42/75 batches, loss: 0.0408Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0406Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0409Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0422Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0421Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0424Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0421Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0418Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0416Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0417Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0423Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0422Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0420Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0418Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0417Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0416Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0421Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0418Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0420Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0422Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0421Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0426Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0430Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0434Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0430Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0428Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0431Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0429Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0427Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0431Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0429Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0427Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0426Epoch 6/15: [==============================] 75/75 batches, loss: 0.0424
[2025-05-07 17:38:32,847][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0424
[2025-05-07 17:38:33,134][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0137, Metrics: {'mse': 0.013609055429697037, 'rmse': 0.11665785627079317, 'r2': 0.2612681984901428}
[2025-05-07 17:38:33,135][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0389Epoch 7/15: [                              ] 2/75 batches, loss: 0.0497Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0420Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0475Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0439Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0414Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0429Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0413Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0400Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0407Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0400Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0394Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0383Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0433Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0421Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0420Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0428Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0420Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0410Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0403Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0398Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0401Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0395Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0398Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0392Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0403Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0402Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0399Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0394Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0388Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0386Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0387Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0385Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0388Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0381Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0375Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0380Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0382Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0379Epoch 7/15: [================              ] 40/75 batches, loss: 0.0376Epoch 7/15: [================              ] 41/75 batches, loss: 0.0385Epoch 7/15: [================              ] 42/75 batches, loss: 0.0386Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0392Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0398Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0393Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0392Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0392Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0388Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0387Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0382Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0380Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0382Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0383Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0382Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0384Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0381Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0381Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0380Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0378Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0381Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0382Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0381Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0379Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0380Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0379Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0375Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0373Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0369Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0368Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0370Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0371Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0371Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0368Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0367Epoch 7/15: [==============================] 75/75 batches, loss: 0.0366
[2025-05-07 17:38:35,502][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0366
[2025-05-07 17:38:35,833][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0135, Metrics: {'mse': 0.013434821739792824, 'rmse': 0.11590867844899631, 'r2': 0.2707260251045227}
[2025-05-07 17:38:35,834][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0315Epoch 8/15: [                              ] 2/75 batches, loss: 0.0258Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0250Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0285Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0359Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0359Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0354Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0336Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0338Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0346Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0340Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0327Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0338Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0340Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0343Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0342Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0338Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0336Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0335Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0331Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0326Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0337Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0329Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0329Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0337Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0335Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0336Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0338Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0333Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0334Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0336Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0336Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0334Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0332Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0329Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0327Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0326Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0326Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0323Epoch 8/15: [================              ] 40/75 batches, loss: 0.0329Epoch 8/15: [================              ] 41/75 batches, loss: 0.0326Epoch 8/15: [================              ] 42/75 batches, loss: 0.0325Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0329Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0325Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0324Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0324Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0325Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0324Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0322Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0318Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0316Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0320Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0316Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0315Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0317Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0313Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0314Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0311Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0311Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0312Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0311Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0312Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0311Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0312Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0311Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0310Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0311Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0312Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0310Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0307Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0309Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0306Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0306Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0306Epoch 8/15: [==============================] 75/75 batches, loss: 0.0317
[2025-05-07 17:38:38,196][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0317
[2025-05-07 17:38:38,428][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0124, Metrics: {'mse': 0.012327517382800579, 'rmse': 0.11102935369892315, 'r2': 0.3308331370353699}
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0538Epoch 9/15: [                              ] 2/75 batches, loss: 0.0472Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0348Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0325Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0300Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0292Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0314Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0331Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0310Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0308Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0296Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0283Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0271Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0270Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0271Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0275Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0273Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0272Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0270Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0271Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0269Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0267Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0275Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0281Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0283Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0282Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0276Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0278Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0280Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0283Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0280Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0280Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0281Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0288Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0284Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0280Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0278Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0279Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0278Epoch 9/15: [================              ] 40/75 batches, loss: 0.0277Epoch 9/15: [================              ] 41/75 batches, loss: 0.0279Epoch 9/15: [================              ] 42/75 batches, loss: 0.0278Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0280Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0281Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0279Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0276Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0275Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0279Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0281Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0278Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0283Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0283Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0279Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0276Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0277Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0275Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0273Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0276Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0279Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0278Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0280Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0283Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0283Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0282Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0281Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0280Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0282Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0280Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0280Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0282Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0281Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0279Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0278Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0276Epoch 9/15: [==============================] 75/75 batches, loss: 0.0273
[2025-05-07 17:38:41,166][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0273
[2025-05-07 17:38:41,415][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0144, Metrics: {'mse': 0.014399941079318523, 'rmse': 0.11999975449690937, 'r2': 0.2183370590209961}
[2025-05-07 17:38:41,416][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0327Epoch 10/15: [                              ] 2/75 batches, loss: 0.0268Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0230Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0242Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0243Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0256Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0251Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0251Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0257Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0275Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0263Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0253Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0252Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0251Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0256Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0254Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0257Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0254Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0252Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0250Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0250Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0249Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0246Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0244Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0240Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0242Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0241Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0250Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0254Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0257Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0258Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0254Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0262Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0266Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0266Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0270Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0266Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0266Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0266Epoch 10/15: [================              ] 40/75 batches, loss: 0.0274Epoch 10/15: [================              ] 41/75 batches, loss: 0.0274Epoch 10/15: [================              ] 42/75 batches, loss: 0.0272Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0271Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0272Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0273Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0272Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0270Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0270Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0271Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0274Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0275Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0273Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0271Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0271Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0271Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0273Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0275Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0273Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0271Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0272Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0271Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0271Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0269Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0269Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0269Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0268Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0267Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0267Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0268Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0266Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0267Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0270Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0270Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0275Epoch 10/15: [==============================] 75/75 batches, loss: 0.0274
[2025-05-07 17:38:43,775][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0274
[2025-05-07 17:38:44,036][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0138, Metrics: {'mse': 0.01374244038015604, 'rmse': 0.11722815523651321, 'r2': 0.254027783870697}
[2025-05-07 17:38:44,036][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0180Epoch 11/15: [                              ] 2/75 batches, loss: 0.0290Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0251Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0268Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0262Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0246Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0240Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0229Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0241Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0236Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0233Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0229Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0220Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0225Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0220Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0218Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0246Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0245Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0248Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0250Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0249Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0251Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0249Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0244Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0243Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0243Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0248Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0253Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0253Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0252Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0248Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0258Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0264Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0260Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0259Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0261Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0265Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0264Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0261Epoch 11/15: [================              ] 40/75 batches, loss: 0.0259Epoch 11/15: [================              ] 41/75 batches, loss: 0.0259Epoch 11/15: [================              ] 42/75 batches, loss: 0.0257Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0255Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0254Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0254Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0251Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0251Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0247Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0248Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0246Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0245Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0242Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0241Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0240Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0239Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0236Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0237Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0238Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0237Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0236Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0236Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0235Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0232Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0231Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0232Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0231Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0230Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0229Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0229Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0228Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0228Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0232Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0232Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0231Epoch 11/15: [==============================] 75/75 batches, loss: 0.0231
[2025-05-07 17:38:46,370][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0231
[2025-05-07 17:38:46,699][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0108, Metrics: {'mse': 0.010772043839097023, 'rmse': 0.10378845715732084, 'r2': 0.4152679443359375}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0337Epoch 12/15: [                              ] 2/75 batches, loss: 0.0215Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0195Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0199Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0212Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0213Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0217Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0236Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0260Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0268Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0274Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0255Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0250Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0248Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0238Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0238Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0235Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0231Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0228Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0224Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0225Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0221Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0217Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0213Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0208Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0216Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0216Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0216Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0213Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0213Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0217Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0217Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0216Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0217Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0219Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0220Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0217Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0217Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0223Epoch 12/15: [================              ] 40/75 batches, loss: 0.0222Epoch 12/15: [================              ] 41/75 batches, loss: 0.0222Epoch 12/15: [================              ] 42/75 batches, loss: 0.0230Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0228Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0226Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0226Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0226Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0225Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0225Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0225Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0224Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0224Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0225Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0223Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0226Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0226Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0227Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0229Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0230Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0232Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0236Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0238Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0241Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0242Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0241Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0239Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0243Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0241Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0242Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0240Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0239Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0237Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0236Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0234Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0238Epoch 12/15: [==============================] 75/75 batches, loss: 0.0238
[2025-05-07 17:38:49,377][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0238
[2025-05-07 17:38:49,700][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0098, Metrics: {'mse': 0.0097507881000638, 'rmse': 0.09874607890981697, 'r2': 0.4707040786743164}
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0282Epoch 13/15: [                              ] 2/75 batches, loss: 0.0252Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0195Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0211Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0218Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0210Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0193Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0207Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0255Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0268Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0267Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0256Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0250Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0244Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0240Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0240Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0241Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0246Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0245Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0242Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0241Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0234Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0235Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0238Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0241Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0238Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0234Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0233Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0232Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0232Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0235Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0236Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0234Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0241Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0240Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0238Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0235Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0233Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0231Epoch 13/15: [================              ] 40/75 batches, loss: 0.0228Epoch 13/15: [================              ] 41/75 batches, loss: 0.0228Epoch 13/15: [================              ] 42/75 batches, loss: 0.0228Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0225Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0230Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0230Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0228Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0225Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0225Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0224Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0222Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0220Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0219Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0217Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0218Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0220Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0219Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0220Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0219Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0218Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0216Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0213Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0213Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0213Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0213Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0212Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0212Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0212Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0213Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0214Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0212Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0212Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0213Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0212Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0211Epoch 13/15: [==============================] 75/75 batches, loss: 0.0210
[2025-05-07 17:38:52,474][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0210
[2025-05-07 17:38:52,822][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0111, Metrics: {'mse': 0.011057324707508087, 'rmse': 0.10515381451715429, 'r2': 0.3997821807861328}
[2025-05-07 17:38:52,823][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0157Epoch 14/15: [                              ] 2/75 batches, loss: 0.0206Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0178Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0154Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0153Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0157Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0147Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0152Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0145Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0163Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0164Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0170Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0167Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0166Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0163Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0164Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0171Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0184Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0185Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0184Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0184Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0184Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0184Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0187Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0189Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0191Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0190Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0194Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0192Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0190Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0188Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0189Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0187Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0187Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0186Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0185Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0185Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0183Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0181Epoch 14/15: [================              ] 40/75 batches, loss: 0.0182Epoch 14/15: [================              ] 41/75 batches, loss: 0.0182Epoch 14/15: [================              ] 42/75 batches, loss: 0.0183Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0183Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0184Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0185Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0183Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0180Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0180Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0179Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0183Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0182Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0183Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0183Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0185Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0184Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0186Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0186Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0186Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0185Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0187Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0185Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0185Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0189Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0188Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0190Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0190Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0191Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0192Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0191Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0189Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0189Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0189Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0189Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0192Epoch 14/15: [==============================] 75/75 batches, loss: 0.0192
[2025-05-07 17:38:55,198][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0192
[2025-05-07 17:38:55,529][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0090, Metrics: {'mse': 0.008986342698335648, 'rmse': 0.09479632217726407, 'r2': 0.5121999979019165}
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0143Epoch 15/15: [                              ] 2/75 batches, loss: 0.0193Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0178Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0187Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0169Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0162Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0163Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0193Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0184Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0180Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0170Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0167Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0184Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0188Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0190Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0186Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0194Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0205Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0200Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0198Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0195Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0196Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0197Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0193Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0195Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0197Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0196Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0196Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0196Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0193Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0192Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0196Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0193Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0192Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0191Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0190Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0188Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0188Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0188Epoch 15/15: [================              ] 40/75 batches, loss: 0.0189Epoch 15/15: [================              ] 41/75 batches, loss: 0.0187Epoch 15/15: [================              ] 42/75 batches, loss: 0.0190Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0190Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0188Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0189Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0187Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0186Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0184Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0182Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0183Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0185Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0182Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0183Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0183Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0186Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0189Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0186Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0186Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0184Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0183Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0182Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0182Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0181Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0182Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0185Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0184Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0183Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0183Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0184Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0185Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0184Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0185Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0187Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0187Epoch 15/15: [==============================] 75/75 batches, loss: 0.0186
[2025-05-07 17:38:58,207][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0186
[2025-05-07 17:38:58,519][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0086, Metrics: {'mse': 0.00855445209890604, 'rmse': 0.0924902811051304, 'r2': 0.5356440544128418}
[2025-05-07 17:38:58,909][src.training.lm_trainer][INFO] - Training completed in 44.00 seconds
[2025-05-07 17:38:58,909][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 17:39:02,009][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.007137726992368698, 'rmse': 0.08448506964173431, 'r2': 0.5130584239959717}
[2025-05-07 17:39:02,009][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.00855445209890604, 'rmse': 0.0924902811051304, 'r2': 0.5356440544128418}
[2025-05-07 17:39:02,009][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.006500615272670984, 'rmse': 0.08062639315181465, 'r2': 0.5298438668251038}
[2025-05-07 17:39:03,668][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/layer2/fi/fi/model.pt
[2025-05-07 17:39:03,669][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▃▂▂▂▁▁▁
wandb:     best_val_mse █▄▃▂▂▂▁▁▁
wandb:      best_val_r2 ▁▅▆▇▇▇███
wandb:    best_val_rmse █▄▃▃▂▂▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▁▆▇▇▇▇▇▇▇▇█▇█
wandb:       train_loss █▄▃▂▂▂▁▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▃▃▂▂▂▂▂▂▂▁▂▁▁
wandb:          val_mse ▇█▃▃▂▂▂▂▂▂▂▁▂▁▁
wandb:           val_r2 ▂▁▆▆▇▇▇▇▇▇▇█▇██
wandb:         val_rmse ▇█▄▃▂▃▃▂▃▃▂▁▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.00861
wandb:     best_val_mse 0.00855
wandb:      best_val_r2 0.53564
wandb:    best_val_rmse 0.09249
wandb:            epoch 15
wandb:   final_test_mse 0.0065
wandb:    final_test_r2 0.52984
wandb:  final_test_rmse 0.08063
wandb:  final_train_mse 0.00714
wandb:   final_train_r2 0.51306
wandb: final_train_rmse 0.08449
wandb:    final_val_mse 0.00855
wandb:     final_val_r2 0.53564
wandb:   final_val_rmse 0.09249
wandb:    learning_rate 0.0001
wandb:       train_loss 0.01861
wandb:       train_time 43.99649
wandb:         val_loss 0.00861
wandb:          val_mse 0.00855
wandb:           val_r2 0.53564
wandb:         val_rmse 0.09249
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_173752-5w92vp2d
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_173752-5w92vp2d/logs
Experiment probe_layer2_n_tokens_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_avg_verb_edges_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_verb_edges_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/layer2/fi"         "wandb.mode=offline" "experiment.submetric=avg_verb_edges"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 17:39:32,788][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/layer2/fi
experiment_name: probe_layer2_avg_verb_edges_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 17:39:32,788][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 17:39:32,788][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 17:39:32,788][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 17:39:32,788][__main__][INFO] - Determined Task Type: regression
[2025-05-07 17:39:32,792][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 17:39:32,793][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 17:39:32,793][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 17:39:36,345][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 17:39:38,882][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 17:39:38,882][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:39:39,048][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:39:39,134][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:39:39,452][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 17:39:39,461][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:39:39,462][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 17:39:39,465][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:39:39,559][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:39:39,646][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:39:39,669][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 17:39:39,670][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:39:39,671][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 17:39:39,673][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:39:39,737][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:39:39,830][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:39:39,855][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 17:39:39,858][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:39:39,858][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 17:39:39,860][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 17:39:39,862][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:39:39,862][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 17:39:39,862][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 17:39:39,862][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 17:39:39,862][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:39:39,862][src.data.datasets][INFO] -   Mean: 0.2055, Std: 0.2125
[2025-05-07 17:39:39,862][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 17:39:39,862][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 17:39:39,863][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:39:39,863][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 17:39:39,863][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 17:39:39,863][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 17:39:39,863][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:39:39,863][src.data.datasets][INFO] -   Mean: 0.2770, Std: 0.2478
[2025-05-07 17:39:39,863][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 17:39:39,863][src.data.datasets][INFO] - Sample label: 0.550000011920929
[2025-05-07 17:39:39,863][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:39:39,864][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 17:39:39,864][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 17:39:39,864][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 17:39:39,864][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:39:39,864][src.data.datasets][INFO] -   Mean: 0.3600, Std: 0.2660
[2025-05-07 17:39:39,864][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 17:39:39,864][src.data.datasets][INFO] - Sample label: 0.6000000238418579
[2025-05-07 17:39:39,864][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 17:39:39,864][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 17:39:39,865][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 17:39:39,865][__main__][INFO] - Using model type: lm_probe for submetric avg_verb_edges
[2025-05-07 17:39:39,865][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 17:39:47,317][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 17:39:47,318][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 17:39:47,318][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 17:39:47,318][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 17:39:47,321][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 17:39:47,321][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 17:39:47,321][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 17:39:47,321][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 17:39:47,322][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 17:39:47,322][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 17:39:47,323][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5056Epoch 1/15: [                              ] 2/75 batches, loss: 0.5688Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5140Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4516Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4561Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4194Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3973Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4370Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4539Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4369Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4263Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4193Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4045Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4213Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4218Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4374Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4250Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4222Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4234Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4182Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4303Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4403Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4299Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4207Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4138Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4082Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4001Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3940Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3928Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3940Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3885Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3847Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3818Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3804Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3789Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3808Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3753Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3730Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3721Epoch 1/15: [================              ] 40/75 batches, loss: 0.3670Epoch 1/15: [================              ] 41/75 batches, loss: 0.3622Epoch 1/15: [================              ] 42/75 batches, loss: 0.3577Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3536Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3535Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3559Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3516Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3498Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3455Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3432Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3423Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3428Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3418Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3387Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3378Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3369Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3351Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3387Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3388Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3373Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3352Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3318Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3313Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3296Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3282Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3265Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3267Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3241Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3205Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3208Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3212Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3190Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3177Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3150Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3125Epoch 1/15: [==============================] 75/75 batches, loss: 0.3108
[2025-05-07 17:39:54,367][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3108
[2025-05-07 17:39:54,664][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1026, Metrics: {'mse': 0.10308706015348434, 'rmse': 0.3210717367715264, 'r2': -0.6785600185394287}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2433Epoch 2/15: [                              ] 2/75 batches, loss: 0.1783Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1443Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1591Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1605Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1529Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1741Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1676Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1731Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1676Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1644Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1602Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1551Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1512Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1523Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1489Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1464Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1439Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1413Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1438Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1467Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1469Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1495Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1485Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1481Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1547Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1568Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1572Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1613Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1623Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1624Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1605Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1599Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1611Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1599Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1596Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1596Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1586Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1598Epoch 2/15: [================              ] 40/75 batches, loss: 0.1598Epoch 2/15: [================              ] 41/75 batches, loss: 0.1598Epoch 2/15: [================              ] 42/75 batches, loss: 0.1575Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1566Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1543Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1544Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1571Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1567Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1574Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1596Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1587Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1582Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1578Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1581Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1582Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1575Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1600Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1589Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1584Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1579Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1562Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1557Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1559Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1547Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1544Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1531Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1531Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1523Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1530Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1527Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1519Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1513Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1509Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1508Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1503Epoch 2/15: [==============================] 75/75 batches, loss: 0.1497
[2025-05-07 17:39:57,338][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1497
[2025-05-07 17:39:57,634][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1008, Metrics: {'mse': 0.10136537998914719, 'rmse': 0.31837930207403115, 'r2': -0.6505259275436401}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1330Epoch 3/15: [                              ] 2/75 batches, loss: 0.1162Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1071Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1072Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1107Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1148Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1113Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1092Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1090Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1107Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1090Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1065Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1119Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1122Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1108Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1088Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1085Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1080Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1086Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1132Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1125Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1112Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1087Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1138Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1137Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1134Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1116Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1121Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1122Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1110Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1125Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1147Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1142Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1144Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1128Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1114Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1111Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1110Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1115Epoch 3/15: [================              ] 40/75 batches, loss: 0.1123Epoch 3/15: [================              ] 41/75 batches, loss: 0.1133Epoch 3/15: [================              ] 42/75 batches, loss: 0.1126Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1120Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1128Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1126Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1122Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1125Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1141Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1149Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1140Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1135Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1132Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1129Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1141Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1129Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1124Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1113Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1106Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1106Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1099Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1093Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1109Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1123Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1130Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1124Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1115Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1116Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1109Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1103Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1105Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1104Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1098Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1091Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1096Epoch 3/15: [==============================] 75/75 batches, loss: 0.1106
[2025-05-07 17:40:00,342][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1106
[2025-05-07 17:40:00,619][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0784, Metrics: {'mse': 0.07875079661607742, 'rmse': 0.2806257233684707, 'r2': -0.28229427337646484}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0567Epoch 4/15: [                              ] 2/75 batches, loss: 0.0576Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0666Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0846Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0776Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0801Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0805Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0770Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0755Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0738Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0765Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0800Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0784Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0790Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0825Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0842Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0855Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0881Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0876Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0866Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0871Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0891Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0883Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0881Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0910Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0902Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0895Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0893Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0898Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0892Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0906Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0909Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0907Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0908Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0911Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0911Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0903Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0908Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0902Epoch 4/15: [================              ] 40/75 batches, loss: 0.0905Epoch 4/15: [================              ] 41/75 batches, loss: 0.0893Epoch 4/15: [================              ] 42/75 batches, loss: 0.0881Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0879Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0874Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0876Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0873Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0872Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0874Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0884Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0884Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0894Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0903Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0898Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0901Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0900Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0917Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0911Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0924Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0927Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0921Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0929Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0931Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0937Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0937Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0936Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0936Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0928Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0934Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0932Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0930Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0925Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0925Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0926Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0932Epoch 4/15: [==============================] 75/75 batches, loss: 0.0931
[2025-05-07 17:40:03,263][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0931
[2025-05-07 17:40:03,527][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0695, Metrics: {'mse': 0.06980504840612411, 'rmse': 0.2642064503491997, 'r2': -0.13663113117218018}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1204Epoch 5/15: [                              ] 2/75 batches, loss: 0.0944Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0928Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0957Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0999Epoch 5/15: [==                            ] 6/75 batches, loss: 0.1007Epoch 5/15: [==                            ] 7/75 batches, loss: 0.1036Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0975Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0934Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0880Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0856Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0866Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0896Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0890Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0905Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0936Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0934Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0922Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0909Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0896Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0895Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0877Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0879Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0874Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0894Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0884Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0867Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0852Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0877Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0865Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0860Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0866Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0857Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0853Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0863Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0851Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0845Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0845Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0845Epoch 5/15: [================              ] 40/75 batches, loss: 0.0862Epoch 5/15: [================              ] 41/75 batches, loss: 0.0872Epoch 5/15: [================              ] 42/75 batches, loss: 0.0864Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0864Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0860Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0853Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0860Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0859Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0866Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0884Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0877Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0866Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0860Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0856Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0853Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0848Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0852Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0852Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0853Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0851Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0851Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0845Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0841Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0837Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0842Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0841Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0838Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0831Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0827Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0827Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0822Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0826Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0822Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0821Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0817Epoch 5/15: [==============================] 75/75 batches, loss: 0.0818
[2025-05-07 17:40:06,165][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0818
[2025-05-07 17:40:06,419][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0827, Metrics: {'mse': 0.08318512886762619, 'rmse': 0.28841832269747736, 'r2': -0.35449814796447754}
[2025-05-07 17:40:06,420][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0549Epoch 6/15: [                              ] 2/75 batches, loss: 0.0689Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0638Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0562Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0642Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0616Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0665Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0755Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0773Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0770Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0798Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0792Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0782Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0787Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0836Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0826Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0807Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0837Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0816Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0815Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0814Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0805Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0798Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0811Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0819Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0821Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0819Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0811Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0803Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0804Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0806Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0803Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0792Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0794Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0792Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0785Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0782Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0766Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0798Epoch 6/15: [================              ] 40/75 batches, loss: 0.0806Epoch 6/15: [================              ] 41/75 batches, loss: 0.0810Epoch 6/15: [================              ] 42/75 batches, loss: 0.0810Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0808Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0812Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0816Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0813Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0809Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0809Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0804Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0796Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0798Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0793Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0802Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0800Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0796Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0789Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0789Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0790Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0786Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0796Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0798Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0795Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0797Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0793Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0795Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0793Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0794Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0799Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0801Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0799Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0800Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0795Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0792Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0789Epoch 6/15: [==============================] 75/75 batches, loss: 0.0788
[2025-05-07 17:40:08,719][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0788
[2025-05-07 17:40:09,037][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0738, Metrics: {'mse': 0.07422135770320892, 'rmse': 0.27243596991441665, 'r2': -0.20854175090789795}
[2025-05-07 17:40:09,037][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0763Epoch 7/15: [                              ] 2/75 batches, loss: 0.0597Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0645Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0766Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0737Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0774Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0740Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0810Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0824Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0817Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0806Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0793Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0765Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0744Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0738Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0749Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0757Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0750Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0737Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0728Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0716Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0706Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0713Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0718Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0718Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0719Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0720Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0722Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0711Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0700Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0695Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0697Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0693Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0697Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0704Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0699Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0702Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0705Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0699Epoch 7/15: [================              ] 40/75 batches, loss: 0.0694Epoch 7/15: [================              ] 41/75 batches, loss: 0.0694Epoch 7/15: [================              ] 42/75 batches, loss: 0.0695Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0698Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0694Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0690Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0684Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0684Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0683Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0681Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0679Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0681Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0686Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0681Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0685Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0685Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0684Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0691Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0693Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0693Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0692Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0694Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0691Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0694Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0696Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0698Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0691Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0690Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0692Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0690Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0691Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0690Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0688Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0688Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0684Epoch 7/15: [==============================] 75/75 batches, loss: 0.0684
[2025-05-07 17:40:11,384][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0684
[2025-05-07 17:40:11,666][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0757, Metrics: {'mse': 0.07615596055984497, 'rmse': 0.2759636942785137, 'r2': -0.2400425672531128}
[2025-05-07 17:40:11,666][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0758Epoch 8/15: [                              ] 2/75 batches, loss: 0.0646Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0697Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0672Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0676Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0644Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0603Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0628Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0618Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0627Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0623Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0638Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0685Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0682Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0683Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0679Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0655Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0645Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0647Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0656Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0644Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0650Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0646Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0632Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0636Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0636Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0644Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0651Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0651Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0658Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0659Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0664Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0663Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0668Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0672Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0664Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0661Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0660Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0661Epoch 8/15: [================              ] 40/75 batches, loss: 0.0678Epoch 8/15: [================              ] 41/75 batches, loss: 0.0676Epoch 8/15: [================              ] 42/75 batches, loss: 0.0677Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0682Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0674Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0674Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0669Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0671Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0674Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0670Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0673Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0673Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0682Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0681Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0675Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0675Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0673Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0674Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0669Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0673Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0675Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0676Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0681Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0680Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0678Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0676Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0672Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0671Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0665Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0664Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0665Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0667Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0665Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0667Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0668Epoch 8/15: [==============================] 75/75 batches, loss: 0.0673
[2025-05-07 17:40:13,977][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0673
[2025-05-07 17:40:14,291][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0761, Metrics: {'mse': 0.07656125724315643, 'rmse': 0.2766970495743611, 'r2': -0.24664199352264404}
[2025-05-07 17:40:14,292][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 17:40:14,292][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-07 17:40:14,292][src.training.lm_trainer][INFO] - Training completed in 22.98 seconds
[2025-05-07 17:40:14,293][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 17:40:17,273][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.04201839491724968, 'rmse': 0.20498388940902082, 'r2': 0.06905150413513184}
[2025-05-07 17:40:17,274][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06980504840612411, 'rmse': 0.2642064503491997, 'r2': -0.13663113117218018}
[2025-05-07 17:40:17,274][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.10424534976482391, 'rmse': 0.32287048450551176, 'r2': -0.47318899631500244}
[2025-05-07 17:40:18,901][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/layer2/fi/fi/model.pt
[2025-05-07 17:40:18,902][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ██▃▁
wandb:     best_val_mse ██▃▁
wandb:      best_val_r2 ▁▁▆█
wandb:    best_val_rmse ██▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▁▄▅▃▄▄
wandb:       train_loss █▃▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ██▃▁▄▂▂▂
wandb:          val_mse ██▃▁▄▂▂▂
wandb:           val_r2 ▁▁▆█▅▇▇▇
wandb:         val_rmse ██▃▁▄▂▂▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06951
wandb:     best_val_mse 0.06981
wandb:      best_val_r2 -0.13663
wandb:    best_val_rmse 0.26421
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.10425
wandb:    final_test_r2 -0.47319
wandb:  final_test_rmse 0.32287
wandb:  final_train_mse 0.04202
wandb:   final_train_r2 0.06905
wandb: final_train_rmse 0.20498
wandb:    final_val_mse 0.06981
wandb:     final_val_r2 -0.13663
wandb:   final_val_rmse 0.26421
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06725
wandb:       train_time 22.97893
wandb:         val_loss 0.07612
wandb:          val_mse 0.07656
wandb:           val_r2 -0.24664
wandb:         val_rmse 0.2767
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_173932-doe7g6tr
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_173932-doe7g6tr/logs
Experiment probe_layer2_avg_verb_edges_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_lexical_density_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_lexical_density_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/layer2/fi"         "wandb.mode=offline" "experiment.submetric=lexical_density"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 17:40:46,055][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/layer2/fi
experiment_name: probe_layer2_lexical_density_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: lexical_density
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 17:40:46,055][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 17:40:46,055][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 17:40:46,055][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 17:40:46,055][__main__][INFO] - Determined Task Type: regression
[2025-05-07 17:40:46,060][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 17:40:46,060][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 17:40:46,061][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 17:40:49,742][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'lexical_density'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 17:40:52,019][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 17:40:52,020][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:40:52,251][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:40:52,342][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:40:52,632][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 17:40:52,640][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:40:52,641][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 17:40:52,644][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:40:52,730][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:40:52,844][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:40:52,873][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 17:40:52,874][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:40:52,875][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 17:40:52,877][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:40:52,995][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:40:53,098][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:40:53,126][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 17:40:53,127][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:40:53,127][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 17:40:53,130][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 17:40:53,131][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:40:53,131][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 17:40:53,131][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 17:40:53,131][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 17:40:53,131][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:40:53,131][src.data.datasets][INFO] -   Mean: 0.6682, Std: 0.2000
[2025-05-07 17:40:53,131][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 17:40:53,131][src.data.datasets][INFO] - Sample label: 0.75
[2025-05-07 17:40:53,132][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:40:53,132][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 17:40:53,132][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 17:40:53,132][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 17:40:53,132][src.data.datasets][INFO] -   Min: 0.1670, Max: 1.0000
[2025-05-07 17:40:53,132][src.data.datasets][INFO] -   Mean: 0.6281, Std: 0.1884
[2025-05-07 17:40:53,132][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 17:40:53,132][src.data.datasets][INFO] - Sample label: 0.7329999804496765
[2025-05-07 17:40:53,132][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:40:53,133][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 17:40:53,133][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 17:40:53,133][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 17:40:53,133][src.data.datasets][INFO] -   Min: 0.0620, Max: 1.0000
[2025-05-07 17:40:53,133][src.data.datasets][INFO] -   Mean: 0.5643, Std: 0.2115
[2025-05-07 17:40:53,133][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 17:40:53,133][src.data.datasets][INFO] - Sample label: 0.25
[2025-05-07 17:40:53,133][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 17:40:53,133][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 17:40:53,133][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 17:40:53,134][__main__][INFO] - Using model type: lm_probe for submetric lexical_density
[2025-05-07 17:40:53,134][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 17:41:03,581][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 17:41:03,582][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 17:41:03,582][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 17:41:03,582][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 17:41:03,585][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 17:41:03,586][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 17:41:03,586][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 17:41:03,586][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 17:41:03,586][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 17:41:03,587][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 17:41:03,587][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4850Epoch 1/15: [                              ] 2/75 batches, loss: 0.5919Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5246Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4716Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4585Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4380Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4188Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4589Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4702Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4624Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4446Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4380Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4217Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4320Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4297Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4491Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4376Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4322Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4288Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4273Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4361Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4394Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4341Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4240Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4160Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4102Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4022Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3989Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3967Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3988Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3927Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3882Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3853Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3840Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3798Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3824Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3785Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3762Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3741Epoch 1/15: [================              ] 40/75 batches, loss: 0.3704Epoch 1/15: [================              ] 41/75 batches, loss: 0.3663Epoch 1/15: [================              ] 42/75 batches, loss: 0.3616Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3586Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3574Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3598Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3548Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3548Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3504Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3487Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3475Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3481Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3476Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3450Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3440Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3440Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3419Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3447Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3448Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3435Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3405Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3367Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3357Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3345Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3318Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3294Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3276Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3249Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3214Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3218Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3209Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3193Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3173Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3148Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3118Epoch 1/15: [==============================] 75/75 batches, loss: 0.3104
[2025-05-07 17:41:10,400][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3104
[2025-05-07 17:41:10,646][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0360, Metrics: {'mse': 0.03603656217455864, 'rmse': 0.18983298494876658, 'r2': -0.01573622226715088}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2036Epoch 2/15: [                              ] 2/75 batches, loss: 0.1624Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1373Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1648Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1589Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1516Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1698Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1660Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1655Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1640Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1610Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1559Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1502Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1487Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1515Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1495Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1508Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1494Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1475Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1459Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1460Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1471Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1482Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1461Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1466Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1529Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1518Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1522Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1557Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1560Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1554Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1542Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1531Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1553Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1568Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1555Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1545Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1534Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1533Epoch 2/15: [================              ] 40/75 batches, loss: 0.1525Epoch 2/15: [================              ] 41/75 batches, loss: 0.1531Epoch 2/15: [================              ] 42/75 batches, loss: 0.1526Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1517Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1511Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1509Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1518Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1515Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1516Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1525Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1519Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1520Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1513Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1520Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1517Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1502Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1510Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1503Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1503Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1493Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1483Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1470Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1464Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1456Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1451Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1443Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1447Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1444Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1444Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1444Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1442Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1441Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1442Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1447Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1439Epoch 2/15: [==============================] 75/75 batches, loss: 0.1431
[2025-05-07 17:41:13,315][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1431
[2025-05-07 17:41:13,543][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0377, Metrics: {'mse': 0.037830743938684464, 'rmse': 0.19450126976111098, 'r2': -0.0663074254989624}
[2025-05-07 17:41:13,543][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1453Epoch 3/15: [                              ] 2/75 batches, loss: 0.1103Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1177Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1071Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1058Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1121Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1059Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1085Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1043Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1061Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1001Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0984Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1018Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1016Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0998Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1000Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1022Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1030Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1025Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1060Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1056Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1058Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1039Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1083Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1070Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1056Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1050Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1049Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1045Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1028Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1042Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1052Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1046Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1050Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1038Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1026Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1019Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1020Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1016Epoch 3/15: [================              ] 40/75 batches, loss: 0.1030Epoch 3/15: [================              ] 41/75 batches, loss: 0.1032Epoch 3/15: [================              ] 42/75 batches, loss: 0.1021Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1027Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1023Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1024Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1021Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1033Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1056Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1063Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1050Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1047Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1054Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1047Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1059Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1055Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1050Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1040Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1034Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1026Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1025Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1023Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1030Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1050Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1052Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1046Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1044Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1044Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1037Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1034Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1038Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1034Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1027Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1020Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1019Epoch 3/15: [==============================] 75/75 batches, loss: 0.1034
[2025-05-07 17:41:15,812][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1034
[2025-05-07 17:41:16,112][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0319, Metrics: {'mse': 0.03202011063694954, 'rmse': 0.17894164031032447, 'r2': 0.09747260808944702}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0599Epoch 4/15: [                              ] 2/75 batches, loss: 0.0776Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0807Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1058Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0994Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0965Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0896Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0884Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0945Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0898Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0962Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0945Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0919Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0905Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0895Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0885Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0897Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0886Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0861Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0846Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0844Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0861Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0845Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0853Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0871Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0867Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0863Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0844Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0840Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0842Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0865Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0874Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0871Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0871Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0864Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0870Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0867Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0865Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0868Epoch 4/15: [================              ] 40/75 batches, loss: 0.0870Epoch 4/15: [================              ] 41/75 batches, loss: 0.0869Epoch 4/15: [================              ] 42/75 batches, loss: 0.0866Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0872Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0867Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0869Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0859Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0856Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0861Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0878Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0872Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0879Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0884Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0882Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0889Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0883Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0883Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0875Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0886Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0885Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0878Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0875Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0867Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0867Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0863Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0858Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0856Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0855Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0863Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0863Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0857Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0849Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0852Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0857Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0866Epoch 4/15: [==============================] 75/75 batches, loss: 0.0859
[2025-05-07 17:41:18,843][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0859
[2025-05-07 17:41:19,089][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0289, Metrics: {'mse': 0.029106775298714638, 'rmse': 0.17060707868876554, 'r2': 0.17958861589431763}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1291Epoch 5/15: [                              ] 2/75 batches, loss: 0.0963Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0878Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1001Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0949Epoch 5/15: [==                            ] 6/75 batches, loss: 0.1065Epoch 5/15: [==                            ] 7/75 batches, loss: 0.1108Epoch 5/15: [===                           ] 8/75 batches, loss: 0.1090Epoch 5/15: [===                           ] 9/75 batches, loss: 0.1043Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0978Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0949Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0940Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0954Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0927Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0962Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0955Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0924Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0925Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0932Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0906Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0921Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0916Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0928Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0901Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0916Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0916Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0916Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0920Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0920Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0899Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0895Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0912Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0916Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0908Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0900Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0897Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0896Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0896Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0881Epoch 5/15: [================              ] 40/75 batches, loss: 0.0898Epoch 5/15: [================              ] 41/75 batches, loss: 0.0911Epoch 5/15: [================              ] 42/75 batches, loss: 0.0903Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0896Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0891Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0895Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0900Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0896Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0893Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0909Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0906Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0897Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0888Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0884Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0883Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0876Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0879Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0877Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0879Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0876Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0872Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0868Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0862Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0864Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0867Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0862Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0860Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0860Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0855Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0850Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0845Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0845Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0844Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0843Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0844Epoch 5/15: [==============================] 75/75 batches, loss: 0.0841
[2025-05-07 17:41:21,733][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0841
[2025-05-07 17:41:21,996][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0302, Metrics: {'mse': 0.030342642217874527, 'rmse': 0.17419139536117886, 'r2': 0.14475423097610474}
[2025-05-07 17:41:21,996][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0554Epoch 6/15: [                              ] 2/75 batches, loss: 0.0723Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0639Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0543Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0630Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0689Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0716Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0693Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0721Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0744Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0756Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0722Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0738Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0709Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0744Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0726Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0717Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0734Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0715Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0738Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0720Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0701Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0730Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0749Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0748Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0778Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0785Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0770Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0759Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0764Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0775Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0766Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0763Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0760Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0759Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0754Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0758Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0748Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0776Epoch 6/15: [================              ] 40/75 batches, loss: 0.0776Epoch 6/15: [================              ] 41/75 batches, loss: 0.0774Epoch 6/15: [================              ] 42/75 batches, loss: 0.0769Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0770Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0779Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0778Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0775Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0776Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0770Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0761Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0759Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0766Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0760Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0761Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0755Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0758Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0755Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0757Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0757Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0755Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0752Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0754Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0746Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0740Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0747Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0746Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0742Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0738Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0743Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0745Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0745Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0746Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0740Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0735Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0737Epoch 6/15: [==============================] 75/75 batches, loss: 0.0734
[2025-05-07 17:41:24,365][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0734
[2025-05-07 17:41:24,652][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0288, Metrics: {'mse': 0.02893190085887909, 'rmse': 0.170093800177664, 'r2': 0.18451768159866333}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.1007Epoch 7/15: [                              ] 2/75 batches, loss: 0.0763Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0761Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0913Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0839Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0774Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0764Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0763Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0777Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0751Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0774Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0762Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0776Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0754Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0735Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0714Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0709Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0680Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0660Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0677Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0677Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0656Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0651Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0640Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0629Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0631Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0622Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0620Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0613Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0599Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0603Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0597Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0591Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0598Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0598Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0591Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0586Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0584Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0580Epoch 7/15: [================              ] 40/75 batches, loss: 0.0581Epoch 7/15: [================              ] 41/75 batches, loss: 0.0580Epoch 7/15: [================              ] 42/75 batches, loss: 0.0593Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0591Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0589Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0592Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0584Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0586Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0589Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0586Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0588Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0584Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0590Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0592Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0596Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0599Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0595Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0595Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0602Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0596Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0597Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0598Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0595Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0599Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0600Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0601Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0598Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0600Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0600Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0597Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0594Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0591Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0590Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0591Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0590Epoch 7/15: [==============================] 75/75 batches, loss: 0.0589
[2025-05-07 17:41:27,418][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0589
[2025-05-07 17:41:27,694][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0288, Metrics: {'mse': 0.02903684973716736, 'rmse': 0.17040202386464592, 'r2': 0.18155962228775024}
[2025-05-07 17:41:27,694][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0862Epoch 8/15: [                              ] 2/75 batches, loss: 0.0697Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0556Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0521Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0500Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0624Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0570Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0548Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0641Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0620Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0601Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0609Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0608Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0602Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0584Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0581Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0582Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0585Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0594Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0593Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0595Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0602Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0595Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0591Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0591Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0592Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0590Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0591Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0597Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0604Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0603Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0614Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0619Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0634Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0629Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0624Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0621Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0613Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0614Epoch 8/15: [================              ] 40/75 batches, loss: 0.0627Epoch 8/15: [================              ] 41/75 batches, loss: 0.0622Epoch 8/15: [================              ] 42/75 batches, loss: 0.0626Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0634Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0632Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0627Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0626Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0622Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0619Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0616Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0616Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0617Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0619Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0613Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0615Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0617Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0612Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0609Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0603Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0603Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0606Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0602Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0600Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0603Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0607Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0606Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0606Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0608Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0604Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0600Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0600Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0599Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0598Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0601Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0601Epoch 8/15: [==============================] 75/75 batches, loss: 0.0604
[2025-05-07 17:41:30,018][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0604
[2025-05-07 17:41:30,328][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0317, Metrics: {'mse': 0.03193097934126854, 'rmse': 0.17869241545535317, 'r2': 0.09998488426208496}
[2025-05-07 17:41:30,329][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0448Epoch 9/15: [                              ] 2/75 batches, loss: 0.0682Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0670Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0581Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0567Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0569Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0560Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0535Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0555Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0523Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0530Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0526Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0533Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0522Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0497Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0494Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0499Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0522Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0555Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0546Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0546Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0539Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0546Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0554Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0566Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0572Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0562Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0556Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0557Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0550Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0554Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0553Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0553Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0562Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0555Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0551Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0551Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0554Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0548Epoch 9/15: [================              ] 40/75 batches, loss: 0.0552Epoch 9/15: [================              ] 41/75 batches, loss: 0.0545Epoch 9/15: [================              ] 42/75 batches, loss: 0.0540Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0546Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0544Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0544Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0541Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0541Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0534Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0535Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0535Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0537Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0535Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0530Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0529Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0527Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0528Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0531Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0535Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0534Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0541Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0541Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0538Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0534Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0537Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0533Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0535Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0533Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0532Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0532Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0527Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0527Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0523Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0524Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0523Epoch 9/15: [==============================] 75/75 batches, loss: 0.0522
[2025-05-07 17:41:32,666][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0522
[2025-05-07 17:41:33,080][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0260, Metrics: {'mse': 0.026251522824168205, 'rmse': 0.16202321692945182, 'r2': 0.26006758213043213}
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0468Epoch 10/15: [                              ] 2/75 batches, loss: 0.0556Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0481Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0551Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0526Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0572Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0594Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0576Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0541Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0554Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0568Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0556Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0550Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0559Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0546Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0543Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0541Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0536Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0535Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0528Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0529Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0522Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0522Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0521Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0525Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0519Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0540Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0538Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0528Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0535Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0528Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0522Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0542Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0543Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0543Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0544Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0538Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0537Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0538Epoch 10/15: [================              ] 40/75 batches, loss: 0.0540Epoch 10/15: [================              ] 41/75 batches, loss: 0.0541Epoch 10/15: [================              ] 42/75 batches, loss: 0.0538Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0535Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0534Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0539Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0538Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0535Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0536Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0532Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0526Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0524Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0522Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0525Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0520Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0525Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0523Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0521Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0519Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0522Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0520Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0519Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0518Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0515Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0517Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0518Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0519Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0518Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0520Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0521Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0518Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0517Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0517Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0520Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0520Epoch 10/15: [==============================] 75/75 batches, loss: 0.0519
[2025-05-07 17:41:35,751][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0519
[2025-05-07 17:41:36,096][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0285, Metrics: {'mse': 0.028719859197735786, 'rmse': 0.16946934589398693, 'r2': 0.19049441814422607}
[2025-05-07 17:41:36,096][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0354Epoch 11/15: [                              ] 2/75 batches, loss: 0.0457Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0411Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0394Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0407Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0386Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0407Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0395Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0441Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0421Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0433Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0417Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0406Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0395Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0428Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0439Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0450Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0452Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0459Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0461Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0450Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0448Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0448Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0440Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0427Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0429Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0436Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0439Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0437Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0437Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0437Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0458Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0458Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0458Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0458Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0469Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0471Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0465Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0461Epoch 11/15: [================              ] 40/75 batches, loss: 0.0466Epoch 11/15: [================              ] 41/75 batches, loss: 0.0468Epoch 11/15: [================              ] 42/75 batches, loss: 0.0463Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0469Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0474Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0473Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0478Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0479Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0478Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0481Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0483Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0483Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0482Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0480Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0480Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0479Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0481Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0482Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0479Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0482Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0479Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0484Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0485Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0482Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0480Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0479Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0481Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0484Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0487Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0490Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0491Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0488Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0492Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0496Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0496Epoch 11/15: [==============================] 75/75 batches, loss: 0.0494
[2025-05-07 17:41:38,488][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0494
[2025-05-07 17:41:38,772][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0256, Metrics: {'mse': 0.025762641802430153, 'rmse': 0.16050745092496532, 'r2': 0.2738473415374756}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0547Epoch 12/15: [                              ] 2/75 batches, loss: 0.0624Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0547Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0589Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0551Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0556Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0530Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0540Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0532Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0578Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0593Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0602Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0603Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0613Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0601Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0581Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0581Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0571Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0558Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0545Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0545Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0541Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0528Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0526Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0518Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0518Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0509Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0516Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0514Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0514Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0507Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0510Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0507Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0512Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0509Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0507Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0501Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0506Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0508Epoch 12/15: [================              ] 40/75 batches, loss: 0.0503Epoch 12/15: [================              ] 41/75 batches, loss: 0.0496Epoch 12/15: [================              ] 42/75 batches, loss: 0.0501Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0497Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0502Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0499Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0494Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0490Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0489Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0491Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0488Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0488Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0487Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0486Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0486Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0487Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0491Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0496Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0494Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0492Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0495Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0495Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0494Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0490Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0491Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0489Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0490Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0490Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0488Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0485Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0482Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0480Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0480Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0477Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0477Epoch 12/15: [==============================] 75/75 batches, loss: 0.0475
[2025-05-07 17:41:41,472][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0475
[2025-05-07 17:41:41,765][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0253, Metrics: {'mse': 0.025504469871520996, 'rmse': 0.15970118932406546, 'r2': 0.28112417459487915}
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0644Epoch 13/15: [                              ] 2/75 batches, loss: 0.0692Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0554Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0560Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0505Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0526Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0480Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0521Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0496Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0487Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0504Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0496Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0493Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0481Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0479Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0476Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0484Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0483Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0502Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0505Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0497Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0497Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0492Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0498Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0504Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0494Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0488Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0498Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0502Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0501Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0503Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0508Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0499Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0495Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0495Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0489Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0485Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0482Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0486Epoch 13/15: [================              ] 40/75 batches, loss: 0.0478Epoch 13/15: [================              ] 41/75 batches, loss: 0.0476Epoch 13/15: [================              ] 42/75 batches, loss: 0.0474Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0474Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0475Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0474Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0470Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0466Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0465Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0464Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0460Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0467Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0467Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0468Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0471Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0473Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0473Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0480Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0479Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0477Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0477Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0474Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0475Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0478Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0477Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0475Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0475Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0474Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0474Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0470Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0468Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0467Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0466Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0467Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0466Epoch 13/15: [==============================] 75/75 batches, loss: 0.0468
[2025-05-07 17:41:44,617][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0468
[2025-05-07 17:41:44,926][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0251, Metrics: {'mse': 0.025284577161073685, 'rmse': 0.15901124853630225, 'r2': 0.28732216358184814}
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0610Epoch 14/15: [                              ] 2/75 batches, loss: 0.0529Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0497Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0459Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0484Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0503Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0468Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0477Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0477Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0503Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0491Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0483Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0500Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0514Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0503Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0497Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0497Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0523Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0521Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0518Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0528Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0530Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0515Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0504Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0502Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0523Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0516Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0512Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0517Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0511Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0512Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0513Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0509Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0506Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0511Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0502Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0501Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0497Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0496Epoch 14/15: [================              ] 40/75 batches, loss: 0.0491Epoch 14/15: [================              ] 41/75 batches, loss: 0.0487Epoch 14/15: [================              ] 42/75 batches, loss: 0.0485Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0482Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0483Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0482Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0481Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0478Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0471Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0472Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0474Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0475Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0476Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0473Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0475Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0476Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0474Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0473Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0474Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0469Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0467Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0467Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0470Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0466Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0463Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0463Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0463Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0462Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0463Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0460Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0459Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0461Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0468Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0465Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0469Epoch 14/15: [==============================] 75/75 batches, loss: 0.0465
[2025-05-07 17:41:47,710][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0465
[2025-05-07 17:41:47,997][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0251, Metrics: {'mse': 0.02534041367471218, 'rmse': 0.15918672581189733, 'r2': 0.28574836254119873}
[2025-05-07 17:41:47,998][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0356Epoch 15/15: [                              ] 2/75 batches, loss: 0.0415Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0444Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0432Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0404Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0388Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0399Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0375Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0377Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0364Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0370Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0375Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0382Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0376Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0374Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0378Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0389Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0380Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0379Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0387Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0378Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0389Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0391Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0398Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0405Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0405Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0406Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0407Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0411Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0411Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0410Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0414Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0422Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0423Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0425Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0422Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0425Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0424Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0421Epoch 15/15: [================              ] 40/75 batches, loss: 0.0422Epoch 15/15: [================              ] 41/75 batches, loss: 0.0426Epoch 15/15: [================              ] 42/75 batches, loss: 0.0422Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0420Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0419Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0421Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0427Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0423Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0427Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0436Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0431Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0434Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0431Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0431Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0429Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0429Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0427Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0425Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0428Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0428Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0426Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0426Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0428Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0427Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0423Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0421Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0426Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0423Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0422Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0421Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0421Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0421Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0423Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0423Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0424Epoch 15/15: [==============================] 75/75 batches, loss: 0.0421
[2025-05-07 17:41:50,313][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0421
[2025-05-07 17:41:50,671][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0251, Metrics: {'mse': 0.0253361277282238, 'rmse': 0.15917326323294312, 'r2': 0.28586912155151367}
[2025-05-07 17:41:50,671][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
[2025-05-07 17:41:50,672][src.training.lm_trainer][INFO] - Training completed in 43.47 seconds
[2025-05-07 17:41:50,672][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 17:41:53,752][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03194558247923851, 'rmse': 0.178733271886458, 'r2': 0.20132899284362793}
[2025-05-07 17:41:53,753][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.025284577161073685, 'rmse': 0.15901124853630225, 'r2': 0.28732216358184814}
[2025-05-07 17:41:53,753][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03944538161158562, 'rmse': 0.1986086141424526, 'r2': 0.11794966459274292}
[2025-05-07 17:41:55,431][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/layer2/fi/fi/model.pt
[2025-05-07 17:41:55,432][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▃▂▁▁▁
wandb:     best_val_mse █▅▃▃▂▁▁▁
wandb:      best_val_r2 ▁▄▆▆▇███
wandb:    best_val_rmse █▆▄▄▂▁▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▃▄▄▄▄▃▅▄▅▅▅▅
wandb:       train_loss █▄▃▂▂▂▁▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▅▃▄▃▃▅▂▃▁▁▁▁▁
wandb:          val_mse ▇█▅▃▄▃▃▅▂▃▁▁▁▁▁
wandb:           val_r2 ▂▁▄▆▅▆▆▄▇▆█████
wandb:         val_rmse ▇█▅▃▄▃▃▅▂▃▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02508
wandb:     best_val_mse 0.02528
wandb:      best_val_r2 0.28732
wandb:    best_val_rmse 0.15901
wandb:            epoch 15
wandb:   final_test_mse 0.03945
wandb:    final_test_r2 0.11795
wandb:  final_test_rmse 0.19861
wandb:  final_train_mse 0.03195
wandb:   final_train_r2 0.20133
wandb: final_train_rmse 0.17873
wandb:    final_val_mse 0.02528
wandb:     final_val_r2 0.28732
wandb:   final_val_rmse 0.15901
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04212
wandb:       train_time 43.4691
wandb:         val_loss 0.02513
wandb:          val_mse 0.02534
wandb:           val_r2 0.28587
wandb:         val_rmse 0.15917
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_174046-8eg8azpx
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_174046-8eg8azpx/logs
Experiment probe_layer2_lexical_density_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/layer2/fi/fi/results.json for layer 2
Running control submetric probing experiments...
=======================
PROBING LAYER 2 (SUBMETRIC CONTROL EXPERIMENTS)
=======================
Running experiment: probe_layer2_n_tokens_control1_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_n_tokens_control1_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control1/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=n_tokens"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 17:42:26,900][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control1/layer2/fi
experiment_name: probe_layer2_n_tokens_control1_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: n_tokens
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 17:42:26,900][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 17:42:26,900][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 17:42:26,901][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 17:42:26,901][__main__][INFO] - Determined Task Type: regression
[2025-05-07 17:42:26,905][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 17:42:26,905][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 17:42:26,905][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 17:42:30,290][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'n_tokens'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 17:42:32,686][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 17:42:32,687][src.data.datasets][INFO] - Loading 'control_n_tokens_seed1' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:42:32,927][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_n_tokens_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:50:23 2025).
[2025-05-07 17:42:33,027][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_n_tokens_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:50:23 2025).
[2025-05-07 17:42:33,328][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 17:42:33,343][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:42:33,346][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 17:42:33,348][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:42:33,404][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:42:33,506][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:42:33,525][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 17:42:33,527][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:42:33,527][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 17:42:33,528][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:42:33,588][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:42:33,662][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:42:33,689][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 17:42:33,690][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:42:33,691][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 17:42:33,693][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 17:42:33,693][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:42:33,693][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 17:42:33,693][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 17:42:33,693][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 17:42:33,694][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:42:33,694][src.data.datasets][INFO] -   Mean: 0.1375, Std: 0.1211
[2025-05-07 17:42:33,694][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 17:42:33,694][src.data.datasets][INFO] - Sample label: 0.07699999958276749
[2025-05-07 17:42:33,694][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:42:33,694][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 17:42:33,694][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 17:42:33,694][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 17:42:33,694][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6150
[2025-05-07 17:42:33,695][src.data.datasets][INFO] -   Mean: 0.1498, Std: 0.1357
[2025-05-07 17:42:33,695][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 17:42:33,695][src.data.datasets][INFO] - Sample label: 0.5860000252723694
[2025-05-07 17:42:33,695][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:42:33,695][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 17:42:33,695][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 17:42:33,695][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 17:42:33,695][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7930
[2025-05-07 17:42:33,695][src.data.datasets][INFO] -   Mean: 0.1144, Std: 0.1176
[2025-05-07 17:42:33,695][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 17:42:33,696][src.data.datasets][INFO] - Sample label: 0.04500000178813934
[2025-05-07 17:42:33,696][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 17:42:33,696][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 17:42:33,696][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 17:42:33,696][__main__][INFO] - Using model type: lm_probe for submetric n_tokens
[2025-05-07 17:42:33,696][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 17:42:40,825][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 17:42:40,826][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 17:42:40,826][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 17:42:40,826][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 17:42:40,829][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 17:42:40,829][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 17:42:40,829][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 17:42:40,829][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 17:42:40,830][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 17:42:40,830][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 17:42:40,830][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6075Epoch 1/15: [                              ] 2/75 batches, loss: 0.6282Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5408Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5043Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4726Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4392Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4240Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4644Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4829Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4619Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4446Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4432Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4234Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4259Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4218Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4328Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4213Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4206Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4169Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4138Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4269Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4302Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4239Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4109Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4034Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3977Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3907Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3862Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3858Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3883Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3821Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3767Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3732Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3698Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3652Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3634Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3592Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3552Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3527Epoch 1/15: [================              ] 40/75 batches, loss: 0.3476Epoch 1/15: [================              ] 41/75 batches, loss: 0.3440Epoch 1/15: [================              ] 42/75 batches, loss: 0.3403Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3364Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3349Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3354Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3310Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3315Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3269Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3252Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3227Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3235Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3210Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3172Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3171Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3149Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3132Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3148Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3137Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3113Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3091Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3054Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3044Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3028Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3012Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2999Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2983Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2962Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2933Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2931Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2932Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2907Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2890Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2861Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2835Epoch 1/15: [==============================] 75/75 batches, loss: 0.2824
[2025-05-07 17:42:47,166][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2824
[2025-05-07 17:42:47,379][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0444, Metrics: {'mse': 0.044232193380594254, 'rmse': 0.2103145106277602, 'r2': -1.4010283946990967}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2025Epoch 2/15: [                              ] 2/75 batches, loss: 0.1570Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1241Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1466Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1370Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1335Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1391Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1352Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1363Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1448Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1398Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1364Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1337Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1272Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1287Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1271Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1267Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1245Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1241Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1253Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1239Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1247Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1262Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1238Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1250Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1297Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1291Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1278Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1299Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1310Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1291Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1278Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1272Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1282Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1292Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1286Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1290Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1276Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1280Epoch 2/15: [================              ] 40/75 batches, loss: 0.1279Epoch 2/15: [================              ] 41/75 batches, loss: 0.1272Epoch 2/15: [================              ] 42/75 batches, loss: 0.1271Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1261Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1243Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1237Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1259Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1261Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1259Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1288Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1275Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1267Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1271Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1270Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1268Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1260Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1269Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1260Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1256Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1247Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1239Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1235Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1233Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1226Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1216Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1212Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1209Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1206Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1206Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1200Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1192Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1186Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1193Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1201Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1192Epoch 2/15: [==============================] 75/75 batches, loss: 0.1186
[2025-05-07 17:42:50,113][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1186
[2025-05-07 17:42:50,389][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0459, Metrics: {'mse': 0.045775964856147766, 'rmse': 0.21395318379530548, 'r2': -1.484827995300293}
[2025-05-07 17:42:50,390][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1361Epoch 3/15: [                              ] 2/75 batches, loss: 0.1013Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1016Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0923Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0885Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0915Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0922Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0856Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0842Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0855Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0851Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0873Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0899Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0904Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0896Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0904Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0921Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0920Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0903Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0891Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0890Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0898Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0893Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0937Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0920Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0904Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0894Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0891Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0898Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0887Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0893Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0894Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0892Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0902Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0893Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0884Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0875Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0871Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0877Epoch 3/15: [================              ] 40/75 batches, loss: 0.0877Epoch 3/15: [================              ] 41/75 batches, loss: 0.0882Epoch 3/15: [================              ] 42/75 batches, loss: 0.0882Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0873Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0873Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0868Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0868Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0876Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0885Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0889Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0881Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0873Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0875Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0871Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0880Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0873Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0871Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0863Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0862Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0853Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0848Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0842Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0839Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0847Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0850Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0845Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0841Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0847Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0840Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0840Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0847Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0843Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0842Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0836Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0835Epoch 3/15: [==============================] 75/75 batches, loss: 0.0842
[2025-05-07 17:42:52,767][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0842
[2025-05-07 17:42:53,019][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0278, Metrics: {'mse': 0.027709010988473892, 'rmse': 0.16646023846094266, 'r2': -0.5041108131408691}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0799Epoch 4/15: [                              ] 2/75 batches, loss: 0.0766Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0680Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0890Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0802Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0811Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0745Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0747Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0758Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0713Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0732Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0724Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0685Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0670Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0705Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0701Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0736Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0737Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0727Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0721Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0720Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0726Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0735Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0735Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0772Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0768Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0764Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0755Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0759Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0753Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0750Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0746Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0739Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0729Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0726Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0720Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0710Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0718Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0709Epoch 4/15: [================              ] 40/75 batches, loss: 0.0713Epoch 4/15: [================              ] 41/75 batches, loss: 0.0720Epoch 4/15: [================              ] 42/75 batches, loss: 0.0714Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0717Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0710Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0709Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0704Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0697Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0696Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0698Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0690Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0687Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0685Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0688Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0693Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0687Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0690Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0686Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0684Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0684Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0685Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0682Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0678Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0676Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0677Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0677Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0674Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0668Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0671Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0672Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0672Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0668Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0664Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0663Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0668Epoch 4/15: [==============================] 75/75 batches, loss: 0.0671
[2025-05-07 17:42:55,780][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0671
[2025-05-07 17:42:56,098][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0200, Metrics: {'mse': 0.019873881712555885, 'rmse': 0.14097475558608316, 'r2': -0.07880139350891113}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1328Epoch 5/15: [                              ] 2/75 batches, loss: 0.1041Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0996Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0907Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0829Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0762Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0759Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0699Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0656Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0631Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0622Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0624Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0616Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0613Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0637Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0642Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0641Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0653Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0637Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0624Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0626Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0609Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0608Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0609Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0638Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0640Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0656Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0659Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0653Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0634Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0624Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0639Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0632Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0629Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0635Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0631Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0627Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0627Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0623Epoch 5/15: [================              ] 40/75 batches, loss: 0.0622Epoch 5/15: [================              ] 41/75 batches, loss: 0.0625Epoch 5/15: [================              ] 42/75 batches, loss: 0.0619Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0613Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0609Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0606Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0604Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0599Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0601Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0619Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0616Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0615Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0609Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0608Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0606Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0602Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0603Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0599Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0594Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0587Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0589Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0587Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0586Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0590Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0590Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0595Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0596Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0594Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0591Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0589Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0591Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0593Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0588Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0585Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0583Epoch 5/15: [==============================] 75/75 batches, loss: 0.0580
[2025-05-07 17:42:58,796][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0580
[2025-05-07 17:42:59,097][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0214, Metrics: {'mse': 0.021325666457414627, 'rmse': 0.14603310055399985, 'r2': -0.15760767459869385}
[2025-05-07 17:42:59,098][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0405Epoch 6/15: [                              ] 2/75 batches, loss: 0.0364Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0459Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0422Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0472Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0490Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0457Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0461Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0460Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0474Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0478Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0470Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0474Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0463Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0506Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0518Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0504Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0516Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0503Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0506Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0499Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0487Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0487Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0493Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0493Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0505Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0512Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0509Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0499Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0489Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0486Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0482Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0477Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0480Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0489Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0479Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0473Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0471Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0496Epoch 6/15: [================              ] 40/75 batches, loss: 0.0489Epoch 6/15: [================              ] 41/75 batches, loss: 0.0495Epoch 6/15: [================              ] 42/75 batches, loss: 0.0505Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0506Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0507Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0517Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0512Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0511Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0508Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0503Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0500Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0506Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0509Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0506Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0506Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0505Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0503Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0498Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0496Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0493Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0494Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0494Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0492Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0492Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0492Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0493Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0491Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0489Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0491Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0491Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0496Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0494Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0492Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0490Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0491Epoch 6/15: [==============================] 75/75 batches, loss: 0.0489
[2025-05-07 17:43:01,534][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0489
[2025-05-07 17:43:01,877][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0210, Metrics: {'mse': 0.02097039856016636, 'rmse': 0.14481159677376104, 'r2': -0.13832294940948486}
[2025-05-07 17:43:01,878][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0422Epoch 7/15: [                              ] 2/75 batches, loss: 0.0409Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0393Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0410Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0399Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0405Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0410Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0442Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0471Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0499Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0493Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0480Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0471Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0462Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0448Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0439Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0443Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0444Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0445Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0440Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0429Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0421Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0411Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0418Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0416Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0422Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0424Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0423Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0421Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0413Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0410Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0406Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0404Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0409Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0403Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0396Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0411Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0407Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0401Epoch 7/15: [================              ] 40/75 batches, loss: 0.0402Epoch 7/15: [================              ] 41/75 batches, loss: 0.0403Epoch 7/15: [================              ] 42/75 batches, loss: 0.0404Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0407Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0416Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0413Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0411Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0428Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0427Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0426Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0423Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0421Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0421Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0419Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0419Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0424Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0424Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0422Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0425Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0421Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0422Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0418Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0415Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0416Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0417Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0418Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0415Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0416Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0415Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0420Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0419Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0417Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0416Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0412Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0410Epoch 7/15: [==============================] 75/75 batches, loss: 0.0410
[2025-05-07 17:43:04,264][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0410
[2025-05-07 17:43:04,512][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0221, Metrics: {'mse': 0.022008245810866356, 'rmse': 0.14835176376055106, 'r2': -0.19465970993041992}
[2025-05-07 17:43:04,513][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0314Epoch 8/15: [                              ] 2/75 batches, loss: 0.0270Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0308Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0346Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0309Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0343Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0348Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0359Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0362Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0368Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0356Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0356Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0353Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0352Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0350Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0372Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0371Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0364Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0372Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0370Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0375Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0381Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0376Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0372Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0371Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0365Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0362Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0368Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0362Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0358Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0356Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0355Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0359Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0358Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0354Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0350Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0347Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0351Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0356Epoch 8/15: [================              ] 40/75 batches, loss: 0.0361Epoch 8/15: [================              ] 41/75 batches, loss: 0.0361Epoch 8/15: [================              ] 42/75 batches, loss: 0.0364Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0363Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0367Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0370Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0365Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0365Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0363Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0360Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0356Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0358Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0359Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0359Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0358Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0363Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0363Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0362Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0366Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0367Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0377Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0374Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0373Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0373Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0372Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0370Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0367Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0373Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0374Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0374Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0372Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0372Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0369Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0368Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0366Epoch 8/15: [==============================] 75/75 batches, loss: 0.0368
[2025-05-07 17:43:06,904][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0368
[2025-05-07 17:43:07,185][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0230, Metrics: {'mse': 0.022900940850377083, 'rmse': 0.1513305681294334, 'r2': -0.2431173324584961}
[2025-05-07 17:43:07,186][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 17:43:07,186][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-07 17:43:07,186][src.training.lm_trainer][INFO] - Training completed in 22.97 seconds
[2025-05-07 17:43:07,186][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 17:43:10,050][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.01603776216506958, 'rmse': 0.12664028650105613, 'r2': -0.09410929679870605}
[2025-05-07 17:43:10,050][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.019873881712555885, 'rmse': 0.14097475558608316, 'r2': -0.07880139350891113}
[2025-05-07 17:43:10,050][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.015630127862095833, 'rmse': 0.12502050976578136, 'r2': -0.13044679164886475}
[2025-05-07 17:43:11,708][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control1/layer2/fi/fi/model.pt
[2025-05-07 17:43:11,709][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▁
wandb:     best_val_mse █▃▁
wandb:      best_val_r2 ▁▆█
wandb:    best_val_rmse █▄▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▁▅▇▆▇▆
wandb:       train_loss █▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss ██▃▁▁▁▂▂
wandb:          val_mse ██▃▁▁▁▂▂
wandb:           val_r2 ▁▁▆███▇▇
wandb:         val_rmse ██▃▁▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01996
wandb:     best_val_mse 0.01987
wandb:      best_val_r2 -0.0788
wandb:    best_val_rmse 0.14097
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.01563
wandb:    final_test_r2 -0.13045
wandb:  final_test_rmse 0.12502
wandb:  final_train_mse 0.01604
wandb:   final_train_r2 -0.09411
wandb: final_train_rmse 0.12664
wandb:    final_val_mse 0.01987
wandb:     final_val_r2 -0.0788
wandb:   final_val_rmse 0.14097
wandb:    learning_rate 0.0001
wandb:       train_loss 0.0368
wandb:       train_time 22.97147
wandb:         val_loss 0.02297
wandb:          val_mse 0.0229
wandb:           val_r2 -0.24312
wandb:         val_rmse 0.15133
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_174226-ayhi7vaw
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_174226-ayhi7vaw/logs
Experiment probe_layer2_n_tokens_control1_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control1/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_n_tokens_control2_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_n_tokens_control2_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control2/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=n_tokens"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 17:43:39,007][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control2/layer2/fi
experiment_name: probe_layer2_n_tokens_control2_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: n_tokens
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 17:43:39,008][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 17:43:39,008][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 17:43:39,008][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 17:43:39,008][__main__][INFO] - Determined Task Type: regression
[2025-05-07 17:43:39,012][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 17:43:39,012][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 17:43:39,012][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 17:43:43,115][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'n_tokens'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 17:43:45,627][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 17:43:45,628][src.data.datasets][INFO] - Loading 'control_n_tokens_seed2' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:43:45,876][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_n_tokens_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:51:35 2025).
[2025-05-07 17:43:45,943][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_n_tokens_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:51:35 2025).
[2025-05-07 17:43:46,372][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 17:43:46,383][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:43:46,384][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 17:43:46,385][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:43:46,431][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:43:46,511][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:43:46,531][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 17:43:46,532][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:43:46,532][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 17:43:46,535][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:43:46,602][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:43:46,693][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:43:46,716][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 17:43:46,719][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:43:46,719][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 17:43:46,722][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 17:43:46,723][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:43:46,723][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 17:43:46,723][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 17:43:46,723][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 17:43:46,723][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:43:46,724][src.data.datasets][INFO] -   Mean: 0.1375, Std: 0.1211
[2025-05-07 17:43:46,724][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 17:43:46,724][src.data.datasets][INFO] - Sample label: 0.5379999876022339
[2025-05-07 17:43:46,724][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:43:46,724][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 17:43:46,724][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 17:43:46,724][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 17:43:46,724][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6150
[2025-05-07 17:43:46,724][src.data.datasets][INFO] -   Mean: 0.1498, Std: 0.1357
[2025-05-07 17:43:46,724][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 17:43:46,725][src.data.datasets][INFO] - Sample label: 0.5860000252723694
[2025-05-07 17:43:46,725][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:43:46,725][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 17:43:46,725][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 17:43:46,725][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 17:43:46,725][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7930
[2025-05-07 17:43:46,725][src.data.datasets][INFO] -   Mean: 0.1144, Std: 0.1176
[2025-05-07 17:43:46,725][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 17:43:46,725][src.data.datasets][INFO] - Sample label: 0.04500000178813934
[2025-05-07 17:43:46,725][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 17:43:46,725][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 17:43:46,726][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 17:43:46,726][__main__][INFO] - Using model type: lm_probe for submetric n_tokens
[2025-05-07 17:43:46,726][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 17:43:54,440][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 17:43:54,441][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 17:43:54,441][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 17:43:54,441][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 17:43:54,444][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 17:43:54,445][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 17:43:54,445][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 17:43:54,445][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 17:43:54,445][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 17:43:54,446][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 17:43:54,446][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5900Epoch 1/15: [                              ] 2/75 batches, loss: 0.6260Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5429Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4846Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4864Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4479Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4203Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4524Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4696Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4524Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4435Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4389Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4232Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4239Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4159Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4366Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4268Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4248Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4218Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4170Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4266Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4281Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4195Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4065Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3998Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3933Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3865Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3813Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3810Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3809Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3726Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3676Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3646Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3622Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3581Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3593Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3541Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3492Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3485Epoch 1/15: [================              ] 40/75 batches, loss: 0.3441Epoch 1/15: [================              ] 41/75 batches, loss: 0.3395Epoch 1/15: [================              ] 42/75 batches, loss: 0.3362Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3327Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3308Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3341Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3302Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3333Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3286Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3268Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3245Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3240Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3214Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3177Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3192Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3173Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3156Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3166Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3161Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3141Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3126Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3087Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3078Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3063Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3050Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3033Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3015Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2990Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2953Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2963Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2956Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2932Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2910Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2885Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2865Epoch 1/15: [==============================] 75/75 batches, loss: 0.2847
[2025-05-07 17:44:00,504][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2847
[2025-05-07 17:44:00,742][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0504, Metrics: {'mse': 0.050213560461997986, 'rmse': 0.2240838246326539, 'r2': -1.7257111072540283}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1425Epoch 2/15: [                              ] 2/75 batches, loss: 0.1285Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1031Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1273Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1290Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1237Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1333Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1304Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1307Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1380Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1355Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1290Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1303Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1244Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1257Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1257Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1231Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1218Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1190Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1198Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1201Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1229Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1240Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1218Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1216Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1264Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1278Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1276Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1298Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1316Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1300Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1292Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1275Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1298Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1327Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1315Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1320Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1313Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1306Epoch 2/15: [================              ] 40/75 batches, loss: 0.1294Epoch 2/15: [================              ] 41/75 batches, loss: 0.1285Epoch 2/15: [================              ] 42/75 batches, loss: 0.1271Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1265Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1248Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1230Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1261Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1259Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1256Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1275Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1262Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1260Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1264Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1263Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1267Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1259Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1275Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1267Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1263Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1255Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1251Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1248Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1256Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1250Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1246Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1237Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1235Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1231Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1232Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1226Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1218Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1217Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1219Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1221Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1221Epoch 2/15: [==============================] 75/75 batches, loss: 0.1220
[2025-05-07 17:44:03,446][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1220
[2025-05-07 17:44:03,726][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0390, Metrics: {'mse': 0.038903333246707916, 'rmse': 0.19723927916798903, 'r2': -1.1117651462554932}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1115Epoch 3/15: [                              ] 2/75 batches, loss: 0.0820Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0801Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0747Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0752Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0787Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0738Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0690Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0674Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0737Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0710Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0727Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0772Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0808Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0790Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0778Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0825Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0815Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0814Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0803Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0812Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0823Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0800Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0872Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0864Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0851Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0849Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0857Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0869Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0858Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0859Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0871Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0868Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0870Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0862Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0851Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0841Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0839Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0839Epoch 3/15: [================              ] 40/75 batches, loss: 0.0837Epoch 3/15: [================              ] 41/75 batches, loss: 0.0838Epoch 3/15: [================              ] 42/75 batches, loss: 0.0832Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0828Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0835Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0838Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0834Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0842Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0851Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0856Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0852Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0847Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0845Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0838Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0849Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0841Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0834Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0829Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0826Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0824Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0816Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0813Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0810Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0813Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0817Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0816Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0809Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0809Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0807Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0814Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0816Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0814Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0807Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0803Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0802Epoch 3/15: [==============================] 75/75 batches, loss: 0.0811
[2025-05-07 17:44:06,493][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0811
[2025-05-07 17:44:06,767][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0279, Metrics: {'mse': 0.027823014184832573, 'rmse': 0.1668023206817956, 'r2': -0.5102992057800293}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0824Epoch 4/15: [                              ] 2/75 batches, loss: 0.0644Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0725Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0801Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0737Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0787Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0759Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0748Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0784Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0748Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0770Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0770Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0728Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0709Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0740Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0731Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0721Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0714Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0709Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0720Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0722Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0735Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0743Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0742Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0760Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0761Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0759Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0749Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0744Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0736Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0734Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0729Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0714Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0706Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0715Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0727Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0726Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0731Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0721Epoch 4/15: [================              ] 40/75 batches, loss: 0.0715Epoch 4/15: [================              ] 41/75 batches, loss: 0.0712Epoch 4/15: [================              ] 42/75 batches, loss: 0.0710Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0713Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0704Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0703Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0700Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0694Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0697Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0698Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0692Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0687Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0689Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0685Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0682Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0678Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0683Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0677Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0681Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0678Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0677Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0679Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0673Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0668Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0667Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0670Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0665Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0666Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0666Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0664Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0662Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0659Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0656Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0654Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0657Epoch 4/15: [==============================] 75/75 batches, loss: 0.0657
[2025-05-07 17:44:09,415][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0657
[2025-05-07 17:44:09,724][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0234, Metrics: {'mse': 0.023275788873434067, 'rmse': 0.15256404843027097, 'r2': -0.2634650468826294}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0790Epoch 5/15: [                              ] 2/75 batches, loss: 0.0759Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0869Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0872Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0836Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0775Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0740Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0710Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0671Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0657Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0650Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0653Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0653Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0684Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0678Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0667Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0653Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0660Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0645Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0633Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0640Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0631Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0627Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0617Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0636Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0637Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0632Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0631Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0629Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0620Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0611Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0612Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0610Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0608Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0616Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0613Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0617Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0617Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0623Epoch 5/15: [================              ] 40/75 batches, loss: 0.0628Epoch 5/15: [================              ] 41/75 batches, loss: 0.0630Epoch 5/15: [================              ] 42/75 batches, loss: 0.0626Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0640Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0634Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0630Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0629Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0621Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0616Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0625Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0620Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0614Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0607Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0605Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0606Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0602Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0598Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0593Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0590Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0584Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0581Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0578Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0580Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0578Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0580Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0576Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0575Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0575Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0572Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0568Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0570Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0571Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0570Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0567Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0566Epoch 5/15: [==============================] 75/75 batches, loss: 0.0562
[2025-05-07 17:44:12,378][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0562
[2025-05-07 17:44:12,666][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0256, Metrics: {'mse': 0.025568239390850067, 'rmse': 0.15990071729310681, 'r2': -0.38790464401245117}
[2025-05-07 17:44:12,667][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0379Epoch 6/15: [                              ] 2/75 batches, loss: 0.0406Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0422Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0362Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0474Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0457Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0450Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0484Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0448Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0443Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0443Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0429Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0451Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0445Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0467Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0472Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0457Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0512Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0503Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0492Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0485Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0474Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0480Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0486Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0494Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0488Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0490Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0486Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0484Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0484Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0479Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0474Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0483Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0481Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0484Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0481Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0476Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0470Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0493Epoch 6/15: [================              ] 40/75 batches, loss: 0.0489Epoch 6/15: [================              ] 41/75 batches, loss: 0.0487Epoch 6/15: [================              ] 42/75 batches, loss: 0.0484Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0487Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0488Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0495Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0491Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0489Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0490Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0484Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0481Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0485Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0486Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0483Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0478Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0477Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0476Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0475Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0475Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0478Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0485Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0487Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0487Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0488Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0487Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0489Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0486Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0485Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0487Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0484Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0482Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0481Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0479Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0475Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0473Epoch 6/15: [==============================] 75/75 batches, loss: 0.0474
[2025-05-07 17:44:15,008][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0474
[2025-05-07 17:44:15,263][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0248, Metrics: {'mse': 0.02475840039551258, 'rmse': 0.1573480231700182, 'r2': -0.3439446687698364}
[2025-05-07 17:44:15,264][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0380Epoch 7/15: [                              ] 2/75 batches, loss: 0.0362Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0370Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0387Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0468Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0458Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0477Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0466Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0462Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0486Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0491Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0493Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0515Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0500Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0501Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0492Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0492Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0500Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0493Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0480Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0482Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0480Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0472Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0478Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0476Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0474Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0472Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0465Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0462Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0454Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0448Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0445Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0437Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0434Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0428Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0423Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0427Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0432Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0428Epoch 7/15: [================              ] 40/75 batches, loss: 0.0424Epoch 7/15: [================              ] 41/75 batches, loss: 0.0426Epoch 7/15: [================              ] 42/75 batches, loss: 0.0426Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0424Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0422Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0417Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0415Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0416Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0412Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0409Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0407Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0411Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0415Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0419Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0421Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0424Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0424Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0424Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0429Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0426Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0429Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0425Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0420Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0424Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0426Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0430Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0428Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0430Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0425Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0426Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0427Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0425Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0424Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0422Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0421Epoch 7/15: [==============================] 75/75 batches, loss: 0.0420
[2025-05-07 17:44:17,547][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0420
[2025-05-07 17:44:17,822][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0251, Metrics: {'mse': 0.025003116577863693, 'rmse': 0.1581237381858388, 'r2': -0.35722851753234863}
[2025-05-07 17:44:17,822][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0364Epoch 8/15: [                              ] 2/75 batches, loss: 0.0268Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0348Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0359Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0339Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0325Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0357Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0348Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0354Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0357Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0364Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0362Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0366Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0358Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0346Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0352Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0349Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0356Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0361Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0379Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0373Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0374Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0365Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0366Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0380Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0374Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0384Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0380Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0376Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0380Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0380Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0379Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0375Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0377Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0378Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0374Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0380Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0377Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0376Epoch 8/15: [================              ] 40/75 batches, loss: 0.0379Epoch 8/15: [================              ] 41/75 batches, loss: 0.0376Epoch 8/15: [================              ] 42/75 batches, loss: 0.0374Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0371Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0374Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0375Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0373Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0376Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0374Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0382Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0381Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0382Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0386Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0381Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0381Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0377Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0380Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0380Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0377Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0378Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0380Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0377Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0378Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0380Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0381Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0379Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0379Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0379Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0377Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0375Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0372Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0371Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0368Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0367Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0367Epoch 8/15: [==============================] 75/75 batches, loss: 0.0370
[2025-05-07 17:44:20,135][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0370
[2025-05-07 17:44:20,409][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0247, Metrics: {'mse': 0.024647308513522148, 'rmse': 0.15699461300796963, 'r2': -0.3379143476486206}
[2025-05-07 17:44:20,410][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 17:44:20,410][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-07 17:44:20,410][src.training.lm_trainer][INFO] - Training completed in 22.87 seconds
[2025-05-07 17:44:20,410][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 17:44:23,392][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.016936879605054855, 'rmse': 0.13014176733491387, 'r2': -0.1554476022720337}
[2025-05-07 17:44:23,392][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.023275788873434067, 'rmse': 0.15256404843027097, 'r2': -0.2634650468826294}
[2025-05-07 17:44:23,392][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.017748691141605377, 'rmse': 0.13322421379616162, 'r2': -0.2836717367172241}
[2025-05-07 17:44:25,043][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control2/layer2/fi/fi/model.pt
[2025-05-07 17:44:25,044][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▂▁
wandb:     best_val_mse █▅▂▁
wandb:      best_val_r2 ▁▄▇█
wandb:    best_val_rmse █▅▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▄▆▆▆▆▆
wandb:       train_loss █▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▂▁▂▁▁▁
wandb:          val_mse █▅▂▁▂▁▁▁
wandb:           val_r2 ▁▄▇█▇███
wandb:         val_rmse █▅▂▁▂▁▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02335
wandb:     best_val_mse 0.02328
wandb:      best_val_r2 -0.26347
wandb:    best_val_rmse 0.15256
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.01775
wandb:    final_test_r2 -0.28367
wandb:  final_test_rmse 0.13322
wandb:  final_train_mse 0.01694
wandb:   final_train_r2 -0.15545
wandb: final_train_rmse 0.13014
wandb:    final_val_mse 0.02328
wandb:     final_val_r2 -0.26347
wandb:   final_val_rmse 0.15256
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03695
wandb:       train_time 22.87429
wandb:         val_loss 0.02471
wandb:          val_mse 0.02465
wandb:           val_r2 -0.33791
wandb:         val_rmse 0.15699
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_174339-17agz26e
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_174339-17agz26e/logs
Experiment probe_layer2_n_tokens_control2_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control2/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_n_tokens_control3_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_n_tokens_control3_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control3/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=n_tokens"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 17:44:51,161][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control3/layer2/fi
experiment_name: probe_layer2_n_tokens_control3_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: n_tokens
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 17:44:51,161][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 17:44:51,161][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 17:44:51,162][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 17:44:51,162][__main__][INFO] - Determined Task Type: regression
[2025-05-07 17:44:51,166][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 17:44:51,166][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 17:44:51,166][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 17:44:55,166][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'n_tokens'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 17:44:57,419][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 17:44:57,420][src.data.datasets][INFO] - Loading 'control_n_tokens_seed3' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:44:57,677][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_n_tokens_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:52:51 2025).
[2025-05-07 17:44:57,814][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_n_tokens_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:52:51 2025).
[2025-05-07 17:44:58,294][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 17:44:58,302][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:44:58,303][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 17:44:58,305][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:44:58,408][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:44:58,486][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:44:58,510][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 17:44:58,511][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:44:58,511][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 17:44:58,513][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:44:58,597][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:44:58,707][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:44:58,754][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 17:44:58,756][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:44:58,756][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 17:44:58,759][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 17:44:58,760][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:44:58,760][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 17:44:58,760][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 17:44:58,760][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 17:44:58,760][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:44:58,760][src.data.datasets][INFO] -   Mean: 0.1375, Std: 0.1211
[2025-05-07 17:44:58,760][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 17:44:58,761][src.data.datasets][INFO] - Sample label: 0.15399999916553497
[2025-05-07 17:44:58,761][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:44:58,761][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 17:44:58,761][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 17:44:58,761][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 17:44:58,761][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6150
[2025-05-07 17:44:58,761][src.data.datasets][INFO] -   Mean: 0.1498, Std: 0.1357
[2025-05-07 17:44:58,761][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 17:44:58,761][src.data.datasets][INFO] - Sample label: 0.5860000252723694
[2025-05-07 17:44:58,762][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:44:58,762][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 17:44:58,762][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 17:44:58,762][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 17:44:58,762][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7930
[2025-05-07 17:44:58,762][src.data.datasets][INFO] -   Mean: 0.1144, Std: 0.1176
[2025-05-07 17:44:58,762][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 17:44:58,762][src.data.datasets][INFO] - Sample label: 0.04500000178813934
[2025-05-07 17:44:58,762][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 17:44:58,762][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 17:44:58,763][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 17:44:58,763][__main__][INFO] - Using model type: lm_probe for submetric n_tokens
[2025-05-07 17:44:58,763][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 17:45:06,645][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 17:45:06,646][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 17:45:06,646][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 17:45:06,646][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 17:45:06,649][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 17:45:06,650][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 17:45:06,650][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 17:45:06,650][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 17:45:06,650][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 17:45:06,651][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 17:45:06,651][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6285Epoch 1/15: [                              ] 2/75 batches, loss: 0.6276Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5495Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4938Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4769Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4450Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4265Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4601Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4664Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4598Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4502Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4456Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4261Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4287Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4197Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4340Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4213Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4243Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4211Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4140Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4287Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4304Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4227Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4103Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4029Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3966Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3889Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3854Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3856Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3833Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3783Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3720Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3679Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3654Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3617Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3634Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3575Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3537Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3525Epoch 1/15: [================              ] 40/75 batches, loss: 0.3480Epoch 1/15: [================              ] 41/75 batches, loss: 0.3437Epoch 1/15: [================              ] 42/75 batches, loss: 0.3395Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3348Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3335Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3351Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3312Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3326Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3280Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3260Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3235Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3227Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3205Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3172Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3170Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3146Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3130Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3146Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3132Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3107Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3089Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3053Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3038Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3017Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3002Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2982Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2985Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2956Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2921Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2918Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2909Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2883Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2874Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2843Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2820Epoch 1/15: [==============================] 75/75 batches, loss: 0.2800
[2025-05-07 17:45:12,761][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2800
[2025-05-07 17:45:12,987][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0416, Metrics: {'mse': 0.04147952049970627, 'rmse': 0.20366521671533966, 'r2': -1.2516067028045654}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1616Epoch 2/15: [                              ] 2/75 batches, loss: 0.1220Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1008Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1241Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1220Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1184Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1340Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1342Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1383Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1480Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1419Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1369Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1328Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1280Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1292Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1264Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1259Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1255Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1245Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1236Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1226Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1241Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1245Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1235Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1223Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1266Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1265Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1273Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1293Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1313Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1308Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1295Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1285Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1308Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1320Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1305Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1289Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1276Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1282Epoch 2/15: [================              ] 40/75 batches, loss: 0.1264Epoch 2/15: [================              ] 41/75 batches, loss: 0.1252Epoch 2/15: [================              ] 42/75 batches, loss: 0.1251Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1246Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1229Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1239Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1262Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1255Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1247Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1276Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1259Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1255Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1256Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1257Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1253Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1244Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1250Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1244Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1239Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1227Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1216Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1213Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1213Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1204Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1202Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1206Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1205Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1204Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1214Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1210Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1201Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1198Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1205Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1208Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1199Epoch 2/15: [==============================] 75/75 batches, loss: 0.1194
[2025-05-07 17:45:15,707][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1194
[2025-05-07 17:45:15,983][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0356, Metrics: {'mse': 0.03549346327781677, 'rmse': 0.18839708935601093, 'r2': -0.9266693592071533}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.0974Epoch 3/15: [                              ] 2/75 batches, loss: 0.0836Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0868Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1066Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0932Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0901Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0839Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0782Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0750Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0794Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0795Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0821Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0849Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0854Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0845Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0837Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0852Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0845Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0848Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0845Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0858Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0862Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0846Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0903Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0895Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0887Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0875Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0873Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0888Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0889Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0891Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0889Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0892Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0893Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0887Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0885Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0875Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0874Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0874Epoch 3/15: [================              ] 40/75 batches, loss: 0.0875Epoch 3/15: [================              ] 41/75 batches, loss: 0.0870Epoch 3/15: [================              ] 42/75 batches, loss: 0.0862Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0855Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0872Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0865Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0862Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0872Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0882Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0885Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0877Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0869Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0864Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0856Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0861Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0854Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0844Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0838Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0836Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0833Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0824Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0818Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0821Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0824Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0836Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0832Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0828Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0829Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0823Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0819Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0821Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0818Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0819Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0814Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0815Epoch 3/15: [==============================] 75/75 batches, loss: 0.0821
[2025-05-07 17:45:18,717][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0821
[2025-05-07 17:45:19,009][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0276, Metrics: {'mse': 0.027468394488096237, 'rmse': 0.16573591791792217, 'r2': -0.49104952812194824}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1642Epoch 4/15: [                              ] 2/75 batches, loss: 0.1106Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0945Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0934Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0834Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0825Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0805Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0785Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0793Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0764Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0833Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0849Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0808Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0788Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0812Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0807Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0787Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0767Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0757Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0749Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0738Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0757Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0768Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0779Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0800Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0808Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0809Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0808Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0798Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0799Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0797Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0789Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0781Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0780Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0776Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0774Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0760Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0769Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0762Epoch 4/15: [================              ] 40/75 batches, loss: 0.0756Epoch 4/15: [================              ] 41/75 batches, loss: 0.0755Epoch 4/15: [================              ] 42/75 batches, loss: 0.0748Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0754Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0755Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0752Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0744Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0739Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0737Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0733Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0727Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0725Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0727Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0725Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0729Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0724Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0728Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0721Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0718Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0716Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0711Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0714Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0709Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0706Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0707Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0703Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0703Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0697Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0698Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0695Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0699Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0694Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0691Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0689Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0694Epoch 4/15: [==============================] 75/75 batches, loss: 0.0692
[2025-05-07 17:45:21,686][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0692
[2025-05-07 17:45:21,990][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0233, Metrics: {'mse': 0.02325899340212345, 'rmse': 0.15250899449581146, 'r2': -0.26255321502685547}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0992Epoch 5/15: [                              ] 2/75 batches, loss: 0.0880Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0997Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0991Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0857Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0797Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0769Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0739Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0696Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0674Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0683Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0675Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0686Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0689Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0707Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0687Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0686Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0688Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0676Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0664Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0670Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0675Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0671Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0666Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0677Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0681Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0676Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0667Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0656Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0644Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0642Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0641Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0635Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0637Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0642Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0635Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0637Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0631Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0624Epoch 5/15: [================              ] 40/75 batches, loss: 0.0619Epoch 5/15: [================              ] 41/75 batches, loss: 0.0625Epoch 5/15: [================              ] 42/75 batches, loss: 0.0624Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0620Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0614Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0613Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0613Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0604Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0599Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0607Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0602Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0599Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0599Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0595Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0597Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0591Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0587Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0585Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0579Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0578Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0579Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0574Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0581Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0587Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0588Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0584Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0581Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0576Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0573Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0572Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0575Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0576Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0572Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0570Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0570Epoch 5/15: [==============================] 75/75 batches, loss: 0.0564
[2025-05-07 17:45:24,682][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0564
[2025-05-07 17:45:24,917][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0241, Metrics: {'mse': 0.024039030075073242, 'rmse': 0.1550452517011509, 'r2': -0.3048955202102661}
[2025-05-07 17:45:24,918][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0678Epoch 6/15: [                              ] 2/75 batches, loss: 0.0562Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0493Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0455Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0473Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0432Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0398Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0435Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0420Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0423Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0427Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0430Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0488Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0477Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0491Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0493Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0475Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0494Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0482Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0470Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0463Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0468Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0478Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0486Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0487Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0487Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0486Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0477Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0470Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0471Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0465Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0462Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0454Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0454Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0462Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0456Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0450Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0447Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0469Epoch 6/15: [================              ] 40/75 batches, loss: 0.0474Epoch 6/15: [================              ] 41/75 batches, loss: 0.0474Epoch 6/15: [================              ] 42/75 batches, loss: 0.0471Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0465Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0463Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0470Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0466Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0465Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0461Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0460Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0459Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0465Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0464Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0468Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0464Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0464Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0463Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0463Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0464Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0463Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0466Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0469Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0466Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0466Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0469Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0469Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0467Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0467Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0472Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0470Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0467Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0469Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0470Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0472Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0470Epoch 6/15: [==============================] 75/75 batches, loss: 0.0469
[2025-05-07 17:45:27,256][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0469
[2025-05-07 17:45:27,489][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0238, Metrics: {'mse': 0.023755930364131927, 'rmse': 0.15412958951522557, 'r2': -0.2895282506942749}
[2025-05-07 17:45:27,490][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0391Epoch 7/15: [                              ] 2/75 batches, loss: 0.0376Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0386Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0385Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0392Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0396Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0391Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0381Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0379Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0393Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0389Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0387Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0400Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0402Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0405Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0416Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0417Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0449Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0433Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0447Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0449Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0442Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0434Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0445Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0446Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0452Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0451Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0451Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0445Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0438Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0432Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0450Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0442Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0441Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0436Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0437Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0432Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0439Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0433Epoch 7/15: [================              ] 40/75 batches, loss: 0.0438Epoch 7/15: [================              ] 41/75 batches, loss: 0.0438Epoch 7/15: [================              ] 42/75 batches, loss: 0.0442Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0441Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0442Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0441Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0438Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0435Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0432Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0430Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0427Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0427Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0427Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0424Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0423Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0425Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0426Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0424Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0425Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0420Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0426Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0424Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0420Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0427Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0427Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0425Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0424Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0421Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0417Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0415Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0414Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0414Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0411Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0408Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0407Epoch 7/15: [==============================] 75/75 batches, loss: 0.0405
[2025-05-07 17:45:29,836][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0405
[2025-05-07 17:45:30,072][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0235, Metrics: {'mse': 0.023453665897250175, 'rmse': 0.15314589742219728, 'r2': -0.2731205224990845}
[2025-05-07 17:45:30,073][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0239Epoch 8/15: [                              ] 2/75 batches, loss: 0.0244Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0271Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0327Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0295Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0301Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0301Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0306Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0340Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0332Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0332Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0328Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0332Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0355Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0353Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0353Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0357Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0352Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0364Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0364Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0362Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0368Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0360Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0362Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0361Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0358Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0357Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0357Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0354Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0353Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0376Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0379Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0380Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0375Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0370Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0364Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0361Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0359Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0360Epoch 8/15: [================              ] 40/75 batches, loss: 0.0360Epoch 8/15: [================              ] 41/75 batches, loss: 0.0363Epoch 8/15: [================              ] 42/75 batches, loss: 0.0359Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0359Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0355Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0356Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0362Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0361Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0358Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0359Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0359Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0359Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0362Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0358Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0357Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0358Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0356Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0356Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0352Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0356Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0357Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0358Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0356Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0356Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0359Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0361Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0361Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0362Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0362Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0359Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0356Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0356Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0354Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0354Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0353Epoch 8/15: [==============================] 75/75 batches, loss: 0.0364
[2025-05-07 17:45:32,325][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0364
[2025-05-07 17:45:32,606][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0233, Metrics: {'mse': 0.023229237645864487, 'rmse': 0.1524114091722286, 'r2': -0.26093804836273193}
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0346Epoch 9/15: [                              ] 2/75 batches, loss: 0.0452Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0362Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0323Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0348Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0383Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0372Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0369Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0349Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0357Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0351Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0336Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0320Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0321Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0335Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0334Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0336Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0344Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0354Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0353Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0351Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0372Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0366Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0369Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0374Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0371Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0361Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0357Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0357Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0355Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0350Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0346Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0348Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0354Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0349Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0344Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0346Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0339Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0336Epoch 9/15: [================              ] 40/75 batches, loss: 0.0341Epoch 9/15: [================              ] 41/75 batches, loss: 0.0348Epoch 9/15: [================              ] 42/75 batches, loss: 0.0346Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0346Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0343Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0344Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0342Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0342Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0340Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0339Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0337Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0337Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0339Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0336Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0335Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0334Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0333Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0332Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0331Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0329Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0327Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0326Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0326Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0326Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0326Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0325Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0322Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0321Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0318Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0318Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0319Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0318Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0317Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0318Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0316Epoch 9/15: [==============================] 75/75 batches, loss: 0.0314
[2025-05-07 17:45:35,336][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0314
[2025-05-07 17:45:35,578][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0237, Metrics: {'mse': 0.02360953576862812, 'rmse': 0.1536539481062173, 'r2': -0.2815816402435303}
[2025-05-07 17:45:35,579][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0484Epoch 10/15: [                              ] 2/75 batches, loss: 0.0428Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0339Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0367Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0364Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0383Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0382Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0385Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0380Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0365Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0352Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0345Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0344Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0353Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0345Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0349Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0343Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0348Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0347Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0365Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0355Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0351Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0347Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0343Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0341Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0340Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0338Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0356Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0353Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0353Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0350Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0346Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0348Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0346Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0343Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0364Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0357Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0357Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0353Epoch 10/15: [================              ] 40/75 batches, loss: 0.0352Epoch 10/15: [================              ] 41/75 batches, loss: 0.0359Epoch 10/15: [================              ] 42/75 batches, loss: 0.0355Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0360Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0360Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0366Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0364Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0363Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0363Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0362Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0360Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0359Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0356Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0359Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0357Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0359Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0355Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0355Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0352Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0354Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0353Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0350Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0348Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0347Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0346Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0346Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0345Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0343Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0345Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0345Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0344Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0344Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0347Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0346Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0348Epoch 10/15: [==============================] 75/75 batches, loss: 0.0346
[2025-05-07 17:45:37,908][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0346
[2025-05-07 17:45:38,248][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0261, Metrics: {'mse': 0.026029858738183975, 'rmse': 0.1613377164155486, 'r2': -0.4129624366760254}
[2025-05-07 17:45:38,248][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0373Epoch 11/15: [                              ] 2/75 batches, loss: 0.0325Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0282Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0272Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0276Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0292Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0299Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0282Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0288Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0287Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0278Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0275Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0263Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0277Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0266Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0263Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0276Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0280Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0285Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0287Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0282Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0286Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0279Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0283Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0282Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0293Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0294Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0294Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0307Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0306Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0303Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0316Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0318Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0318Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0312Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0311Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0309Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0309Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0305Epoch 11/15: [================              ] 40/75 batches, loss: 0.0302Epoch 11/15: [================              ] 41/75 batches, loss: 0.0303Epoch 11/15: [================              ] 42/75 batches, loss: 0.0300Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0298Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0296Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0298Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0298Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0298Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0295Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0308Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0307Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0304Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0301Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0298Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0296Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0300Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0300Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0302Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0301Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0301Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0299Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0300Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0302Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0300Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0298Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0296Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0295Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0296Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0297Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0296Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0295Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0296Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0299Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0299Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0297Epoch 11/15: [==============================] 75/75 batches, loss: 0.0296
[2025-05-07 17:45:40,551][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0296
[2025-05-07 17:45:40,801][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0210, Metrics: {'mse': 0.020909123122692108, 'rmse': 0.14459987248504788, 'r2': -0.13499677181243896}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0508Epoch 12/15: [                              ] 2/75 batches, loss: 0.0405Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0349Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0336Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0324Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0296Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0311Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0333Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0320Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0312Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0302Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0289Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0289Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0278Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0273Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0292Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0292Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0292Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0293Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0289Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0293Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0291Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0293Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0296Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0293Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0297Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0297Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0299Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0299Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0298Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0295Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0300Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0301Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0297Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0301Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0310Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0305Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0301Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0302Epoch 12/15: [================              ] 40/75 batches, loss: 0.0302Epoch 12/15: [================              ] 41/75 batches, loss: 0.0302Epoch 12/15: [================              ] 42/75 batches, loss: 0.0304Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0299Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0300Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0298Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0296Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0294Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0295Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0294Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0290Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0288Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0289Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0295Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0304Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0302Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0301Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0303Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0302Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0303Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0303Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0301Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0303Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0302Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0301Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0302Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0302Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0302Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0301Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0300Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0300Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0300Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0298Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0296Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0299Epoch 12/15: [==============================] 75/75 batches, loss: 0.0297
[2025-05-07 17:45:43,457][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0297
[2025-05-07 17:45:43,716][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0200, Metrics: {'mse': 0.019917307421565056, 'rmse': 0.141128690993593, 'r2': -0.08115875720977783}
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0268Epoch 13/15: [                              ] 2/75 batches, loss: 0.0343Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0274Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0265Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0234Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0349Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0324Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0304Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0305Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0327Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0314Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0302Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0297Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0298Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0284Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0298Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0292Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0290Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0283Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0277Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0275Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0267Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0275Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0279Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0273Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0267Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0262Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0265Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0266Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0264Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0263Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0267Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0265Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0271Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0270Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0279Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0281Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0278Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0277Epoch 13/15: [================              ] 40/75 batches, loss: 0.0273Epoch 13/15: [================              ] 41/75 batches, loss: 0.0272Epoch 13/15: [================              ] 42/75 batches, loss: 0.0269Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0269Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0270Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0269Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0265Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0265Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0267Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0266Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0263Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0262Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0268Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0269Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0269Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0269Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0269Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0270Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0269Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0269Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0267Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0265Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0264Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0268Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0267Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0266Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0268Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0267Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0265Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0265Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0268Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0268Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0266Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0264Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0263Epoch 13/15: [==============================] 75/75 batches, loss: 0.0264
[2025-05-07 17:45:46,470][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0264
[2025-05-07 17:45:46,749][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0268, Metrics: {'mse': 0.02678818814456463, 'rmse': 0.1636709752661254, 'r2': -0.45412635803222656}
[2025-05-07 17:45:46,749][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0208Epoch 14/15: [                              ] 2/75 batches, loss: 0.0186Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0256Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0238Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0316Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0301Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0281Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0280Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0298Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0309Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0299Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0305Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0295Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0282Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0277Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0268Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0260Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0265Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0264Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0260Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0263Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0262Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0260Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0272Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0264Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0267Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0265Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0263Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0260Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0271Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0270Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0270Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0269Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0268Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0272Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0271Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0275Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0272Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0269Epoch 14/15: [================              ] 40/75 batches, loss: 0.0266Epoch 14/15: [================              ] 41/75 batches, loss: 0.0264Epoch 14/15: [================              ] 42/75 batches, loss: 0.0264Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0264Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0264Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0265Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0263Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0261Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0267Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0269Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0267Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0268Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0266Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0269Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0269Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0267Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0265Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0263Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0261Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0260Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0259Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0259Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0261Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0259Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0261Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0260Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0262Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0260Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0259Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0265Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0267Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0265Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0265Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0264Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0266Epoch 14/15: [==============================] 75/75 batches, loss: 0.0266
[2025-05-07 17:45:49,040][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0266
[2025-05-07 17:45:49,314][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0221, Metrics: {'mse': 0.02200234867632389, 'rmse': 0.14833188691688612, 'r2': -0.19433963298797607}
[2025-05-07 17:45:49,314][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0130Epoch 15/15: [                              ] 2/75 batches, loss: 0.0197Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0208Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0201Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0231Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0251Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0246Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0251Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0249Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0233Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0236Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0231Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0238Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0232Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0235Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0229Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0239Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0235Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0252Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0255Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0248Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0251Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0247Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0240Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0237Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0245Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0244Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0242Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0244Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0241Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0244Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0247Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0246Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0245Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0246Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0245Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0243Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0250Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0246Epoch 15/15: [================              ] 40/75 batches, loss: 0.0245Epoch 15/15: [================              ] 41/75 batches, loss: 0.0244Epoch 15/15: [================              ] 42/75 batches, loss: 0.0249Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0260Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0261Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0258Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0256Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0255Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0255Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0254Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0253Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0252Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0250Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0249Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0250Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0250Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0248Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0247Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0246Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0246Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0246Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0243Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0242Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0242Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0243Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0241Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0240Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0240Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0241Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0241Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0246Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0244Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0245Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0245Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0244Epoch 15/15: [==============================] 75/75 batches, loss: 0.0243
[2025-05-07 17:45:51,714][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0243
[2025-05-07 17:45:52,070][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0188, Metrics: {'mse': 0.01875155046582222, 'rmse': 0.13693630075996, 'r2': -0.01787865161895752}
[2025-05-07 17:45:52,449][src.training.lm_trainer][INFO] - Training completed in 42.82 seconds
[2025-05-07 17:45:52,449][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 17:45:55,517][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.01498534344136715, 'rmse': 0.12241463736566452, 'r2': -0.022312283515930176}
[2025-05-07 17:45:55,517][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.01875155046582222, 'rmse': 0.13693630075996, 'r2': -0.01787865161895752}
[2025-05-07 17:45:55,517][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.013514876365661621, 'rmse': 0.11625350044476777, 'r2': 0.022538363933563232}
[2025-05-07 17:45:57,155][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control3/layer2/fi/fi/model.pt
[2025-05-07 17:45:57,157][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▄▂▂▂▁▁
wandb:     best_val_mse █▆▄▂▂▂▁▁
wandb:      best_val_r2 ▁▃▅▇▇▇██
wandb:    best_val_rmse █▆▄▃▃▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▅▆▆▆▆▆▆▅▆▆▅▆
wandb:       train_loss █▄▃▂▂▂▁▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▄▂▃▃▂▂▂▃▂▁▃▂▁
wandb:          val_mse █▆▄▂▃▃▂▂▂▃▂▁▃▂▁
wandb:           val_r2 ▁▃▅▇▆▆▇▇▇▆▇█▆▇█
wandb:         val_rmse █▆▄▃▃▃▃▃▃▄▂▁▄▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01882
wandb:     best_val_mse 0.01875
wandb:      best_val_r2 -0.01788
wandb:    best_val_rmse 0.13694
wandb:            epoch 15
wandb:   final_test_mse 0.01351
wandb:    final_test_r2 0.02254
wandb:  final_test_rmse 0.11625
wandb:  final_train_mse 0.01499
wandb:   final_train_r2 -0.02231
wandb: final_train_rmse 0.12241
wandb:    final_val_mse 0.01875
wandb:     final_val_r2 -0.01788
wandb:   final_val_rmse 0.13694
wandb:    learning_rate 0.0001
wandb:       train_loss 0.02426
wandb:       train_time 42.81794
wandb:         val_loss 0.01882
wandb:          val_mse 0.01875
wandb:           val_r2 -0.01788
wandb:         val_rmse 0.13694
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_174451-t0d8u5ab
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_174451-t0d8u5ab/logs
Experiment probe_layer2_n_tokens_control3_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control3/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_avg_verb_edges_control1_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_verb_edges_control1_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control1/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_verb_edges"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 17:46:22,163][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control1/layer2/fi
experiment_name: probe_layer2_avg_verb_edges_control1_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 17:46:22,163][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 17:46:22,163][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 17:46:22,163][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 17:46:22,163][__main__][INFO] - Determined Task Type: regression
[2025-05-07 17:46:22,168][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 17:46:22,168][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 17:46:22,168][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 17:46:25,478][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 17:46:27,956][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 17:46:27,957][src.data.datasets][INFO] - Loading 'control_avg_verb_edges_seed1' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:46:28,219][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_verb_edges_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:29:02 2025).
[2025-05-07 17:46:28,320][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_verb_edges_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:29:02 2025).
[2025-05-07 17:46:28,616][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 17:46:28,624][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:46:28,625][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 17:46:28,628][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:46:28,726][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:46:28,860][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:46:28,904][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 17:46:28,906][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:46:28,906][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 17:46:28,909][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:46:29,017][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:46:29,158][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:46:29,199][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 17:46:29,201][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:46:29,201][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 17:46:29,204][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 17:46:29,204][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:46:29,204][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 17:46:29,204][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 17:46:29,204][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 17:46:29,204][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:46:29,205][src.data.datasets][INFO] -   Mean: 0.2055, Std: 0.2125
[2025-05-07 17:46:29,205][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 17:46:29,205][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 17:46:29,205][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:46:29,205][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 17:46:29,205][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 17:46:29,205][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 17:46:29,205][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:46:29,206][src.data.datasets][INFO] -   Mean: 0.2770, Std: 0.2478
[2025-05-07 17:46:29,206][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 17:46:29,206][src.data.datasets][INFO] - Sample label: 0.550000011920929
[2025-05-07 17:46:29,206][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:46:29,206][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 17:46:29,206][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 17:46:29,206][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 17:46:29,206][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:46:29,206][src.data.datasets][INFO] -   Mean: 0.3600, Std: 0.2660
[2025-05-07 17:46:29,206][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 17:46:29,207][src.data.datasets][INFO] - Sample label: 0.6000000238418579
[2025-05-07 17:46:29,207][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 17:46:29,207][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 17:46:29,207][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 17:46:29,207][__main__][INFO] - Using model type: lm_probe for submetric avg_verb_edges
[2025-05-07 17:46:29,207][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 17:46:37,677][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 17:46:37,678][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 17:46:37,678][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 17:46:37,678][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 17:46:37,681][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 17:46:37,681][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 17:46:37,681][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 17:46:37,681][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 17:46:37,682][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 17:46:37,682][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 17:46:37,682][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5949Epoch 1/15: [                              ] 2/75 batches, loss: 0.6205Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5635Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5020Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4821Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4667Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4402Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4646Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4759Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4559Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4434Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4427Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4305Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4343Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4298Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4478Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4376Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4365Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4384Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4322Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4494Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4527Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4420Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4316Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4232Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4173Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4085Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4043Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4022Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4010Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3957Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3929Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3911Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3889Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3847Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3860Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3822Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3793Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3777Epoch 1/15: [================              ] 40/75 batches, loss: 0.3737Epoch 1/15: [================              ] 41/75 batches, loss: 0.3690Epoch 1/15: [================              ] 42/75 batches, loss: 0.3642Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3592Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3566Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3563Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3528Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3531Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3494Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3471Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3460Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3476Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3479Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3444Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3461Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3443Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3440Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3452Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3431Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3410Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3386Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3350Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3347Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3318Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3307Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3285Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3279Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3249Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3214Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3214Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3193Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3173Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3161Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3134Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3112Epoch 1/15: [==============================] 75/75 batches, loss: 0.3111
[2025-05-07 17:46:44,692][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3111
[2025-05-07 17:46:44,914][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1141, Metrics: {'mse': 0.11456954479217529, 'rmse': 0.33848123255532986, 'r2': -0.8655285835266113}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1924Epoch 2/15: [                              ] 2/75 batches, loss: 0.1816Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1356Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1720Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1568Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1577Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1664Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1562Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1495Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1519Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1479Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1484Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1473Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1433Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1463Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1420Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1459Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1439Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1427Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1425Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1430Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1456Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1471Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1445Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1453Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1520Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1511Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1529Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1574Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1590Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1593Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1577Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1557Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1583Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1599Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1601Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1596Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1567Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1553Epoch 2/15: [================              ] 40/75 batches, loss: 0.1543Epoch 2/15: [================              ] 41/75 batches, loss: 0.1532Epoch 2/15: [================              ] 42/75 batches, loss: 0.1524Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1525Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1503Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1497Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1503Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1495Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1486Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1504Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1493Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1482Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1486Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1495Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1486Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1485Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1492Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1492Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1499Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1501Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1490Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1495Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1498Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1498Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1501Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1494Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1496Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1490Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1494Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1497Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1492Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1492Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1497Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1501Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1490Epoch 2/15: [==============================] 75/75 batches, loss: 0.1480
[2025-05-07 17:46:47,629][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1480
[2025-05-07 17:46:47,867][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1017, Metrics: {'mse': 0.1021592989563942, 'rmse': 0.31962368334714214, 'r2': -0.6634533405303955}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1382Epoch 3/15: [                              ] 2/75 batches, loss: 0.1094Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1344Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1260Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1170Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1180Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1174Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1133Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1134Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1153Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1142Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1104Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1139Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1186Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1169Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1181Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1183Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1178Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1183Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1169Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1160Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1175Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1167Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1237Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1227Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1222Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1201Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1186Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1195Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1179Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1180Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1178Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1186Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1185Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1182Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1170Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1154Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1150Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1157Epoch 3/15: [================              ] 40/75 batches, loss: 0.1158Epoch 3/15: [================              ] 41/75 batches, loss: 0.1162Epoch 3/15: [================              ] 42/75 batches, loss: 0.1160Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1151Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1154Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1157Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1162Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1161Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1165Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1164Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1165Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1161Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1164Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1156Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1164Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1155Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1149Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1145Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1142Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1139Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1129Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1126Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1133Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1149Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1157Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1154Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1151Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1144Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1138Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1132Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1135Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1131Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1142Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1135Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1134Epoch 3/15: [==============================] 75/75 batches, loss: 0.1142
[2025-05-07 17:46:50,608][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1142
[2025-05-07 17:46:50,884][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0863, Metrics: {'mse': 0.08668919652700424, 'rmse': 0.2944302914562363, 'r2': -0.4115546941757202}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0928Epoch 4/15: [                              ] 2/75 batches, loss: 0.1226Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1129Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1163Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1185Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1171Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1115Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1104Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1078Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1041Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1034Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1059Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1027Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0991Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1023Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1023Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1019Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1003Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1000Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1015Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1007Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1008Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1022Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1027Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1046Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1046Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1039Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1034Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1040Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1030Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1031Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1028Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1020Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.1013Epoch 4/15: [==============                ] 35/75 batches, loss: 0.1010Epoch 4/15: [==============                ] 36/75 batches, loss: 0.1007Epoch 4/15: [==============                ] 37/75 batches, loss: 0.1005Epoch 4/15: [===============               ] 38/75 batches, loss: 0.1018Epoch 4/15: [===============               ] 39/75 batches, loss: 0.1011Epoch 4/15: [================              ] 40/75 batches, loss: 0.1013Epoch 4/15: [================              ] 41/75 batches, loss: 0.1010Epoch 4/15: [================              ] 42/75 batches, loss: 0.1026Epoch 4/15: [=================             ] 43/75 batches, loss: 0.1021Epoch 4/15: [=================             ] 44/75 batches, loss: 0.1020Epoch 4/15: [==================            ] 45/75 batches, loss: 0.1016Epoch 4/15: [==================            ] 46/75 batches, loss: 0.1012Epoch 4/15: [==================            ] 47/75 batches, loss: 0.1010Epoch 4/15: [===================           ] 48/75 batches, loss: 0.1014Epoch 4/15: [===================           ] 49/75 batches, loss: 0.1015Epoch 4/15: [====================          ] 50/75 batches, loss: 0.1008Epoch 4/15: [====================          ] 51/75 batches, loss: 0.1008Epoch 4/15: [====================          ] 52/75 batches, loss: 0.1008Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.1008Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.1014Epoch 4/15: [======================        ] 55/75 batches, loss: 0.1016Epoch 4/15: [======================        ] 56/75 batches, loss: 0.1014Epoch 4/15: [======================        ] 57/75 batches, loss: 0.1016Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.1007Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.1006Epoch 4/15: [========================      ] 60/75 batches, loss: 0.1008Epoch 4/15: [========================      ] 61/75 batches, loss: 0.1008Epoch 4/15: [========================      ] 62/75 batches, loss: 0.1004Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0994Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0987Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0982Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0977Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0978Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0985Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0989Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0987Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0985Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0985Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0986Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0989Epoch 4/15: [==============================] 75/75 batches, loss: 0.0986
[2025-05-07 17:46:53,521][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0986
[2025-05-07 17:46:53,826][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0792, Metrics: {'mse': 0.07947543263435364, 'rmse': 0.2819138744977864, 'r2': -0.29409337043762207}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1486Epoch 5/15: [                              ] 2/75 batches, loss: 0.1101Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1121Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1155Epoch 5/15: [==                            ] 5/75 batches, loss: 0.1067Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0981Epoch 5/15: [==                            ] 7/75 batches, loss: 0.1058Epoch 5/15: [===                           ] 8/75 batches, loss: 0.1022Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0973Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0961Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0961Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0941Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0937Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0938Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0970Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0977Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0965Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0969Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0959Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0961Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0952Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0935Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0970Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0970Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.1010Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.1020Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.1012Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0996Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0995Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0977Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0983Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0987Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0982Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0987Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0989Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0979Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0977Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0979Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0976Epoch 5/15: [================              ] 40/75 batches, loss: 0.0971Epoch 5/15: [================              ] 41/75 batches, loss: 0.0970Epoch 5/15: [================              ] 42/75 batches, loss: 0.0961Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0955Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0949Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0945Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0942Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0935Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0929Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0947Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0948Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0943Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0941Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0934Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0934Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0934Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0923Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0921Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0918Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0915Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0917Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0913Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0912Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0921Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0918Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0914Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0915Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0906Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0903Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0900Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0902Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0906Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0905Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0907Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0905Epoch 5/15: [==============================] 75/75 batches, loss: 0.0902
[2025-05-07 17:46:56,473][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0902
[2025-05-07 17:46:56,831][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0854, Metrics: {'mse': 0.08585493266582489, 'rmse': 0.2930101238282133, 'r2': -0.39797043800354004}
[2025-05-07 17:46:56,832][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0523Epoch 6/15: [                              ] 2/75 batches, loss: 0.0712Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0675Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0725Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0736Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0673Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0664Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0714Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0698Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0726Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0723Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0731Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0751Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0760Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0763Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0779Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0774Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0781Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0769Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0753Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0738Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0740Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0747Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0747Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0736Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0746Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0749Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0749Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0743Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0742Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0737Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0727Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0721Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0719Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0719Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0718Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0717Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0720Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0739Epoch 6/15: [================              ] 40/75 batches, loss: 0.0746Epoch 6/15: [================              ] 41/75 batches, loss: 0.0755Epoch 6/15: [================              ] 42/75 batches, loss: 0.0752Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0750Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0759Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0763Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0766Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0772Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0773Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0781Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0783Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0793Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0801Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0797Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0795Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0793Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0794Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0789Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0796Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0794Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0801Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0808Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0807Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0806Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0805Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0808Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0813Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0810Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0810Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0810Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0806Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0802Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0797Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0792Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0796Epoch 6/15: [==============================] 75/75 batches, loss: 0.0797
[2025-05-07 17:46:59,144][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0797
[2025-05-07 17:46:59,383][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0975, Metrics: {'mse': 0.09801537543535233, 'rmse': 0.3130740734001337, 'r2': -0.5959781408309937}
[2025-05-07 17:46:59,383][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0803Epoch 7/15: [                              ] 2/75 batches, loss: 0.0703Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0712Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0750Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0743Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0765Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0776Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0776Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0779Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0809Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0810Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0798Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0768Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0746Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0768Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0784Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0765Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0794Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0776Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0777Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0765Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0756Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0743Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0770Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0769Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0776Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0770Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0781Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0773Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0773Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0759Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0751Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0744Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0742Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0738Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0744Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0755Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0759Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0758Epoch 7/15: [================              ] 40/75 batches, loss: 0.0757Epoch 7/15: [================              ] 41/75 batches, loss: 0.0758Epoch 7/15: [================              ] 42/75 batches, loss: 0.0763Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0767Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0765Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0763Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0763Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0760Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0769Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0767Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0766Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0761Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0765Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0761Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0752Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0765Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0767Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0767Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0767Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0765Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0775Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0771Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0769Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0768Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0770Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0777Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0774Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0774Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0769Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0774Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0778Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0779Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0778Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0775Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0769Epoch 7/15: [==============================] 75/75 batches, loss: 0.0766
[2025-05-07 17:47:01,666][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0766
[2025-05-07 17:47:01,924][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0909, Metrics: {'mse': 0.09137581288814545, 'rmse': 0.3022843245822473, 'r2': -0.48786652088165283}
[2025-05-07 17:47:01,925][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.1128Epoch 8/15: [                              ] 2/75 batches, loss: 0.0893Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0856Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0808Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0804Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0830Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0765Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0734Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0709Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0675Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0659Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0664Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0649Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0627Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0631Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0629Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0651Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0652Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0645Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0634Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0644Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0666Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0660Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0665Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0654Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0644Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0637Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0640Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0634Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0640Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0639Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0645Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0638Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0636Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0645Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0645Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0640Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0651Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0655Epoch 8/15: [================              ] 40/75 batches, loss: 0.0667Epoch 8/15: [================              ] 41/75 batches, loss: 0.0662Epoch 8/15: [================              ] 42/75 batches, loss: 0.0682Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0686Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0695Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0689Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0684Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0693Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0688Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0683Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0686Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0684Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0677Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0678Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0675Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0677Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0676Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0681Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0678Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0679Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0693Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0690Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0686Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0685Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0688Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0684Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0682Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0687Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0685Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0685Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0684Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0679Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0679Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0679Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0677Epoch 8/15: [==============================] 75/75 batches, loss: 0.0678
[2025-05-07 17:47:04,204][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0678
[2025-05-07 17:47:04,436][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0878, Metrics: {'mse': 0.08829224854707718, 'rmse': 0.2971401160178093, 'r2': -0.43765711784362793}
[2025-05-07 17:47:04,437][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 17:47:04,437][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-07 17:47:04,437][src.training.lm_trainer][INFO] - Training completed in 22.85 seconds
[2025-05-07 17:47:04,437][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 17:47:07,326][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.046357542276382446, 'rmse': 0.21530801721343878, 'r2': -0.027085423469543457}
[2025-05-07 17:47:07,327][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07947543263435364, 'rmse': 0.2819138744977864, 'r2': -0.29409337043762207}
[2025-05-07 17:47:07,327][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.11366827040910721, 'rmse': 0.337147253302036, 'r2': -0.6063530445098877}
[2025-05-07 17:47:09,007][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control1/layer2/fi/fi/model.pt
[2025-05-07 17:47:09,008][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▂▁
wandb:     best_val_mse █▆▂▁
wandb:      best_val_r2 ▁▃▇█
wandb:    best_val_rmse █▆▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▄▅▄▃▄
wandb:       train_loss █▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▂▁▂▅▃▃
wandb:          val_mse █▆▂▁▂▅▃▃
wandb:           val_r2 ▁▃▇█▇▄▆▆
wandb:         val_rmse █▆▃▁▂▅▄▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07916
wandb:     best_val_mse 0.07948
wandb:      best_val_r2 -0.29409
wandb:    best_val_rmse 0.28191
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.11367
wandb:    final_test_r2 -0.60635
wandb:  final_test_rmse 0.33715
wandb:  final_train_mse 0.04636
wandb:   final_train_r2 -0.02709
wandb: final_train_rmse 0.21531
wandb:    final_val_mse 0.07948
wandb:     final_val_r2 -0.29409
wandb:   final_val_rmse 0.28191
wandb:    learning_rate 0.0001
wandb:       train_loss 0.0678
wandb:       train_time 22.85384
wandb:         val_loss 0.08783
wandb:          val_mse 0.08829
wandb:           val_r2 -0.43766
wandb:         val_rmse 0.29714
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_174622-nfp9nwj3
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_174622-nfp9nwj3/logs
Experiment probe_layer2_avg_verb_edges_control1_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control1/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_avg_verb_edges_control2_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_verb_edges_control2_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control2/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=avg_verb_edges"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 17:47:39,435][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control2/layer2/fi
experiment_name: probe_layer2_avg_verb_edges_control2_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 17:47:39,436][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 17:47:39,436][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 17:47:39,436][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 17:47:39,436][__main__][INFO] - Determined Task Type: regression
[2025-05-07 17:47:39,440][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 17:47:39,440][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 17:47:39,440][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 17:47:42,044][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 17:47:44,291][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 17:47:44,292][src.data.datasets][INFO] - Loading 'control_avg_verb_edges_seed2' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:47:44,467][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_verb_edges_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:30:51 2025).
[2025-05-07 17:47:44,538][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_verb_edges_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:30:51 2025).
[2025-05-07 17:47:44,883][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 17:47:44,891][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:47:44,892][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 17:47:44,893][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:47:44,929][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:47:45,038][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:47:45,061][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 17:47:45,062][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:47:45,062][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 17:47:45,064][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:47:45,107][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:47:45,170][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:47:45,188][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 17:47:45,189][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:47:45,189][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 17:47:45,191][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 17:47:45,192][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:47:45,192][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 17:47:45,192][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 17:47:45,192][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 17:47:45,192][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:47:45,192][src.data.datasets][INFO] -   Mean: 0.2055, Std: 0.2125
[2025-05-07 17:47:45,192][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 17:47:45,192][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 17:47:45,193][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:47:45,193][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 17:47:45,193][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 17:47:45,193][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 17:47:45,193][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:47:45,193][src.data.datasets][INFO] -   Mean: 0.2770, Std: 0.2478
[2025-05-07 17:47:45,193][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 17:47:45,193][src.data.datasets][INFO] - Sample label: 0.550000011920929
[2025-05-07 17:47:45,193][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:47:45,194][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 17:47:45,194][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 17:47:45,194][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 17:47:45,194][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:47:45,194][src.data.datasets][INFO] -   Mean: 0.3600, Std: 0.2660
[2025-05-07 17:47:45,194][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 17:47:45,194][src.data.datasets][INFO] - Sample label: 0.6000000238418579
[2025-05-07 17:47:45,194][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 17:47:45,194][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 17:47:45,195][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 17:47:45,195][__main__][INFO] - Using model type: lm_probe for submetric avg_verb_edges
[2025-05-07 17:47:45,195][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 17:47:52,000][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 17:47:52,001][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 17:47:52,001][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 17:47:52,001][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 17:47:52,004][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 17:47:52,004][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 17:47:52,004][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 17:47:52,005][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 17:47:52,005][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 17:47:52,006][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 17:47:52,006][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7268Epoch 1/15: [                              ] 2/75 batches, loss: 0.7827Epoch 1/15: [=                             ] 3/75 batches, loss: 0.6360Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5818Epoch 1/15: [==                            ] 5/75 batches, loss: 0.5506Epoch 1/15: [==                            ] 6/75 batches, loss: 0.5122Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4783Epoch 1/15: [===                           ] 8/75 batches, loss: 0.5283Epoch 1/15: [===                           ] 9/75 batches, loss: 0.5321Epoch 1/15: [====                          ] 10/75 batches, loss: 0.5016Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4931Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4872Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4751Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4777Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4699Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4860Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4723Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4659Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4581Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4519Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4615Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4695Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4621Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4476Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4399Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4299Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4244Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4191Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4196Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4162Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4084Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4068Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4031Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3980Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3918Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3924Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3863Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3807Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3793Epoch 1/15: [================              ] 40/75 batches, loss: 0.3747Epoch 1/15: [================              ] 41/75 batches, loss: 0.3708Epoch 1/15: [================              ] 42/75 batches, loss: 0.3662Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3645Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3621Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3651Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3611Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3609Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3563Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3544Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3520Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3511Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3499Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3468Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3464Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3449Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3441Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3449Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3463Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3433Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3406Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3372Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3352Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3336Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3317Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3300Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3296Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3269Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3239Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3247Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3241Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3226Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3203Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3178Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3150Epoch 1/15: [==============================] 75/75 batches, loss: 0.3132
[2025-05-07 17:47:58,276][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3132
[2025-05-07 17:47:58,597][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1038, Metrics: {'mse': 0.10412031412124634, 'rmse': 0.32267679513910874, 'r2': -0.6953845024108887}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2611Epoch 2/15: [                              ] 2/75 batches, loss: 0.2039Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1615Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1723Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1728Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1741Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1929Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1889Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1849Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1878Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1812Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1749Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1741Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1707Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1754Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1740Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1727Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1726Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1687Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1683Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1690Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1677Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1675Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1669Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1694Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1718Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1762Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1772Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1758Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1760Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1740Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1710Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1696Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1708Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1741Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1712Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1705Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1701Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1705Epoch 2/15: [================              ] 40/75 batches, loss: 0.1695Epoch 2/15: [================              ] 41/75 batches, loss: 0.1691Epoch 2/15: [================              ] 42/75 batches, loss: 0.1685Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1683Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1664Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1649Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1678Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1678Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1667Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1690Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1676Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1669Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1662Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1671Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1662Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1652Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1662Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1648Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1646Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1642Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1625Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1634Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1626Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1619Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1620Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1614Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1607Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1604Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1601Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1599Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1585Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1575Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1579Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1574Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1567Epoch 2/15: [==============================] 75/75 batches, loss: 0.1576
[2025-05-07 17:48:01,328][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1576
[2025-05-07 17:48:01,597][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1167, Metrics: {'mse': 0.1172989085316658, 'rmse': 0.342489282360289, 'r2': -0.9099706411361694}
[2025-05-07 17:48:01,598][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1304Epoch 3/15: [                              ] 2/75 batches, loss: 0.1433Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1269Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1227Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1161Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1103Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1110Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1073Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1034Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1128Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1104Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1097Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1140Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1138Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1136Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1116Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1178Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1173Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1187Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1176Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1169Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1153Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1142Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1194Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1166Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1191Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1177Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1176Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1182Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1156Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1161Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1152Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1144Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1136Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1134Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1135Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1125Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1125Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1128Epoch 3/15: [================              ] 40/75 batches, loss: 0.1139Epoch 3/15: [================              ] 41/75 batches, loss: 0.1141Epoch 3/15: [================              ] 42/75 batches, loss: 0.1127Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1118Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1126Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1119Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1122Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1130Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1129Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1134Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1135Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1129Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1135Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1125Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1136Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1138Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1130Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1127Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1124Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1117Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1107Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1095Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1098Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1098Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1116Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1111Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1110Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1113Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1101Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1100Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1104Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1103Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1095Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1093Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1091Epoch 3/15: [==============================] 75/75 batches, loss: 0.1101
[2025-05-07 17:48:03,922][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1101
[2025-05-07 17:48:04,234][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0917, Metrics: {'mse': 0.09211966395378113, 'rmse': 0.3035122138461336, 'r2': -0.4999786615371704}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0844Epoch 4/15: [                              ] 2/75 batches, loss: 0.0914Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0897Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1016Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0965Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0982Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1039Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1031Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1019Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0972Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1023Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1024Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0997Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0968Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0977Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0949Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0952Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0945Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0935Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0943Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0931Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0946Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0952Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0962Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0977Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1001Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1005Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0986Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0995Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0980Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0990Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0985Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0976Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0967Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0971Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0958Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0949Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0970Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0966Epoch 4/15: [================              ] 40/75 batches, loss: 0.0958Epoch 4/15: [================              ] 41/75 batches, loss: 0.0972Epoch 4/15: [================              ] 42/75 batches, loss: 0.0967Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0962Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0950Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0954Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0962Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0970Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0973Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0987Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0984Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0984Epoch 4/15: [====================          ] 52/75 batches, loss: 0.1004Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.1007Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.1002Epoch 4/15: [======================        ] 55/75 batches, loss: 0.1003Epoch 4/15: [======================        ] 56/75 batches, loss: 0.1016Epoch 4/15: [======================        ] 57/75 batches, loss: 0.1010Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.1006Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.1006Epoch 4/15: [========================      ] 60/75 batches, loss: 0.1005Epoch 4/15: [========================      ] 61/75 batches, loss: 0.1012Epoch 4/15: [========================      ] 62/75 batches, loss: 0.1011Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.1008Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.1008Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.1004Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.1000Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0998Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0999Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.1002Epoch 4/15: [============================  ] 70/75 batches, loss: 0.1000Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0992Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0988Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0987Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0998Epoch 4/15: [==============================] 75/75 batches, loss: 0.0996
[2025-05-07 17:48:07,026][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0996
[2025-05-07 17:48:07,360][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0944, Metrics: {'mse': 0.09493271261453629, 'rmse': 0.30811152626043753, 'r2': -0.5457834005355835}
[2025-05-07 17:48:07,361][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0870Epoch 5/15: [                              ] 2/75 batches, loss: 0.0827Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1198Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1289Epoch 5/15: [==                            ] 5/75 batches, loss: 0.1129Epoch 5/15: [==                            ] 6/75 batches, loss: 0.1038Epoch 5/15: [==                            ] 7/75 batches, loss: 0.1053Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0994Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0946Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0921Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0914Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0888Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0884Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0893Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0890Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0880Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0905Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0903Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0886Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0885Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0899Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0896Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0898Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0900Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0935Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0936Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0928Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0923Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0919Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0912Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0909Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0924Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0923Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0918Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0931Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0923Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0925Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0932Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0921Epoch 5/15: [================              ] 40/75 batches, loss: 0.0923Epoch 5/15: [================              ] 41/75 batches, loss: 0.0920Epoch 5/15: [================              ] 42/75 batches, loss: 0.0916Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0916Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0911Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0900Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0910Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0910Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0904Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0917Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0912Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0906Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0900Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0897Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0899Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0895Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0892Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0893Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0888Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0883Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0881Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0876Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0882Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0885Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0881Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0886Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0880Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0877Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0875Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0873Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0876Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0877Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0877Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0878Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0876Epoch 5/15: [==============================] 75/75 batches, loss: 0.0872
[2025-05-07 17:48:09,795][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0872
[2025-05-07 17:48:10,033][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0749, Metrics: {'mse': 0.07513264566659927, 'rmse': 0.27410334851402174, 'r2': -0.22338008880615234}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0715Epoch 6/15: [                              ] 2/75 batches, loss: 0.0778Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0775Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0731Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0795Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0817Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0774Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0751Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0711Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0707Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0703Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0705Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0729Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0753Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0770Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0770Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0753Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0756Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0733Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0731Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0727Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0725Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0745Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0746Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0759Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0759Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0758Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0744Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0734Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0741Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0745Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0741Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0732Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0739Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0741Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0736Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0742Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0740Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0769Epoch 6/15: [================              ] 40/75 batches, loss: 0.0772Epoch 6/15: [================              ] 41/75 batches, loss: 0.0769Epoch 6/15: [================              ] 42/75 batches, loss: 0.0774Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0784Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0783Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0791Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0786Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0785Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0775Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0768Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0763Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0769Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0771Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0785Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0783Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0782Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0788Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0797Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0801Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0801Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0802Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0805Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0803Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0801Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0802Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0802Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0804Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0801Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0800Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0800Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0796Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0801Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0800Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0796Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0798Epoch 6/15: [==============================] 75/75 batches, loss: 0.0797
[2025-05-07 17:48:12,721][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0797
[2025-05-07 17:48:12,966][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0784, Metrics: {'mse': 0.07875221222639084, 'rmse': 0.28062824559618166, 'r2': -0.28231728076934814}
[2025-05-07 17:48:12,967][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0679Epoch 7/15: [                              ] 2/75 batches, loss: 0.0829Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0713Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0743Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0729Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0719Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0730Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0726Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0723Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0690Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0715Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0738Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0746Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0766Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0779Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0780Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0788Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0787Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0779Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0779Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0787Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0783Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0774Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0790Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0784Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0785Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0788Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0782Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0777Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0769Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0760Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0751Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0756Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0764Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0761Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0758Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0767Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0757Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0755Epoch 7/15: [================              ] 40/75 batches, loss: 0.0758Epoch 7/15: [================              ] 41/75 batches, loss: 0.0752Epoch 7/15: [================              ] 42/75 batches, loss: 0.0754Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0751Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0744Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0740Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0741Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0754Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0754Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0757Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0757Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0757Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0763Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0763Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0763Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0763Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0764Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0761Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0762Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0758Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0762Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0760Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0754Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0753Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0753Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0750Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0752Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0748Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0741Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0743Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0743Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0744Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0739Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0738Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0733Epoch 7/15: [==============================] 75/75 batches, loss: 0.0735
[2025-05-07 17:48:15,315][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0735
[2025-05-07 17:48:15,625][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0803, Metrics: {'mse': 0.08068966120481491, 'rmse': 0.2840592565026088, 'r2': -0.31386470794677734}
[2025-05-07 17:48:15,626][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0629Epoch 8/15: [                              ] 2/75 batches, loss: 0.0639Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0584Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0556Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0579Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0626Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0629Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0707Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0709Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0705Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0683Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0656Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0670Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0648Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0665Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0668Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0683Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0684Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0717Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0730Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0730Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0745Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0741Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0743Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0748Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0738Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0751Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0740Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0732Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0745Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0745Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0744Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0742Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0741Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0746Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0741Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0744Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0739Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0735Epoch 8/15: [================              ] 40/75 batches, loss: 0.0736Epoch 8/15: [================              ] 41/75 batches, loss: 0.0731Epoch 8/15: [================              ] 42/75 batches, loss: 0.0726Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0726Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0724Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0728Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0736Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0739Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0731Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0729Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0725Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0729Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0728Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0730Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0728Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0725Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0720Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0715Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0712Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0710Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0712Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0711Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0711Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0714Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0716Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0717Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0710Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0707Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0706Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0707Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0707Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0708Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0704Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0700Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0702Epoch 8/15: [==============================] 75/75 batches, loss: 0.0701
[2025-05-07 17:48:18,002][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0701
[2025-05-07 17:48:18,257][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0840, Metrics: {'mse': 0.08442147076129913, 'rmse': 0.2905537312809786, 'r2': -0.3746293783187866}
[2025-05-07 17:48:18,258][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0450Epoch 9/15: [                              ] 2/75 batches, loss: 0.0470Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0478Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0488Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0491Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0508Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0487Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0511Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0574Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0577Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0603Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0589Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0576Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0587Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0580Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0590Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0586Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0596Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0615Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0619Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0607Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0609Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0598Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0592Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0595Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0590Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0586Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0581Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0577Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0588Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0591Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0585Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0580Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0589Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0591Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0597Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0600Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0599Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0602Epoch 9/15: [================              ] 40/75 batches, loss: 0.0610Epoch 9/15: [================              ] 41/75 batches, loss: 0.0608Epoch 9/15: [================              ] 42/75 batches, loss: 0.0607Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0617Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0619Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0616Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0613Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0607Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0603Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0604Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0604Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0604Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0611Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0616Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0612Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0617Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0618Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0620Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0616Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0621Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0618Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0619Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0627Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0626Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0627Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0623Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0635Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0631Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0631Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0639Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0639Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0642Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0642Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0638Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0639Epoch 9/15: [==============================] 75/75 batches, loss: 0.0634
[2025-05-07 17:48:20,534][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0634
[2025-05-07 17:48:20,830][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0819, Metrics: {'mse': 0.08235234022140503, 'rmse': 0.2869709745277474, 'r2': -0.3409379720687866}
[2025-05-07 17:48:20,830][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 17:48:20,830][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 9
[2025-05-07 17:48:20,831][src.training.lm_trainer][INFO] - Training completed in 25.69 seconds
[2025-05-07 17:48:20,831][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 17:48:23,703][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.045375969260931015, 'rmse': 0.21301635913922437, 'r2': -0.005337953567504883}
[2025-05-07 17:48:23,703][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07513264566659927, 'rmse': 0.27410334851402174, 'r2': -0.22338008880615234}
[2025-05-07 17:48:23,704][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.10762333124876022, 'rmse': 0.32805995069310157, 'r2': -0.5209263563156128}
[2025-05-07 17:48:25,442][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control2/layer2/fi/fi/model.pt
[2025-05-07 17:48:25,443][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▁
wandb:     best_val_mse █▅▁
wandb:      best_val_r2 ▁▄█
wandb:    best_val_rmse █▅▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▄▃▅▅▅▄
wandb:       train_loss █▄▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆█▄▄▁▂▂▃▂
wandb:          val_mse ▆█▄▄▁▂▂▃▂
wandb:           val_r2 ▃▁▅▅█▇▇▆▇
wandb:         val_rmse ▆█▄▄▁▂▂▃▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07485
wandb:     best_val_mse 0.07513
wandb:      best_val_r2 -0.22338
wandb:    best_val_rmse 0.2741
wandb: early_stop_epoch 9
wandb:            epoch 9
wandb:   final_test_mse 0.10762
wandb:    final_test_r2 -0.52093
wandb:  final_test_rmse 0.32806
wandb:  final_train_mse 0.04538
wandb:   final_train_r2 -0.00534
wandb: final_train_rmse 0.21302
wandb:    final_val_mse 0.07513
wandb:     final_val_r2 -0.22338
wandb:   final_val_rmse 0.2741
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06336
wandb:       train_time 25.68647
wandb:         val_loss 0.08194
wandb:          val_mse 0.08235
wandb:           val_r2 -0.34094
wandb:         val_rmse 0.28697
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_174739-vo7k0hrk
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_174739-vo7k0hrk/logs
Experiment probe_layer2_avg_verb_edges_control2_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control2/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_avg_verb_edges_control3_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_verb_edges_control3_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control3/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=avg_verb_edges"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 17:48:54,501][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control3/layer2/fi
experiment_name: probe_layer2_avg_verb_edges_control3_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 17:48:54,501][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 17:48:54,501][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 17:48:54,501][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 17:48:54,501][__main__][INFO] - Determined Task Type: regression
[2025-05-07 17:48:54,505][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 17:48:54,506][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 17:48:54,506][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 17:48:58,087][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 17:49:00,534][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 17:49:00,535][src.data.datasets][INFO] - Loading 'control_avg_verb_edges_seed3' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:49:00,683][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_verb_edges_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:32:16 2025).
[2025-05-07 17:49:00,765][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_verb_edges_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:32:16 2025).
[2025-05-07 17:49:01,067][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 17:49:01,075][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:49:01,076][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 17:49:01,078][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:49:01,143][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:49:01,211][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:49:01,229][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 17:49:01,230][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:49:01,230][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 17:49:01,232][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:49:01,287][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:49:01,393][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:49:01,449][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 17:49:01,450][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:49:01,450][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 17:49:01,452][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 17:49:01,453][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:49:01,453][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 17:49:01,453][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 17:49:01,453][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 17:49:01,453][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:49:01,453][src.data.datasets][INFO] -   Mean: 0.2055, Std: 0.2125
[2025-05-07 17:49:01,453][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 17:49:01,453][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 17:49:01,454][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:49:01,454][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 17:49:01,454][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 17:49:01,454][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 17:49:01,454][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:49:01,454][src.data.datasets][INFO] -   Mean: 0.2770, Std: 0.2478
[2025-05-07 17:49:01,454][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 17:49:01,454][src.data.datasets][INFO] - Sample label: 0.550000011920929
[2025-05-07 17:49:01,454][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:49:01,455][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 17:49:01,455][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 17:49:01,455][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 17:49:01,455][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:49:01,455][src.data.datasets][INFO] -   Mean: 0.3600, Std: 0.2660
[2025-05-07 17:49:01,455][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 17:49:01,455][src.data.datasets][INFO] - Sample label: 0.6000000238418579
[2025-05-07 17:49:01,455][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 17:49:01,455][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 17:49:01,455][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 17:49:01,456][__main__][INFO] - Using model type: lm_probe for submetric avg_verb_edges
[2025-05-07 17:49:01,456][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 17:49:08,538][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 17:49:08,539][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 17:49:08,539][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 17:49:08,539][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 17:49:08,542][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 17:49:08,543][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 17:49:08,543][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 17:49:08,543][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 17:49:08,543][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 17:49:08,544][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 17:49:08,544][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5622Epoch 1/15: [                              ] 2/75 batches, loss: 0.6707Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5764Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5073Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4801Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4567Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4303Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4567Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4594Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4394Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4277Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4279Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4097Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4208Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4157Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4308Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4248Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4197Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4163Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4134Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4232Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4279Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4188Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4087Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4023Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3933Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3878Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3861Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3848Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3853Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3777Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3762Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3748Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3715Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3657Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3661Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3644Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3595Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3588Epoch 1/15: [================              ] 40/75 batches, loss: 0.3530Epoch 1/15: [================              ] 41/75 batches, loss: 0.3492Epoch 1/15: [================              ] 42/75 batches, loss: 0.3480Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3440Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3431Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3462Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3422Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3443Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3398Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3360Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3349Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3354Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3352Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3326Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3340Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3325Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3310Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3350Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3351Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3327Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3307Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3276Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3258Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3252Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3234Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3218Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3230Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3198Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3167Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3162Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3138Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3119Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3097Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3070Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3049Epoch 1/15: [==============================] 75/75 batches, loss: 0.3035
[2025-05-07 17:49:15,040][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3035
[2025-05-07 17:49:15,360][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1025, Metrics: {'mse': 0.10289965569972992, 'rmse': 0.32077976198589886, 'r2': -0.6755086183547974}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2004Epoch 2/15: [                              ] 2/75 batches, loss: 0.1650Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1444Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1669Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1629Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1577Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1722Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1610Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1597Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1659Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1637Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1602Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1597Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1570Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1567Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1565Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1572Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1584Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1548Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1536Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1552Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1580Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1577Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1544Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1564Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1579Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1578Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1578Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1605Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1594Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1596Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1597Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1584Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1609Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1631Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1616Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1600Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1586Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1600Epoch 2/15: [================              ] 40/75 batches, loss: 0.1592Epoch 2/15: [================              ] 41/75 batches, loss: 0.1598Epoch 2/15: [================              ] 42/75 batches, loss: 0.1570Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1567Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1551Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1536Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1537Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1529Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1530Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1570Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1562Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1565Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1575Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1574Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1573Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1561Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1565Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1568Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1561Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1556Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1539Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1529Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1532Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1519Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1518Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1509Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1504Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1507Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1502Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1494Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1481Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1472Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1483Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1486Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1480Epoch 2/15: [==============================] 75/75 batches, loss: 0.1478
[2025-05-07 17:49:18,056][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1478
[2025-05-07 17:49:18,334][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1309, Metrics: {'mse': 0.13160422444343567, 'rmse': 0.36277296542525833, 'r2': -1.1429033279418945}
[2025-05-07 17:49:18,334][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1058Epoch 3/15: [                              ] 2/75 batches, loss: 0.0776Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0983Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1026Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1041Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1084Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1030Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1026Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1041Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1061Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1066Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1081Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1131Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1201Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1180Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1177Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1182Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1155Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1142Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1144Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1130Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1136Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1120Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1188Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1185Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1177Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1169Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1163Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1163Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1166Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1159Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1143Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1139Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1136Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1138Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1122Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1105Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1106Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1120Epoch 3/15: [================              ] 40/75 batches, loss: 0.1117Epoch 3/15: [================              ] 41/75 batches, loss: 0.1112Epoch 3/15: [================              ] 42/75 batches, loss: 0.1100Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1085Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1101Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1104Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1095Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1108Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1106Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1108Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1110Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1100Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1105Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1105Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1099Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1099Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1086Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1083Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1094Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1086Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1087Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1081Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1089Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1089Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1090Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1104Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1098Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1099Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1091Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1089Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1090Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1096Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1091Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1088Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1094Epoch 3/15: [==============================] 75/75 batches, loss: 0.1100
[2025-05-07 17:49:20,645][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1100
[2025-05-07 17:49:20,874][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0911, Metrics: {'mse': 0.09157372266054153, 'rmse': 0.30261150450791113, 'r2': -0.4910891056060791}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0984Epoch 4/15: [                              ] 2/75 batches, loss: 0.0818Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0824Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0796Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0881Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0912Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0994Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0969Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1003Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1004Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1119Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1171Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1129Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1090Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1148Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1176Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1159Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1154Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1149Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1130Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1128Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1149Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1135Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1125Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1141Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1131Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1113Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1091Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1111Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1104Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1112Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1110Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1101Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.1099Epoch 4/15: [==============                ] 35/75 batches, loss: 0.1105Epoch 4/15: [==============                ] 36/75 batches, loss: 0.1087Epoch 4/15: [==============                ] 37/75 batches, loss: 0.1081Epoch 4/15: [===============               ] 38/75 batches, loss: 0.1090Epoch 4/15: [===============               ] 39/75 batches, loss: 0.1081Epoch 4/15: [================              ] 40/75 batches, loss: 0.1086Epoch 4/15: [================              ] 41/75 batches, loss: 0.1088Epoch 4/15: [================              ] 42/75 batches, loss: 0.1075Epoch 4/15: [=================             ] 43/75 batches, loss: 0.1092Epoch 4/15: [=================             ] 44/75 batches, loss: 0.1083Epoch 4/15: [==================            ] 45/75 batches, loss: 0.1076Epoch 4/15: [==================            ] 46/75 batches, loss: 0.1071Epoch 4/15: [==================            ] 47/75 batches, loss: 0.1063Epoch 4/15: [===================           ] 48/75 batches, loss: 0.1061Epoch 4/15: [===================           ] 49/75 batches, loss: 0.1067Epoch 4/15: [====================          ] 50/75 batches, loss: 0.1057Epoch 4/15: [====================          ] 51/75 batches, loss: 0.1044Epoch 4/15: [====================          ] 52/75 batches, loss: 0.1041Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.1034Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.1031Epoch 4/15: [======================        ] 55/75 batches, loss: 0.1030Epoch 4/15: [======================        ] 56/75 batches, loss: 0.1036Epoch 4/15: [======================        ] 57/75 batches, loss: 0.1029Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.1020Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.1023Epoch 4/15: [========================      ] 60/75 batches, loss: 0.1017Epoch 4/15: [========================      ] 61/75 batches, loss: 0.1020Epoch 4/15: [========================      ] 62/75 batches, loss: 0.1011Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.1012Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.1014Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.1016Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.1007Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.1003Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.1002Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.1005Epoch 4/15: [============================  ] 70/75 batches, loss: 0.1003Epoch 4/15: [============================  ] 71/75 batches, loss: 0.1002Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0997Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0996Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0997Epoch 4/15: [==============================] 75/75 batches, loss: 0.0990
[2025-05-07 17:49:23,562][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0990
[2025-05-07 17:49:23,829][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0839, Metrics: {'mse': 0.08427531272172928, 'rmse': 0.2903021059546921, 'r2': -0.3722496032714844}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1104Epoch 5/15: [                              ] 2/75 batches, loss: 0.1159Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1396Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1199Epoch 5/15: [==                            ] 5/75 batches, loss: 0.1104Epoch 5/15: [==                            ] 6/75 batches, loss: 0.1083Epoch 5/15: [==                            ] 7/75 batches, loss: 0.1116Epoch 5/15: [===                           ] 8/75 batches, loss: 0.1093Epoch 5/15: [===                           ] 9/75 batches, loss: 0.1041Epoch 5/15: [====                          ] 10/75 batches, loss: 0.1018Epoch 5/15: [====                          ] 11/75 batches, loss: 0.1025Epoch 5/15: [====                          ] 12/75 batches, loss: 0.1049Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.1098Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.1080Epoch 5/15: [======                        ] 15/75 batches, loss: 0.1118Epoch 5/15: [======                        ] 16/75 batches, loss: 0.1095Epoch 5/15: [======                        ] 17/75 batches, loss: 0.1091Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.1094Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.1094Epoch 5/15: [========                      ] 20/75 batches, loss: 0.1088Epoch 5/15: [========                      ] 21/75 batches, loss: 0.1093Epoch 5/15: [========                      ] 22/75 batches, loss: 0.1079Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.1071Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.1066Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.1079Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.1075Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.1054Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.1043Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.1023Epoch 5/15: [============                  ] 30/75 batches, loss: 0.1013Epoch 5/15: [============                  ] 31/75 batches, loss: 0.1005Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0998Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0994Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.1000Epoch 5/15: [==============                ] 35/75 batches, loss: 0.1008Epoch 5/15: [==============                ] 36/75 batches, loss: 0.1003Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0988Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0986Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0977Epoch 5/15: [================              ] 40/75 batches, loss: 0.0977Epoch 5/15: [================              ] 41/75 batches, loss: 0.0976Epoch 5/15: [================              ] 42/75 batches, loss: 0.0977Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0971Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0962Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0970Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0971Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0964Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0954Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0958Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0949Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0941Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0932Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0928Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0927Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0925Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0930Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0929Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0922Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0921Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0918Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0914Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0923Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0928Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0931Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0926Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0925Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0919Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0918Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0912Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0912Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0915Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0910Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0905Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0904Epoch 5/15: [==============================] 75/75 batches, loss: 0.0900
[2025-05-07 17:49:26,501][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0900
[2025-05-07 17:49:26,768][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0768, Metrics: {'mse': 0.07706257700920105, 'rmse': 0.2776014715544589, 'r2': -0.2548050880432129}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0636Epoch 6/15: [                              ] 2/75 batches, loss: 0.0646Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0598Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0563Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0627Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0613Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0617Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0689Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0677Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0681Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0696Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0709Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0740Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0744Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0750Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0726Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0712Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0738Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0737Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0734Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0727Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0727Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0733Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0755Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0781Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0801Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0808Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0796Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0793Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0789Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0787Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0779Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0774Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0765Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0771Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0768Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0755Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0755Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0785Epoch 6/15: [================              ] 40/75 batches, loss: 0.0780Epoch 6/15: [================              ] 41/75 batches, loss: 0.0788Epoch 6/15: [================              ] 42/75 batches, loss: 0.0791Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0789Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0789Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0796Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0793Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0799Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0799Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0802Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0802Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0801Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0798Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0807Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0809Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0804Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0806Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0811Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0817Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0815Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0821Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0823Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0828Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0829Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0833Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0830Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0831Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0832Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0833Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0831Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0828Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0836Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0831Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0825Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0823Epoch 6/15: [==============================] 75/75 batches, loss: 0.0823
[2025-05-07 17:49:29,458][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0823
[2025-05-07 17:49:29,733][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0751, Metrics: {'mse': 0.07537826150655746, 'rmse': 0.27455101803955756, 'r2': -0.2273794412612915}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.1098Epoch 7/15: [                              ] 2/75 batches, loss: 0.0872Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0895Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0880Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0845Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0839Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0864Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0836Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0818Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0865Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0838Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0822Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0797Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0785Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0787Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0797Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0820Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0834Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0824Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0813Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0806Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0801Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0788Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0811Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0801Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0798Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0797Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0788Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0780Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0769Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0756Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0758Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0753Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0766Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0758Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0754Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0769Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0761Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0758Epoch 7/15: [================              ] 40/75 batches, loss: 0.0764Epoch 7/15: [================              ] 41/75 batches, loss: 0.0762Epoch 7/15: [================              ] 42/75 batches, loss: 0.0774Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0769Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0763Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0754Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0751Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0752Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0745Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0741Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0740Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0741Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0744Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0746Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0748Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0754Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0754Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0758Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0753Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0754Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0758Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0756Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0753Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0756Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0755Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0761Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0759Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0761Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0755Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0751Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0748Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0746Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0743Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0744Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0738Epoch 7/15: [==============================] 75/75 batches, loss: 0.0735
[2025-05-07 17:49:32,461][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0735
[2025-05-07 17:49:32,737][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0829, Metrics: {'mse': 0.08328413218259811, 'rmse': 0.2885899031196312, 'r2': -0.3561103343963623}
[2025-05-07 17:49:32,738][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0639Epoch 8/15: [                              ] 2/75 batches, loss: 0.0867Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0849Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0774Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0717Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0711Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0736Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0729Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0706Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0704Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0708Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0716Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0699Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0690Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0678Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0675Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0671Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0666Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0682Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0676Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0701Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0710Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0705Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0738Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0735Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0735Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0732Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0734Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0727Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0744Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0747Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0754Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0754Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0742Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0740Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0739Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0732Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0731Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0724Epoch 8/15: [================              ] 40/75 batches, loss: 0.0728Epoch 8/15: [================              ] 41/75 batches, loss: 0.0729Epoch 8/15: [================              ] 42/75 batches, loss: 0.0724Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0724Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0732Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0735Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0740Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0741Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0736Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0733Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0732Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0737Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0738Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0731Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0729Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0725Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0722Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0725Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0720Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0723Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0731Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0728Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0726Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0725Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0731Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0731Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0731Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0731Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0727Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0724Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0721Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0720Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0718Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0718Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0712Epoch 8/15: [==============================] 75/75 batches, loss: 0.0715
[2025-05-07 17:49:35,053][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0715
[2025-05-07 17:49:35,385][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0881, Metrics: {'mse': 0.0885983556509018, 'rmse': 0.297654759160511, 'r2': -0.44264137744903564}
[2025-05-07 17:49:35,386][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0498Epoch 9/15: [                              ] 2/75 batches, loss: 0.0688Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0718Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0711Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0672Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0641Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0661Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0630Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0652Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0643Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0653Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0654Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0636Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0653Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0651Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0648Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0636Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0621Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0608Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0607Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0600Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0597Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0602Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0600Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0595Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0584Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0589Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0590Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0593Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0589Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0583Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0589Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0588Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0600Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0601Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0605Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0604Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0599Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0606Epoch 9/15: [================              ] 40/75 batches, loss: 0.0609Epoch 9/15: [================              ] 41/75 batches, loss: 0.0610Epoch 9/15: [================              ] 42/75 batches, loss: 0.0614Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0619Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0624Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0626Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0619Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0619Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0622Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0624Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0631Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0626Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0620Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0622Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0624Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0625Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0626Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0628Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0631Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0625Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0627Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0628Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0633Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0634Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0635Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0639Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0638Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0638Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0634Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0638Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0633Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0638Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0635Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0635Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0634Epoch 9/15: [==============================] 75/75 batches, loss: 0.0633
[2025-05-07 17:49:37,757][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0633
[2025-05-07 17:49:38,086][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0783, Metrics: {'mse': 0.07866327464580536, 'rmse': 0.28046973926932894, 'r2': -0.28086912631988525}
[2025-05-07 17:49:38,087][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0539Epoch 10/15: [                              ] 2/75 batches, loss: 0.0626Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0578Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0599Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0671Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0737Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0750Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0751Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0734Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0722Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0716Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0710Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0721Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0751Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0748Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0769Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0770Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0767Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0772Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0763Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0753Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0746Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0749Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0738Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0726Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0721Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0716Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0720Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0715Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0699Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0695Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0694Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0687Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0684Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0691Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0692Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0687Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0685Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0691Epoch 10/15: [================              ] 40/75 batches, loss: 0.0685Epoch 10/15: [================              ] 41/75 batches, loss: 0.0678Epoch 10/15: [================              ] 42/75 batches, loss: 0.0669Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0672Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0674Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0670Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0673Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0681Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0681Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0681Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0685Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0690Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0686Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0688Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0683Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0679Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0680Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0679Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0675Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0676Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0673Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0672Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0671Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0670Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0668Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0664Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0663Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0664Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0667Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0661Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0662Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0660Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0661Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0657Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0657Epoch 10/15: [==============================] 75/75 batches, loss: 0.0658
[2025-05-07 17:49:40,465][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0658
[2025-05-07 17:49:40,794][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0762, Metrics: {'mse': 0.07646860182285309, 'rmse': 0.2765295677189929, 'r2': -0.2451333999633789}
[2025-05-07 17:49:40,795][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 17:49:40,795][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 17:49:40,795][src.training.lm_trainer][INFO] - Training completed in 28.99 seconds
[2025-05-07 17:49:40,795][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 17:49:43,763][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.0457734540104866, 'rmse': 0.21394731596934466, 'r2': -0.014144539833068848}
[2025-05-07 17:49:43,764][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07537826150655746, 'rmse': 0.27455101803955756, 'r2': -0.2273794412612915}
[2025-05-07 17:49:43,764][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.10655824840068817, 'rmse': 0.3264326092789876, 'r2': -0.5058746337890625}
[2025-05-07 17:49:45,419][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control3/layer2/fi/fi/model.pt
[2025-05-07 17:49:45,421][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▁▁
wandb:     best_val_mse █▅▃▁▁
wandb:      best_val_r2 ▁▄▆██
wandb:    best_val_rmse █▅▃▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▄▁▅▅▆▆▅▅▅
wandb:       train_loss █▃▂▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▄█▃▂▁▁▂▃▁▁
wandb:          val_mse ▄█▃▂▁▁▂▃▁▁
wandb:           val_r2 ▅▁▆▇██▇▆██
wandb:         val_rmse ▅█▃▂▁▁▂▃▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07511
wandb:     best_val_mse 0.07538
wandb:      best_val_r2 -0.22738
wandb:    best_val_rmse 0.27455
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.10656
wandb:    final_test_r2 -0.50587
wandb:  final_test_rmse 0.32643
wandb:  final_train_mse 0.04577
wandb:   final_train_r2 -0.01414
wandb: final_train_rmse 0.21395
wandb:    final_val_mse 0.07538
wandb:     final_val_r2 -0.22738
wandb:   final_val_rmse 0.27455
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06578
wandb:       train_time 28.99057
wandb:         val_loss 0.07616
wandb:          val_mse 0.07647
wandb:           val_r2 -0.24513
wandb:         val_rmse 0.27653
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_174854-q18t51rg
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_174854-q18t51rg/logs
Experiment probe_layer2_avg_verb_edges_control3_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control3/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_lexical_density_control1_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_lexical_density_control1_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control1/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=lexical_density"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 17:50:29,398][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control1/layer2/fi
experiment_name: probe_layer2_lexical_density_control1_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: lexical_density
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 17:50:29,398][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 17:50:29,398][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 17:50:29,398][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 17:50:29,398][__main__][INFO] - Determined Task Type: regression
[2025-05-07 17:50:29,403][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 17:50:29,403][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 17:50:29,403][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 17:50:33,505][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'lexical_density'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 17:50:35,842][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 17:50:35,842][src.data.datasets][INFO] - Loading 'control_lexical_density_seed1' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:50:36,008][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_lexical_density_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:47:48 2025).
[2025-05-07 17:50:36,107][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_lexical_density_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:47:48 2025).
[2025-05-07 17:50:36,450][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 17:50:36,459][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:50:36,459][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 17:50:36,461][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:50:36,560][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:50:36,670][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:50:36,700][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 17:50:36,701][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:50:36,701][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 17:50:36,715][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:50:36,807][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:50:36,903][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:50:36,937][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 17:50:36,939][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:50:36,939][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 17:50:36,941][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 17:50:36,941][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:50:36,941][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 17:50:36,941][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 17:50:36,941][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 17:50:36,942][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:50:36,942][src.data.datasets][INFO] -   Mean: 0.6682, Std: 0.2000
[2025-05-07 17:50:36,942][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 17:50:36,942][src.data.datasets][INFO] - Sample label: 0.625
[2025-05-07 17:50:36,942][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:50:36,942][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 17:50:36,942][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 17:50:36,942][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 17:50:36,942][src.data.datasets][INFO] -   Min: 0.1670, Max: 1.0000
[2025-05-07 17:50:36,943][src.data.datasets][INFO] -   Mean: 0.6281, Std: 0.1884
[2025-05-07 17:50:36,943][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 17:50:36,943][src.data.datasets][INFO] - Sample label: 0.7329999804496765
[2025-05-07 17:50:36,943][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:50:36,943][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 17:50:36,943][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 17:50:36,943][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 17:50:36,943][src.data.datasets][INFO] -   Min: 0.0620, Max: 1.0000
[2025-05-07 17:50:36,943][src.data.datasets][INFO] -   Mean: 0.5643, Std: 0.2115
[2025-05-07 17:50:36,943][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 17:50:36,944][src.data.datasets][INFO] - Sample label: 0.25
[2025-05-07 17:50:36,944][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 17:50:36,944][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 17:50:36,944][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 17:50:36,944][__main__][INFO] - Using model type: lm_probe for submetric lexical_density
[2025-05-07 17:50:36,944][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 17:50:44,765][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 17:50:44,766][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 17:50:44,766][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 17:50:44,766][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 17:50:44,769][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 17:50:44,770][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 17:50:44,770][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 17:50:44,770][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 17:50:44,770][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 17:50:44,771][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 17:50:44,771][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6184Epoch 1/15: [                              ] 2/75 batches, loss: 0.7446Epoch 1/15: [=                             ] 3/75 batches, loss: 0.6398Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5832Epoch 1/15: [==                            ] 5/75 batches, loss: 0.5511Epoch 1/15: [==                            ] 6/75 batches, loss: 0.5318Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4868Epoch 1/15: [===                           ] 8/75 batches, loss: 0.5191Epoch 1/15: [===                           ] 9/75 batches, loss: 0.5129Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4920Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4722Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4714Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4500Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4530Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4437Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4541Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4439Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4360Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4304Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4223Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4268Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4322Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4275Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4166Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4119Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4083Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4021Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3958Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3952Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3964Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3936Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3900Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3890Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3853Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3815Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3813Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3764Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3771Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3726Epoch 1/15: [================              ] 40/75 batches, loss: 0.3698Epoch 1/15: [================              ] 41/75 batches, loss: 0.3652Epoch 1/15: [================              ] 42/75 batches, loss: 0.3621Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3596Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3554Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3568Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3531Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3536Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3497Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3464Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3465Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3482Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3466Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3429Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3438Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3425Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3413Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3430Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3414Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3402Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3384Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3350Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3336Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3312Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3284Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3264Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3247Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3225Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3193Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3201Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3189Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3169Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3151Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3123Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3099Epoch 1/15: [==============================] 75/75 batches, loss: 0.3094
[2025-05-07 17:50:51,859][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3094
[2025-05-07 17:50:52,134][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0417, Metrics: {'mse': 0.04169238731265068, 'rmse': 0.20418713797066326, 'r2': -0.17515265941619873}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1395Epoch 2/15: [                              ] 2/75 batches, loss: 0.1425Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1199Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1473Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1480Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1554Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1579Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1519Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1493Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1497Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1436Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1411Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1410Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1378Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1393Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1359Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1382Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1373Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1346Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1323Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1324Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1350Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1373Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1346Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1372Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1457Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1457Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1465Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1503Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1511Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1494Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1474Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1452Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1490Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1501Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1492Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1475Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1462Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1452Epoch 2/15: [================              ] 40/75 batches, loss: 0.1435Epoch 2/15: [================              ] 41/75 batches, loss: 0.1430Epoch 2/15: [================              ] 42/75 batches, loss: 0.1420Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1426Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1411Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1420Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1444Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1439Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1432Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1436Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1427Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1417Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1417Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1428Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1420Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1420Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1437Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1433Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1444Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1439Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1426Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1439Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1437Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1431Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1436Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1433Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1434Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1434Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1433Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1435Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1434Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1426Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1427Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1430Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1417Epoch 2/15: [==============================] 75/75 batches, loss: 0.1412
[2025-05-07 17:50:54,878][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1412
[2025-05-07 17:50:55,185][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0425, Metrics: {'mse': 0.04255487024784088, 'rmse': 0.2062883182534602, 'r2': -0.199462890625}
[2025-05-07 17:50:55,186][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1198Epoch 3/15: [                              ] 2/75 batches, loss: 0.1070Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1291Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1117Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1037Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1055Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1054Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1057Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1106Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1191Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1140Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1099Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1148Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1139Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1131Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1136Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1160Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1162Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1143Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1122Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1118Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1127Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1132Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1190Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1176Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1167Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1165Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1154Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1171Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1157Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1142Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1134Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1130Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1129Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1124Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1130Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1109Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1108Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1098Epoch 3/15: [================              ] 40/75 batches, loss: 0.1105Epoch 3/15: [================              ] 41/75 batches, loss: 0.1106Epoch 3/15: [================              ] 42/75 batches, loss: 0.1091Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1082Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1107Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1113Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1107Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1101Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1098Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1095Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1086Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1087Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1086Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1081Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1094Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1083Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1081Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1071Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1069Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1064Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1055Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1052Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1061Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1070Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1075Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1078Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1073Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1067Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1062Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1055Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1064Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1056Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1051Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1048Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1053Epoch 3/15: [==============================] 75/75 batches, loss: 0.1057
[2025-05-07 17:50:57,549][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1057
[2025-05-07 17:50:57,786][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0385, Metrics: {'mse': 0.03854738175868988, 'rmse': 0.19633487147903675, 'r2': -0.08650684356689453}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0854Epoch 4/15: [                              ] 2/75 batches, loss: 0.1053Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0935Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0979Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0940Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1036Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0974Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0963Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0944Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0940Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1002Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1043Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1013Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0970Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0996Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1017Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1007Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1025Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1030Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1032Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1012Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1026Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1035Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1044Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1066Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1071Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1069Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1057Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1048Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1047Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1053Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1053Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1039Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.1030Epoch 4/15: [==============                ] 35/75 batches, loss: 0.1022Epoch 4/15: [==============                ] 36/75 batches, loss: 0.1008Epoch 4/15: [==============                ] 37/75 batches, loss: 0.1002Epoch 4/15: [===============               ] 38/75 batches, loss: 0.1004Epoch 4/15: [===============               ] 39/75 batches, loss: 0.1000Epoch 4/15: [================              ] 40/75 batches, loss: 0.1015Epoch 4/15: [================              ] 41/75 batches, loss: 0.1016Epoch 4/15: [================              ] 42/75 batches, loss: 0.1009Epoch 4/15: [=================             ] 43/75 batches, loss: 0.1019Epoch 4/15: [=================             ] 44/75 batches, loss: 0.1015Epoch 4/15: [==================            ] 45/75 batches, loss: 0.1004Epoch 4/15: [==================            ] 46/75 batches, loss: 0.1001Epoch 4/15: [==================            ] 47/75 batches, loss: 0.1001Epoch 4/15: [===================           ] 48/75 batches, loss: 0.1002Epoch 4/15: [===================           ] 49/75 batches, loss: 0.1005Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0997Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0994Epoch 4/15: [====================          ] 52/75 batches, loss: 0.1006Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.1007Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.1011Epoch 4/15: [======================        ] 55/75 batches, loss: 0.1015Epoch 4/15: [======================        ] 56/75 batches, loss: 0.1023Epoch 4/15: [======================        ] 57/75 batches, loss: 0.1023Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.1023Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.1021Epoch 4/15: [========================      ] 60/75 batches, loss: 0.1022Epoch 4/15: [========================      ] 61/75 batches, loss: 0.1025Epoch 4/15: [========================      ] 62/75 batches, loss: 0.1024Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.1017Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.1006Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.1003Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0997Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.1000Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.1001Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.1012Epoch 4/15: [============================  ] 70/75 batches, loss: 0.1008Epoch 4/15: [============================  ] 71/75 batches, loss: 0.1009Epoch 4/15: [============================  ] 72/75 batches, loss: 0.1010Epoch 4/15: [============================= ] 73/75 batches, loss: 0.1009Epoch 4/15: [============================= ] 74/75 batches, loss: 0.1005Epoch 4/15: [==============================] 75/75 batches, loss: 0.1003
[2025-05-07 17:51:00,504][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1003
[2025-05-07 17:51:00,759][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0365, Metrics: {'mse': 0.036582402884960175, 'rmse': 0.19126526837081576, 'r2': -0.031121373176574707}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.2261Epoch 5/15: [                              ] 2/75 batches, loss: 0.1536Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1320Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1274Epoch 5/15: [==                            ] 5/75 batches, loss: 0.1175Epoch 5/15: [==                            ] 6/75 batches, loss: 0.1104Epoch 5/15: [==                            ] 7/75 batches, loss: 0.1194Epoch 5/15: [===                           ] 8/75 batches, loss: 0.1187Epoch 5/15: [===                           ] 9/75 batches, loss: 0.1097Epoch 5/15: [====                          ] 10/75 batches, loss: 0.1061Epoch 5/15: [====                          ] 11/75 batches, loss: 0.1052Epoch 5/15: [====                          ] 12/75 batches, loss: 0.1037Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.1064Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.1083Epoch 5/15: [======                        ] 15/75 batches, loss: 0.1144Epoch 5/15: [======                        ] 16/75 batches, loss: 0.1136Epoch 5/15: [======                        ] 17/75 batches, loss: 0.1114Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.1106Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.1073Epoch 5/15: [========                      ] 20/75 batches, loss: 0.1062Epoch 5/15: [========                      ] 21/75 batches, loss: 0.1042Epoch 5/15: [========                      ] 22/75 batches, loss: 0.1026Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.1047Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.1040Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.1063Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.1097Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.1084Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.1076Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.1068Epoch 5/15: [============                  ] 30/75 batches, loss: 0.1042Epoch 5/15: [============                  ] 31/75 batches, loss: 0.1025Epoch 5/15: [============                  ] 32/75 batches, loss: 0.1032Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.1029Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.1016Epoch 5/15: [==============                ] 35/75 batches, loss: 0.1014Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0995Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0982Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0981Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0972Epoch 5/15: [================              ] 40/75 batches, loss: 0.0960Epoch 5/15: [================              ] 41/75 batches, loss: 0.0966Epoch 5/15: [================              ] 42/75 batches, loss: 0.0959Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0950Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0944Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0943Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0937Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0933Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0925Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0949Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0951Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0940Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0937Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0937Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0939Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0944Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0939Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0933Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0927Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0922Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0921Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0918Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0914Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0910Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0909Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0903Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0900Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0897Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0899Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0898Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0895Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0901Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0903Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0899Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0898Epoch 5/15: [==============================] 75/75 batches, loss: 0.0896
[2025-05-07 17:51:03,395][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0896
[2025-05-07 17:51:03,633][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0357, Metrics: {'mse': 0.03584280610084534, 'rmse': 0.18932196412684224, 'r2': -0.010274887084960938}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0282Epoch 6/15: [                              ] 2/75 batches, loss: 0.0399Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0512Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0572Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0604Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0565Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0598Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0611Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0594Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0593Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0627Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0635Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0680Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0682Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0694Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0696Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0703Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0716Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0718Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0720Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0702Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0697Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0701Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0704Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0699Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0727Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0727Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0721Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0729Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0724Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0719Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0714Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0717Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0715Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0720Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0718Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0721Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0719Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0744Epoch 6/15: [================              ] 40/75 batches, loss: 0.0748Epoch 6/15: [================              ] 41/75 batches, loss: 0.0756Epoch 6/15: [================              ] 42/75 batches, loss: 0.0750Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0744Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0740Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0758Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0755Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0765Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0761Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0757Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0761Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0764Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0763Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0762Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0759Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0762Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0762Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0756Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0764Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0763Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0759Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0761Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0760Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0758Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0758Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0766Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0763Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0756Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0757Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0758Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0755Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0751Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0749Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0744Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0742Epoch 6/15: [==============================] 75/75 batches, loss: 0.0743
[2025-05-07 17:51:06,267][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0743
[2025-05-07 17:51:06,496][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0369, Metrics: {'mse': 0.03701571002602577, 'rmse': 0.19239467255105006, 'r2': -0.0433347225189209}
[2025-05-07 17:51:06,497][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0683Epoch 7/15: [                              ] 2/75 batches, loss: 0.0531Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0566Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0599Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0621Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0600Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0615Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0594Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0577Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0584Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0654Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0655Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0660Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0630Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0639Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0643Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0658Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0669Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0656Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0650Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0655Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0656Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0653Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0664Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0655Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0649Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0657Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0674Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0664Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0673Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0667Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0664Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0656Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0653Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0654Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0648Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0656Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0655Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0653Epoch 7/15: [================              ] 40/75 batches, loss: 0.0645Epoch 7/15: [================              ] 41/75 batches, loss: 0.0655Epoch 7/15: [================              ] 42/75 batches, loss: 0.0662Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0664Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0670Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0667Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0665Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0666Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0673Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0673Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0678Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0678Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0681Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0685Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0680Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0680Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0680Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0676Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0673Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0675Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0682Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0683Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0678Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0681Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0676Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0676Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0674Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0674Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0672Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0673Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0674Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0673Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0668Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0664Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0662Epoch 7/15: [==============================] 75/75 batches, loss: 0.0666
[2025-05-07 17:51:08,760][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0666
[2025-05-07 17:51:09,013][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0380, Metrics: {'mse': 0.03812292590737343, 'rmse': 0.1952509306184568, 'r2': -0.07454299926757812}
[2025-05-07 17:51:09,014][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0706Epoch 8/15: [                              ] 2/75 batches, loss: 0.0601Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0565Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0568Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0575Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0562Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0541Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0562Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0571Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0559Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0565Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0580Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0567Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0555Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0547Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0562Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0546Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0543Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0548Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0554Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0558Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0572Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0568Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0589Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0598Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0598Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0613Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0610Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0601Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0613Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0600Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0608Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0608Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0609Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0622Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0638Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0637Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0643Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0658Epoch 8/15: [================              ] 40/75 batches, loss: 0.0658Epoch 8/15: [================              ] 41/75 batches, loss: 0.0657Epoch 8/15: [================              ] 42/75 batches, loss: 0.0665Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0667Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0667Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0668Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0667Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0672Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0661Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0661Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0657Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0651Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0646Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0644Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0640Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0644Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0642Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0643Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0637Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0636Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0642Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0640Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0639Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0634Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0633Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0631Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0629Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0626Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0626Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0621Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0624Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0625Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0623Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0623Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0622Epoch 8/15: [==============================] 75/75 batches, loss: 0.0623
[2025-05-07 17:51:11,403][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0623
[2025-05-07 17:51:11,657][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0366, Metrics: {'mse': 0.03673965111374855, 'rmse': 0.1916759012336933, 'r2': -0.035553693771362305}
[2025-05-07 17:51:11,658][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0374Epoch 9/15: [                              ] 2/75 batches, loss: 0.0516Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0533Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0540Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0543Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0544Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0583Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0606Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0644Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0615Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0609Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0585Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0569Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0563Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0547Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0573Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0581Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0581Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0576Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0567Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0573Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0566Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0582Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0582Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0594Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0614Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0599Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0593Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0589Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0584Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0578Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0578Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0574Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0583Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0583Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0580Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0599Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0599Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0603Epoch 9/15: [================              ] 40/75 batches, loss: 0.0600Epoch 9/15: [================              ] 41/75 batches, loss: 0.0599Epoch 9/15: [================              ] 42/75 batches, loss: 0.0598Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0609Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0608Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0608Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0607Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0619Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0624Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0625Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0624Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0630Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0639Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0643Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0640Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0637Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0632Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0628Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0624Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0625Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0619Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0621Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0622Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0617Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0612Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0611Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0608Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0602Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0603Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0603Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0601Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0604Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0602Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0599Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0599Epoch 9/15: [==============================] 75/75 batches, loss: 0.0594
[2025-05-07 17:51:13,926][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0594
[2025-05-07 17:51:14,245][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0354, Metrics: {'mse': 0.03553289175033569, 'rmse': 0.18850170224784626, 'r2': -0.0015395879745483398}
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0551Epoch 10/15: [                              ] 2/75 batches, loss: 0.0530Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0562Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0665Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0647Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0702Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0663Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0681Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0670Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0652Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0651Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0653Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0640Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0650Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0643Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0642Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0631Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0635Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0632Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0630Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0628Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0614Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0610Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0606Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0599Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0593Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0591Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0593Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0588Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0587Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0605Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0598Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0593Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0605Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0598Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0605Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0604Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0599Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0593Epoch 10/15: [================              ] 40/75 batches, loss: 0.0592Epoch 10/15: [================              ] 41/75 batches, loss: 0.0597Epoch 10/15: [================              ] 42/75 batches, loss: 0.0600Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0597Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0598Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0605Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0602Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0600Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0596Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0601Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0608Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0614Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0612Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0614Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0612Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0607Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0607Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0610Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0609Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0610Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0607Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0605Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0603Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0604Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0602Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0607Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0604Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0602Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0604Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0604Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0601Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0600Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0598Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0598Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0603Epoch 10/15: [==============================] 75/75 batches, loss: 0.0605
[2025-05-07 17:51:16,984][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0605
[2025-05-07 17:51:17,251][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0355, Metrics: {'mse': 0.035625290125608444, 'rmse': 0.18874662944171597, 'r2': -0.004143953323364258}
[2025-05-07 17:51:17,251][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0822Epoch 11/15: [                              ] 2/75 batches, loss: 0.0746Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0617Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0614Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0604Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0602Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0610Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0610Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0623Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0602Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0599Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0627Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0621Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0596Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0612Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0611Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0603Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0596Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0603Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0600Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0589Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0583Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0585Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0584Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0587Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0587Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0580Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0577Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0573Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0580Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0579Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0590Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0589Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0585Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0588Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0594Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0586Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0584Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0576Epoch 11/15: [================              ] 40/75 batches, loss: 0.0573Epoch 11/15: [================              ] 41/75 batches, loss: 0.0573Epoch 11/15: [================              ] 42/75 batches, loss: 0.0578Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0580Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0572Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0570Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0562Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0566Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0568Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0567Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0565Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0560Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0556Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0552Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0555Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0556Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0556Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0561Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0558Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0557Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0557Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0557Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0562Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0560Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0557Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0555Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0553Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0553Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0551Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0553Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0555Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0552Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0555Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0559Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0557Epoch 11/15: [==============================] 75/75 batches, loss: 0.0553
[2025-05-07 17:51:19,592][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0553
[2025-05-07 17:51:19,843][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0363, Metrics: {'mse': 0.036396585404872894, 'rmse': 0.19077889140277782, 'r2': -0.025883913040161133}
[2025-05-07 17:51:19,844][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0722Epoch 12/15: [                              ] 2/75 batches, loss: 0.0687Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0659Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0682Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0663Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0624Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0608Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0586Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0586Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0576Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0573Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0584Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0578Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0575Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0575Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0568Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0565Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0579Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0570Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0558Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0569Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0569Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0571Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0587Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0580Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0574Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0567Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0567Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0566Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0560Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0574Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0576Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0570Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0566Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0567Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0563Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0565Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0569Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0572Epoch 12/15: [================              ] 40/75 batches, loss: 0.0566Epoch 12/15: [================              ] 41/75 batches, loss: 0.0558Epoch 12/15: [================              ] 42/75 batches, loss: 0.0561Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0564Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0563Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0570Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0564Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0564Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0560Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0558Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0551Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0555Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0551Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0552Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0553Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0554Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0552Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0547Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0552Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0547Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0545Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0543Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0545Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0546Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0545Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0545Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0541Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0539Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0545Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0545Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0544Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0547Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0545Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0549Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0548Epoch 12/15: [==============================] 75/75 batches, loss: 0.0551
[2025-05-07 17:51:22,147][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0551
[2025-05-07 17:51:22,399][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0346, Metrics: {'mse': 0.034707486629486084, 'rmse': 0.18629945418461666, 'r2': 0.02172553539276123}
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0419Epoch 13/15: [                              ] 2/75 batches, loss: 0.0389Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0437Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0471Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0495Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0484Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0492Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0488Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0469Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0459Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0494Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0503Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0495Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0503Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0507Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0509Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0510Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0496Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0490Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0498Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0492Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0507Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0512Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0512Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0503Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0494Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0498Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0491Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0496Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0500Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0508Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0506Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0508Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0541Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0541Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0548Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0547Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0542Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0549Epoch 13/15: [================              ] 40/75 batches, loss: 0.0542Epoch 13/15: [================              ] 41/75 batches, loss: 0.0552Epoch 13/15: [================              ] 42/75 batches, loss: 0.0556Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0553Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0554Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0563Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0562Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0556Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0554Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0549Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0545Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0545Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0543Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0540Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0535Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0537Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0541Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0538Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0537Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0535Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0533Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0535Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0541Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0541Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0539Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0539Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0544Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0541Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0541Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0548Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0550Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0550Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0546Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0547Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0547Epoch 13/15: [==============================] 75/75 batches, loss: 0.0544
[2025-05-07 17:51:25,109][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0544
[2025-05-07 17:51:25,365][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0368, Metrics: {'mse': 0.03692493587732315, 'rmse': 0.1921586216575336, 'r2': -0.04077613353729248}
[2025-05-07 17:51:25,365][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0538Epoch 14/15: [                              ] 2/75 batches, loss: 0.0536Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0426Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0450Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0422Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0407Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0388Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0439Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0487Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0488Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0481Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0480Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0467Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0478Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0463Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0474Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0481Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0484Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0482Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0480Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0487Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0487Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0490Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0492Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0496Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0496Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0493Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0497Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0493Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0498Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0505Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0512Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0524Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0523Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0526Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0532Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0531Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0527Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0521Epoch 14/15: [================              ] 40/75 batches, loss: 0.0524Epoch 14/15: [================              ] 41/75 batches, loss: 0.0523Epoch 14/15: [================              ] 42/75 batches, loss: 0.0525Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0522Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0526Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0523Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0524Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0522Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0521Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0525Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0528Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0528Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0527Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0530Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0527Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0530Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0529Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0530Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0533Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0529Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0526Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0528Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0528Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0526Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0526Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0529Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0529Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0527Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0529Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0527Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0525Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0524Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0524Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0525Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0521Epoch 14/15: [==============================] 75/75 batches, loss: 0.0526
[2025-05-07 17:51:27,689][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0526
[2025-05-07 17:51:27,943][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0348, Metrics: {'mse': 0.03494725748896599, 'rmse': 0.18694185590435863, 'r2': 0.014967203140258789}
[2025-05-07 17:51:27,944][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0560Epoch 15/15: [                              ] 2/75 batches, loss: 0.0560Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0532Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0534Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0543Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0526Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0529Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0531Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0542Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0503Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0484Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0471Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0479Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0473Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0478Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0486Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0472Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0471Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0461Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0464Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0465Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0467Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0471Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0463Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0458Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0461Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0464Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0469Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0468Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0472Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0471Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0471Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0469Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0466Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0467Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0460Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0472Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0472Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0473Epoch 15/15: [================              ] 40/75 batches, loss: 0.0472Epoch 15/15: [================              ] 41/75 batches, loss: 0.0469Epoch 15/15: [================              ] 42/75 batches, loss: 0.0469Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0466Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0470Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0472Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0472Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0480Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0486Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0487Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0483Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0487Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0485Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0486Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0488Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0490Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0488Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0484Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0485Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0488Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0487Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0486Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0489Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0489Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0487Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0491Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0492Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0493Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0494Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0496Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0495Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0492Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0492Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0494Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0493Epoch 15/15: [==============================] 75/75 batches, loss: 0.0493
[2025-05-07 17:51:30,266][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0493
[2025-05-07 17:51:30,555][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0345, Metrics: {'mse': 0.0346665233373642, 'rmse': 0.1861894823489345, 'r2': 0.022880136966705322}
[2025-05-07 17:51:30,941][src.training.lm_trainer][INFO] - Training completed in 42.13 seconds
[2025-05-07 17:51:30,941][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 17:51:33,996][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.04204091802239418, 'rmse': 0.205038820769127, 'r2': -0.051064372062683105}
[2025-05-07 17:51:33,997][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.0346665233373642, 'rmse': 0.1861894823489345, 'r2': 0.022880136966705322}
[2025-05-07 17:51:33,997][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04672267287969589, 'rmse': 0.21615428027151323, 'r2': -0.04478001594543457}
[2025-05-07 17:51:35,664][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control1/layer2/fi/fi/model.pt
[2025-05-07 17:51:35,665][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▂▂▁▁
wandb:     best_val_mse █▅▃▂▂▁▁
wandb:      best_val_r2 ▁▄▆▇▇██
wandb:    best_val_rmse █▅▃▂▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▁▂▃▃▃▂▃▃▃▃▃▃▃
wandb:       train_loss █▃▃▂▂▂▁▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▄▃▂▃▄▃▂▂▃▁▃▁▁
wandb:          val_mse ▇█▄▃▂▃▄▃▂▂▃▁▃▁▁
wandb:           val_r2 ▂▁▅▆▇▆▅▆▇▇▆█▆██
wandb:         val_rmse ▇█▅▃▂▃▄▃▂▂▃▁▃▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03454
wandb:     best_val_mse 0.03467
wandb:      best_val_r2 0.02288
wandb:    best_val_rmse 0.18619
wandb:            epoch 15
wandb:   final_test_mse 0.04672
wandb:    final_test_r2 -0.04478
wandb:  final_test_rmse 0.21615
wandb:  final_train_mse 0.04204
wandb:   final_train_r2 -0.05106
wandb: final_train_rmse 0.20504
wandb:    final_val_mse 0.03467
wandb:     final_val_r2 0.02288
wandb:   final_val_rmse 0.18619
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04928
wandb:       train_time 42.12723
wandb:         val_loss 0.03454
wandb:          val_mse 0.03467
wandb:           val_r2 0.02288
wandb:         val_rmse 0.18619
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_175029-pfyejyps
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_175029-pfyejyps/logs
Experiment probe_layer2_lexical_density_control1_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control1/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_lexical_density_control2_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_lexical_density_control2_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control2/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=lexical_density"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 17:52:20,408][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control2/layer2/fi
experiment_name: probe_layer2_lexical_density_control2_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: lexical_density
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 17:52:20,408][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 17:52:20,408][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 17:52:20,408][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 17:52:20,408][__main__][INFO] - Determined Task Type: regression
[2025-05-07 17:52:20,412][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 17:52:20,412][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 17:52:20,413][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 17:52:23,994][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'lexical_density'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 17:52:26,291][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 17:52:26,292][src.data.datasets][INFO] - Loading 'control_lexical_density_seed2' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:52:26,538][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_lexical_density_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:48:37 2025).
[2025-05-07 17:52:26,646][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_lexical_density_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:48:37 2025).
[2025-05-07 17:52:27,254][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 17:52:27,262][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:52:27,263][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 17:52:27,265][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:52:27,344][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:52:27,434][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:52:27,453][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 17:52:27,455][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:52:27,455][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 17:52:27,467][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:52:27,542][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:52:27,618][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:52:27,645][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 17:52:27,647][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:52:27,647][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 17:52:27,648][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 17:52:27,649][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:52:27,649][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 17:52:27,649][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 17:52:27,649][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 17:52:27,650][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:52:27,650][src.data.datasets][INFO] -   Mean: 0.6682, Std: 0.2000
[2025-05-07 17:52:27,650][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 17:52:27,650][src.data.datasets][INFO] - Sample label: 0.699999988079071
[2025-05-07 17:52:27,650][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:52:27,650][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 17:52:27,650][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 17:52:27,650][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 17:52:27,651][src.data.datasets][INFO] -   Min: 0.1670, Max: 1.0000
[2025-05-07 17:52:27,651][src.data.datasets][INFO] -   Mean: 0.6281, Std: 0.1884
[2025-05-07 17:52:27,651][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 17:52:27,651][src.data.datasets][INFO] - Sample label: 0.7329999804496765
[2025-05-07 17:52:27,651][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:52:27,651][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 17:52:27,651][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 17:52:27,651][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 17:52:27,651][src.data.datasets][INFO] -   Min: 0.0620, Max: 1.0000
[2025-05-07 17:52:27,651][src.data.datasets][INFO] -   Mean: 0.5643, Std: 0.2115
[2025-05-07 17:52:27,652][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 17:52:27,652][src.data.datasets][INFO] - Sample label: 0.25
[2025-05-07 17:52:27,652][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 17:52:27,652][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 17:52:27,652][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 17:52:27,652][__main__][INFO] - Using model type: lm_probe for submetric lexical_density
[2025-05-07 17:52:27,653][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 17:52:34,902][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 17:52:34,903][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 17:52:34,903][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 17:52:34,903][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 17:52:34,906][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 17:52:34,907][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 17:52:34,907][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 17:52:34,907][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 17:52:34,907][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 17:52:34,908][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 17:52:34,908][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6887Epoch 1/15: [                              ] 2/75 batches, loss: 0.7326Epoch 1/15: [=                             ] 3/75 batches, loss: 0.6024Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5419Epoch 1/15: [==                            ] 5/75 batches, loss: 0.5344Epoch 1/15: [==                            ] 6/75 batches, loss: 0.5039Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4802Epoch 1/15: [===                           ] 8/75 batches, loss: 0.5391Epoch 1/15: [===                           ] 9/75 batches, loss: 0.5408Epoch 1/15: [====                          ] 10/75 batches, loss: 0.5233Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4992Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4909Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4704Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4763Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4814Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4959Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4847Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4762Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4681Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4636Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4680Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4689Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4618Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4513Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4481Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4429Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4358Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4275Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4252Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4236Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4147Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4089Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4052Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4015Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3968Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3980Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3944Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3911Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3898Epoch 1/15: [================              ] 40/75 batches, loss: 0.3856Epoch 1/15: [================              ] 41/75 batches, loss: 0.3811Epoch 1/15: [================              ] 42/75 batches, loss: 0.3755Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3750Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3716Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3749Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3708Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3697Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3651Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3617Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3626Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3626Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3610Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3582Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3585Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3561Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3544Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3550Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3562Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3534Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3498Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3463Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3441Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3423Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3400Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3382Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3379Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3357Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3323Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3339Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3343Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3325Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3298Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3280Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3248Epoch 1/15: [==============================] 75/75 batches, loss: 0.3221
[2025-05-07 17:52:41,921][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3221
[2025-05-07 17:52:42,178][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0443, Metrics: {'mse': 0.04426565021276474, 'rmse': 0.21039403559218295, 'r2': -0.24768340587615967}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1983Epoch 2/15: [                              ] 2/75 batches, loss: 0.1514Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1255Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1470Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1509Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1467Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1587Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1562Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1527Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1633Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1617Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1560Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1540Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1517Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1539Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1560Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1597Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1644Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1634Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1644Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1660Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1658Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1668Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1647Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1669Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1700Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1730Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1735Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1755Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1767Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1739Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1716Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1701Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1712Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1714Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1683Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1663Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1654Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1659Epoch 2/15: [================              ] 40/75 batches, loss: 0.1661Epoch 2/15: [================              ] 41/75 batches, loss: 0.1673Epoch 2/15: [================              ] 42/75 batches, loss: 0.1671Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1664Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1642Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1623Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1643Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1643Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1634Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1658Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1644Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1631Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1629Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1641Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1627Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1621Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1637Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1620Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1611Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1608Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1596Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1600Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1588Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1586Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1585Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1581Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1578Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1572Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1567Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1571Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1557Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1549Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1545Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1543Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1535Epoch 2/15: [==============================] 75/75 batches, loss: 0.1535
[2025-05-07 17:52:44,944][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1535
[2025-05-07 17:52:45,214][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0417, Metrics: {'mse': 0.04177728295326233, 'rmse': 0.204394919098451, 'r2': -0.1775456666946411}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1027Epoch 3/15: [                              ] 2/75 batches, loss: 0.1003Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1034Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1078Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1034Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0965Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0959Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1011Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0984Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1079Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1059Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1017Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1025Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1024Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1028Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1048Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1096Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1108Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1147Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1121Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1100Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1105Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1095Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1188Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1173Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1163Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1150Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1150Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1151Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1140Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1140Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1139Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1139Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1148Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1135Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1121Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1104Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1096Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1088Epoch 3/15: [================              ] 40/75 batches, loss: 0.1098Epoch 3/15: [================              ] 41/75 batches, loss: 0.1095Epoch 3/15: [================              ] 42/75 batches, loss: 0.1089Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1081Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1086Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1087Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1088Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1102Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1107Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1117Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1116Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1105Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1112Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1102Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1112Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1102Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1100Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1092Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1088Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1081Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1077Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1068Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1081Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1078Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1085Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1079Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1073Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1076Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1076Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1074Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1079Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1076Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1068Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1063Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1060Epoch 3/15: [==============================] 75/75 batches, loss: 0.1065
[2025-05-07 17:52:47,959][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1065
[2025-05-07 17:52:48,207][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0373, Metrics: {'mse': 0.03734137490391731, 'rmse': 0.19323916503627653, 'r2': -0.052513957023620605}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0955Epoch 4/15: [                              ] 2/75 batches, loss: 0.0971Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0905Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1035Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0964Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0941Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1002Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1019Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1022Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0984Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1057Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1084Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1049Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1027Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1048Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1026Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0995Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0991Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0964Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0953Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0944Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0933Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0941Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0939Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0968Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0983Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0984Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0974Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0988Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0978Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0995Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0988Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0976Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0971Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0972Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0958Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0940Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0938Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0931Epoch 4/15: [================              ] 40/75 batches, loss: 0.0932Epoch 4/15: [================              ] 41/75 batches, loss: 0.0933Epoch 4/15: [================              ] 42/75 batches, loss: 0.0924Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0925Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0912Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0917Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0907Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0911Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0924Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0930Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0931Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0932Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0927Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0930Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0928Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0938Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0964Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0962Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0967Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0969Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0976Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0982Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0981Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0982Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0977Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0975Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0977Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0973Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0976Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0975Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0972Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0971Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0964Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0960Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0962Epoch 4/15: [==============================] 75/75 batches, loss: 0.0967
[2025-05-07 17:52:50,822][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0967
[2025-05-07 17:52:51,076][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0387, Metrics: {'mse': 0.03878658637404442, 'rmse': 0.1969431044084672, 'r2': -0.09324908256530762}
[2025-05-07 17:52:51,077][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0887Epoch 5/15: [                              ] 2/75 batches, loss: 0.0791Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1171Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1176Epoch 5/15: [==                            ] 5/75 batches, loss: 0.1055Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0943Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0939Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0921Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0947Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0925Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0919Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0926Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0933Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0917Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0893Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0877Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0875Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0849Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0861Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0849Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0853Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0841Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0868Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0867Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0910Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0902Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0891Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0904Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0904Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0885Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0889Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0907Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0899Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0895Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0908Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0901Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0903Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0898Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0893Epoch 5/15: [================              ] 40/75 batches, loss: 0.0896Epoch 5/15: [================              ] 41/75 batches, loss: 0.0895Epoch 5/15: [================              ] 42/75 batches, loss: 0.0888Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0884Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0890Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0885Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0888Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0881Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0872Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0889Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0880Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0869Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0867Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0867Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0868Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0866Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0863Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0862Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0863Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0858Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0860Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0857Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0854Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0855Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0853Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0862Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0860Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0856Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0853Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0853Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0854Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0862Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0857Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0856Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0855Epoch 5/15: [==============================] 75/75 batches, loss: 0.0852
[2025-05-07 17:52:53,366][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0852
[2025-05-07 17:52:53,601][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0360, Metrics: {'mse': 0.036094944924116135, 'rmse': 0.18998669670299584, 'r2': -0.017381787300109863}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0575Epoch 6/15: [                              ] 2/75 batches, loss: 0.0944Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0817Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0767Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0886Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0847Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0836Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0805Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0739Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0710Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0735Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0749Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0756Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0757Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0796Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0809Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0785Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0800Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0782Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0776Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0759Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0751Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0762Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0754Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0763Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0758Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0758Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0749Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0733Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0734Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0736Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0736Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0733Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0732Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0733Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0726Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0735Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0730Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0756Epoch 6/15: [================              ] 40/75 batches, loss: 0.0759Epoch 6/15: [================              ] 41/75 batches, loss: 0.0756Epoch 6/15: [================              ] 42/75 batches, loss: 0.0755Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0751Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0750Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0768Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0763Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0761Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0757Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0752Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0745Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0752Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0751Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0761Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0757Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0756Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0755Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0758Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0758Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0757Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0761Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0772Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0770Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0771Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0778Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0780Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0780Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0778Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0779Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0781Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0777Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0780Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0776Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0775Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0772Epoch 6/15: [==============================] 75/75 batches, loss: 0.0772
[2025-05-07 17:52:56,266][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0772
[2025-05-07 17:52:56,539][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0371, Metrics: {'mse': 0.03718157112598419, 'rmse': 0.19282523467115031, 'r2': -0.048009634017944336}
[2025-05-07 17:52:56,540][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0678Epoch 7/15: [                              ] 2/75 batches, loss: 0.0791Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0733Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0697Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0680Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0677Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0713Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0720Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0693Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0667Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0667Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0650Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0647Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0663Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0667Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0667Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0683Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0683Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0673Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0670Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0651Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0638Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0626Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0626Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0628Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0632Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0633Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0642Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0665Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0660Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0668Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0663Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0659Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0664Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0657Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0659Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0676Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0676Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0676Epoch 7/15: [================              ] 40/75 batches, loss: 0.0680Epoch 7/15: [================              ] 41/75 batches, loss: 0.0675Epoch 7/15: [================              ] 42/75 batches, loss: 0.0681Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0683Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0682Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0675Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0675Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0674Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0674Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0680Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0681Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0687Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0686Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0685Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0685Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0688Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0685Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0685Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0690Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0691Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0702Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0701Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0696Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0699Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0698Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0695Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0699Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0698Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0695Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0693Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0696Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0696Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0698Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0692Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0688Epoch 7/15: [==============================] 75/75 batches, loss: 0.0688
[2025-05-07 17:52:58,839][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0688
[2025-05-07 17:52:59,058][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0351, Metrics: {'mse': 0.03523451089859009, 'rmse': 0.18770857971491364, 'r2': 0.006870627403259277}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0660Epoch 8/15: [                              ] 2/75 batches, loss: 0.0594Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0546Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0529Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0526Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0586Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0592Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0627Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0623Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0619Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0627Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0618Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0621Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0621Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0631Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0630Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0625Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0623Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0620Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0631Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0655Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0656Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0648Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0650Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0652Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0644Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0656Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0645Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0636Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0655Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0659Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0667Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0662Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0666Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0660Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0659Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0656Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0649Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0647Epoch 8/15: [================              ] 40/75 batches, loss: 0.0650Epoch 8/15: [================              ] 41/75 batches, loss: 0.0647Epoch 8/15: [================              ] 42/75 batches, loss: 0.0648Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0642Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0646Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0652Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0648Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0652Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0644Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0645Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0644Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0651Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0655Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0658Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0652Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0659Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0662Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0660Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0659Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0654Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0659Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0654Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0650Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0655Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0656Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0656Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0652Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0647Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0646Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0644Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0642Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0641Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0637Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0632Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0631Epoch 8/15: [==============================] 75/75 batches, loss: 0.0634
[2025-05-07 17:53:01,779][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0634
[2025-05-07 17:53:02,040][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0382, Metrics: {'mse': 0.038332499563694, 'rmse': 0.19578687280738205, 'r2': -0.08045005798339844}
[2025-05-07 17:53:02,040][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0149Epoch 9/15: [                              ] 2/75 batches, loss: 0.0220Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0344Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0335Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0315Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0363Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0398Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0470Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0486Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0495Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0485Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0495Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0488Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0494Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0509Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0514Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0508Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0521Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0537Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0552Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0555Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0546Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0546Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0548Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0557Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0560Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0560Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0559Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0562Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0564Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0562Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0563Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0567Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0573Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0578Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0584Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0577Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0578Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0577Epoch 9/15: [================              ] 40/75 batches, loss: 0.0576Epoch 9/15: [================              ] 41/75 batches, loss: 0.0579Epoch 9/15: [================              ] 42/75 batches, loss: 0.0579Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0586Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0586Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0582Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0585Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0588Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0593Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0594Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0588Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0593Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0596Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0598Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0599Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0599Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0599Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0600Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0601Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0603Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0600Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0602Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0607Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0602Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0605Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0606Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0609Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0608Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0604Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0610Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0608Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0606Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0607Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0608Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0612Epoch 9/15: [==============================] 75/75 batches, loss: 0.0609
[2025-05-07 17:53:04,387][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0609
[2025-05-07 17:53:04,650][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0353, Metrics: {'mse': 0.035413142293691635, 'rmse': 0.1881837992328023, 'r2': 0.001835644245147705}
[2025-05-07 17:53:04,651][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0581Epoch 10/15: [                              ] 2/75 batches, loss: 0.0442Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0474Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0487Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0509Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0615Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0600Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0623Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0624Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0666Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0693Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0696Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0683Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0708Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0688Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0693Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0697Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0677Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0670Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0659Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0663Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0659Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0662Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0659Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0644Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0641Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0645Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0644Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0651Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0646Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0645Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0643Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0656Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0646Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0656Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0650Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0646Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0648Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0656Epoch 10/15: [================              ] 40/75 batches, loss: 0.0659Epoch 10/15: [================              ] 41/75 batches, loss: 0.0665Epoch 10/15: [================              ] 42/75 batches, loss: 0.0662Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0665Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0656Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0660Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0659Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0657Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0663Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0662Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0672Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0671Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0668Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0667Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0665Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0658Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0654Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0649Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0648Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0646Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0646Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0643Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0642Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0639Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0638Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0640Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0642Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0640Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0638Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0637Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0635Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0632Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0634Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0629Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0633Epoch 10/15: [==============================] 75/75 batches, loss: 0.0632
[2025-05-07 17:53:07,008][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0632
[2025-05-07 17:53:07,332][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0428, Metrics: {'mse': 0.0429331511259079, 'rmse': 0.20720316388971452, 'r2': -0.21012520790100098}
[2025-05-07 17:53:07,333][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0767Epoch 11/15: [                              ] 2/75 batches, loss: 0.0633Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0639Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0602Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0576Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0554Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0562Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0541Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0545Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0598Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0612Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0594Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0579Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0570Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0560Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0559Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0564Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0588Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0585Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0585Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0590Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0598Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0587Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0596Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0587Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0589Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0597Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0596Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0590Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0589Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0587Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0590Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0595Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0596Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0594Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0594Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0591Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0590Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0589Epoch 11/15: [================              ] 40/75 batches, loss: 0.0592Epoch 11/15: [================              ] 41/75 batches, loss: 0.0595Epoch 11/15: [================              ] 42/75 batches, loss: 0.0588Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0589Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0589Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0586Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0586Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0580Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0581Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0577Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0574Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0571Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0569Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0571Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0575Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0574Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0575Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0575Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0571Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0570Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0567Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0566Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0564Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0560Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0557Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0557Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0556Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0558Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0554Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0560Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0557Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0555Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0556Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0556Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0556Epoch 11/15: [==============================] 75/75 batches, loss: 0.0552
[2025-05-07 17:53:09,622][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0552
[2025-05-07 17:53:09,876][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0377, Metrics: {'mse': 0.03784230723977089, 'rmse': 0.19453099300566706, 'r2': -0.06663334369659424}
[2025-05-07 17:53:09,877][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 17:53:09,877][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 11
[2025-05-07 17:53:09,877][src.training.lm_trainer][INFO] - Training completed in 31.15 seconds
[2025-05-07 17:53:09,877][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 17:53:12,838][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.04152265936136246, 'rmse': 0.20377109550022657, 'r2': -0.03810739517211914}
[2025-05-07 17:53:12,838][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03523451089859009, 'rmse': 0.18770857971491364, 'r2': 0.006870627403259277}
[2025-05-07 17:53:12,838][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04816669225692749, 'rmse': 0.21946911458546392, 'r2': -0.07707023620605469}
[2025-05-07 17:53:14,505][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control2/layer2/fi/fi/model.pt
[2025-05-07 17:53:14,506][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▃▂▁
wandb:     best_val_mse █▆▃▂▁
wandb:      best_val_r2 ▁▃▆▇█
wandb:    best_val_rmse █▆▃▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▃▃▃▃▄▃▄▁
wandb:       train_loss █▄▂▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▃▄▂▂▁▃▁▇▃
wandb:          val_mse █▆▃▄▂▃▁▃▁▇▃
wandb:           val_r2 ▁▃▆▅▇▆█▆█▂▆
wandb:         val_rmse █▆▃▄▂▃▁▃▁▇▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03513
wandb:     best_val_mse 0.03523
wandb:      best_val_r2 0.00687
wandb:    best_val_rmse 0.18771
wandb: early_stop_epoch 11
wandb:            epoch 11
wandb:   final_test_mse 0.04817
wandb:    final_test_r2 -0.07707
wandb:  final_test_rmse 0.21947
wandb:  final_train_mse 0.04152
wandb:   final_train_r2 -0.03811
wandb: final_train_rmse 0.20377
wandb:    final_val_mse 0.03523
wandb:     final_val_r2 0.00687
wandb:   final_val_rmse 0.18771
wandb:    learning_rate 0.0001
wandb:       train_loss 0.05524
wandb:       train_time 31.14588
wandb:         val_loss 0.03772
wandb:          val_mse 0.03784
wandb:           val_r2 -0.06663
wandb:         val_rmse 0.19453
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_175220-6yizgby1
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_175220-6yizgby1/logs
Experiment probe_layer2_lexical_density_control2_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control2/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_lexical_density_control3_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_lexical_density_control3_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control3/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=lexical_density"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 17:53:51,926][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control3/layer2/fi
experiment_name: probe_layer2_lexical_density_control3_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: lexical_density
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 17:53:51,926][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 17:53:51,926][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 17:53:51,926][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 17:53:51,926][__main__][INFO] - Determined Task Type: regression
[2025-05-07 17:53:51,931][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 17:53:51,931][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 17:53:51,931][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 17:53:56,860][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'lexical_density'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 17:53:59,214][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 17:53:59,214][src.data.datasets][INFO] - Loading 'control_lexical_density_seed3' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:53:59,551][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_lexical_density_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:49:28 2025).
[2025-05-07 17:53:59,700][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_lexical_density_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:49:28 2025).
[2025-05-07 17:54:00,281][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 17:54:00,290][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:54:00,290][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 17:54:00,294][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:54:00,361][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:54:00,475][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:54:00,503][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 17:54:00,504][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:54:00,505][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 17:54:00,514][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 17:54:00,640][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:54:00,783][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 17:54:00,801][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 17:54:00,803][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 17:54:00,803][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 17:54:00,804][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 17:54:00,805][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:54:00,805][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 17:54:00,805][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 17:54:00,805][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 17:54:00,805][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 17:54:00,805][src.data.datasets][INFO] -   Mean: 0.6682, Std: 0.2000
[2025-05-07 17:54:00,805][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 17:54:00,806][src.data.datasets][INFO] - Sample label: 0.699999988079071
[2025-05-07 17:54:00,806][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:54:00,806][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 17:54:00,806][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 17:54:00,806][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 17:54:00,806][src.data.datasets][INFO] -   Min: 0.1670, Max: 1.0000
[2025-05-07 17:54:00,806][src.data.datasets][INFO] -   Mean: 0.6281, Std: 0.1884
[2025-05-07 17:54:00,806][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 17:54:00,806][src.data.datasets][INFO] - Sample label: 0.7329999804496765
[2025-05-07 17:54:00,807][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 17:54:00,807][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 17:54:00,807][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 17:54:00,807][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 17:54:00,807][src.data.datasets][INFO] -   Min: 0.0620, Max: 1.0000
[2025-05-07 17:54:00,807][src.data.datasets][INFO] -   Mean: 0.5643, Std: 0.2115
[2025-05-07 17:54:00,807][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 17:54:00,807][src.data.datasets][INFO] - Sample label: 0.25
[2025-05-07 17:54:00,807][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 17:54:00,807][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 17:54:00,808][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 17:54:00,808][__main__][INFO] - Using model type: lm_probe for submetric lexical_density
[2025-05-07 17:54:00,808][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 17:54:10,230][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 17:54:10,231][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 17:54:10,231][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 17:54:10,231][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 17:54:10,234][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 17:54:10,234][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 17:54:10,234][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 17:54:10,235][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 17:54:10,235][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 17:54:10,235][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 17:54:10,236][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5968Epoch 1/15: [                              ] 2/75 batches, loss: 0.7169Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5909Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5362Epoch 1/15: [==                            ] 5/75 batches, loss: 0.5274Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4939Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4653Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4953Epoch 1/15: [===                           ] 9/75 batches, loss: 0.5008Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4847Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4694Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4617Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4379Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4478Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4347Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4415Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4318Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4204Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4174Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4144Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4182Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4252Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4157Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4041Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3972Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3905Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3863Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3833Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3796Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3834Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3784Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3750Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3730Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3695Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3660Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3662Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3635Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3590Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3574Epoch 1/15: [================              ] 40/75 batches, loss: 0.3524Epoch 1/15: [================              ] 41/75 batches, loss: 0.3481Epoch 1/15: [================              ] 42/75 batches, loss: 0.3457Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3421Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3405Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3447Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3406Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3417Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3378Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3349Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3340Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3337Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3328Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3306Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3312Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3297Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3273Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3346Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3347Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3324Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3307Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3279Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3260Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3264Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3233Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3217Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3197Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3165Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3140Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3134Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3118Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3095Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3079Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3054Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3032Epoch 1/15: [==============================] 75/75 batches, loss: 0.3013
[2025-05-07 17:54:18,433][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3013
[2025-05-07 17:54:18,723][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0422, Metrics: {'mse': 0.04220518097281456, 'rmse': 0.20543899574524443, 'r2': -0.18960654735565186}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1328Epoch 2/15: [                              ] 2/75 batches, loss: 0.1158Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1106Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1420Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1530Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1556Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1745Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1710Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1697Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1749Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1661Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1635Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1613Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1579Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1560Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1568Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1584Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1629Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1617Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1604Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1609Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1636Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1629Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1610Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1641Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1636Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1649Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1640Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1681Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1681Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1665Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1642Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1639Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1648Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1667Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1659Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1638Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1616Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1623Epoch 2/15: [================              ] 40/75 batches, loss: 0.1611Epoch 2/15: [================              ] 41/75 batches, loss: 0.1610Epoch 2/15: [================              ] 42/75 batches, loss: 0.1589Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1581Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1568Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1556Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1578Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1568Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1570Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1595Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1584Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1574Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1586Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1584Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1583Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1564Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1569Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1568Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1560Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1557Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1543Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1527Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1525Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1522Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1530Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1525Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1517Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1516Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1511Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1506Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1496Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1493Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1504Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1504Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1498Epoch 2/15: [==============================] 75/75 batches, loss: 0.1498
[2025-05-07 17:54:21,399][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1498
[2025-05-07 17:54:21,664][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0493, Metrics: {'mse': 0.04938981682062149, 'rmse': 0.2222381983832246, 'r2': -0.392114520072937}
[2025-05-07 17:54:21,665][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1126Epoch 3/15: [                              ] 2/75 batches, loss: 0.1104Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1230Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1266Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1118Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1130Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1102Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1075Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1106Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1196Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1166Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1158Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1176Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1236Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1212Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1230Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1214Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1172Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1142Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1140Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1129Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1132Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1111Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1187Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1185Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1165Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1159Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1145Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1159Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1143Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1133Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1121Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1114Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1139Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1127Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1117Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1096Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1091Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1093Epoch 3/15: [================              ] 40/75 batches, loss: 0.1087Epoch 3/15: [================              ] 41/75 batches, loss: 0.1080Epoch 3/15: [================              ] 42/75 batches, loss: 0.1078Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1088Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1099Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1106Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1100Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1103Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1110Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1108Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1107Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1106Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1106Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1098Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1097Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1093Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1081Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1072Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1069Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1063Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1060Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1050Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1061Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1064Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1073Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1074Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1065Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1069Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1060Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1059Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1067Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1065Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1062Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1056Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1063Epoch 3/15: [==============================] 75/75 batches, loss: 0.1065
[2025-05-07 17:54:23,955][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1065
[2025-05-07 17:54:24,203][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0395, Metrics: {'mse': 0.03961101174354553, 'rmse': 0.19902515354483596, 'r2': -0.1164865493774414}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0799Epoch 4/15: [                              ] 2/75 batches, loss: 0.0893Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0858Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0854Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0833Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0898Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0958Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0931Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0940Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0910Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0976Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0992Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0954Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0943Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0975Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1010Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1004Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0987Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0961Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0956Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0977Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0998Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0989Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1001Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1014Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1007Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0982Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0967Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0978Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0967Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0978Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0977Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0964Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0961Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0963Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0959Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0957Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0966Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0956Epoch 4/15: [================              ] 40/75 batches, loss: 0.0945Epoch 4/15: [================              ] 41/75 batches, loss: 0.0952Epoch 4/15: [================              ] 42/75 batches, loss: 0.0943Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0955Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0954Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0947Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0943Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0940Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0941Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0953Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0954Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0952Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0960Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0964Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0962Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0956Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0957Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0949Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0949Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0957Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0967Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0970Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0965Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0963Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0960Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0958Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0949Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0951Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0950Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0957Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0962Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0962Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0962Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0958Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0955Epoch 4/15: [==============================] 75/75 batches, loss: 0.0950
[2025-05-07 17:54:26,928][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0950
[2025-05-07 17:54:27,163][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0380, Metrics: {'mse': 0.03812580928206444, 'rmse': 0.19525831424567927, 'r2': -0.07462430000305176}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0904Epoch 5/15: [                              ] 2/75 batches, loss: 0.0975Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1079Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0988Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0878Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0899Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0921Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0946Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0933Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0899Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0901Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0939Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0956Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0942Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0959Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0932Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0916Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0911Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0946Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0938Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0955Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0950Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0963Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0967Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0976Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0973Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0967Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0960Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0944Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0933Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0933Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0957Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0936Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0931Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0933Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0930Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0935Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0924Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0917Epoch 5/15: [================              ] 40/75 batches, loss: 0.0914Epoch 5/15: [================              ] 41/75 batches, loss: 0.0912Epoch 5/15: [================              ] 42/75 batches, loss: 0.0905Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0895Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0889Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0885Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0886Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0880Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0883Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0888Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0876Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0870Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0866Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0864Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0870Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0872Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0866Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0860Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0857Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0858Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0856Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0858Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0865Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0862Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0860Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0854Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0855Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0855Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0852Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0845Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0857Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0864Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0863Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0860Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0858Epoch 5/15: [==============================] 75/75 batches, loss: 0.0857
[2025-05-07 17:54:29,815][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0857
[2025-05-07 17:54:30,061][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0374, Metrics: {'mse': 0.03749687224626541, 'rmse': 0.19364109131655247, 'r2': -0.05689692497253418}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0601Epoch 6/15: [                              ] 2/75 batches, loss: 0.0609Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0545Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0590Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0642Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0727Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0752Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0767Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0761Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0751Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0750Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0756Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0793Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0789Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0791Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0784Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0752Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0764Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0757Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0742Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0745Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0741Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0742Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0762Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0785Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0791Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0793Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0777Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0771Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0766Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0773Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0767Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0760Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0752Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0745Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0739Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0735Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0732Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0753Epoch 6/15: [================              ] 40/75 batches, loss: 0.0748Epoch 6/15: [================              ] 41/75 batches, loss: 0.0750Epoch 6/15: [================              ] 42/75 batches, loss: 0.0756Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0749Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0748Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0761Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0752Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0759Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0754Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0750Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0741Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0745Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0738Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0741Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0739Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0733Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0738Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0741Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0738Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0731Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0745Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0748Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0749Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0745Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0747Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0750Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0748Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0758Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0758Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0757Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0756Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0756Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0752Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0752Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0752Epoch 6/15: [==============================] 75/75 batches, loss: 0.0749
[2025-05-07 17:54:32,922][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0749
[2025-05-07 17:54:33,222][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0348, Metrics: {'mse': 0.0348743312060833, 'rmse': 0.18674670333390975, 'r2': 0.01702284812927246}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0954Epoch 7/15: [                              ] 2/75 batches, loss: 0.0982Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0792Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0790Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0739Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0723Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0791Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0797Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0787Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0806Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0793Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0779Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0766Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0760Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0766Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0776Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0755Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0755Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0725Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0707Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0706Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0713Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0723Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0744Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0747Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0754Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0760Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0757Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0756Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0741Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0739Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0736Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0720Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0733Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0737Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0732Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0755Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0761Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0753Epoch 7/15: [================              ] 40/75 batches, loss: 0.0748Epoch 7/15: [================              ] 41/75 batches, loss: 0.0745Epoch 7/15: [================              ] 42/75 batches, loss: 0.0748Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0748Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0747Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0743Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0735Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0741Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0738Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0732Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0732Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0728Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0733Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0727Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0729Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0732Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0732Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0732Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0729Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0725Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0729Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0723Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0722Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0726Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0725Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0722Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0719Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0713Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0708Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0705Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0701Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0697Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0695Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0697Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0696Epoch 7/15: [==============================] 75/75 batches, loss: 0.0694
[2025-05-07 17:54:36,011][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0694
[2025-05-07 17:54:36,306][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0387, Metrics: {'mse': 0.03880889341235161, 'rmse': 0.19699972947278788, 'r2': -0.09387791156768799}
[2025-05-07 17:54:36,307][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0849Epoch 8/15: [                              ] 2/75 batches, loss: 0.0707Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0683Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0590Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0541Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0580Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0587Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0600Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0582Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0571Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0575Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0570Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0563Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0587Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0585Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0599Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0592Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0590Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0620Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0628Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0640Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0658Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0656Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0661Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0651Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0644Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0642Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0635Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0637Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0645Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0649Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0652Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0663Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0653Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0656Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0648Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0645Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0648Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0653Epoch 8/15: [================              ] 40/75 batches, loss: 0.0659Epoch 8/15: [================              ] 41/75 batches, loss: 0.0652Epoch 8/15: [================              ] 42/75 batches, loss: 0.0650Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0654Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0652Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0659Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0664Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0669Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0665Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0666Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0664Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0666Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0666Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0666Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0664Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0668Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0663Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0668Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0669Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0676Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0674Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0670Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0669Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0668Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0669Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0668Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0668Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0671Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0670Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0669Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0666Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0669Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0671Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0669Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0667Epoch 8/15: [==============================] 75/75 batches, loss: 0.0665
[2025-05-07 17:54:38,658][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0665
[2025-05-07 17:54:38,990][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0371, Metrics: {'mse': 0.03720897063612938, 'rmse': 0.19289626910889018, 'r2': -0.04878199100494385}
[2025-05-07 17:54:38,991][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0264Epoch 9/15: [                              ] 2/75 batches, loss: 0.0590Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0560Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0590Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0669Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0646Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0643Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0625Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0669Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0656Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0677Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0650Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0644Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0672Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0648Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0690Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0668Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0658Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0662Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0650Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0642Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0662Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0662Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0669Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0664Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0656Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0642Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0633Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0627Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0630Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0624Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0619Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0610Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0626Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0617Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0610Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0616Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0631Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0628Epoch 9/15: [================              ] 40/75 batches, loss: 0.0627Epoch 9/15: [================              ] 41/75 batches, loss: 0.0623Epoch 9/15: [================              ] 42/75 batches, loss: 0.0634Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0636Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0639Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0640Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0638Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0638Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0637Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0637Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0634Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0628Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0628Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0632Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0627Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0632Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0631Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0631Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0628Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0622Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0626Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0623Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0628Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0625Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0624Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0625Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0625Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0622Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0622Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0624Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0623Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0633Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0628Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0627Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0628Epoch 9/15: [==============================] 75/75 batches, loss: 0.0631
[2025-05-07 17:54:41,349][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0631
[2025-05-07 17:54:41,710][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0362, Metrics: {'mse': 0.03633532300591469, 'rmse': 0.19061826514244296, 'r2': -0.024157166481018066}
[2025-05-07 17:54:41,711][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0799Epoch 10/15: [                              ] 2/75 batches, loss: 0.0566Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0505Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0500Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0517Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0570Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0541Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0558Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0551Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0547Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0553Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0566Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0566Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0581Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0582Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0596Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0599Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0601Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0618Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0620Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0617Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0609Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0618Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0618Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0611Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0608Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0606Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0605Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0607Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0602Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0606Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0610Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0606Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0602Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0605Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0610Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0615Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0613Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0611Epoch 10/15: [================              ] 40/75 batches, loss: 0.0614Epoch 10/15: [================              ] 41/75 batches, loss: 0.0614Epoch 10/15: [================              ] 42/75 batches, loss: 0.0613Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0615Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0608Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0605Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0605Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0611Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0611Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0611Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0615Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0618Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0610Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0611Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0617Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0618Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0617Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0622Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0618Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0621Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0620Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0617Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0615Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0611Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0615Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0616Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0618Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0613Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0613Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0616Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0614Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0613Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0616Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0616Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0620Epoch 10/15: [==============================] 75/75 batches, loss: 0.0618
[2025-05-07 17:54:44,052][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0618
[2025-05-07 17:54:44,358][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0378, Metrics: {'mse': 0.0378609262406826, 'rmse': 0.1945788432504485, 'r2': -0.06715822219848633}
[2025-05-07 17:54:44,359][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 17:54:44,359][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 17:54:44,359][src.training.lm_trainer][INFO] - Training completed in 29.07 seconds
[2025-05-07 17:54:44,359][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 17:54:47,323][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.040928930044174194, 'rmse': 0.20230899644893252, 'r2': -0.02326345443725586}
[2025-05-07 17:54:47,323][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.0348743312060833, 'rmse': 0.18674670333390975, 'r2': 0.01702284812927246}
[2025-05-07 17:54:47,323][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04948452487587929, 'rmse': 0.22245117413913393, 'r2': -0.10653877258300781}
[2025-05-07 17:54:48,980][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control3/layer2/fi/fi/model.pt
[2025-05-07 17:54:48,981][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▄▃▁
wandb:     best_val_mse █▆▄▄▁
wandb:      best_val_r2 ▁▃▅▅█
wandb:    best_val_rmse █▆▄▄▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▁▄▄▄▅▄▄▄
wandb:       train_loss █▄▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▅█▃▃▂▁▃▂▂▂
wandb:          val_mse ▅█▃▃▂▁▃▂▂▂
wandb:           val_r2 ▄▁▆▆▇█▆▇▇▇
wandb:         val_rmse ▅█▃▃▂▁▃▂▂▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03476
wandb:     best_val_mse 0.03487
wandb:      best_val_r2 0.01702
wandb:    best_val_rmse 0.18675
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.04948
wandb:    final_test_r2 -0.10654
wandb:  final_test_rmse 0.22245
wandb:  final_train_mse 0.04093
wandb:   final_train_r2 -0.02326
wandb: final_train_rmse 0.20231
wandb:    final_val_mse 0.03487
wandb:     final_val_r2 0.01702
wandb:   final_val_rmse 0.18675
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06176
wandb:       train_time 29.06708
wandb:         val_loss 0.03776
wandb:          val_mse 0.03786
wandb:           val_r2 -0.06716
wandb:         val_rmse 0.19458
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_175352-tjdqk933
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_175352-tjdqk933/logs
Experiment probe_layer2_lexical_density_control3_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control3/layer2/fi/fi/results.json for layer 2
Some experiments failed. See /scratch/leuven/371/vsc37132/makeup_probes_output/failed_experiments.log for details.
Failed experiments (2):
probe_layer2_n_tokens,_fi
probe_layer2_avg_verb_edges,_fi
==============================================
probing experiments completed!
 planned experiments: 16
 completed: 64
