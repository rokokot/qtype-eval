SLURM_JOB_ID: 64466444
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: qtype_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Wed May  7 18:58:07 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main probing experiments (non-control)...
=======================
PROBING LAYER 2
=======================
Experiment probe_layer2_complexity_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/ru/ru/results.json for layer 2
Experiment probe_layer2_complexity_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/ja/ja/results.json for layer 2
=======================
PROBING LAYER 6
=======================
Experiment probe_layer6_complexity_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer6/ru/ru/results.json for layer 6
Experiment probe_layer6_complexity_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer6/ja/ja/results.json for layer 6
Running control probing experiments...
=======================
PROBING LAYER 2 (CONTROL EXPERIMENTS)
=======================
Experiment probe_layer2_complexity_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/ru/ru/results.json for layer 2
Experiment probe_layer2_complexity_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/ru/ru/results.json for layer 2
Experiment probe_layer2_complexity_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/ru/ru/results.json for layer 2
Experiment probe_layer2_complexity_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/ja/ja/results.json for layer 2
Experiment probe_layer2_complexity_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/ja/ja/results.json for layer 2
Experiment probe_layer2_complexity_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/ja/ja/results.json for layer 2
=======================
PROBING LAYER 6 (CONTROL EXPERIMENTS)
=======================
Experiment probe_layer6_complexity_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer6/ru/ru/results.json for layer 6
Experiment probe_layer6_complexity_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer6/ru/ru/results.json for layer 6
Experiment probe_layer6_complexity_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer6/ru/ru/results.json for layer 6
Experiment probe_layer6_complexity_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer6/ja/ja/results.json for layer 6
Experiment probe_layer6_complexity_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer6/ja/ja/results.json for layer 6
Experiment probe_layer6_complexity_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer6/ja/ja/results.json for layer 6
Running submetric probing experiments...
=======================
PROBING LAYER 2 (SUBMETRIC EXPERIMENTS)
=======================
Experiment probe_layer2_avg_links_len_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/ru/ru/results.json for layer 2
Experiment probe_layer2_avg_links_len_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/ja/ja/results.json for layer 2
=======================
PROBING LAYER 6 (SUBMETRIC EXPERIMENTS)
=======================
Experiment probe_layer6_avg_links_len_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer6/ru/ru/results.json for layer 6
Running experiment: probe_layer6_avg_links_len_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_avg_links_len_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer6/ja"         "wandb.mode=offline" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 18:58:55,006][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer6/ja
experiment_name: probe_layer6_avg_links_len_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 18:58:55,006][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 18:58:55,006][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 18:58:55,006][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 18:58:55,006][__main__][INFO] - Determined Task Type: regression
[2025-05-07 18:58:55,010][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 18:58:55,010][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 18:58:55,010][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 18:58:59,298][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 18:59:01,829][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 18:59:01,830][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:59:02,101][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:59:02,252][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:59:02,471][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 18:59:02,479][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:59:02,480][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 18:59:02,483][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:59:02,553][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:59:02,650][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:59:02,676][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 18:59:02,677][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:59:02,677][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 18:59:02,680][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:59:02,750][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:59:02,850][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:59:02,887][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 18:59:02,888][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:59:02,888][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 18:59:02,890][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 18:59:02,891][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 18:59:02,891][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 18:59:02,891][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 18:59:02,891][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 18:59:02,891][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:59:02,892][src.data.datasets][INFO] -   Mean: 0.1654, Std: 0.0964
[2025-05-07 18:59:02,892][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 18:59:02,892][src.data.datasets][INFO] - Sample label: 0.2070000022649765
[2025-05-07 18:59:02,892][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 18:59:02,892][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 18:59:02,892][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 18:59:02,892][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 18:59:02,892][src.data.datasets][INFO] -   Min: 0.0580, Max: 0.6190
[2025-05-07 18:59:02,892][src.data.datasets][INFO] -   Mean: 0.2214, Std: 0.1335
[2025-05-07 18:59:02,893][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 18:59:02,893][src.data.datasets][INFO] - Sample label: 0.3889999985694885
[2025-05-07 18:59:02,893][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 18:59:02,893][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 18:59:02,893][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 18:59:02,893][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 18:59:02,893][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:59:02,893][src.data.datasets][INFO] -   Mean: 0.4217, Std: 0.2062
[2025-05-07 18:59:02,893][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 18:59:02,893][src.data.datasets][INFO] - Sample label: 0.3799999952316284
[2025-05-07 18:59:02,894][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 18:59:02,894][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 18:59:02,894][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 18:59:02,894][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 18:59:02,894][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 18:59:10,569][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 18:59:10,571][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 18:59:10,571][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-07 18:59:10,571][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 18:59:10,574][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 18:59:10,574][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 18:59:10,574][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 18:59:10,574][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 18:59:10,574][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 18:59:10,575][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 18:59:10,575][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.3080Epoch 1/15: [                              ] 2/75 batches, loss: 0.3692Epoch 1/15: [=                             ] 3/75 batches, loss: 0.2986Epoch 1/15: [=                             ] 4/75 batches, loss: 0.3316Epoch 1/15: [==                            ] 5/75 batches, loss: 0.3546Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3543Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3534Epoch 1/15: [===                           ] 8/75 batches, loss: 0.3564Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3742Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3704Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3579Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3663Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3556Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3709Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3608Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3616Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3709Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3668Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3602Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3595Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3547Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3630Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3537Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3491Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3421Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3373Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3375Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3337Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3296Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3277Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3231Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3199Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3154Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3124Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3090Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3108Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3077Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3024Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3037Epoch 1/15: [================              ] 40/75 batches, loss: 0.3001Epoch 1/15: [================              ] 41/75 batches, loss: 0.2966Epoch 1/15: [================              ] 42/75 batches, loss: 0.2934Epoch 1/15: [=================             ] 43/75 batches, loss: 0.2901Epoch 1/15: [=================             ] 44/75 batches, loss: 0.2883Epoch 1/15: [==================            ] 45/75 batches, loss: 0.2883Epoch 1/15: [==================            ] 46/75 batches, loss: 0.2839Epoch 1/15: [==================            ] 47/75 batches, loss: 0.2815Epoch 1/15: [===================           ] 48/75 batches, loss: 0.2806Epoch 1/15: [===================           ] 49/75 batches, loss: 0.2768Epoch 1/15: [====================          ] 50/75 batches, loss: 0.2746Epoch 1/15: [====================          ] 51/75 batches, loss: 0.2730Epoch 1/15: [====================          ] 52/75 batches, loss: 0.2701Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.2698Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.2693Epoch 1/15: [======================        ] 55/75 batches, loss: 0.2677Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2655Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2624Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2637Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2615Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2593Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2571Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2553Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2545Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2531Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2514Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2489Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2467Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2451Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2450Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2446Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2434Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2450Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2427Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2422Epoch 1/15: [==============================] 75/75 batches, loss: 0.2403
[2025-05-07 18:59:18,228][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2403
[2025-05-07 18:59:18,599][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0319, Metrics: {'mse': 0.032094549387693405, 'rmse': 0.17914951685029298, 'r2': -0.8002573251724243}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.3107Epoch 2/15: [                              ] 2/75 batches, loss: 0.2262Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1774Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1579Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1572Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1414Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1498Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1468Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1523Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1619Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1632Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1587Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1556Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1557Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1495Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1489Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1444Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1475Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1450Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1416Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1396Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1367Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1362Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1344Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1333Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1354Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1370Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1360Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1351Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1366Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1364Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1358Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1346Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1326Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1338Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1321Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1337Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1334Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1328Epoch 2/15: [================              ] 40/75 batches, loss: 0.1326Epoch 2/15: [================              ] 41/75 batches, loss: 0.1313Epoch 2/15: [================              ] 42/75 batches, loss: 0.1300Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1311Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1309Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1298Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1293Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1284Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1300Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1297Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1291Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1291Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1279Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1274Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1259Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1251Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1268Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1258Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1248Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1237Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1224Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1218Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1222Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1217Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1208Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1195Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1194Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1189Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1188Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1182Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1182Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1173Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1168Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1161Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1157Epoch 2/15: [==============================] 75/75 batches, loss: 0.1159
[2025-05-07 18:59:21,603][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1159
[2025-05-07 18:59:22,140][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0386, Metrics: {'mse': 0.03847459703683853, 'rmse': 0.19614942527786955, 'r2': -1.1581292152404785}
[2025-05-07 18:59:22,141][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1265Epoch 3/15: [                              ] 2/75 batches, loss: 0.1070Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0964Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0818Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0835Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0922Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0937Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0897Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0902Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0874Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0887Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0847Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0806Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0800Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0775Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0758Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0735Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0746Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0757Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0762Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0767Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0774Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0781Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0773Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0768Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0767Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0770Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0785Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0788Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0770Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0772Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0824Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0827Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0848Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0835Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0820Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0818Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0812Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0803Epoch 3/15: [================              ] 40/75 batches, loss: 0.0810Epoch 3/15: [================              ] 41/75 batches, loss: 0.0815Epoch 3/15: [================              ] 42/75 batches, loss: 0.0816Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0806Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0803Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0813Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0818Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0813Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0809Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0818Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0813Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0815Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0814Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0809Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0803Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0795Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0794Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0801Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0804Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0817Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0810Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0811Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0807Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0807Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0797Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0796Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0790Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0784Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0792Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0795Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0794Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0791Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0788Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0793Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0789Epoch 3/15: [==============================] 75/75 batches, loss: 0.0785
[2025-05-07 18:59:24,737][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0785
[2025-05-07 18:59:25,047][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0450, Metrics: {'mse': 0.04497002437710762, 'rmse': 0.2120613693653505, 'r2': -1.522472620010376}
[2025-05-07 18:59:25,048][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0640Epoch 4/15: [                              ] 2/75 batches, loss: 0.0747Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0726Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0640Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0570Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0549Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0516Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0497Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0582Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0611Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0644Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0644Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0626Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0619Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0607Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0622Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0622Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0629Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0633Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0637Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0635Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0629Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0623Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0623Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0637Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0621Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0650Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0648Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0643Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0638Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0639Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0639Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0641Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0636Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0638Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0642Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0634Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0633Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0630Epoch 4/15: [================              ] 40/75 batches, loss: 0.0628Epoch 4/15: [================              ] 41/75 batches, loss: 0.0632Epoch 4/15: [================              ] 42/75 batches, loss: 0.0630Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0631Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0631Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0634Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0638Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0634Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0628Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0631Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0629Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0623Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0619Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0623Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0622Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0615Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0614Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0613Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0610Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0615Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0620Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0627Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0629Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0626Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0624Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0622Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0625Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0629Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0630Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0627Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0626Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0622Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0620Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0618Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0619Epoch 4/15: [==============================] 75/75 batches, loss: 0.0625
[2025-05-07 18:59:27,455][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0625
[2025-05-07 18:59:27,831][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0362, Metrics: {'mse': 0.03637167811393738, 'rmse': 0.19071360233066068, 'r2': -1.0401716232299805}
[2025-05-07 18:59:27,832][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0860Epoch 5/15: [                              ] 2/75 batches, loss: 0.0795Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0662Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0679Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0596Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0723Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0705Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0676Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0646Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0623Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0625Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0640Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0632Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0623Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0648Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0637Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0625Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0616Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0608Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0610Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0591Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0587Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0582Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0575Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0572Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0558Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0555Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0543Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0539Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0546Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0544Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0546Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0541Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0541Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0543Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0539Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0535Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0531Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0531Epoch 5/15: [================              ] 40/75 batches, loss: 0.0532Epoch 5/15: [================              ] 41/75 batches, loss: 0.0537Epoch 5/15: [================              ] 42/75 batches, loss: 0.0535Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0530Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0538Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0531Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0530Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0525Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0520Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0517Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0514Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0510Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0509Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0511Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0509Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0505Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0504Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0504Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0504Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0503Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0503Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0506Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0506Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0506Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0503Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0511Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0512Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0512Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0509Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0507Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0505Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0504Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0503Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0498Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0496Epoch 5/15: [==============================] 75/75 batches, loss: 0.0497
[2025-05-07 18:59:30,253][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0497
[2025-05-07 18:59:30,708][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0254, Metrics: {'mse': 0.025619709864258766, 'rmse': 0.1600615814749397, 'r2': -0.4370687007904053}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0577Epoch 6/15: [                              ] 2/75 batches, loss: 0.0421Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0373Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0358Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0399Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0423Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0506Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0498Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0480Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0478Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0476Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0498Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0487Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0481Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0467Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0480Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0472Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0459Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0449Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0442Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0443Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0445Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0434Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0436Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0443Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0444Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0442Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0439Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0429Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0428Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0422Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0418Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0420Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0416Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0409Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0408Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0413Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0411Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0412Epoch 6/15: [================              ] 40/75 batches, loss: 0.0413Epoch 6/15: [================              ] 41/75 batches, loss: 0.0416Epoch 6/15: [================              ] 42/75 batches, loss: 0.0415Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0409Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0409Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0405Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0406Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0405Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0403Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0407Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0406Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0408Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0407Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0405Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0402Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0401Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0401Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0397Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0399Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0399Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0396Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0393Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0390Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0388Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0388Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0386Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0384Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0382Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0381Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0378Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0378Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0377Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0374Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0372Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0373Epoch 6/15: [==============================] 75/75 batches, loss: 0.0380
[2025-05-07 18:59:33,498][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0380
[2025-05-07 18:59:33,766][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0229, Metrics: {'mse': 0.023198043927550316, 'rmse': 0.1523090408595311, 'r2': -0.30123186111450195}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0888Epoch 7/15: [                              ] 2/75 batches, loss: 0.0696Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0598Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0501Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0451Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0420Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0407Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0422Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0399Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0375Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0367Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0350Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0359Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0352Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0343Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0339Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0342Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0335Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0357Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0353Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0351Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0344Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0344Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0338Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0336Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0340Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0340Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0339Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0350Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0364Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0363Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0355Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0352Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0358Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0356Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0352Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0350Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0349Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0359Epoch 7/15: [================              ] 40/75 batches, loss: 0.0359Epoch 7/15: [================              ] 41/75 batches, loss: 0.0358Epoch 7/15: [================              ] 42/75 batches, loss: 0.0361Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0360Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0357Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0359Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0357Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0360Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0358Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0358Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0354Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0353Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0349Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0352Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0350Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0348Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0345Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0345Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0345Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0344Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0343Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0342Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0341Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0341Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0340Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0339Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0339Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0338Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0341Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0340Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0340Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0338Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0337Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0337Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0338Epoch 7/15: [==============================] 75/75 batches, loss: 0.0340
[2025-05-07 18:59:36,634][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0340
[2025-05-07 18:59:36,947][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0283, Metrics: {'mse': 0.02849888801574707, 'rmse': 0.16881613671609438, 'r2': -0.5985684394836426}
[2025-05-07 18:59:36,947][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0327Epoch 8/15: [                              ] 2/75 batches, loss: 0.0406Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0323Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0274Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0321Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0300Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0341Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0337Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0329Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0316Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0310Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0303Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0293Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0296Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0315Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0303Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0323Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0321Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0323Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0331Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0325Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0324Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0332Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0329Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0333Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0326Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0336Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0334Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0329Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0328Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0333Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0341Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0360Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0358Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0357Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0351Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0346Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0344Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0342Epoch 8/15: [================              ] 40/75 batches, loss: 0.0343Epoch 8/15: [================              ] 41/75 batches, loss: 0.0340Epoch 8/15: [================              ] 42/75 batches, loss: 0.0343Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0342Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0341Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0340Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0340Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0336Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0335Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0332Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0336Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0334Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0331Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0333Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0332Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0330Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0328Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0330Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0327Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0326Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0325Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0327Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0324Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0325Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0324Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0323Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0322Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0323Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0321Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0320Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0320Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0320Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0321Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0319Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0320Epoch 8/15: [==============================] 75/75 batches, loss: 0.0319
[2025-05-07 18:59:39,368][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0319
[2025-05-07 18:59:39,638][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0275, Metrics: {'mse': 0.027729550376534462, 'rmse': 0.16652192160954205, 'r2': -0.5554145574569702}
[2025-05-07 18:59:39,639][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0184Epoch 9/15: [                              ] 2/75 batches, loss: 0.0305Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0301Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0298Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0286Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0286Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0290Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0289Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0289Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0281Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0282Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0276Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0267Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0275Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0273Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0279Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0287Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0294Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0305Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0305Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0306Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0311Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0317Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0316Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0316Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0316Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0311Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0306Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0305Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0301Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0301Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0306Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0304Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0302Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0309Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0303Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0302Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0305Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0305Epoch 9/15: [================              ] 40/75 batches, loss: 0.0302Epoch 9/15: [================              ] 41/75 batches, loss: 0.0302Epoch 9/15: [================              ] 42/75 batches, loss: 0.0300Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0302Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0303Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0298Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0297Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0294Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0294Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0293Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0290Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0292Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0289Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0285Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0286Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0284Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0289Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0287Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0285Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0284Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0283Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0282Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0280Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0280Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0279Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0277Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0277Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0278Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0278Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0279Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0279Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0279Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0277Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0278Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0279Epoch 9/15: [==============================] 75/75 batches, loss: 0.0277
[2025-05-07 18:59:42,032][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0277
[2025-05-07 18:59:42,331][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0244, Metrics: {'mse': 0.02468043565750122, 'rmse': 0.1571000816597535, 'r2': -0.38438260555267334}
[2025-05-07 18:59:42,332][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0456Epoch 10/15: [                              ] 2/75 batches, loss: 0.0435Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0396Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0394Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0383Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0358Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0337Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0323Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0310Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0304Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0289Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0295Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0290Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0292Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0287Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0284Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0289Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0291Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0291Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0283Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0282Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0282Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0279Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0279Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0281Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0278Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0274Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0270Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0278Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0277Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0275Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0271Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0268Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0266Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0278Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0278Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0276Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0272Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0269Epoch 10/15: [================              ] 40/75 batches, loss: 0.0270Epoch 10/15: [================              ] 41/75 batches, loss: 0.0268Epoch 10/15: [================              ] 42/75 batches, loss: 0.0265Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0263Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0263Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0263Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0261Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0262Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0263Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0260Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0259Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0259Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0263Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0261Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0260Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0262Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0259Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0261Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0260Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0259Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0257Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0257Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0259Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0260Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0258Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0255Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0254Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0252Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0252Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0256Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0256Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0255Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0258Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0257Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0259Epoch 10/15: [==============================] 75/75 batches, loss: 0.0260
[2025-05-07 18:59:44,722][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0260
[2025-05-07 18:59:44,936][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0208, Metrics: {'mse': 0.021112624555826187, 'rmse': 0.14530183947846698, 'r2': -0.18425583839416504}
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0133Epoch 11/15: [                              ] 2/75 batches, loss: 0.0168Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0224Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0193Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0182Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0188Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0291Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0271Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0273Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0277Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0264Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0261Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0269Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0268Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0261Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0251Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0251Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0261Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0260Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0255Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0261Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0256Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0252Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0250Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0246Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0249Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0246Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0254Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0254Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0252Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0248Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0247Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0245Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0244Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0242Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0242Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0238Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0235Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0235Epoch 11/15: [================              ] 40/75 batches, loss: 0.0231Epoch 11/15: [================              ] 41/75 batches, loss: 0.0232Epoch 11/15: [================              ] 42/75 batches, loss: 0.0233Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0231Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0229Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0229Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0227Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0227Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0226Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0227Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0225Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0224Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0224Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0221Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0224Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0226Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0224Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0222Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0221Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0219Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0218Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0218Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0218Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0218Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0217Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0216Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0220Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0221Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0221Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0222Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0224Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0222Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0223Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0224Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0228Epoch 11/15: [==============================] 75/75 batches, loss: 0.0227
[2025-05-07 18:59:47,682][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0227
[2025-05-07 18:59:47,982][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0205, Metrics: {'mse': 0.02083647809922695, 'rmse': 0.1443484606749478, 'r2': -0.16876614093780518}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0255Epoch 12/15: [                              ] 2/75 batches, loss: 0.0288Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0293Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0265Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0266Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0255Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0241Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0225Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0219Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0209Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0202Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0195Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0199Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0196Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0187Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0193Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0193Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0194Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0192Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0193Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0195Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0195Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0193Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0193Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0194Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0196Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0194Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0202Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0210Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0212Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0212Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0216Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0223Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0222Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0218Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0218Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0217Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0218Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0215Epoch 12/15: [================              ] 40/75 batches, loss: 0.0215Epoch 12/15: [================              ] 41/75 batches, loss: 0.0215Epoch 12/15: [================              ] 42/75 batches, loss: 0.0215Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0215Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0213Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0213Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0212Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0210Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0209Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0208Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0206Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0213Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0212Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0211Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0211Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0213Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0212Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0212Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0210Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0212Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0213Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0213Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0212Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0212Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0211Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0210Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0213Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0213Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0212Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0211Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0211Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0209Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0209Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0208Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0206Epoch 12/15: [==============================] 75/75 batches, loss: 0.0205
[2025-05-07 18:59:50,713][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0205
[2025-05-07 18:59:50,990][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0285, Metrics: {'mse': 0.028779173269867897, 'rmse': 0.16964425504527966, 'r2': -0.6142902374267578}
[2025-05-07 18:59:50,991][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0684Epoch 13/15: [                              ] 2/75 batches, loss: 0.0422Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0339Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0292Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0285Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0266Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0249Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0246Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0242Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0230Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0227Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0220Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0221Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0221Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0215Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0209Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0205Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0207Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0210Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0207Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0208Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0212Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0214Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0215Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0214Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0219Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0213Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0211Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0209Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0211Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0210Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0210Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0208Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0209Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0213Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0214Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0214Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0214Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0213Epoch 13/15: [================              ] 40/75 batches, loss: 0.0213Epoch 13/15: [================              ] 41/75 batches, loss: 0.0211Epoch 13/15: [================              ] 42/75 batches, loss: 0.0210Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0209Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0208Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0209Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0208Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0205Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0205Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0204Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0204Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0203Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0203Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0201Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0201Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0202Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0200Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0200Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0198Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0198Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0198Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0196Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0198Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0198Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0201Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0200Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0199Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0199Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0200Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0200Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0201Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0199Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0200Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0200Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0201Epoch 13/15: [==============================] 75/75 batches, loss: 0.0204
[2025-05-07 18:59:53,424][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0204
[2025-05-07 18:59:54,025][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0266, Metrics: {'mse': 0.026936125010252, 'rmse': 0.16412228675671076, 'r2': -0.5109094381332397}
[2025-05-07 18:59:54,025][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0266Epoch 14/15: [                              ] 2/75 batches, loss: 0.0230Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0188Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0183Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0226Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0221Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0219Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0231Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0225Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0222Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0216Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0213Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0227Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0237Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0230Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0231Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0227Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0222Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0214Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0215Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0216Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0212Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0210Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0208Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0207Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0205Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0205Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0211Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0208Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0206Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0205Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0203Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0201Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0201Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0205Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0202Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0202Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0203Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0202Epoch 14/15: [================              ] 40/75 batches, loss: 0.0202Epoch 14/15: [================              ] 41/75 batches, loss: 0.0199Epoch 14/15: [================              ] 42/75 batches, loss: 0.0202Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0205Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0204Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0203Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0202Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0213Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0212Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0211Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0214Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0213Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0211Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0209Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0208Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0208Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0208Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0208Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0209Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0210Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0209Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0207Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0208Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0210Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0211Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0208Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0206Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0205Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0205Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0204Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0204Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0202Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0201Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0200Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0200Epoch 14/15: [==============================] 75/75 batches, loss: 0.0201
[2025-05-07 18:59:56,657][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0201
[2025-05-07 18:59:57,457][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0241, Metrics: {'mse': 0.024420030415058136, 'rmse': 0.15626909616126325, 'r2': -0.3697758913040161}
[2025-05-07 18:59:57,457][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0054Epoch 15/15: [                              ] 2/75 batches, loss: 0.0098Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0136Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0132Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0160Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0151Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0167Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0160Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0154Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0150Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0159Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0155Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0161Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0155Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0152Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0156Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0159Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0156Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0158Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0167Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0168Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0165Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0168Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0163Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0165Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0161Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0161Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0159Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0161Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0160Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0159Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0161Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0167Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0166Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0167Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0168Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0167Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0168Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0171Epoch 15/15: [================              ] 40/75 batches, loss: 0.0172Epoch 15/15: [================              ] 41/75 batches, loss: 0.0170Epoch 15/15: [================              ] 42/75 batches, loss: 0.0173Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0174Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0175Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0174Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0175Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0178Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0176Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0176Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0178Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0176Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0177Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0181Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0182Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0182Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0181Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0182Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0183Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0184Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0185Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0184Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0183Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0183Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0183Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0184Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0183Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0182Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0183Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0183Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0183Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0183Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0183Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0182Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0181Epoch 15/15: [==============================] 75/75 batches, loss: 0.0179
[2025-05-07 19:00:00,126][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0179
[2025-05-07 19:00:00,480][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0263, Metrics: {'mse': 0.026581790298223495, 'rmse': 0.1630392293229562, 'r2': -0.49103403091430664}
[2025-05-07 19:00:00,481][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 19:00:00,481][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 15
[2025-05-07 19:00:00,481][src.training.lm_trainer][INFO] - Training completed in 45.29 seconds
[2025-05-07 19:00:00,481][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 19:00:03,672][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.008158918470144272, 'rmse': 0.09032673175834645, 'r2': 0.12285882234573364}
[2025-05-07 19:00:03,672][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.02083647809922695, 'rmse': 0.1443484606749478, 'r2': -0.16876614093780518}
[2025-05-07 19:00:03,672][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.11350969970226288, 'rmse': 0.3369120058743275, 'r2': -1.669618844985962}
[2025-05-07 19:00:06,285][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer6/ja/ja/model.pt
[2025-05-07 19:00:06,286][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁▁
wandb:     best_val_mse █▄▂▁▁
wandb:      best_val_r2 ▁▅▇██
wandb:    best_val_rmse █▄▃▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▄▃▁▃▆▆▅▅▆▇▇▅▆▆
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▄▆█▆▂▂▃▃▂▁▁▃▃▂▃
wandb:          val_mse ▄▆█▆▂▂▃▃▂▁▁▃▃▂▃
wandb:           val_r2 ▅▃▁▃▇▇▆▆▇██▆▆▇▆
wandb:         val_rmse ▅▆█▆▃▂▄▃▂▁▁▄▃▂▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02051
wandb:     best_val_mse 0.02084
wandb:      best_val_r2 -0.16877
wandb:    best_val_rmse 0.14435
wandb: early_stop_epoch 15
wandb:            epoch 15
wandb:   final_test_mse 0.11351
wandb:    final_test_r2 -1.66962
wandb:  final_test_rmse 0.33691
wandb:  final_train_mse 0.00816
wandb:   final_train_r2 0.12286
wandb: final_train_rmse 0.09033
wandb:    final_val_mse 0.02084
wandb:     final_val_r2 -0.16877
wandb:   final_val_rmse 0.14435
wandb:    learning_rate 0.0001
wandb:       train_loss 0.01788
wandb:       train_time 45.28766
wandb:         val_loss 0.02625
wandb:          val_mse 0.02658
wandb:           val_r2 -0.49103
wandb:         val_rmse 0.16304
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_185855-wrkn8a6d
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_185855-wrkn8a6d/logs
Experiment probe_layer6_avg_links_len_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer6/ja/ja/results.json for layer 6
Running control submetric probing experiments...
=======================
PROBING LAYER 2 (SUBMETRIC CONTROL EXPERIMENTS)
=======================
Running experiment: probe_layer2_avg_links_len_control1_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_control1_ru"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/ru"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 19:00:43,134][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/ru
experiment_name: probe_layer2_avg_links_len_control1_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 19:00:43,134][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 19:00:43,134][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 19:00:43,135][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 19:00:43,135][__main__][INFO] - Determined Task Type: regression
[2025-05-07 19:00:43,139][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-05-07 19:00:43,139][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 19:00:43,139][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 19:00:47,529][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 19:00:49,838][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 19:00:49,839][src.data.datasets][INFO] - Loading 'control_avg_links_len_seed1' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 19:00:50,149][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_links_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:14:19 2025).
[2025-05-07 19:00:50,247][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_links_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:14:19 2025).
[2025-05-07 19:00:50,546][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-07 19:00:50,561][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 19:00:50,564][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-07 19:00:50,566][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 19:00:50,650][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:00:50,771][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:00:50,816][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-07 19:00:50,817][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 19:00:50,817][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-07 19:00:50,820][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 19:00:50,905][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:00:51,016][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:00:51,046][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-07 19:00:51,047][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 19:00:51,047][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-07 19:00:51,050][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-07 19:00:51,050][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 19:00:51,050][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 19:00:51,050][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 19:00:51,050][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 19:00:51,051][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9000
[2025-05-07 19:00:51,051][src.data.datasets][INFO] -   Mean: 0.2497, Std: 0.1826
[2025-05-07 19:00:51,051][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-07 19:00:51,051][src.data.datasets][INFO] - Sample label: 0.5139999985694885
[2025-05-07 19:00:51,051][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 19:00:51,051][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 19:00:51,051][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 19:00:51,051][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 19:00:51,052][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8000
[2025-05-07 19:00:51,052][src.data.datasets][INFO] -   Mean: 0.2557, Std: 0.1728
[2025-05-07 19:00:51,052][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-07 19:00:51,052][src.data.datasets][INFO] - Sample label: 0.23399999737739563
[2025-05-07 19:00:51,052][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 19:00:51,052][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 19:00:51,052][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 19:00:51,052][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 19:00:51,052][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6270
[2025-05-07 19:00:51,052][src.data.datasets][INFO] -   Mean: 0.2617, Std: 0.1298
[2025-05-07 19:00:51,053][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-07 19:00:51,053][src.data.datasets][INFO] - Sample label: 0.14000000059604645
[2025-05-07 19:00:51,053][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-07 19:00:51,053][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 19:00:51,053][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 19:00:51,053][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 19:00:51,054][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 19:00:58,145][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 19:00:58,146][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 19:00:58,146][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 19:00:58,147][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 19:00:58,150][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 19:00:58,150][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 19:00:58,151][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 19:00:58,151][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 19:00:58,151][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-07 19:00:58,152][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 19:00:58,152][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.2565Epoch 1/15: [                              ] 2/75 batches, loss: 0.4971Epoch 1/15: [=                             ] 3/75 batches, loss: 0.3957Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4080Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4175Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3912Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4476Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4367Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4710Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4615Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4340Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4316Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4130Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4081Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3974Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3959Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3814Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3797Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3805Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3753Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3726Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3731Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3681Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3660Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3585Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3534Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3583Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3520Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3550Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3532Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3535Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3475Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3448Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3463Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3455Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3507Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3448Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3457Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3431Epoch 1/15: [================              ] 40/75 batches, loss: 0.3396Epoch 1/15: [================              ] 41/75 batches, loss: 0.3365Epoch 1/15: [================              ] 42/75 batches, loss: 0.3355Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3338Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3307Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3291Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3242Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3246Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3230Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3194Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3177Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3141Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3115Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3087Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3073Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3061Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3056Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3069Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3051Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3031Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3005Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2978Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2962Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2939Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2917Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2911Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2892Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2868Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2843Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2845Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2846Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2824Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2833Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2815Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2796Epoch 1/15: [==============================] 75/75 batches, loss: 0.2780
[2025-05-07 19:01:04,972][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2780
[2025-05-07 19:01:05,264][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0540, Metrics: {'mse': 0.05658837780356407, 'rmse': 0.23788311794569214, 'r2': -0.8960814476013184}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1115Epoch 2/15: [                              ] 2/75 batches, loss: 0.1754Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1439Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1986Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1843Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1770Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1787Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1808Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1735Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1764Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1695Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1734Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1769Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1684Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1646Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1638Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1594Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1647Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1714Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1714Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1706Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1683Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1691Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1704Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1690Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1702Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1694Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1699Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1736Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1707Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1697Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1679Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1689Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1695Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1705Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1681Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1666Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1675Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1697Epoch 2/15: [================              ] 40/75 batches, loss: 0.1682Epoch 2/15: [================              ] 41/75 batches, loss: 0.1670Epoch 2/15: [================              ] 42/75 batches, loss: 0.1672Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1663Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1637Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1638Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1630Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1612Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1617Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1606Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1595Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1598Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1590Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1595Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1585Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1580Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1572Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1569Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1580Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1580Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1563Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1559Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1557Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1548Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1542Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1542Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1533Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1531Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1530Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1520Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1508Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1495Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1485Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1478Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1470Epoch 2/15: [==============================] 75/75 batches, loss: 0.1464
[2025-05-07 19:01:08,004][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1464
[2025-05-07 19:01:08,356][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0532, Metrics: {'mse': 0.05738629400730133, 'rmse': 0.23955436545239858, 'r2': -0.9228167533874512}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1257Epoch 3/15: [                              ] 2/75 batches, loss: 0.1567Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1640Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1395Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1219Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1217Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1222Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1180Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1140Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1135Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1157Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1111Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1167Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1126Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1115Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1069Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1059Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1060Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1032Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1003Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0999Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0989Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0988Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0997Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0981Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0975Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0987Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0972Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0972Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0973Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0982Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0983Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0977Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0978Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0975Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0978Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0969Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0967Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0970Epoch 3/15: [================              ] 40/75 batches, loss: 0.0963Epoch 3/15: [================              ] 41/75 batches, loss: 0.0984Epoch 3/15: [================              ] 42/75 batches, loss: 0.0994Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0993Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1000Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0996Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1005Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1003Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1011Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1016Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1009Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1009Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1001Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0993Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0989Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0987Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0988Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0981Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0988Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0976Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0970Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0962Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0968Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0976Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0983Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0977Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0973Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0967Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0966Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0972Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0970Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0972Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0973Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0976Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0970Epoch 3/15: [==============================] 75/75 batches, loss: 0.0984
[2025-05-07 19:01:11,160][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0984
[2025-05-07 19:01:11,494][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0362, Metrics: {'mse': 0.037883345037698746, 'rmse': 0.19463644324149254, 'r2': -0.26934027671813965}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0822Epoch 4/15: [                              ] 2/75 batches, loss: 0.0968Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0908Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0837Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0829Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0863Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0875Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0855Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0904Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0919Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0927Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0978Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0961Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0978Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0951Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0957Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0941Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0930Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0939Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0928Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0930Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0939Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0931Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0922Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0939Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0927Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0926Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0927Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0931Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0919Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0925Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0907Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0902Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0890Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0896Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0892Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0877Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0905Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0905Epoch 4/15: [================              ] 40/75 batches, loss: 0.0900Epoch 4/15: [================              ] 41/75 batches, loss: 0.0900Epoch 4/15: [================              ] 42/75 batches, loss: 0.0900Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0908Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0906Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0904Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0903Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0898Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0894Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0890Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0886Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0903Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0908Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0896Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0896Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0893Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0890Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0883Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0889Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0899Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0900Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0900Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0890Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0890Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0887Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0885Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0892Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0891Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0890Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0895Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0888Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0883Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0879Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0877Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0880Epoch 4/15: [==============================] 75/75 batches, loss: 0.0879
[2025-05-07 19:01:14,158][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0879
[2025-05-07 19:01:14,552][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0348, Metrics: {'mse': 0.03634073957800865, 'rmse': 0.19063247251716758, 'r2': -0.21765291690826416}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1048Epoch 5/15: [                              ] 2/75 batches, loss: 0.1061Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0943Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1071Epoch 5/15: [==                            ] 5/75 batches, loss: 0.1019Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0990Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0993Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0936Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0863Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0839Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0821Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0769Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0746Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0754Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0756Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0751Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0770Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0807Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0814Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0810Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0799Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0784Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0791Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0778Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0764Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0762Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0754Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0789Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0785Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0771Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0767Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0768Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0762Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0767Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0767Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0759Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0755Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0745Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0742Epoch 5/15: [================              ] 40/75 batches, loss: 0.0733Epoch 5/15: [================              ] 41/75 batches, loss: 0.0736Epoch 5/15: [================              ] 42/75 batches, loss: 0.0748Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0745Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0751Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0752Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0763Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0762Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0763Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0769Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0774Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0770Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0766Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0768Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0774Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0777Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0778Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0785Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0776Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0774Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0769Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0766Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0763Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0757Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0751Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0746Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0741Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0738Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0731Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0732Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0729Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0731Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0734Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0734Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0729Epoch 5/15: [==============================] 75/75 batches, loss: 0.0732
[2025-05-07 19:01:17,437][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0732
[2025-05-07 19:01:17,822][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0371, Metrics: {'mse': 0.0396556630730629, 'rmse': 0.19913729704167146, 'r2': -0.3287245035171509}
[2025-05-07 19:01:17,823][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0808Epoch 6/15: [                              ] 2/75 batches, loss: 0.0743Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0769Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0748Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0784Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0828Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0782Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0782Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0779Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0745Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0737Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0753Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0746Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0737Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0712Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0718Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0702Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0734Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0731Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0713Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0709Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0714Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0723Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0726Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0734Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0730Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0728Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0716Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0702Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0702Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0698Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0687Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0682Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0681Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0681Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0678Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0665Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0663Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0663Epoch 6/15: [================              ] 40/75 batches, loss: 0.0670Epoch 6/15: [================              ] 41/75 batches, loss: 0.0665Epoch 6/15: [================              ] 42/75 batches, loss: 0.0670Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0669Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0682Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0700Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0691Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0695Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0693Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0695Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0689Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0686Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0679Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0682Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0676Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0673Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0680Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0678Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0680Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0680Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0680Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0683Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0678Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0675Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0681Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0675Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0681Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0680Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0678Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0677Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0676Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0672Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0667Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0666Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0663Epoch 6/15: [==============================] 75/75 batches, loss: 0.0656
[2025-05-07 19:01:20,416][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0656
[2025-05-07 19:01:20,794][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0342, Metrics: {'mse': 0.036049824208021164, 'rmse': 0.18986791252873975, 'r2': -0.20790529251098633}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0538Epoch 7/15: [                              ] 2/75 batches, loss: 0.0662Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0592Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0518Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0491Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0497Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0542Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0548Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0554Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0595Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0569Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0576Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0589Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0600Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0581Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0564Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0581Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0595Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0582Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0593Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0602Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0606Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0590Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0588Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0587Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0589Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0589Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0591Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0590Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0595Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0586Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0586Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0577Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0579Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0591Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0584Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0583Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0583Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0589Epoch 7/15: [================              ] 40/75 batches, loss: 0.0594Epoch 7/15: [================              ] 41/75 batches, loss: 0.0588Epoch 7/15: [================              ] 42/75 batches, loss: 0.0590Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0594Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0592Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0591Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0589Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0594Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0590Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0590Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0584Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0580Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0576Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0571Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0575Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0574Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0571Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0572Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0573Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0570Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0573Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0576Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0573Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0574Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0574Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0573Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0573Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0572Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0568Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0574Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0571Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0574Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0573Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0571Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0572Epoch 7/15: [==============================] 75/75 batches, loss: 0.0571
[2025-05-07 19:01:23,753][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0571
[2025-05-07 19:01:24,167][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0394, Metrics: {'mse': 0.04232867807149887, 'rmse': 0.20573934497683924, 'r2': -0.418287992477417}
[2025-05-07 19:01:24,168][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0706Epoch 8/15: [                              ] 2/75 batches, loss: 0.0577Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0533Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0504Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0488Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0486Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0535Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0524Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0571Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0586Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0569Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0602Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0634Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0613Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0595Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0575Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0568Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0584Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0584Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0578Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0564Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0562Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0557Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0543Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0544Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0539Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0537Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0550Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0569Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0576Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0574Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0567Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0564Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0559Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0565Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0570Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0573Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0573Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0572Epoch 8/15: [================              ] 40/75 batches, loss: 0.0574Epoch 8/15: [================              ] 41/75 batches, loss: 0.0574Epoch 8/15: [================              ] 42/75 batches, loss: 0.0575Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0573Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0568Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0568Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0564Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0557Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0556Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0556Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0552Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0551Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0554Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0551Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0547Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0544Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0548Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0552Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0551Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0549Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0548Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0546Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0545Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0547Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0549Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0549Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0547Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0547Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0551Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0556Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0559Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0557Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0558Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0559Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0561Epoch 8/15: [==============================] 75/75 batches, loss: 0.0559
[2025-05-07 19:01:26,591][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0559
[2025-05-07 19:01:26,957][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0328, Metrics: {'mse': 0.03469889983534813, 'rmse': 0.1862764070819172, 'r2': -0.16264045238494873}
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0577Epoch 9/15: [                              ] 2/75 batches, loss: 0.0444Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0377Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0378Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0373Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0450Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0443Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0495Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0490Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0486Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0472Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0477Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0463Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0474Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0493Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0491Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0500Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0511Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0540Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0527Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0514Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0502Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0495Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0497Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0500Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0507Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0511Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0507Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0516Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0526Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0517Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0513Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0509Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0515Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0526Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0524Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0532Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0530Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0542Epoch 9/15: [================              ] 40/75 batches, loss: 0.0551Epoch 9/15: [================              ] 41/75 batches, loss: 0.0550Epoch 9/15: [================              ] 42/75 batches, loss: 0.0556Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0562Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0569Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0563Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0560Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0563Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0559Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0556Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0558Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0557Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0553Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0552Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0553Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0555Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0558Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0554Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0552Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0555Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0551Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0553Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0552Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0553Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0552Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0549Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0546Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0542Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0537Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0533Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0532Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0534Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0535Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0538Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0541Epoch 9/15: [==============================] 75/75 batches, loss: 0.0540
[2025-05-07 19:01:29,725][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0540
[2025-05-07 19:01:30,111][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0336, Metrics: {'mse': 0.03564129397273064, 'rmse': 0.18878901973560494, 'r2': -0.19421684741973877}
[2025-05-07 19:01:30,112][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0475Epoch 10/15: [                              ] 2/75 batches, loss: 0.0660Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0569Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0511Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0499Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0496Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0511Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0543Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0588Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0577Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0557Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0551Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0543Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0553Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0559Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0558Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0563Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0553Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0541Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0543Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0540Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0533Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0534Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0522Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0515Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0508Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0514Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0541Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0534Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0530Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0535Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0536Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0529Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0528Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0532Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0532Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0528Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0530Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0526Epoch 10/15: [================              ] 40/75 batches, loss: 0.0532Epoch 10/15: [================              ] 41/75 batches, loss: 0.0533Epoch 10/15: [================              ] 42/75 batches, loss: 0.0532Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0534Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0534Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0536Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0528Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0535Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0551Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0548Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0548Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0543Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0546Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0541Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0535Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0534Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0532Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0532Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0531Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0529Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0526Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0529Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0529Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0528Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0526Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0531Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0528Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0526Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0525Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0529Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0529Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0535Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0537Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0536Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0539Epoch 10/15: [==============================] 75/75 batches, loss: 0.0539
[2025-05-07 19:01:32,535][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0539
[2025-05-07 19:01:32,932][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0370, Metrics: {'mse': 0.0398666113615036, 'rmse': 0.19966624993098758, 'r2': -0.3357926607131958}
[2025-05-07 19:01:32,932][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0560Epoch 11/15: [                              ] 2/75 batches, loss: 0.0548Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0474Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0506Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0587Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0553Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0568Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0524Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0532Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0528Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0524Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0510Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0506Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0510Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0507Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0505Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0499Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0499Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0497Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0495Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0526Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0514Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0513Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0501Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0497Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0500Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0497Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0489Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0488Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0486Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0492Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0495Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0493Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0490Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0488Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0491Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0492Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0487Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0488Epoch 11/15: [================              ] 40/75 batches, loss: 0.0493Epoch 11/15: [================              ] 41/75 batches, loss: 0.0496Epoch 11/15: [================              ] 42/75 batches, loss: 0.0500Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0497Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0500Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0497Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0504Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0500Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0508Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0505Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0505Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0501Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0498Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0501Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0498Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0500Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0497Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0496Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0495Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0490Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0490Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0486Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0482Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0479Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0485Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0486Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0484Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0479Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0476Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0477Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0479Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0482Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0487Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0493Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0495Epoch 11/15: [==============================] 75/75 batches, loss: 0.0491
[2025-05-07 19:01:35,366][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0491
[2025-05-07 19:01:35,658][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0308, Metrics: {'mse': 0.03194235637784004, 'rmse': 0.17872424675415488, 'r2': -0.07027828693389893}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0616Epoch 12/15: [                              ] 2/75 batches, loss: 0.0455Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0476Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0415Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0409Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0385Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0363Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0379Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0381Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0409Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0413Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0418Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0431Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0431Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0422Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0406Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0429Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0442Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0440Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0455Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0446Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0438Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0450Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0448Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0444Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0446Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0445Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0450Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0450Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0456Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0449Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0455Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0452Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0452Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0455Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0458Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0459Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0466Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0465Epoch 12/15: [================              ] 40/75 batches, loss: 0.0466Epoch 12/15: [================              ] 41/75 batches, loss: 0.0474Epoch 12/15: [================              ] 42/75 batches, loss: 0.0480Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0488Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0485Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0483Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0488Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0492Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0493Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0499Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0500Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0502Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0508Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0507Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0501Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0500Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0503Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0500Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0496Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0492Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0494Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0493Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0490Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0488Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0486Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0488Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0489Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0496Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0501Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0502Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0501Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0497Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0496Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0496Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0496Epoch 12/15: [==============================] 75/75 batches, loss: 0.0495
[2025-05-07 19:01:38,363][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0495
[2025-05-07 19:01:38,649][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0307, Metrics: {'mse': 0.03091435320675373, 'rmse': 0.17582477984275632, 'r2': -0.03583335876464844}
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0539Epoch 13/15: [                              ] 2/75 batches, loss: 0.0572Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0594Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0563Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0592Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0523Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0531Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0518Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0526Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0521Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0502Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0498Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0526Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0519Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0524Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0529Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0520Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0511Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0514Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0506Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0500Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0495Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0497Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0500Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0488Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0488Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0478Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0478Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0477Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0477Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0469Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0464Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0455Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0460Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0459Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0453Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0448Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0465Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0467Epoch 13/15: [================              ] 40/75 batches, loss: 0.0462Epoch 13/15: [================              ] 41/75 batches, loss: 0.0457Epoch 13/15: [================              ] 42/75 batches, loss: 0.0454Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0451Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0454Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0463Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0474Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0471Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0474Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0480Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0475Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0477Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0479Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0477Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0473Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0469Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0470Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0468Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0469Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0472Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0473Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0473Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0473Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0474Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0474Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0471Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0468Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0473Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0472Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0474Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0474Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0476Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0475Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0475Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0472Epoch 13/15: [==============================] 75/75 batches, loss: 0.0471
[2025-05-07 19:01:41,492][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0471
[2025-05-07 19:01:41,882][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0323, Metrics: {'mse': 0.03393947705626488, 'rmse': 0.18422670017200243, 'r2': -0.13719487190246582}
[2025-05-07 19:01:41,883][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0468Epoch 14/15: [                              ] 2/75 batches, loss: 0.0407Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0414Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0467Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0507Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0494Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0487Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0473Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0431Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0427Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0452Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0443Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0430Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0435Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0435Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0441Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0433Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0440Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0438Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0433Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0431Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0426Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0416Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0426Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0426Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0422Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0427Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0426Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0433Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0435Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0435Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0432Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0435Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0443Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0440Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0449Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0444Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0441Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0443Epoch 14/15: [================              ] 40/75 batches, loss: 0.0443Epoch 14/15: [================              ] 41/75 batches, loss: 0.0441Epoch 14/15: [================              ] 42/75 batches, loss: 0.0441Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0437Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0438Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0438Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0440Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0435Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0431Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0427Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0424Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0427Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0429Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0424Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0423Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0418Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0416Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0416Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0422Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0419Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0420Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0419Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0419Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0418Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0419Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0420Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0419Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0425Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0428Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0424Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0425Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0428Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0426Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0426Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0423Epoch 14/15: [==============================] 75/75 batches, loss: 0.0425
[2025-05-07 19:01:44,264][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0425
[2025-05-07 19:01:44,611][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0300, Metrics: {'mse': 0.03050069697201252, 'rmse': 0.17464448737939747, 'r2': -0.021973133087158203}
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0259Epoch 15/15: [                              ] 2/75 batches, loss: 0.0346Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0361Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0390Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0401Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0377Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0387Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0373Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0382Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0396Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0384Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0443Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0468Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0463Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0459Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0463Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0470Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0468Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0480Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0473Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0464Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0474Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0467Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0466Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0471Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0463Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0456Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0450Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0448Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0441Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0434Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0436Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0430Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0424Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0422Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0426Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0429Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0444Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0442Epoch 15/15: [================              ] 40/75 batches, loss: 0.0439Epoch 15/15: [================              ] 41/75 batches, loss: 0.0442Epoch 15/15: [================              ] 42/75 batches, loss: 0.0438Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0438Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0435Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0435Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0433Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0430Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0430Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0429Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0430Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0430Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0429Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0427Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0433Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0432Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0428Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0430Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0430Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0429Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0429Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0429Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0427Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0428Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0428Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0430Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0428Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0428Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0429Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0427Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0431Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0431Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0429Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0427Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0427Epoch 15/15: [==============================] 75/75 batches, loss: 0.0426
[2025-05-07 19:01:47,362][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0426
[2025-05-07 19:01:47,774][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0306, Metrics: {'mse': 0.03182360157370567, 'rmse': 0.17839170825379097, 'r2': -0.0662992000579834}
[2025-05-07 19:01:47,775][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-07 19:01:47,775][src.training.lm_trainer][INFO] - Training completed in 45.90 seconds
[2025-05-07 19:01:47,775][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 19:01:51,256][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03325984627008438, 'rmse': 0.18237282218051126, 'r2': 0.002364933490753174}
[2025-05-07 19:01:51,256][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03050069697201252, 'rmse': 0.17464448737939747, 'r2': -0.021973133087158203}
[2025-05-07 19:01:51,256][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.017720989882946014, 'rmse': 0.1331202083943156, 'r2': -0.05200004577636719}
[2025-05-07 19:01:53,777][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/ru/ru/model.pt
[2025-05-07 19:01:53,778][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ██▃▂▂▂▁▁▁
wandb:     best_val_mse ██▃▃▂▂▁▁▁
wandb:      best_val_r2 ▁▁▆▆▇▇███
wandb:    best_val_rmse ██▃▃▃▂▁▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▁▅▅▅▅▄▅▅▅▆▆▆▆
wandb:       train_loss █▄▃▂▂▂▁▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ██▃▂▃▂▄▂▂▃▁▁▂▁▁
wandb:          val_mse ██▃▃▃▂▄▂▂▃▁▁▂▁▁
wandb:           val_r2 ▁▁▆▆▆▇▅▇▇▆██▇██
wandb:         val_rmse ██▃▃▄▃▄▂▃▄▁▁▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02997
wandb:     best_val_mse 0.0305
wandb:      best_val_r2 -0.02197
wandb:    best_val_rmse 0.17464
wandb:            epoch 15
wandb:   final_test_mse 0.01772
wandb:    final_test_r2 -0.052
wandb:  final_test_rmse 0.13312
wandb:  final_train_mse 0.03326
wandb:   final_train_r2 0.00236
wandb: final_train_rmse 0.18237
wandb:    final_val_mse 0.0305
wandb:     final_val_r2 -0.02197
wandb:   final_val_rmse 0.17464
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04263
wandb:       train_time 45.89884
wandb:         val_loss 0.03065
wandb:          val_mse 0.03182
wandb:           val_r2 -0.0663
wandb:         val_rmse 0.17839
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_190043-uu4mq53n
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_190043-uu4mq53n/logs
Experiment probe_layer2_avg_links_len_control1_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/ru/ru/results.json for layer 2
Running experiment: probe_layer2_avg_links_len_control2_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_control2_ru"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/ru"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 19:02:27,051][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/ru
experiment_name: probe_layer2_avg_links_len_control2_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 19:02:27,051][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 19:02:27,052][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 19:02:27,052][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 19:02:27,052][__main__][INFO] - Determined Task Type: regression
[2025-05-07 19:02:27,057][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-05-07 19:02:27,057][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 19:02:27,057][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 19:02:31,629][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 19:02:33,985][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 19:02:33,985][src.data.datasets][INFO] - Loading 'control_avg_links_len_seed2' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 19:02:34,251][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_links_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:16:02 2025).
[2025-05-07 19:02:34,422][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_links_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:16:02 2025).
[2025-05-07 19:02:34,876][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-07 19:02:34,886][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 19:02:34,887][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-07 19:02:34,889][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 19:02:34,977][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:02:35,102][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:02:35,154][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-07 19:02:35,155][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 19:02:35,155][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-07 19:02:35,158][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 19:02:35,281][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:02:35,402][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:02:35,432][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-07 19:02:35,434][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 19:02:35,434][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-07 19:02:35,437][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-07 19:02:35,438][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 19:02:35,438][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 19:02:35,438][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 19:02:35,438][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 19:02:35,438][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9000
[2025-05-07 19:02:35,438][src.data.datasets][INFO] -   Mean: 0.2497, Std: 0.1826
[2025-05-07 19:02:35,438][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-07 19:02:35,438][src.data.datasets][INFO] - Sample label: 0.0560000017285347
[2025-05-07 19:02:35,439][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 19:02:35,439][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 19:02:35,439][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 19:02:35,439][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 19:02:35,439][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8000
[2025-05-07 19:02:35,439][src.data.datasets][INFO] -   Mean: 0.2557, Std: 0.1728
[2025-05-07 19:02:35,439][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-07 19:02:35,439][src.data.datasets][INFO] - Sample label: 0.23399999737739563
[2025-05-07 19:02:35,439][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 19:02:35,439][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 19:02:35,440][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 19:02:35,440][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 19:02:35,440][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6270
[2025-05-07 19:02:35,440][src.data.datasets][INFO] -   Mean: 0.2617, Std: 0.1298
[2025-05-07 19:02:35,440][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-07 19:02:35,440][src.data.datasets][INFO] - Sample label: 0.14000000059604645
[2025-05-07 19:02:35,440][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-07 19:02:35,440][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 19:02:35,440][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 19:02:35,441][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 19:02:35,441][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 19:02:43,837][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 19:02:43,838][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 19:02:43,838][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 19:02:43,838][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 19:02:43,841][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 19:02:43,842][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 19:02:43,842][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 19:02:43,842][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 19:02:43,842][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-07 19:02:43,843][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 19:02:43,843][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.2238Epoch 1/15: [                              ] 2/75 batches, loss: 0.4027Epoch 1/15: [=                             ] 3/75 batches, loss: 0.3404Epoch 1/15: [=                             ] 4/75 batches, loss: 0.3767Epoch 1/15: [==                            ] 5/75 batches, loss: 0.3920Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3723Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4455Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4428Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4890Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4707Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4422Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4430Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4267Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4128Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3995Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3905Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3770Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3888Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3915Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3892Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3895Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3903Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3818Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3827Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3783Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3786Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3775Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3744Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3742Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3729Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3694Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3680Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3642Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3658Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3645Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3664Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3597Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3592Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3567Epoch 1/15: [================              ] 40/75 batches, loss: 0.3522Epoch 1/15: [================              ] 41/75 batches, loss: 0.3503Epoch 1/15: [================              ] 42/75 batches, loss: 0.3475Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3446Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3420Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3393Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3342Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3318Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3291Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3264Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3273Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3242Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3209Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3191Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3169Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3152Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3144Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3174Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3167Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3147Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3118Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3099Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3080Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3050Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3035Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3020Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2994Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2967Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2934Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2937Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2919Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2894Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2890Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2870Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2852Epoch 1/15: [==============================] 75/75 batches, loss: 0.2835
[2025-05-07 19:02:50,508][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2835
[2025-05-07 19:02:50,815][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0585, Metrics: {'mse': 0.06232462823390961, 'rmse': 0.24964901007997128, 'r2': -1.0882833003997803}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2196Epoch 2/15: [                              ] 2/75 batches, loss: 0.2323Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1957Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2292Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2077Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1945Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1954Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1834Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1730Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1862Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1850Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1861Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1857Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1820Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1778Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1714Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1666Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1699Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1685Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1675Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1658Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1639Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1712Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1693Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1691Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1706Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1687Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1682Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1726Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1691Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1690Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1680Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1691Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1676Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1686Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1671Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1653Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1656Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1658Epoch 2/15: [================              ] 40/75 batches, loss: 0.1647Epoch 2/15: [================              ] 41/75 batches, loss: 0.1627Epoch 2/15: [================              ] 42/75 batches, loss: 0.1630Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1619Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1607Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1592Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1580Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1562Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1563Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1551Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1543Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1543Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1539Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1544Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1530Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1525Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1531Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1512Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1509Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1512Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1499Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1495Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1486Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1481Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1472Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1475Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1478Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1475Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1464Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1454Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1446Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1440Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1432Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1421Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1419Epoch 2/15: [==============================] 75/75 batches, loss: 0.1416
[2025-05-07 19:02:53,562][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1416
[2025-05-07 19:02:53,879][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0718, Metrics: {'mse': 0.07800493389368057, 'rmse': 0.27929363382232786, 'r2': -1.6136763095855713}
[2025-05-07 19:02:53,880][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1526Epoch 3/15: [                              ] 2/75 batches, loss: 0.1597Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1470Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1257Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1196Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1242Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1289Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1197Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1210Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1243Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1200Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1181Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1206Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1238Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1229Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1188Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1174Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1178Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1163Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1127Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1095Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1093Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1084Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1106Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1084Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1070Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1068Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1074Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1100Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1100Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1123Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1124Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1107Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1101Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1108Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1090Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1088Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1092Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1077Epoch 3/15: [================              ] 40/75 batches, loss: 0.1088Epoch 3/15: [================              ] 41/75 batches, loss: 0.1102Epoch 3/15: [================              ] 42/75 batches, loss: 0.1105Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1108Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1115Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1110Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1106Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1110Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1118Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1113Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1101Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1108Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1096Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1088Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1087Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1075Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1079Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1076Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1073Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1066Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1064Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1058Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1070Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1080Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1081Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1069Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1066Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1060Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1051Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1049Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1040Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1052Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1045Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1042Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1043Epoch 3/15: [==============================] 75/75 batches, loss: 0.1049
[2025-05-07 19:02:56,213][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1049
[2025-05-07 19:02:56,615][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0390, Metrics: {'mse': 0.04141158610582352, 'rmse': 0.2034983688038396, 'r2': -0.3875594139099121}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0784Epoch 4/15: [                              ] 2/75 batches, loss: 0.0971Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0789Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0765Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0790Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0800Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0873Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0848Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0883Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0864Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0897Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0912Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0931Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0898Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0920Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0909Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0890Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0874Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0877Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0889Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0895Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0916Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0933Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0952Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0953Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0936Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0939Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0942Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0953Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0942Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0936Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0925Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0928Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0911Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0916Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0912Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0905Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0902Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0893Epoch 4/15: [================              ] 40/75 batches, loss: 0.0880Epoch 4/15: [================              ] 41/75 batches, loss: 0.0877Epoch 4/15: [================              ] 42/75 batches, loss: 0.0878Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0891Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0893Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0904Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0902Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0895Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0891Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0885Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0878Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0897Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0896Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0902Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0898Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0899Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0898Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0892Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0884Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0883Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0883Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0895Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0897Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0901Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0897Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0895Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0890Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0890Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0889Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0885Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0880Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0874Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0876Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0875Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0881Epoch 4/15: [==============================] 75/75 batches, loss: 0.0893
[2025-05-07 19:02:59,376][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0893
[2025-05-07 19:02:59,750][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0390, Metrics: {'mse': 0.04159696400165558, 'rmse': 0.20395333780464486, 'r2': -0.39377081394195557}
[2025-05-07 19:02:59,751][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0488Epoch 5/15: [                              ] 2/75 batches, loss: 0.0572Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0690Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0828Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0819Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0789Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0782Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0765Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0742Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0698Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0741Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0776Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0817Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0820Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0816Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0826Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0818Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0824Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0826Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0827Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0824Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0804Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0813Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0812Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0807Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0799Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0795Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0796Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0790Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0773Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0777Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0799Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0794Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0792Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0789Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0782Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0776Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0766Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0762Epoch 5/15: [================              ] 40/75 batches, loss: 0.0765Epoch 5/15: [================              ] 41/75 batches, loss: 0.0776Epoch 5/15: [================              ] 42/75 batches, loss: 0.0770Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0773Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0768Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0778Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0773Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0774Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0767Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0764Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0761Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0765Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0765Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0766Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0761Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0763Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0761Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0759Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0756Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0752Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0757Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0756Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0753Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0753Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0748Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0751Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0757Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0758Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0753Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0750Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0746Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0750Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0747Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0745Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0740Epoch 5/15: [==============================] 75/75 batches, loss: 0.0737
[2025-05-07 19:03:02,208][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0737
[2025-05-07 19:03:02,505][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0323, Metrics: {'mse': 0.03397493436932564, 'rmse': 0.18432290787996383, 'r2': -0.1383829116821289}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0863Epoch 6/15: [                              ] 2/75 batches, loss: 0.0681Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0697Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0719Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0770Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0765Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0824Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0785Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0797Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0786Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0761Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0740Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0700Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0700Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0684Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0695Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0693Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0697Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0721Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0703Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0702Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0733Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0726Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0710Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0724Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0724Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0741Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0728Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0737Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0741Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0742Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0732Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0728Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0720Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0723Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0717Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0722Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0711Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0718Epoch 6/15: [================              ] 40/75 batches, loss: 0.0713Epoch 6/15: [================              ] 41/75 batches, loss: 0.0707Epoch 6/15: [================              ] 42/75 batches, loss: 0.0710Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0711Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0712Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0721Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0719Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0727Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0728Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0730Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0727Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0725Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0726Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0719Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0710Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0709Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0707Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0709Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0707Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0710Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0712Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0713Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0708Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0708Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0707Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0706Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0707Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0707Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0705Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0703Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0701Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0701Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0695Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0690Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0687Epoch 6/15: [==============================] 75/75 batches, loss: 0.0681
[2025-05-07 19:03:05,186][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0681
[2025-05-07 19:03:05,471][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0342, Metrics: {'mse': 0.03613683953881264, 'rmse': 0.19009692143433737, 'r2': -0.21082079410552979}
[2025-05-07 19:03:05,472][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0916Epoch 7/15: [                              ] 2/75 batches, loss: 0.0840Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0798Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0787Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0687Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0688Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0669Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0679Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0663Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0644Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0608Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0626Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0641Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0658Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0639Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0633Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0682Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0681Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0676Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0660Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0663Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0656Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0653Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0652Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0651Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0645Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0649Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0646Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0648Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0644Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0642Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0637Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0644Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0643Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0645Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0644Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0638Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0646Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0645Epoch 7/15: [================              ] 40/75 batches, loss: 0.0643Epoch 7/15: [================              ] 41/75 batches, loss: 0.0639Epoch 7/15: [================              ] 42/75 batches, loss: 0.0637Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0639Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0636Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0637Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0637Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0635Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0632Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0629Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0624Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0623Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0624Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0624Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0624Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0627Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0623Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0618Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0621Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0620Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0617Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0615Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0612Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0616Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0617Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0620Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0615Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0622Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0619Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0617Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0618Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0616Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0612Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0612Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0613Epoch 7/15: [==============================] 75/75 batches, loss: 0.0614
[2025-05-07 19:03:07,845][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0614
[2025-05-07 19:03:08,141][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0344, Metrics: {'mse': 0.03638117387890816, 'rmse': 0.19073849605915466, 'r2': -0.2190077304840088}
[2025-05-07 19:03:08,142][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0402Epoch 8/15: [                              ] 2/75 batches, loss: 0.0549Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0522Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0507Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0471Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0494Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0485Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0493Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0534Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0603Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0646Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0623Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0633Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0610Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0619Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0618Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0627Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0628Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0610Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0613Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0605Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0594Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0601Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0600Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0609Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0605Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0605Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0609Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0608Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0619Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0619Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0617Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0616Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0605Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0600Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0594Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0599Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0590Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0588Epoch 8/15: [================              ] 40/75 batches, loss: 0.0593Epoch 8/15: [================              ] 41/75 batches, loss: 0.0590Epoch 8/15: [================              ] 42/75 batches, loss: 0.0587Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0589Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0600Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0599Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0604Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0602Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0602Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0605Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0605Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0601Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0602Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0597Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0596Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0597Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0602Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0596Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0594Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0594Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0595Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0593Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0592Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0588Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0589Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0588Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0587Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0585Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0584Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0589Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0588Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0586Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0585Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0583Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0591Epoch 8/15: [==============================] 75/75 batches, loss: 0.0586
[2025-05-07 19:03:10,509][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0586
[2025-05-07 19:03:10,921][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0349, Metrics: {'mse': 0.037088360637426376, 'rmse': 0.19258338619264742, 'r2': -0.24270308017730713}
[2025-05-07 19:03:10,921][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0418Epoch 9/15: [                              ] 2/75 batches, loss: 0.0449Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0468Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0501Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0448Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0484Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0522Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0532Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0537Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0536Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0535Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0517Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0518Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0521Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0516Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0515Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0507Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0508Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0512Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0508Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0501Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0511Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0508Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0501Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0501Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0496Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0490Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0485Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0492Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0497Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0496Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0499Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0493Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0499Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0497Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0491Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0490Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0486Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0504Epoch 9/15: [================              ] 40/75 batches, loss: 0.0500Epoch 9/15: [================              ] 41/75 batches, loss: 0.0494Epoch 9/15: [================              ] 42/75 batches, loss: 0.0491Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0495Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0492Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0499Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0496Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0493Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0490Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0486Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0495Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0497Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0499Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0505Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0501Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0508Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0508Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0506Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0501Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0502Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0505Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0507Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0508Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0509Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0511Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0512Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0516Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0515Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0513Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0516Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0516Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0512Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0511Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0512Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0510Epoch 9/15: [==============================] 75/75 batches, loss: 0.0509
[2025-05-07 19:03:13,325][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0509
[2025-05-07 19:03:13,753][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0329, Metrics: {'mse': 0.03447489067912102, 'rmse': 0.18567415188744238, 'r2': -0.1551346778869629}
[2025-05-07 19:03:13,754][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 19:03:13,754][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 9
[2025-05-07 19:03:13,754][src.training.lm_trainer][INFO] - Training completed in 26.33 seconds
[2025-05-07 19:03:13,754][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 19:03:17,163][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03499756380915642, 'rmse': 0.18707635823148905, 'r2': -0.049758195877075195}
[2025-05-07 19:03:17,163][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03397493436932564, 'rmse': 0.18432290787996383, 'r2': -0.1383829116821289}
[2025-05-07 19:03:17,163][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.02098778448998928, 'rmse': 0.14487161381716324, 'r2': -0.2459319829940796}
[2025-05-07 19:03:20,299][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/ru/ru/model.pt
[2025-05-07 19:03:20,300][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▁
wandb:     best_val_mse █▃▁
wandb:      best_val_r2 ▁▆█
wandb:    best_val_rmse █▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▁▆▆▇▆▆▆
wandb:       train_loss █▄▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆█▂▂▁▁▁▁▁
wandb:          val_mse ▆█▂▂▁▁▁▁▁
wandb:           val_r2 ▃▁▇▇█████
wandb:         val_rmse ▆█▂▂▁▁▁▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03225
wandb:     best_val_mse 0.03397
wandb:      best_val_r2 -0.13838
wandb:    best_val_rmse 0.18432
wandb: early_stop_epoch 9
wandb:            epoch 9
wandb:   final_test_mse 0.02099
wandb:    final_test_r2 -0.24593
wandb:  final_test_rmse 0.14487
wandb:  final_train_mse 0.035
wandb:   final_train_r2 -0.04976
wandb: final_train_rmse 0.18708
wandb:    final_val_mse 0.03397
wandb:     final_val_r2 -0.13838
wandb:   final_val_rmse 0.18432
wandb:    learning_rate 0.0001
wandb:       train_loss 0.05094
wandb:       train_time 26.32927
wandb:         val_loss 0.03289
wandb:          val_mse 0.03447
wandb:           val_r2 -0.15513
wandb:         val_rmse 0.18567
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_190227-17x5p9n2
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_190227-17x5p9n2/logs
Experiment probe_layer2_avg_links_len_control2_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/ru/ru/results.json for layer 2
Running experiment: probe_layer2_avg_links_len_control3_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_control3_ru"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control3/layer2/ru"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 19:04:08,638][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control3/layer2/ru
experiment_name: probe_layer2_avg_links_len_control3_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 19:04:08,638][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 19:04:08,638][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 19:04:08,638][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 19:04:08,638][__main__][INFO] - Determined Task Type: regression
[2025-05-07 19:04:08,643][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-05-07 19:04:08,643][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 19:04:08,643][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 19:04:12,145][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 19:04:14,492][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 19:04:14,492][src.data.datasets][INFO] - Loading 'control_avg_links_len_seed3' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 19:04:14,686][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_links_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:17:58 2025).
[2025-05-07 19:04:14,786][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_links_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:17:58 2025).
[2025-05-07 19:04:15,178][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-07 19:04:15,188][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 19:04:15,189][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-07 19:04:15,194][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 19:04:15,300][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:04:15,437][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:04:15,463][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-07 19:04:15,464][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 19:04:15,465][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-07 19:04:15,472][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 19:04:15,592][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:04:15,708][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:04:15,747][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-07 19:04:15,749][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 19:04:15,749][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-07 19:04:15,752][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-07 19:04:15,753][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 19:04:15,753][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 19:04:15,753][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 19:04:15,753][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 19:04:15,753][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9000
[2025-05-07 19:04:15,753][src.data.datasets][INFO] -   Mean: 0.2497, Std: 0.1826
[2025-05-07 19:04:15,753][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-07 19:04:15,754][src.data.datasets][INFO] - Sample label: 0.4000000059604645
[2025-05-07 19:04:15,754][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 19:04:15,754][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 19:04:15,754][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 19:04:15,754][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 19:04:15,754][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8000
[2025-05-07 19:04:15,754][src.data.datasets][INFO] -   Mean: 0.2557, Std: 0.1728
[2025-05-07 19:04:15,754][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-07 19:04:15,754][src.data.datasets][INFO] - Sample label: 0.23399999737739563
[2025-05-07 19:04:15,755][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 19:04:15,755][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 19:04:15,755][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 19:04:15,755][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 19:04:15,755][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6270
[2025-05-07 19:04:15,755][src.data.datasets][INFO] -   Mean: 0.2617, Std: 0.1298
[2025-05-07 19:04:15,755][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-07 19:04:15,755][src.data.datasets][INFO] - Sample label: 0.14000000059604645
[2025-05-07 19:04:15,755][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-07 19:04:15,755][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 19:04:15,756][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 19:04:15,756][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 19:04:15,756][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 19:04:24,711][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 19:04:24,712][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 19:04:24,712][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 19:04:24,712][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 19:04:24,715][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 19:04:24,716][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 19:04:24,716][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 19:04:24,716][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 19:04:24,716][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-07 19:04:24,717][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 19:04:24,717][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.2316Epoch 1/15: [                              ] 2/75 batches, loss: 0.4329Epoch 1/15: [=                             ] 3/75 batches, loss: 0.3621Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4053Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4206Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3822Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4405Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4407Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4828Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4665Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4499Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4443Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4236Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4086Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3953Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3924Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3833Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3891Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3883Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3820Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3873Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3988Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3940Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3906Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3821Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3775Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3759Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3719Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3764Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3806Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3781Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3732Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3676Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3683Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3674Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3676Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3604Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3608Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3578Epoch 1/15: [================              ] 40/75 batches, loss: 0.3550Epoch 1/15: [================              ] 41/75 batches, loss: 0.3539Epoch 1/15: [================              ] 42/75 batches, loss: 0.3523Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3506Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3477Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3444Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3402Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3387Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3384Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3347Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3330Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3303Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3276Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3248Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3235Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3220Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3204Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3212Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3192Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3174Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3163Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3146Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3127Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3097Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3081Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3070Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3047Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3015Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2983Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2986Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2966Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2950Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2953Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2934Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2918Epoch 1/15: [==============================] 75/75 batches, loss: 0.2901
[2025-05-07 19:04:31,959][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2901
[2025-05-07 19:04:32,264][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0603, Metrics: {'mse': 0.06452462077140808, 'rmse': 0.2540169694556017, 'r2': -1.1619973182678223}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1014Epoch 2/15: [                              ] 2/75 batches, loss: 0.1394Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1335Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1507Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1366Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1359Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1521Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1580Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1550Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1649Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1666Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1699Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1728Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1730Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1679Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1648Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1627Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1665Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1654Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1624Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1624Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1609Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1645Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1634Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1623Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1638Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1648Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1622Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1636Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1608Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1606Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1589Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1596Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1588Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1592Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1570Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1566Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1561Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1578Epoch 2/15: [================              ] 40/75 batches, loss: 0.1568Epoch 2/15: [================              ] 41/75 batches, loss: 0.1562Epoch 2/15: [================              ] 42/75 batches, loss: 0.1569Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1556Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1539Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1526Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1516Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1499Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1499Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1503Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1494Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1486Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1477Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1475Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1471Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1464Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1461Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1456Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1454Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1452Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1447Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1439Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1430Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1428Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1425Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1420Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1418Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1414Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1411Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1402Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1397Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1390Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1386Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1396Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1393Epoch 2/15: [==============================] 75/75 batches, loss: 0.1385
[2025-05-07 19:04:34,960][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1385
[2025-05-07 19:04:35,219][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0512, Metrics: {'mse': 0.05506761744618416, 'rmse': 0.23466490458989422, 'r2': -0.8451260328292847}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1850Epoch 3/15: [                              ] 2/75 batches, loss: 0.1622Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1708Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1532Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1366Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1278Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1329Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1255Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1261Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1216Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1199Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1182Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1206Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1171Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1178Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1149Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1149Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1174Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1151Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1146Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1137Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1117Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1105Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1119Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1107Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1097Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1081Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1068Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1073Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1074Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1063Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1061Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1058Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1053Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1043Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1033Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1031Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1022Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1015Epoch 3/15: [================              ] 40/75 batches, loss: 0.1012Epoch 3/15: [================              ] 41/75 batches, loss: 0.1020Epoch 3/15: [================              ] 42/75 batches, loss: 0.1016Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1039Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1052Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1045Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1064Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1074Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1078Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1083Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1080Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1073Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1065Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1065Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1069Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1056Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1055Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1044Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1047Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1035Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1031Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1023Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1031Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1027Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1029Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1027Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1025Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1026Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1022Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1026Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1021Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1029Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1029Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1031Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1027Epoch 3/15: [==============================] 75/75 batches, loss: 0.1038
[2025-05-07 19:04:37,957][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1038
[2025-05-07 19:04:38,364][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0389, Metrics: {'mse': 0.04100706800818443, 'rmse': 0.20250201976322219, 'r2': -0.37400543689727783}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0374Epoch 4/15: [                              ] 2/75 batches, loss: 0.0635Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0637Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0671Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0751Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0787Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0815Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0855Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0848Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0814Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0848Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0814Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0819Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0813Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0804Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0853Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0826Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0849Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0863Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0872Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0862Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0896Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0910Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0910Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0957Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0967Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0963Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0955Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0952Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0948Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0949Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0944Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0948Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0939Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0947Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0950Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0940Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0946Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0939Epoch 4/15: [================              ] 40/75 batches, loss: 0.0929Epoch 4/15: [================              ] 41/75 batches, loss: 0.0932Epoch 4/15: [================              ] 42/75 batches, loss: 0.0927Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0929Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0926Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0932Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0923Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0923Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0919Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0916Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0914Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0930Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0931Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0925Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0921Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0917Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0920Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0920Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0921Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0918Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0917Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0917Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0907Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0907Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0901Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0904Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0900Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0893Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0889Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0889Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0885Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0878Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0875Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0875Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0877Epoch 4/15: [==============================] 75/75 batches, loss: 0.0876
[2025-05-07 19:04:41,017][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0876
[2025-05-07 19:04:41,393][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0418, Metrics: {'mse': 0.044893138110637665, 'rmse': 0.21188000875646024, 'r2': -0.5042142868041992}
[2025-05-07 19:04:41,394][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0416Epoch 5/15: [                              ] 2/75 batches, loss: 0.0518Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0734Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0738Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0751Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0792Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0898Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0893Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0870Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0859Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0857Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0841Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0818Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0832Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0843Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0875Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0880Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0908Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0905Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0893Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0871Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0856Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0865Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0846Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0861Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0854Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0848Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0839Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0829Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0821Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0812Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0817Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0818Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0814Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0813Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0810Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0808Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0812Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0806Epoch 5/15: [================              ] 40/75 batches, loss: 0.0799Epoch 5/15: [================              ] 41/75 batches, loss: 0.0810Epoch 5/15: [================              ] 42/75 batches, loss: 0.0808Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0807Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0809Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0800Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0802Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0796Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0790Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0791Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0801Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0798Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0789Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0792Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0787Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0791Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0787Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0785Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0782Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0778Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0780Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0780Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0784Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0779Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0778Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0776Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0779Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0777Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0778Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0777Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0771Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0774Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0776Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0775Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0769Epoch 5/15: [==============================] 75/75 batches, loss: 0.0775
[2025-05-07 19:04:43,753][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0775
[2025-05-07 19:04:44,062][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0339, Metrics: {'mse': 0.03484348952770233, 'rmse': 0.18666410883644002, 'r2': -0.16748511791229248}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0410Epoch 6/15: [                              ] 2/75 batches, loss: 0.0665Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0722Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0724Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0713Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0744Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0728Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0714Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0683Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0671Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0659Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0684Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0671Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0662Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0669Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0678Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0656Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0678Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0687Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0704Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0717Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0722Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0716Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0704Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0698Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0694Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0694Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0680Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0681Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0685Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0695Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0691Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0684Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0690Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0690Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0689Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0688Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0683Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0694Epoch 6/15: [================              ] 40/75 batches, loss: 0.0687Epoch 6/15: [================              ] 41/75 batches, loss: 0.0689Epoch 6/15: [================              ] 42/75 batches, loss: 0.0697Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0693Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0700Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0708Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0703Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0703Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0704Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0703Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0705Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0700Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0698Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0695Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0693Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0686Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0696Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0700Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0694Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0691Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0685Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0693Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0689Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0687Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0687Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0684Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0683Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0682Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0688Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0684Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0686Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0683Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0687Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0682Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0679Epoch 6/15: [==============================] 75/75 batches, loss: 0.0676
[2025-05-07 19:04:46,787][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0676
[2025-05-07 19:04:47,077][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0340, Metrics: {'mse': 0.03564692288637161, 'rmse': 0.1888039270946757, 'r2': -0.1944054365158081}
[2025-05-07 19:04:47,078][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0682Epoch 7/15: [                              ] 2/75 batches, loss: 0.0732Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0643Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0789Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0798Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0732Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0681Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0675Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0650Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0654Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0651Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0646Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0645Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0676Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0662Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0664Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0652Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0643Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0631Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0634Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0646Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0651Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0652Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0654Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0640Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0639Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0641Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0650Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0658Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0658Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0654Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0647Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0645Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0660Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0660Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0656Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0654Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0656Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0648Epoch 7/15: [================              ] 40/75 batches, loss: 0.0652Epoch 7/15: [================              ] 41/75 batches, loss: 0.0652Epoch 7/15: [================              ] 42/75 batches, loss: 0.0664Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0669Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0668Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0664Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0664Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0663Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0663Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0665Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0658Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0651Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0654Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0650Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0645Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0646Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0643Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0642Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0642Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0640Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0643Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0640Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0640Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0642Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0641Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0640Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0637Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0637Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0639Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0637Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0636Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0634Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0632Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0632Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0629Epoch 7/15: [==============================] 75/75 batches, loss: 0.0628
[2025-05-07 19:04:49,430][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0628
[2025-05-07 19:04:49,761][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0351, Metrics: {'mse': 0.037277381867170334, 'rmse': 0.1930735141524345, 'r2': -0.24903643131256104}
[2025-05-07 19:04:49,762][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0882Epoch 8/15: [                              ] 2/75 batches, loss: 0.0638Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0614Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0663Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0624Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0624Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0610Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0636Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0619Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0602Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0583Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0570Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0587Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0587Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0600Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0591Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0594Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0598Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0587Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0588Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0578Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0578Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0577Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0580Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0582Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0578Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0570Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0570Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0568Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0567Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0569Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0568Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0562Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0558Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0551Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0546Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0542Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0540Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0548Epoch 8/15: [================              ] 40/75 batches, loss: 0.0555Epoch 8/15: [================              ] 41/75 batches, loss: 0.0555Epoch 8/15: [================              ] 42/75 batches, loss: 0.0550Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0547Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0551Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0549Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0566Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0567Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0564Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0565Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0572Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0569Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0581Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0579Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0574Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0569Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0568Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0566Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0563Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0562Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0564Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0563Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0560Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0564Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0564Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0560Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0560Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0560Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0558Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0554Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0550Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0548Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0549Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0544Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0545Epoch 8/15: [==============================] 75/75 batches, loss: 0.0549
[2025-05-07 19:04:52,076][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0549
[2025-05-07 19:04:52,453][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0416, Metrics: {'mse': 0.04508938267827034, 'rmse': 0.21234260683685302, 'r2': -0.5107897520065308}
[2025-05-07 19:04:52,454][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0163Epoch 9/15: [                              ] 2/75 batches, loss: 0.0245Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0402Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0416Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0395Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0399Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0454Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0447Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0460Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0459Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0475Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0503Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0490Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0495Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0491Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0498Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0485Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0473Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0484Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0487Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0487Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0494Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0485Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0486Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0495Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0489Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0485Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0487Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0490Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0484Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0493Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0490Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0493Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0502Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0504Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0497Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0508Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0500Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0505Epoch 9/15: [================              ] 40/75 batches, loss: 0.0501Epoch 9/15: [================              ] 41/75 batches, loss: 0.0500Epoch 9/15: [================              ] 42/75 batches, loss: 0.0501Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0500Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0498Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0497Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0495Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0492Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0495Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0501Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0501Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0507Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0502Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0504Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0502Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0509Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0512Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0512Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0514Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0513Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0509Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0517Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0520Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0521Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0520Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0520Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0524Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0521Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0519Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0521Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0519Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0519Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0531Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0532Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0530Epoch 9/15: [==============================] 75/75 batches, loss: 0.0528
[2025-05-07 19:04:54,849][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0528
[2025-05-07 19:04:55,144][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0321, Metrics: {'mse': 0.03336309641599655, 'rmse': 0.18265567720713352, 'r2': -0.1178823709487915}
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0678Epoch 10/15: [                              ] 2/75 batches, loss: 0.0619Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0542Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0552Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0560Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0513Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0500Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0517Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0517Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0540Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0563Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0558Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0548Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0532Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0507Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0511Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0511Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0512Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0502Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0507Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0501Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0496Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0491Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0478Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0473Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0467Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0480Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0480Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0471Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0472Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0466Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0475Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0468Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0475Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0483Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0484Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0494Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0497Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0502Epoch 10/15: [================              ] 40/75 batches, loss: 0.0506Epoch 10/15: [================              ] 41/75 batches, loss: 0.0503Epoch 10/15: [================              ] 42/75 batches, loss: 0.0498Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0493Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0486Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0484Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0487Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0494Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0496Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0493Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0500Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0499Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0497Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0494Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0493Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0491Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0493Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0495Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0490Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0493Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0492Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0496Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0493Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0496Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0491Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0490Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0487Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0488Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0493Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0493Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0491Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0492Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0495Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0494Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0496Epoch 10/15: [==============================] 75/75 batches, loss: 0.0495
[2025-05-07 19:04:57,855][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0495
[2025-05-07 19:04:58,151][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0362, Metrics: {'mse': 0.03873887285590172, 'rmse': 0.19682193184678815, 'r2': -0.2980060577392578}
[2025-05-07 19:04:58,151][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0656Epoch 11/15: [                              ] 2/75 batches, loss: 0.0428Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0432Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0599Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0585Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0555Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0564Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0538Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0531Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0508Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0506Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0499Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0493Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0478Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0486Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0492Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0477Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0490Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0495Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0486Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0490Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0498Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0489Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0486Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0485Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0494Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0504Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0494Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0513Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0505Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0505Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0502Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0517Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0514Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0529Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0528Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0528Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0524Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0519Epoch 11/15: [================              ] 40/75 batches, loss: 0.0524Epoch 11/15: [================              ] 41/75 batches, loss: 0.0520Epoch 11/15: [================              ] 42/75 batches, loss: 0.0511Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0507Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0507Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0511Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0511Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0508Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0504Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0508Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0502Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0504Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0503Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0500Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0502Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0504Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0501Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0498Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0494Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0496Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0494Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0492Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0487Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0486Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0488Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0487Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0489Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0485Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0483Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0483Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0481Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0477Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0477Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0476Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0474Epoch 11/15: [==============================] 75/75 batches, loss: 0.0479
[2025-05-07 19:05:00,530][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0479
[2025-05-07 19:05:02,856][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0314, Metrics: {'mse': 0.03265834599733353, 'rmse': 0.1807162029186468, 'r2': -0.0942685604095459}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0590Epoch 12/15: [                              ] 2/75 batches, loss: 0.0577Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0699Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0666Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0588Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0533Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0507Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0473Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0462Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0469Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0492Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0483Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0497Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0488Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0483Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0487Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0481Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0466Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0466Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0463Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0456Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0455Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0451Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0445Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0455Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0459Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0461Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0470Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0475Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0473Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0470Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0468Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0474Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0471Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0477Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0482Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0476Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0470Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0474Epoch 12/15: [================              ] 40/75 batches, loss: 0.0468Epoch 12/15: [================              ] 41/75 batches, loss: 0.0468Epoch 12/15: [================              ] 42/75 batches, loss: 0.0476Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0478Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0471Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0472Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0467Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0468Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0468Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0469Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0467Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0475Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0481Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0477Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0477Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0476Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0481Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0483Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0479Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0480Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0479Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0477Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0475Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0478Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0484Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0484Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0483Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0481Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0481Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0483Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0484Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0483Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0478Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0480Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0482Epoch 12/15: [==============================] 75/75 batches, loss: 0.0478
[2025-05-07 19:05:05,544][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0478
[2025-05-07 19:05:05,965][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0310, Metrics: {'mse': 0.031546205282211304, 'rmse': 0.17761251443018114, 'r2': -0.057004570960998535}
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0618Epoch 13/15: [                              ] 2/75 batches, loss: 0.0763Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0612Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0611Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0545Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0499Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0452Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0467Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0448Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0491Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0494Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0497Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0484Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0489Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0490Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0502Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0495Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0493Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0486Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0494Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0499Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0492Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0491Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0496Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0494Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0496Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0494Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0495Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0488Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0484Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0484Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0487Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0484Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0487Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0490Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0491Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0488Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0484Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0477Epoch 13/15: [================              ] 40/75 batches, loss: 0.0471Epoch 13/15: [================              ] 41/75 batches, loss: 0.0478Epoch 13/15: [================              ] 42/75 batches, loss: 0.0475Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0472Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0471Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0472Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0469Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0476Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0479Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0478Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0476Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0471Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0474Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0475Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0481Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0483Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0480Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0482Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0485Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0487Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0483Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0481Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0481Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0478Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0479Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0477Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0475Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0476Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0474Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0477Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0475Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0476Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0473Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0471Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0473Epoch 13/15: [==============================] 75/75 batches, loss: 0.0472
[2025-05-07 19:05:08,774][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0472
[2025-05-07 19:05:09,331][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0354, Metrics: {'mse': 0.037670448422431946, 'rmse': 0.19408876428694152, 'r2': -0.26220691204071045}
[2025-05-07 19:05:09,332][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0641Epoch 14/15: [                              ] 2/75 batches, loss: 0.0518Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0588Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0520Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0550Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0527Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0506Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0504Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0512Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0503Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0507Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0523Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0503Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0510Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0495Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0486Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0484Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0485Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0477Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0475Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0473Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0462Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0459Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0465Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0462Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0461Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0458Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0466Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0459Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0454Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0448Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0452Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0450Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0452Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0452Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0455Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0454Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0453Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0459Epoch 14/15: [================              ] 40/75 batches, loss: 0.0457Epoch 14/15: [================              ] 41/75 batches, loss: 0.0451Epoch 14/15: [================              ] 42/75 batches, loss: 0.0451Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0457Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0456Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0459Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0456Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0457Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0459Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0457Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0455Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0459Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0455Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0458Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0457Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0459Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0459Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0458Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0454Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0452Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0453Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0450Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0448Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0446Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0443Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0441Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0447Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0449Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0449Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0449Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0448Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0454Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0453Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0455Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0454Epoch 14/15: [==============================] 75/75 batches, loss: 0.0452
[2025-05-07 19:05:11,908][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0452
[2025-05-07 19:05:12,314][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0323, Metrics: {'mse': 0.03344745934009552, 'rmse': 0.1828864657105482, 'r2': -0.12070894241333008}
[2025-05-07 19:05:12,315][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0655Epoch 15/15: [                              ] 2/75 batches, loss: 0.0519Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0549Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0508Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0465Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0448Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0529Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0499Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0498Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0482Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0457Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0470Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0478Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0473Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0475Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0479Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0488Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0484Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0478Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0473Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0477Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0467Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0465Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0462Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0464Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0460Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0452Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0459Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0456Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0452Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0460Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0454Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0454Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0456Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0454Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0450Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0444Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0437Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0434Epoch 15/15: [================              ] 40/75 batches, loss: 0.0430Epoch 15/15: [================              ] 41/75 batches, loss: 0.0433Epoch 15/15: [================              ] 42/75 batches, loss: 0.0429Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0423Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0419Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0428Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0431Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0429Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0432Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0434Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0437Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0437Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0437Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0441Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0442Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0443Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0442Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0440Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0440Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0439Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0437Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0437Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0436Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0434Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0433Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0437Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0438Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0436Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0435Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0441Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0442Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0440Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0437Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0438Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0438Epoch 15/15: [==============================] 75/75 batches, loss: 0.0434
[2025-05-07 19:05:14,842][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0434
[2025-05-07 19:05:15,239][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0331, Metrics: {'mse': 0.03492844104766846, 'rmse': 0.18689152213963173, 'r2': -0.17033159732818604}
[2025-05-07 19:05:15,240][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
[2025-05-07 19:05:15,240][src.training.lm_trainer][INFO] - Training completed in 46.28 seconds
[2025-05-07 19:05:15,240][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 19:05:19,413][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.033301398158073425, 'rmse': 0.18248670679825812, 'r2': 0.0011186599731445312}
[2025-05-07 19:05:19,414][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.031546205282211304, 'rmse': 0.17761251443018114, 'r2': -0.057004570960998535}
[2025-05-07 19:05:19,414][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.019425299018621445, 'rmse': 0.13937467136686438, 'r2': -0.1531757116317749}
[2025-05-07 19:05:22,766][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control3/layer2/ru/ru/model.pt
[2025-05-07 19:05:22,767][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▃▂▁▁▁
wandb:     best_val_mse █▆▃▂▁▁▁
wandb:      best_val_r2 ▁▃▆▇███
wandb:    best_val_rmse █▆▃▂▁▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▅▅▆▆▆▅▆▅▆▆▆▆
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▃▄▂▂▂▄▁▂▁▁▂▁▂
wandb:          val_mse █▆▃▄▂▂▂▄▁▃▁▁▂▁▂
wandb:           val_r2 ▁▃▆▅▇▇▇▅█▆██▇█▇
wandb:         val_rmse █▆▃▄▂▂▂▄▁▃▁▁▃▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03097
wandb:     best_val_mse 0.03155
wandb:      best_val_r2 -0.057
wandb:    best_val_rmse 0.17761
wandb:            epoch 15
wandb:   final_test_mse 0.01943
wandb:    final_test_r2 -0.15318
wandb:  final_test_rmse 0.13937
wandb:  final_train_mse 0.0333
wandb:   final_train_r2 0.00112
wandb: final_train_rmse 0.18249
wandb:    final_val_mse 0.03155
wandb:     final_val_r2 -0.057
wandb:   final_val_rmse 0.17761
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04339
wandb:       train_time 46.27854
wandb:         val_loss 0.03309
wandb:          val_mse 0.03493
wandb:           val_r2 -0.17033
wandb:         val_rmse 0.18689
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_190408-p4p1leon
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_190408-p4p1leon/logs
Experiment probe_layer2_avg_links_len_control3_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control3/layer2/ru/ru/results.json for layer 2
Running experiment: probe_layer2_avg_links_len_control1_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_control1_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 19:06:08,398][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/ja
experiment_name: probe_layer2_avg_links_len_control1_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 19:06:08,398][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 19:06:08,398][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 19:06:08,398][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 19:06:08,398][__main__][INFO] - Determined Task Type: regression
[2025-05-07 19:06:08,402][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 19:06:08,402][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 19:06:08,403][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
slurmstepd: error: *** JOB 64466444 ON k28i22 CANCELLED AT 2025-05-07T19:06:08 ***
