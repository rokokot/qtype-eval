SLURM_JOB_ID: 64466766
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: qtype_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Wed May  7 20:11:22 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main probing experiments (non-control)...
=======================
PROBING LAYER 2
=======================
Experiment probe_layer2_complexity_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/id/id/results.json for layer 2
Running control probing experiments...
=======================
PROBING LAYER 2 (CONTROL EXPERIMENTS)
=======================
Experiment probe_layer2_complexity_control1_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/id/id/results.json for layer 2
Experiment probe_layer2_complexity_control2_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/id/id/results.json for layer 2
Experiment probe_layer2_complexity_control3_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/id/id/results.json for layer 2
Running submetric probing experiments...
=======================
PROBING LAYER 2 (SUBMETRIC EXPERIMENTS)
=======================
Experiment probe_layer2_avg_links_len_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/id/id/results.json for layer 2
Experiment probe_layer2_avg_max_depth_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer2/id/id/results.json for layer 2
Experiment probe_layer2_avg_subordinate_chain_len_id already completed successfully. Extracting metrics...
Warning: No test metrics found in /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/id/id/results.json
Failed to extract metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/id/id/results.json for layer 2
Experiment probe_layer2_avg_verb_edges_id already completed successfully. Extracting metrics...
Warning: No test metrics found in /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/layer2/id/id/results.json
Failed to extract metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/layer2/id/id/results.json for layer 2
Experiment probe_layer2_lexical_density_id already completed successfully. Extracting metrics...
Warning: No test metrics found in /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/layer2/id/id/results.json
Failed to extract metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_n_tokens_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_n_tokens_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/layer2/id"         "wandb.mode=offline" "experiment.submetric=n_tokens"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:12:06,151][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/layer2/id
experiment_name: probe_layer2_n_tokens_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: n_tokens
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:12:06,151][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:12:06,151][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 20:12:06,151][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:12:06,151][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:12:06,156][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:12:06,156][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 20:12:06,157][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:12:10,033][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'n_tokens'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:12:12,389][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:12:12,389][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:12:12,768][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:12:12,981][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:12:13,326][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:12:13,334][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:12:13,334][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:12:13,337][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:12:13,481][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:12:13,591][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:12:13,633][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:12:13,635][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:12:13,635][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:12:13,638][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:12:13,744][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:12:13,866][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:12:13,907][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:12:13,908][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:12:13,908][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:12:13,912][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:12:13,912][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:12:13,912][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 20:12:13,912][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 20:12:13,912][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 20:12:13,912][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:12:13,913][src.data.datasets][INFO] -   Mean: 0.2923, Std: 0.1789
[2025-05-07 20:12:13,913][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:12:13,913][src.data.datasets][INFO] - Sample label: 0.5
[2025-05-07 20:12:13,913][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:12:13,913][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 20:12:13,913][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 20:12:13,913][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 20:12:13,913][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:12:13,914][src.data.datasets][INFO] -   Mean: 0.2550, Std: 0.1823
[2025-05-07 20:12:13,914][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:12:13,914][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:12:13,914][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:12:13,914][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 20:12:13,914][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 20:12:13,914][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 20:12:13,914][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:12:13,914][src.data.datasets][INFO] -   Mean: 0.1905, Std: 0.1719
[2025-05-07 20:12:13,914][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:12:13,914][src.data.datasets][INFO] - Sample label: 0.28600001335144043
[2025-05-07 20:12:13,914][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:12:13,915][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:12:13,915][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:12:13,915][__main__][INFO] - Using model type: lm_probe for submetric n_tokens
[2025-05-07 20:12:13,915][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:12:23,646][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:12:23,647][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:12:23,647][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:12:23,647][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:12:23,650][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:12:23,651][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:12:23,651][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:12:23,651][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:12:23,651][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:12:23,652][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:12:23,652][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.3697Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4106Epoch 1/15: [=                             ] 3/60 batches, loss: 0.4888Epoch 1/15: [==                            ] 4/60 batches, loss: 0.4616Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4485Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4313Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4767Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4651Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4831Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4795Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4656Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4631Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4486Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4493Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4394Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4337Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4262Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4302Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4164Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4091Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4010Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4010Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.3954Epoch 1/15: [============                  ] 24/60 batches, loss: 0.3875Epoch 1/15: [============                  ] 25/60 batches, loss: 0.3843Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.3798Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3720Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3685Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3673Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3674Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3633Epoch 1/15: [================              ] 32/60 batches, loss: 0.3591Epoch 1/15: [================              ] 33/60 batches, loss: 0.3546Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3524Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3483Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3475Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3405Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3355Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3314Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3289Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3265Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3258Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3235Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3227Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3213Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3164Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3149Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3116Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3095Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3063Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3044Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3026Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.2992Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.2990Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.2977Epoch 1/15: [============================  ] 56/60 batches, loss: 0.2976Epoch 1/15: [============================  ] 57/60 batches, loss: 0.2998Epoch 1/15: [============================= ] 58/60 batches, loss: 0.2988Epoch 1/15: [============================= ] 59/60 batches, loss: 0.2976Epoch 1/15: [==============================] 60/60 batches, loss: 0.2946
[2025-05-07 20:12:30,875][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2946
[2025-05-07 20:12:31,190][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0417, Metrics: {'mse': 0.03966832160949707, 'rmse': 0.19916907794508934, 'r2': -0.19348037242889404}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.3002Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2193Epoch 2/15: [=                             ] 3/60 batches, loss: 0.1929Epoch 2/15: [==                            ] 4/60 batches, loss: 0.1877Epoch 2/15: [==                            ] 5/60 batches, loss: 0.1637Epoch 2/15: [===                           ] 6/60 batches, loss: 0.1749Epoch 2/15: [===                           ] 7/60 batches, loss: 0.1787Epoch 2/15: [====                          ] 8/60 batches, loss: 0.1828Epoch 2/15: [====                          ] 9/60 batches, loss: 0.1843Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.1743Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.1779Epoch 2/15: [======                        ] 12/60 batches, loss: 0.1777Epoch 2/15: [======                        ] 13/60 batches, loss: 0.1854Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.1904Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.1829Epoch 2/15: [========                      ] 16/60 batches, loss: 0.1842Epoch 2/15: [========                      ] 17/60 batches, loss: 0.1860Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.1827Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.1844Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1844Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1811Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1876Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1867Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1816Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1861Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1829Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1807Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1792Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1766Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1750Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1739Epoch 2/15: [================              ] 32/60 batches, loss: 0.1717Epoch 2/15: [================              ] 33/60 batches, loss: 0.1698Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1690Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1705Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1698Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1705Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1696Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1687Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1690Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1697Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1680Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1670Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1691Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1674Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1664Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1639Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1631Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1632Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1627Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1610Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1599Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1588Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1575Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1564Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1550Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1546Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1547Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1545Epoch 2/15: [==============================] 60/60 batches, loss: 0.1527
[2025-05-07 20:12:33,481][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1527
[2025-05-07 20:12:33,856][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0378, Metrics: {'mse': 0.035797469317913055, 'rmse': 0.18920219163083987, 'r2': -0.07702004909515381}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.0846Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1197Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1082Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1283Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1272Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1253Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1279Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1340Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1274Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1230Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1226Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1258Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1242Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1231Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1176Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1158Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1147Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1132Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1098Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1118Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1102Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1094Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1069Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1044Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1039Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1064Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1048Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1025Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1023Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1007Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1028Epoch 3/15: [================              ] 32/60 batches, loss: 0.1034Epoch 3/15: [================              ] 33/60 batches, loss: 0.1078Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1081Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1070Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1079Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1066Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1057Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1060Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1050Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1051Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1052Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1056Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1055Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1047Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1049Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1048Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1048Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1053Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1052Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1046Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1048Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1042Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1048Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1047Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1036Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1034Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1031Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1021Epoch 3/15: [==============================] 60/60 batches, loss: 0.1050
[2025-05-07 20:12:36,194][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1050
[2025-05-07 20:12:36,555][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0330, Metrics: {'mse': 0.03157101571559906, 'rmse': 0.17768234497439261, 'r2': 0.05013912916183472}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0450Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0548Epoch 4/15: [=                             ] 3/60 batches, loss: 0.0718Epoch 4/15: [==                            ] 4/60 batches, loss: 0.0926Epoch 4/15: [==                            ] 5/60 batches, loss: 0.0908Epoch 4/15: [===                           ] 6/60 batches, loss: 0.0939Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1024Epoch 4/15: [====                          ] 8/60 batches, loss: 0.0971Epoch 4/15: [====                          ] 9/60 batches, loss: 0.0934Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.0868Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.0916Epoch 4/15: [======                        ] 12/60 batches, loss: 0.0904Epoch 4/15: [======                        ] 13/60 batches, loss: 0.0912Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.0905Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.0896Epoch 4/15: [========                      ] 16/60 batches, loss: 0.0916Epoch 4/15: [========                      ] 17/60 batches, loss: 0.0948Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.0962Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.0958Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.0970Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.0985Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.0984Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.0987Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1020Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1001Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1019Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1003Epoch 4/15: [==============                ] 28/60 batches, loss: 0.0997Epoch 4/15: [==============                ] 29/60 batches, loss: 0.0993Epoch 4/15: [===============               ] 30/60 batches, loss: 0.0979Epoch 4/15: [===============               ] 31/60 batches, loss: 0.0977Epoch 4/15: [================              ] 32/60 batches, loss: 0.0978Epoch 4/15: [================              ] 33/60 batches, loss: 0.0966Epoch 4/15: [=================             ] 34/60 batches, loss: 0.0969Epoch 4/15: [=================             ] 35/60 batches, loss: 0.0961Epoch 4/15: [==================            ] 36/60 batches, loss: 0.0949Epoch 4/15: [==================            ] 37/60 batches, loss: 0.0940Epoch 4/15: [===================           ] 38/60 batches, loss: 0.0939Epoch 4/15: [===================           ] 39/60 batches, loss: 0.0938Epoch 4/15: [====================          ] 40/60 batches, loss: 0.0936Epoch 4/15: [====================          ] 41/60 batches, loss: 0.0930Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.0937Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.0947Epoch 4/15: [======================        ] 44/60 batches, loss: 0.0935Epoch 4/15: [======================        ] 45/60 batches, loss: 0.0928Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.0923Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.0908Epoch 4/15: [========================      ] 48/60 batches, loss: 0.0903Epoch 4/15: [========================      ] 49/60 batches, loss: 0.0896Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.0890Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.0892Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.0890Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.0887Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.0880Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.0881Epoch 4/15: [============================  ] 56/60 batches, loss: 0.0888Epoch 4/15: [============================  ] 57/60 batches, loss: 0.0881Epoch 4/15: [============================= ] 58/60 batches, loss: 0.0877Epoch 4/15: [============================= ] 59/60 batches, loss: 0.0873Epoch 4/15: [==============================] 60/60 batches, loss: 0.0865
[2025-05-07 20:12:38,848][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0865
[2025-05-07 20:12:39,214][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0262, Metrics: {'mse': 0.025531835854053497, 'rmse': 0.15978684505945256, 'r2': 0.23183679580688477}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.1231Epoch 5/15: [=                             ] 2/60 batches, loss: 0.1001Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0894Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0912Epoch 5/15: [==                            ] 5/60 batches, loss: 0.0894Epoch 5/15: [===                           ] 6/60 batches, loss: 0.0858Epoch 5/15: [===                           ] 7/60 batches, loss: 0.0873Epoch 5/15: [====                          ] 8/60 batches, loss: 0.0844Epoch 5/15: [====                          ] 9/60 batches, loss: 0.0845Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.0851Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.0860Epoch 5/15: [======                        ] 12/60 batches, loss: 0.0870Epoch 5/15: [======                        ] 13/60 batches, loss: 0.0897Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.0919Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.0886Epoch 5/15: [========                      ] 16/60 batches, loss: 0.0884Epoch 5/15: [========                      ] 17/60 batches, loss: 0.0892Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.0885Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.0887Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.0917Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.0896Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.0887Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.0896Epoch 5/15: [============                  ] 24/60 batches, loss: 0.0882Epoch 5/15: [============                  ] 25/60 batches, loss: 0.0883Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.0889Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.0881Epoch 5/15: [==============                ] 28/60 batches, loss: 0.0880Epoch 5/15: [==============                ] 29/60 batches, loss: 0.0908Epoch 5/15: [===============               ] 30/60 batches, loss: 0.0912Epoch 5/15: [===============               ] 31/60 batches, loss: 0.0921Epoch 5/15: [================              ] 32/60 batches, loss: 0.0909Epoch 5/15: [================              ] 33/60 batches, loss: 0.0902Epoch 5/15: [=================             ] 34/60 batches, loss: 0.0902Epoch 5/15: [=================             ] 35/60 batches, loss: 0.0915Epoch 5/15: [==================            ] 36/60 batches, loss: 0.0910Epoch 5/15: [==================            ] 37/60 batches, loss: 0.0909Epoch 5/15: [===================           ] 38/60 batches, loss: 0.0901Epoch 5/15: [===================           ] 39/60 batches, loss: 0.0890Epoch 5/15: [====================          ] 40/60 batches, loss: 0.0882Epoch 5/15: [====================          ] 41/60 batches, loss: 0.0869Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.0865Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.0871Epoch 5/15: [======================        ] 44/60 batches, loss: 0.0865Epoch 5/15: [======================        ] 45/60 batches, loss: 0.0861Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.0854Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0845Epoch 5/15: [========================      ] 48/60 batches, loss: 0.0851Epoch 5/15: [========================      ] 49/60 batches, loss: 0.0841Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.0837Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0831Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0824Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0818Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0813Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0806Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0799Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0794Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0800Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0790Epoch 5/15: [==============================] 60/60 batches, loss: 0.0786
[2025-05-07 20:12:41,540][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0786
[2025-05-07 20:12:41,867][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0231, Metrics: {'mse': 0.02234385348856449, 'rmse': 0.14947860545430738, 'r2': 0.32775193452835083}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.0544Epoch 6/15: [=                             ] 2/60 batches, loss: 0.0532Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0659Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0574Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0574Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0577Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0607Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0658Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0685Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0655Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0659Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0662Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0680Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0675Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0678Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0685Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0664Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0655Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0641Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0647Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0639Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0661Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0656Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0649Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0652Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0654Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0648Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0644Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0643Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0656Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0649Epoch 6/15: [================              ] 32/60 batches, loss: 0.0649Epoch 6/15: [================              ] 33/60 batches, loss: 0.0653Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0653Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0650Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0643Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0646Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0647Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0647Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0652Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0647Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0647Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0640Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0641Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0640Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0636Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0640Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0633Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0626Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0625Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0639Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0636Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0632Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0629Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0639Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0636Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0630Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0626Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0622Epoch 6/15: [==============================] 60/60 batches, loss: 0.0620
[2025-05-07 20:12:44,114][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0620
[2025-05-07 20:12:44,421][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0214, Metrics: {'mse': 0.020841924473643303, 'rmse': 0.1443673248129344, 'r2': 0.37293970584869385}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0358Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0321Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0525Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0569Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0609Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0631Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0643Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0633Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0638Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0586Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0560Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0557Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0564Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0591Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0589Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0583Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0563Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0553Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0552Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0549Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0546Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0548Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0578Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0590Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0575Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0566Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0568Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0570Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0582Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0579Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0585Epoch 7/15: [================              ] 32/60 batches, loss: 0.0577Epoch 7/15: [================              ] 33/60 batches, loss: 0.0585Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0581Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0575Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0573Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0570Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0564Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0568Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0571Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0568Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0580Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0571Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0563Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0564Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0569Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0566Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0569Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0579Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0577Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0580Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0580Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0580Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0574Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0574Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0568Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0569Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0564Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0565Epoch 7/15: [==============================] 60/60 batches, loss: 0.0572
[2025-05-07 20:12:46,739][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0572
[2025-05-07 20:12:47,047][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0207, Metrics: {'mse': 0.020132627338171005, 'rmse': 0.14188948987916972, 'r2': 0.39427995681762695}
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0692Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0663Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0693Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0631Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0584Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0577Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0555Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0594Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0569Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0566Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0547Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0534Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0511Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0528Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0533Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0530Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0531Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0515Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0521Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0517Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0520Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0538Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0536Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0536Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0527Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0516Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0520Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0518Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0515Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0519Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0511Epoch 8/15: [================              ] 32/60 batches, loss: 0.0500Epoch 8/15: [================              ] 33/60 batches, loss: 0.0500Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0501Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0499Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0496Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0505Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0497Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0501Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0496Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0508Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0508Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0503Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0503Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0502Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0498Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0497Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0502Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0510Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0511Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0517Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0519Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0518Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0515Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0519Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0525Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0526Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0535Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0530Epoch 8/15: [==============================] 60/60 batches, loss: 0.0526
[2025-05-07 20:12:49,347][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0526
[2025-05-07 20:12:49,665][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0195, Metrics: {'mse': 0.019147071987390518, 'rmse': 0.138372945286969, 'r2': 0.42393189668655396}
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.0432Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0375Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0372Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0338Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0384Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0389Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0414Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0427Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0415Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0412Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0452Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0442Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0434Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0455Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0459Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0470Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0462Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0468Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0461Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0453Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0447Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0449Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0456Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0452Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0462Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0457Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0469Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0480Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0474Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0477Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0484Epoch 9/15: [================              ] 32/60 batches, loss: 0.0478Epoch 9/15: [================              ] 33/60 batches, loss: 0.0475Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0471Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0473Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0480Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0477Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0471Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0473Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0474Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0482Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0490Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0483Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0481Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0478Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0486Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0485Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0482Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0484Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0481Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0480Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0478Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0476Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0473Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0470Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0469Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0467Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0471Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0471Epoch 9/15: [==============================] 60/60 batches, loss: 0.0467
[2025-05-07 20:12:51,940][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0467
[2025-05-07 20:12:52,260][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0189, Metrics: {'mse': 0.01861976832151413, 'rmse': 0.13645427190643072, 'r2': 0.4397965669631958}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0407Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0424Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0443Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0412Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0409Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0370Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0366Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0380Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0412Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0449Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0432Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0417Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0454Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0455Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0457Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0467Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0452Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0449Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0451Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0460Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0450Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0456Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0459Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0460Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0470Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0464Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0458Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0455Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0462Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0458Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0456Epoch 10/15: [================              ] 32/60 batches, loss: 0.0471Epoch 10/15: [================              ] 33/60 batches, loss: 0.0470Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0471Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0470Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0464Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0462Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0457Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0458Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0452Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0458Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0458Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0451Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0450Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0448Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0444Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0441Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0440Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0439Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0439Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0437Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0436Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0432Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0429Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0427Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0426Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0430Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0433Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0434Epoch 10/15: [==============================] 60/60 batches, loss: 0.0439
[2025-05-07 20:12:54,636][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0439
[2025-05-07 20:12:54,953][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0187, Metrics: {'mse': 0.018493805080652237, 'rmse': 0.13599193020415673, 'r2': 0.44358640909194946}
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0607Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0565Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0456Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0410Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0418Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0423Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0435Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0439Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0438Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0418Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0414Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0445Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0431Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0427Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0414Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0411Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0414Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0452Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0451Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0446Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0439Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0434Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0433Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0425Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0421Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0426Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0423Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0420Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0419Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0419Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0417Epoch 11/15: [================              ] 32/60 batches, loss: 0.0418Epoch 11/15: [================              ] 33/60 batches, loss: 0.0418Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0422Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0423Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0421Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0420Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0415Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0414Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0416Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0420Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0420Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0417Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0415Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0413Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0414Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0413Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0413Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0417Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0411Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0409Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0412Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0412Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0415Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0413Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0417Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0414Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0412Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0420Epoch 11/15: [==============================] 60/60 batches, loss: 0.0418
[2025-05-07 20:12:57,251][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0418
[2025-05-07 20:12:57,634][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0173, Metrics: {'mse': 0.01711234450340271, 'rmse': 0.1308141601792509, 'r2': 0.48514968156814575}
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0390Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0407Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0402Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0421Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0402Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0409Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0376Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0392Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0408Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0404Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0402Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0418Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0407Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0406Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0426Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0431Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0422Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0430Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0455Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0454Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0443Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0434Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0423Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0432Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0444Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0443Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0437Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0433Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0453Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0449Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0453Epoch 12/15: [================              ] 32/60 batches, loss: 0.0452Epoch 12/15: [================              ] 33/60 batches, loss: 0.0448Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0452Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0451Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0444Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0442Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0443Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0444Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0438Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0445Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0445Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0450Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0455Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0456Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0451Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0446Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0441Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0439Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0435Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0438Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0431Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0430Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0428Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0428Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0427Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0426Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0433Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0433Epoch 12/15: [==============================] 60/60 batches, loss: 0.0431
[2025-05-07 20:12:59,988][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0431
[2025-05-07 20:13:00,359][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0172, Metrics: {'mse': 0.017020920291543007, 'rmse': 0.13046424909354673, 'r2': 0.4879003167152405}
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0368Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0410Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0348Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0308Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0291Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0270Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0284Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0289Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0305Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0293Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0296Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0301Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0309Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0322Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0335Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0329Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0337Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0331Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0328Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0340Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0341Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0355Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0357Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0351Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0356Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0348Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0353Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0354Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0352Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0347Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0341Epoch 13/15: [================              ] 32/60 batches, loss: 0.0336Epoch 13/15: [================              ] 33/60 batches, loss: 0.0341Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0342Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0338Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0333Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0341Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0342Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0345Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0340Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0349Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0345Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0343Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0344Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0346Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0349Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0348Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0347Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0346Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0347Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0351Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0357Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0357Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0356Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0357Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0355Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0356Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0360Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0359Epoch 13/15: [==============================] 60/60 batches, loss: 0.0359
[2025-05-07 20:13:02,614][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0359
[2025-05-07 20:13:02,946][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0192, Metrics: {'mse': 0.018980802968144417, 'rmse': 0.13777083496932294, 'r2': 0.42893433570861816}
[2025-05-07 20:13:02,947][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0193Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0240Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0244Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0296Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0309Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0302Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0319Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0359Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0338Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0351Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0342Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0345Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0360Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0367Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0354Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0350Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0345Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0348Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0352Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0359Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0367Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0364Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0361Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0354Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0358Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0350Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0357Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0351Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0352Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0355Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0360Epoch 14/15: [================              ] 32/60 batches, loss: 0.0356Epoch 14/15: [================              ] 33/60 batches, loss: 0.0354Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0354Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0352Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0354Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0360Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0357Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0362Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0360Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0355Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0356Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0352Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0358Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0356Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0357Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0357Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0354Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0361Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0365Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0367Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0364Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0360Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0361Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0359Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0356Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0355Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0352Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0348Epoch 14/15: [==============================] 60/60 batches, loss: 0.0346
[2025-05-07 20:13:04,892][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0346
[2025-05-07 20:13:05,231][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0160, Metrics: {'mse': 0.015846114605665207, 'rmse': 0.12588135130218933, 'r2': 0.5232460498809814}
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0254Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0207Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0248Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0216Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0262Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0262Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0281Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0304Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0311Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0309Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0320Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0324Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0317Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0321Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0316Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0319Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0316Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0331Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0330Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0325Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0320Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0314Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0316Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0318Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0319Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0326Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0324Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0318Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0316Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0317Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0316Epoch 15/15: [================              ] 32/60 batches, loss: 0.0322Epoch 15/15: [================              ] 33/60 batches, loss: 0.0321Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0321Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0319Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0318Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0316Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0313Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0312Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0311Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0308Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0307Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0306Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0304Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0307Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0312Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0308Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0305Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0303Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0305Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0308Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0310Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0311Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0309Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0313Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0314Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0314Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0316Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0319Epoch 15/15: [==============================] 60/60 batches, loss: 0.0317
[2025-05-07 20:13:07,547][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0317
[2025-05-07 20:13:07,873][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0175, Metrics: {'mse': 0.01718324050307274, 'rmse': 0.131084859930782, 'r2': 0.4830166697502136}
[2025-05-07 20:13:07,874][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-07 20:13:07,874][src.training.lm_trainer][INFO] - Training completed in 39.74 seconds
[2025-05-07 20:13:07,874][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:13:10,529][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.014924537390470505, 'rmse': 0.12216602387927056, 'r2': 0.5338941216468811}
[2025-05-07 20:13:10,530][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.015846114605665207, 'rmse': 0.12588135130218933, 'r2': 0.5232460498809814}
[2025-05-07 20:13:10,530][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.026748938485980034, 'rmse': 0.16355102716271774, 'r2': 0.09449851512908936}
[2025-05-07 20:13:12,674][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/layer2/id/id/model.pt
[2025-05-07 20:13:12,676][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▆▄▃▂▂▂▂▂▁▁▁
wandb:     best_val_mse █▇▆▄▃▂▂▂▂▂▁▁▁
wandb:      best_val_r2 ▁▂▃▅▆▇▇▇▇▇███
wandb:    best_val_rmse █▇▆▄▃▃▃▂▂▂▁▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▄▅▆▆▆▆▇▇▇▇▇▇
wandb:       train_loss █▄▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▆▄▃▂▂▂▂▂▁▁▂▁▁
wandb:          val_mse █▇▆▄▃▂▂▂▂▂▁▁▂▁▁
wandb:           val_r2 ▁▂▃▅▆▇▇▇▇▇██▇██
wandb:         val_rmse █▇▆▄▃▃▃▂▂▂▁▁▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01598
wandb:     best_val_mse 0.01585
wandb:      best_val_r2 0.52325
wandb:    best_val_rmse 0.12588
wandb:            epoch 15
wandb:   final_test_mse 0.02675
wandb:    final_test_r2 0.0945
wandb:  final_test_rmse 0.16355
wandb:  final_train_mse 0.01492
wandb:   final_train_r2 0.53389
wandb: final_train_rmse 0.12217
wandb:    final_val_mse 0.01585
wandb:     final_val_r2 0.52325
wandb:   final_val_rmse 0.12588
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03166
wandb:       train_time 39.74129
wandb:         val_loss 0.01748
wandb:          val_mse 0.01718
wandb:           val_r2 0.48302
wandb:         val_rmse 0.13108
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_201206-kkt3acar
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_201206-kkt3acar/logs
Experiment probe_layer2_n_tokens_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/layer2/id/id/results.json for layer 2
Running control submetric probing experiments...
=======================
PROBING LAYER 2 (SUBMETRIC CONTROL EXPERIMENTS)
=======================
Running experiment: probe_layer2_avg_links_len_control1_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_control1_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:13:47,695][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/id
experiment_name: probe_layer2_avg_links_len_control1_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:13:47,695][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:13:47,695][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 20:13:47,695][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:13:47,695][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:13:47,744][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:13:47,744][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 20:13:47,744][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:13:51,907][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:13:54,470][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:13:54,471][src.data.datasets][INFO] - Loading 'control_avg_links_len_seed1' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:13:54,785][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_links_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:14:19 2025).
[2025-05-07 20:13:54,922][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_links_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:14:19 2025).
[2025-05-07 20:13:55,318][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:13:55,325][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:13:55,325][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:13:55,329][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:13:55,435][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:13:55,565][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:13:55,605][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:13:55,606][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:13:55,606][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:13:55,610][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:13:55,751][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:13:55,870][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:13:55,902][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:13:55,903][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:13:55,903][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:13:55,907][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:13:55,908][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:13:55,908][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:13:55,908][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:13:55,908][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:13:55,908][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:13:55,908][src.data.datasets][INFO] -   Mean: 0.1820, Std: 0.1352
[2025-05-07 20:13:55,908][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:13:55,908][src.data.datasets][INFO] - Sample label: 0.12700000405311584
[2025-05-07 20:13:55,909][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:13:55,909][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:13:55,909][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:13:55,909][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:13:55,909][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9170
[2025-05-07 20:13:55,909][src.data.datasets][INFO] -   Mean: 0.2802, Std: 0.1921
[2025-05-07 20:13:55,909][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:13:55,909][src.data.datasets][INFO] - Sample label: 0.1289999932050705
[2025-05-07 20:13:55,909][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:13:55,910][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:13:55,910][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:13:55,910][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:13:55,910][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7260
[2025-05-07 20:13:55,910][src.data.datasets][INFO] -   Mean: 0.2993, Std: 0.1490
[2025-05-07 20:13:55,910][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:13:55,910][src.data.datasets][INFO] - Sample label: 0.22599999606609344
[2025-05-07 20:13:55,910][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:13:55,910][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:13:55,911][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:13:55,911][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 20:13:55,911][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:14:04,068][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:14:04,069][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:14:04,069][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:14:04,069][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:14:04,072][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:14:04,073][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:14:04,073][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:14:04,073][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:14:04,073][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:14:04,074][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:14:04,074][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.3467Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4117Epoch 1/15: [=                             ] 3/60 batches, loss: 0.4667Epoch 1/15: [==                            ] 4/60 batches, loss: 0.4423Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4126Epoch 1/15: [===                           ] 6/60 batches, loss: 0.3812Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4262Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4295Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4381Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4355Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4198Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4239Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4128Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4058Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4014Epoch 1/15: [========                      ] 16/60 batches, loss: 0.3990Epoch 1/15: [========                      ] 17/60 batches, loss: 0.3926Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.3995Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.3873Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.3789Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.3728Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.3730Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.3640Epoch 1/15: [============                  ] 24/60 batches, loss: 0.3572Epoch 1/15: [============                  ] 25/60 batches, loss: 0.3543Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.3491Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3434Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3414Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3442Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3473Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3421Epoch 1/15: [================              ] 32/60 batches, loss: 0.3378Epoch 1/15: [================              ] 33/60 batches, loss: 0.3331Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3346Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3301Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3285Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3225Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3175Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3141Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3120Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3111Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3110Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3070Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3062Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3049Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3012Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3016Epoch 1/15: [========================      ] 48/60 batches, loss: 0.2973Epoch 1/15: [========================      ] 49/60 batches, loss: 0.2963Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.2936Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.2922Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.2907Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.2884Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.2877Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.2859Epoch 1/15: [============================  ] 56/60 batches, loss: 0.2879Epoch 1/15: [============================  ] 57/60 batches, loss: 0.2892Epoch 1/15: [============================= ] 58/60 batches, loss: 0.2882Epoch 1/15: [============================= ] 59/60 batches, loss: 0.2858Epoch 1/15: [==============================] 60/60 batches, loss: 0.2833
[2025-05-07 20:14:09,982][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2833
[2025-05-07 20:14:10,239][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1631, Metrics: {'mse': 0.15164834260940552, 'rmse': 0.38942052155658863, 'r2': -3.109524726867676}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.3163Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2259Epoch 2/15: [=                             ] 3/60 batches, loss: 0.1911Epoch 2/15: [==                            ] 4/60 batches, loss: 0.1872Epoch 2/15: [==                            ] 5/60 batches, loss: 0.1652Epoch 2/15: [===                           ] 6/60 batches, loss: 0.1887Epoch 2/15: [===                           ] 7/60 batches, loss: 0.1941Epoch 2/15: [====                          ] 8/60 batches, loss: 0.1957Epoch 2/15: [====                          ] 9/60 batches, loss: 0.1970Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.1922Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.1868Epoch 2/15: [======                        ] 12/60 batches, loss: 0.1879Epoch 2/15: [======                        ] 13/60 batches, loss: 0.1955Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.1948Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.1869Epoch 2/15: [========                      ] 16/60 batches, loss: 0.1885Epoch 2/15: [========                      ] 17/60 batches, loss: 0.1866Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.1822Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.1820Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1844Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1805Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1809Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1783Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1750Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1780Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1756Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1735Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1729Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1692Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1688Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1677Epoch 2/15: [================              ] 32/60 batches, loss: 0.1644Epoch 2/15: [================              ] 33/60 batches, loss: 0.1622Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1636Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1652Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1641Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1635Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1632Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1621Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1646Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1655Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1643Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1638Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1634Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1622Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1605Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1583Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1578Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1581Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1580Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1561Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1554Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1557Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1543Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1541Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1530Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1528Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1522Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1508Epoch 2/15: [==============================] 60/60 batches, loss: 0.1492
[2025-05-07 20:14:12,520][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1492
[2025-05-07 20:14:12,781][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1278, Metrics: {'mse': 0.11765122413635254, 'rmse': 0.34300324216594885, 'r2': -2.188235282897949}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.0735Epoch 3/15: [=                             ] 2/60 batches, loss: 0.0825Epoch 3/15: [=                             ] 3/60 batches, loss: 0.0868Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1055Epoch 3/15: [==                            ] 5/60 batches, loss: 0.0996Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1072Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1122Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1176Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1134Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1107Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1084Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1100Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1137Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1118Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1092Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1089Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1080Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1057Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1032Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1019Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1013Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.0994Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.0980Epoch 3/15: [============                  ] 24/60 batches, loss: 0.0996Epoch 3/15: [============                  ] 25/60 batches, loss: 0.0991Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1014Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.0992Epoch 3/15: [==============                ] 28/60 batches, loss: 0.0981Epoch 3/15: [==============                ] 29/60 batches, loss: 0.0997Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1002Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1024Epoch 3/15: [================              ] 32/60 batches, loss: 0.1018Epoch 3/15: [================              ] 33/60 batches, loss: 0.1045Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1033Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1022Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1024Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1019Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1018Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1023Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1015Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1018Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1021Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1034Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1034Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1024Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1015Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1023Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1014Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1021Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1019Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1019Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1014Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1003Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1018Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1013Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1004Epoch 3/15: [============================  ] 57/60 batches, loss: 0.0998Epoch 3/15: [============================= ] 58/60 batches, loss: 0.0990Epoch 3/15: [============================= ] 59/60 batches, loss: 0.0987Epoch 3/15: [==============================] 60/60 batches, loss: 0.1002
[2025-05-07 20:14:15,108][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1002
[2025-05-07 20:14:15,382][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1492, Metrics: {'mse': 0.13793686032295227, 'rmse': 0.37139851954868136, 'r2': -2.7379565238952637}
[2025-05-07 20:14:15,382][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0276Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0509Epoch 4/15: [=                             ] 3/60 batches, loss: 0.0716Epoch 4/15: [==                            ] 4/60 batches, loss: 0.0984Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1003Epoch 4/15: [===                           ] 6/60 batches, loss: 0.0971Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1022Epoch 4/15: [====                          ] 8/60 batches, loss: 0.0991Epoch 4/15: [====                          ] 9/60 batches, loss: 0.0936Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.0896Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.0893Epoch 4/15: [======                        ] 12/60 batches, loss: 0.0888Epoch 4/15: [======                        ] 13/60 batches, loss: 0.0875Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.0883Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.0864Epoch 4/15: [========                      ] 16/60 batches, loss: 0.0882Epoch 4/15: [========                      ] 17/60 batches, loss: 0.0936Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.0918Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.0895Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.0901Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.0928Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.0979Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.0965Epoch 4/15: [============                  ] 24/60 batches, loss: 0.0990Epoch 4/15: [============                  ] 25/60 batches, loss: 0.0968Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.0982Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.0970Epoch 4/15: [==============                ] 28/60 batches, loss: 0.0959Epoch 4/15: [==============                ] 29/60 batches, loss: 0.0943Epoch 4/15: [===============               ] 30/60 batches, loss: 0.0928Epoch 4/15: [===============               ] 31/60 batches, loss: 0.0922Epoch 4/15: [================              ] 32/60 batches, loss: 0.0906Epoch 4/15: [================              ] 33/60 batches, loss: 0.0910Epoch 4/15: [=================             ] 34/60 batches, loss: 0.0897Epoch 4/15: [=================             ] 35/60 batches, loss: 0.0892Epoch 4/15: [==================            ] 36/60 batches, loss: 0.0892Epoch 4/15: [==================            ] 37/60 batches, loss: 0.0888Epoch 4/15: [===================           ] 38/60 batches, loss: 0.0887Epoch 4/15: [===================           ] 39/60 batches, loss: 0.0878Epoch 4/15: [====================          ] 40/60 batches, loss: 0.0867Epoch 4/15: [====================          ] 41/60 batches, loss: 0.0867Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.0875Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.0869Epoch 4/15: [======================        ] 44/60 batches, loss: 0.0859Epoch 4/15: [======================        ] 45/60 batches, loss: 0.0860Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.0853Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.0846Epoch 4/15: [========================      ] 48/60 batches, loss: 0.0843Epoch 4/15: [========================      ] 49/60 batches, loss: 0.0842Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.0841Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.0841Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.0840Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.0836Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.0838Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.0838Epoch 4/15: [============================  ] 56/60 batches, loss: 0.0832Epoch 4/15: [============================  ] 57/60 batches, loss: 0.0825Epoch 4/15: [============================= ] 58/60 batches, loss: 0.0820Epoch 4/15: [============================= ] 59/60 batches, loss: 0.0816Epoch 4/15: [==============================] 60/60 batches, loss: 0.0810
[2025-05-07 20:14:17,289][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0810
[2025-05-07 20:14:17,603][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1208, Metrics: {'mse': 0.11123339086771011, 'rmse': 0.3335167025318374, 'r2': -2.0143182277679443}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.0448Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0591Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0647Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0615Epoch 5/15: [==                            ] 5/60 batches, loss: 0.0805Epoch 5/15: [===                           ] 6/60 batches, loss: 0.0807Epoch 5/15: [===                           ] 7/60 batches, loss: 0.0817Epoch 5/15: [====                          ] 8/60 batches, loss: 0.0859Epoch 5/15: [====                          ] 9/60 batches, loss: 0.0825Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.0824Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.0819Epoch 5/15: [======                        ] 12/60 batches, loss: 0.0826Epoch 5/15: [======                        ] 13/60 batches, loss: 0.0833Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.0879Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.0854Epoch 5/15: [========                      ] 16/60 batches, loss: 0.0856Epoch 5/15: [========                      ] 17/60 batches, loss: 0.0851Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.0848Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.0837Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.0847Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.0857Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.0853Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.0856Epoch 5/15: [============                  ] 24/60 batches, loss: 0.0850Epoch 5/15: [============                  ] 25/60 batches, loss: 0.0836Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.0842Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.0822Epoch 5/15: [==============                ] 28/60 batches, loss: 0.0811Epoch 5/15: [==============                ] 29/60 batches, loss: 0.0823Epoch 5/15: [===============               ] 30/60 batches, loss: 0.0827Epoch 5/15: [===============               ] 31/60 batches, loss: 0.0842Epoch 5/15: [================              ] 32/60 batches, loss: 0.0843Epoch 5/15: [================              ] 33/60 batches, loss: 0.0838Epoch 5/15: [=================             ] 34/60 batches, loss: 0.0826Epoch 5/15: [=================             ] 35/60 batches, loss: 0.0826Epoch 5/15: [==================            ] 36/60 batches, loss: 0.0823Epoch 5/15: [==================            ] 37/60 batches, loss: 0.0820Epoch 5/15: [===================           ] 38/60 batches, loss: 0.0813Epoch 5/15: [===================           ] 39/60 batches, loss: 0.0811Epoch 5/15: [====================          ] 40/60 batches, loss: 0.0799Epoch 5/15: [====================          ] 41/60 batches, loss: 0.0791Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.0793Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.0797Epoch 5/15: [======================        ] 44/60 batches, loss: 0.0786Epoch 5/15: [======================        ] 45/60 batches, loss: 0.0785Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.0790Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0788Epoch 5/15: [========================      ] 48/60 batches, loss: 0.0788Epoch 5/15: [========================      ] 49/60 batches, loss: 0.0779Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.0780Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0775Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0775Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0773Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0766Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0759Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0752Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0749Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0749Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0745Epoch 5/15: [==============================] 60/60 batches, loss: 0.0736
[2025-05-07 20:14:19,940][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0736
[2025-05-07 20:14:20,277][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1020, Metrics: {'mse': 0.09329000860452652, 'rmse': 0.305434131368003, 'r2': -1.5280697345733643}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.0858Epoch 6/15: [=                             ] 2/60 batches, loss: 0.0749Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0767Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0689Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0626Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0590Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0611Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0736Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0732Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0695Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0683Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0673Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0664Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0675Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0657Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0667Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0660Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0644Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0626Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0611Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0604Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0605Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0604Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0608Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0604Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0613Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0613Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0604Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0601Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0594Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0588Epoch 6/15: [================              ] 32/60 batches, loss: 0.0586Epoch 6/15: [================              ] 33/60 batches, loss: 0.0603Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0604Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0595Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0624Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0617Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0612Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0607Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0605Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0599Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0599Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0591Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0594Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0590Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0591Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0591Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0596Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0590Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0587Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0602Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0598Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0594Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0595Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0599Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0595Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0599Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0595Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0591Epoch 6/15: [==============================] 60/60 batches, loss: 0.0589
[2025-05-07 20:14:22,567][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0589
[2025-05-07 20:14:22,947][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0997, Metrics: {'mse': 0.0912037193775177, 'rmse': 0.3019995353928838, 'r2': -1.4715332984924316}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0224Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0473Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0494Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0475Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0524Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0485Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0500Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0503Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0497Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0471Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0494Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0498Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0545Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0552Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0555Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0555Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0554Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0559Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0564Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0558Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0541Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0544Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0552Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0562Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0581Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0582Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0575Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0561Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0559Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0550Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0547Epoch 7/15: [================              ] 32/60 batches, loss: 0.0541Epoch 7/15: [================              ] 33/60 batches, loss: 0.0539Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0532Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0528Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0526Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0517Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0516Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0509Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0512Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0513Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0532Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0529Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0541Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0538Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0542Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0539Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0538Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0545Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0541Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0545Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0555Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0552Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0553Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0553Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0554Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0554Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0552Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0556Epoch 7/15: [==============================] 60/60 batches, loss: 0.0563
[2025-05-07 20:14:25,279][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0563
[2025-05-07 20:14:25,637][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1065, Metrics: {'mse': 0.09788328409194946, 'rmse': 0.31286304366599366, 'r2': -1.652543067932129}
[2025-05-07 20:14:25,638][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0568Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0554Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0526Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0478Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0477Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0509Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0471Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0447Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0438Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0444Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0429Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0465Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0458Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0445Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0443Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0434Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0463Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0458Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0453Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0451Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0441Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0437Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0434Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0441Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0444Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0434Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0443Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0460Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0461Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0463Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0457Epoch 8/15: [================              ] 32/60 batches, loss: 0.0454Epoch 8/15: [================              ] 33/60 batches, loss: 0.0453Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0452Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0448Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0448Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0451Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0453Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0461Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0459Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0464Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0456Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0454Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0459Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0457Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0459Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0456Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0472Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0472Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0469Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0467Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0467Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0466Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0466Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0465Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0464Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0461Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0469Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0466Epoch 8/15: [==============================] 60/60 batches, loss: 0.0463
[2025-05-07 20:14:27,603][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0463
[2025-05-07 20:14:27,965][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0981, Metrics: {'mse': 0.0901009663939476, 'rmse': 0.3001682301542713, 'r2': -1.4416496753692627}
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.0811Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0581Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0460Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0399Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0595Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0550Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0522Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0500Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0477Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0449Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0463Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0462Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0461Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0481Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0497Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0477Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0475Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0496Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0489Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0480Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0477Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0482Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0483Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0478Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0477Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0482Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0478Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0484Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0484Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0478Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0470Epoch 9/15: [================              ] 32/60 batches, loss: 0.0465Epoch 9/15: [================              ] 33/60 batches, loss: 0.0458Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0456Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0456Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0455Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0454Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0451Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0452Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0454Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0451Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0446Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0445Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0444Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0448Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0443Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0443Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0446Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0454Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0461Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0463Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0468Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0465Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0463Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0460Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0461Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0458Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0463Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0461Epoch 9/15: [==============================] 60/60 batches, loss: 0.0456
[2025-05-07 20:14:30,268][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0456
[2025-05-07 20:14:30,596][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0743, Metrics: {'mse': 0.06758707016706467, 'rmse': 0.2599751337475657, 'r2': -0.8315447568893433}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0525Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0491Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0538Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0621Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0642Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0599Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0578Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0585Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0584Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0550Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0511Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0483Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0509Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0499Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0487Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0466Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0455Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0454Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0458Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0450Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0447Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0437Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0432Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0441Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0433Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0436Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0445Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0449Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0442Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0440Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0441Epoch 10/15: [================              ] 32/60 batches, loss: 0.0438Epoch 10/15: [================              ] 33/60 batches, loss: 0.0446Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0444Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0442Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0451Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0447Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0443Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0440Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0440Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0436Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0433Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0429Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0423Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0419Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0417Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0422Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0420Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0418Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0419Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0415Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0413Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0412Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0409Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0408Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0412Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0415Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0414Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0413Epoch 10/15: [==============================] 60/60 batches, loss: 0.0414
[2025-05-07 20:14:32,849][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0414
[2025-05-07 20:14:33,185][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0716, Metrics: {'mse': 0.06520052999258041, 'rmse': 0.2553439444995327, 'r2': -0.7668716907501221}
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0273Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0498Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0484Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0465Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0446Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0428Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0449Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0432Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0434Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0417Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0407Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0409Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0393Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0402Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0419Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0409Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0394Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0398Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0402Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0401Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0393Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0394Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0391Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0388Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0389Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0387Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0382Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0379Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0381Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0387Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0387Epoch 11/15: [================              ] 32/60 batches, loss: 0.0381Epoch 11/15: [================              ] 33/60 batches, loss: 0.0385Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0383Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0378Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0377Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0377Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0373Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0375Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0382Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0395Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0392Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0390Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0392Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0390Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0387Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0386Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0383Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0380Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0383Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0380Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0380Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0379Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0386Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0386Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0389Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0388Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0387Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0388Epoch 11/15: [==============================] 60/60 batches, loss: 0.0383
[2025-05-07 20:14:35,445][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0383
[2025-05-07 20:14:35,832][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0881, Metrics: {'mse': 0.08102533221244812, 'rmse': 0.2846494900969403, 'r2': -1.1957087516784668}
[2025-05-07 20:14:35,833][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0206Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0478Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0364Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0408Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0389Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0358Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0372Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0356Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0342Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0338Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0353Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0354Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0340Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0338Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0351Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0384Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0374Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0377Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0377Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0379Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0366Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0374Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0366Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0364Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0374Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0372Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0367Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0367Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0365Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0370Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0370Epoch 12/15: [================              ] 32/60 batches, loss: 0.0374Epoch 12/15: [================              ] 33/60 batches, loss: 0.0379Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0389Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0390Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0385Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0380Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0379Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0379Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0383Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0390Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0387Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0385Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0386Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0382Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0382Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0386Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0388Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0383Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0380Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0381Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0384Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0381Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0388Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0384Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0379Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0379Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0377Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0373Epoch 12/15: [==============================] 60/60 batches, loss: 0.0382
[2025-05-07 20:14:37,786][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0382
[2025-05-07 20:14:38,060][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0744, Metrics: {'mse': 0.06813541054725647, 'rmse': 0.26102760495253463, 'r2': -0.8464043140411377}
[2025-05-07 20:14:38,061][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0395Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0411Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0383Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0372Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0409Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0388Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0364Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0344Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0339Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0324Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0312Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0313Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0324Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0332Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0335Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0335Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0340Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0338Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0330Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0329Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0324Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0326Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0333Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0333Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0329Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0321Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0322Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0324Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0326Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0325Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0323Epoch 13/15: [================              ] 32/60 batches, loss: 0.0322Epoch 13/15: [================              ] 33/60 batches, loss: 0.0322Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0325Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0326Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0326Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0328Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0324Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0320Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0316Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0331Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0340Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0344Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0344Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0343Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0350Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0350Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0351Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0350Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0350Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0350Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0351Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0349Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0346Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0346Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0345Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0345Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0343Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0340Epoch 13/15: [==============================] 60/60 batches, loss: 0.0338
[2025-05-07 20:14:39,965][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0338
[2025-05-07 20:14:40,285][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0824, Metrics: {'mse': 0.07567749917507172, 'rmse': 0.2750954364853618, 'r2': -1.0507876873016357}
[2025-05-07 20:14:40,286][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0191Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0302Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0323Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0311Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0298Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0368Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0359Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0350Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0380Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0374Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0385Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0372Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0370Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0382Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0374Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0367Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0362Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0358Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0355Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0352Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0343Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0343Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0341Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0334Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0332Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0324Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0331Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0327Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0323Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0326Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0321Epoch 14/15: [================              ] 32/60 batches, loss: 0.0317Epoch 14/15: [================              ] 33/60 batches, loss: 0.0317Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0312Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0309Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0310Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0323Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0323Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0331Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0331Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0331Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0330Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0329Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0330Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0328Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0329Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0326Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0330Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0334Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0332Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0330Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0331Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0332Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0329Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0328Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0328Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0327Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0326Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0327Epoch 14/15: [==============================] 60/60 batches, loss: 0.0324
[2025-05-07 20:14:42,223][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0324
[2025-05-07 20:14:42,599][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0706, Metrics: {'mse': 0.06462165713310242, 'rmse': 0.25420790139785665, 'r2': -0.7511849403381348}
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0357Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0366Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0351Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0321Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0302Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0341Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0326Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0331Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0325Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0345Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0339Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0337Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0327Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0320Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0317Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0306Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0304Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0301Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0318Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0311Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0311Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0303Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0304Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0316Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0314Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0309Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0310Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0307Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0307Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0302Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0298Epoch 15/15: [================              ] 32/60 batches, loss: 0.0292Epoch 15/15: [================              ] 33/60 batches, loss: 0.0296Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0291Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0290Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0289Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0284Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0290Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0296Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0305Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0300Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0309Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0309Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0307Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0303Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0320Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0321Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0318Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0315Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0312Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0313Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0311Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0311Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0308Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0305Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0307Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0309Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0307Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0307Epoch 15/15: [==============================] 60/60 batches, loss: 0.0305
[2025-05-07 20:14:44,937][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0305
[2025-05-07 20:14:45,294][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0862, Metrics: {'mse': 0.07935410737991333, 'rmse': 0.2816986108945398, 'r2': -1.1504201889038086}
[2025-05-07 20:14:45,294][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-07 20:14:45,295][src.training.lm_trainer][INFO] - Training completed in 38.08 seconds
[2025-05-07 20:14:45,295][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:14:47,958][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.02041933313012123, 'rmse': 0.14289623203612203, 'r2': -0.11782658100128174}
[2025-05-07 20:14:47,958][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06462165713310242, 'rmse': 0.25420790139785665, 'r2': -0.7511849403381348}
[2025-05-07 20:14:47,958][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.05507507175207138, 'rmse': 0.23468078692571187, 'r2': -1.480769157409668}
[2025-05-07 20:14:49,758][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/id/id/model.pt
[2025-05-07 20:14:49,759][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▅▃▃▃▁▁▁
wandb:     best_val_mse █▅▅▃▃▃▁▁▁
wandb:      best_val_r2 ▁▄▄▆▆▆███
wandb:    best_val_rmse █▆▅▄▃▃▁▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▄▂▄▅▅▅▅▇▇▆▇▆▇
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▇▅▃▃▄▃▁▁▂▁▂▁▂
wandb:          val_mse █▅▇▅▃▃▄▃▁▁▂▁▂▁▂
wandb:           val_r2 ▁▄▂▄▆▆▅▆██▇█▇█▇
wandb:         val_rmse █▆▇▅▄▃▄▃▁▁▃▁▂▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07058
wandb:     best_val_mse 0.06462
wandb:      best_val_r2 -0.75118
wandb:    best_val_rmse 0.25421
wandb:            epoch 15
wandb:   final_test_mse 0.05508
wandb:    final_test_r2 -1.48077
wandb:  final_test_rmse 0.23468
wandb:  final_train_mse 0.02042
wandb:   final_train_r2 -0.11783
wandb: final_train_rmse 0.1429
wandb:    final_val_mse 0.06462
wandb:     final_val_r2 -0.75118
wandb:   final_val_rmse 0.25421
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03051
wandb:       train_time 38.07727
wandb:         val_loss 0.0862
wandb:          val_mse 0.07935
wandb:           val_r2 -1.15042
wandb:         val_rmse 0.2817
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_201347-ckkech4r
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_201347-ckkech4r/logs
Experiment probe_layer2_avg_links_len_control1_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_avg_links_len_control2_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_control2_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:15:24,112][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/id
experiment_name: probe_layer2_avg_links_len_control2_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:15:24,113][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:15:24,113][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 20:15:24,113][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:15:24,113][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:15:24,117][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:15:24,117][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 20:15:24,117][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:15:27,719][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:15:30,176][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:15:30,177][src.data.datasets][INFO] - Loading 'control_avg_links_len_seed2' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:15:30,463][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_links_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:16:02 2025).
[2025-05-07 20:15:30,602][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_links_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:16:02 2025).
[2025-05-07 20:15:30,971][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:15:30,978][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:15:30,979][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:15:30,981][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:15:31,087][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:15:31,196][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:15:31,255][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:15:31,256][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:15:31,256][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:15:31,261][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:15:31,379][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:15:31,528][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:15:31,585][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:15:31,586][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:15:31,587][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:15:31,590][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:15:31,590][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:15:31,591][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:15:31,591][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:15:31,591][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:15:31,591][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:15:31,591][src.data.datasets][INFO] -   Mean: 0.1820, Std: 0.1352
[2025-05-07 20:15:31,591][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:15:31,591][src.data.datasets][INFO] - Sample label: 0.04600000008940697
[2025-05-07 20:15:31,591][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:15:31,592][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:15:31,592][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:15:31,592][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:15:31,592][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9170
[2025-05-07 20:15:31,592][src.data.datasets][INFO] -   Mean: 0.2802, Std: 0.1921
[2025-05-07 20:15:31,592][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:15:31,592][src.data.datasets][INFO] - Sample label: 0.1289999932050705
[2025-05-07 20:15:31,592][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:15:31,592][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:15:31,592][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:15:31,593][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:15:31,593][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7260
[2025-05-07 20:15:31,593][src.data.datasets][INFO] -   Mean: 0.2993, Std: 0.1490
[2025-05-07 20:15:31,593][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:15:31,593][src.data.datasets][INFO] - Sample label: 0.22599999606609344
[2025-05-07 20:15:31,593][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:15:31,593][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:15:31,593][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:15:31,593][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 20:15:31,594][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:15:40,407][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:15:40,408][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:15:40,408][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:15:40,408][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:15:40,411][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:15:40,412][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:15:40,412][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:15:40,412][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:15:40,412][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:15:40,413][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:15:40,413][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.3566Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4304Epoch 1/15: [=                             ] 3/60 batches, loss: 0.4726Epoch 1/15: [==                            ] 4/60 batches, loss: 0.4574Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4381Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4141Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4402Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4424Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4487Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4470Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4350Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4352Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4216Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4242Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4162Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4177Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4125Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4215Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4095Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4003Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.3944Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.3947Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.3858Epoch 1/15: [============                  ] 24/60 batches, loss: 0.3793Epoch 1/15: [============                  ] 25/60 batches, loss: 0.3746Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.3696Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3625Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3563Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3571Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3584Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3536Epoch 1/15: [================              ] 32/60 batches, loss: 0.3476Epoch 1/15: [================              ] 33/60 batches, loss: 0.3433Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3420Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3402Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3380Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3308Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3256Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3222Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3189Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3190Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3171Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3136Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3141Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3109Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3070Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3056Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3009Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3000Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.2963Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.2952Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.2945Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.2917Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.2924Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.2913Epoch 1/15: [============================  ] 56/60 batches, loss: 0.2925Epoch 1/15: [============================  ] 57/60 batches, loss: 0.2938Epoch 1/15: [============================= ] 58/60 batches, loss: 0.2931Epoch 1/15: [============================= ] 59/60 batches, loss: 0.2908Epoch 1/15: [==============================] 60/60 batches, loss: 0.2882
[2025-05-07 20:15:47,243][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2882
[2025-05-07 20:15:47,556][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1708, Metrics: {'mse': 0.15874914824962616, 'rmse': 0.3984333673898638, 'r2': -3.3019495010375977}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.2790Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2030Epoch 2/15: [=                             ] 3/60 batches, loss: 0.1817Epoch 2/15: [==                            ] 4/60 batches, loss: 0.1997Epoch 2/15: [==                            ] 5/60 batches, loss: 0.1749Epoch 2/15: [===                           ] 6/60 batches, loss: 0.1915Epoch 2/15: [===                           ] 7/60 batches, loss: 0.1896Epoch 2/15: [====                          ] 8/60 batches, loss: 0.1923Epoch 2/15: [====                          ] 9/60 batches, loss: 0.1956Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.1868Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.1811Epoch 2/15: [======                        ] 12/60 batches, loss: 0.1805Epoch 2/15: [======                        ] 13/60 batches, loss: 0.1840Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.1843Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.1772Epoch 2/15: [========                      ] 16/60 batches, loss: 0.1792Epoch 2/15: [========                      ] 17/60 batches, loss: 0.1796Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.1749Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.1734Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1741Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1707Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1722Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1699Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1672Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1698Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1660Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1628Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1615Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1595Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1580Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1580Epoch 2/15: [================              ] 32/60 batches, loss: 0.1575Epoch 2/15: [================              ] 33/60 batches, loss: 0.1550Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1551Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1585Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1570Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1566Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1570Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1573Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1602Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1621Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1622Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1603Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1606Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1591Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1583Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1569Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1567Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1574Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1571Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1556Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1542Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1530Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1520Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1508Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1495Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1491Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1486Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1477Epoch 2/15: [==============================] 60/60 batches, loss: 0.1463
[2025-05-07 20:15:49,824][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1463
[2025-05-07 20:15:50,137][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1559, Metrics: {'mse': 0.14448192715644836, 'rmse': 0.3801077836041356, 'r2': -2.9153213500976562}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.0895Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1219Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1105Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1216Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1092Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1063Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1080Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1191Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1135Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1092Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1067Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1104Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1148Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1131Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1104Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1097Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1091Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1064Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1050Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1049Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1024Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1002Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.0981Epoch 3/15: [============                  ] 24/60 batches, loss: 0.0978Epoch 3/15: [============                  ] 25/60 batches, loss: 0.0968Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.0987Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.0983Epoch 3/15: [==============                ] 28/60 batches, loss: 0.0961Epoch 3/15: [==============                ] 29/60 batches, loss: 0.0965Epoch 3/15: [===============               ] 30/60 batches, loss: 0.0955Epoch 3/15: [===============               ] 31/60 batches, loss: 0.0948Epoch 3/15: [================              ] 32/60 batches, loss: 0.0951Epoch 3/15: [================              ] 33/60 batches, loss: 0.0981Epoch 3/15: [=================             ] 34/60 batches, loss: 0.0964Epoch 3/15: [=================             ] 35/60 batches, loss: 0.0951Epoch 3/15: [==================            ] 36/60 batches, loss: 0.0960Epoch 3/15: [==================            ] 37/60 batches, loss: 0.0958Epoch 3/15: [===================           ] 38/60 batches, loss: 0.0952Epoch 3/15: [===================           ] 39/60 batches, loss: 0.0954Epoch 3/15: [====================          ] 40/60 batches, loss: 0.0952Epoch 3/15: [====================          ] 41/60 batches, loss: 0.0950Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.0952Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.0951Epoch 3/15: [======================        ] 44/60 batches, loss: 0.0949Epoch 3/15: [======================        ] 45/60 batches, loss: 0.0943Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.0936Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.0938Epoch 3/15: [========================      ] 48/60 batches, loss: 0.0936Epoch 3/15: [========================      ] 49/60 batches, loss: 0.0939Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.0932Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.0925Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.0930Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.0928Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.0944Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.0940Epoch 3/15: [============================  ] 56/60 batches, loss: 0.0934Epoch 3/15: [============================  ] 57/60 batches, loss: 0.0929Epoch 3/15: [============================= ] 58/60 batches, loss: 0.0923Epoch 3/15: [============================= ] 59/60 batches, loss: 0.0923Epoch 3/15: [==============================] 60/60 batches, loss: 0.0933
[2025-05-07 20:15:52,462][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0933
[2025-05-07 20:15:52,732][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1343, Metrics: {'mse': 0.12395203113555908, 'rmse': 0.3520682194341873, 'r2': -2.358981132507324}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0566Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0717Epoch 4/15: [=                             ] 3/60 batches, loss: 0.0673Epoch 4/15: [==                            ] 4/60 batches, loss: 0.0902Epoch 4/15: [==                            ] 5/60 batches, loss: 0.0921Epoch 4/15: [===                           ] 6/60 batches, loss: 0.0855Epoch 4/15: [===                           ] 7/60 batches, loss: 0.0919Epoch 4/15: [====                          ] 8/60 batches, loss: 0.0880Epoch 4/15: [====                          ] 9/60 batches, loss: 0.0868Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.0801Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.0798Epoch 4/15: [======                        ] 12/60 batches, loss: 0.0813Epoch 4/15: [======                        ] 13/60 batches, loss: 0.0807Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.0809Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.0790Epoch 4/15: [========                      ] 16/60 batches, loss: 0.0827Epoch 4/15: [========                      ] 17/60 batches, loss: 0.0854Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.0845Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.0831Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.0835Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.0870Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.0866Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.0852Epoch 4/15: [============                  ] 24/60 batches, loss: 0.0871Epoch 4/15: [============                  ] 25/60 batches, loss: 0.0867Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.0868Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.0857Epoch 4/15: [==============                ] 28/60 batches, loss: 0.0866Epoch 4/15: [==============                ] 29/60 batches, loss: 0.0869Epoch 4/15: [===============               ] 30/60 batches, loss: 0.0869Epoch 4/15: [===============               ] 31/60 batches, loss: 0.0863Epoch 4/15: [================              ] 32/60 batches, loss: 0.0851Epoch 4/15: [================              ] 33/60 batches, loss: 0.0846Epoch 4/15: [=================             ] 34/60 batches, loss: 0.0844Epoch 4/15: [=================             ] 35/60 batches, loss: 0.0838Epoch 4/15: [==================            ] 36/60 batches, loss: 0.0821Epoch 4/15: [==================            ] 37/60 batches, loss: 0.0821Epoch 4/15: [===================           ] 38/60 batches, loss: 0.0828Epoch 4/15: [===================           ] 39/60 batches, loss: 0.0824Epoch 4/15: [====================          ] 40/60 batches, loss: 0.0820Epoch 4/15: [====================          ] 41/60 batches, loss: 0.0820Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.0828Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.0821Epoch 4/15: [======================        ] 44/60 batches, loss: 0.0811Epoch 4/15: [======================        ] 45/60 batches, loss: 0.0810Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.0798Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.0788Epoch 4/15: [========================      ] 48/60 batches, loss: 0.0781Epoch 4/15: [========================      ] 49/60 batches, loss: 0.0776Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.0775Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.0779Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.0784Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.0787Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.0782Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.0786Epoch 4/15: [============================  ] 56/60 batches, loss: 0.0787Epoch 4/15: [============================  ] 57/60 batches, loss: 0.0783Epoch 4/15: [============================= ] 58/60 batches, loss: 0.0783Epoch 4/15: [============================= ] 59/60 batches, loss: 0.0781Epoch 4/15: [==============================] 60/60 batches, loss: 0.0773
[2025-05-07 20:15:55,048][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0773
[2025-05-07 20:15:55,310][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1101, Metrics: {'mse': 0.10113009065389633, 'rmse': 0.31800957635564425, 'r2': -1.7405283451080322}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.1078Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0878Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0833Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0930Epoch 5/15: [==                            ] 5/60 batches, loss: 0.0933Epoch 5/15: [===                           ] 6/60 batches, loss: 0.0894Epoch 5/15: [===                           ] 7/60 batches, loss: 0.0853Epoch 5/15: [====                          ] 8/60 batches, loss: 0.0823Epoch 5/15: [====                          ] 9/60 batches, loss: 0.0790Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.0783Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.0819Epoch 5/15: [======                        ] 12/60 batches, loss: 0.0827Epoch 5/15: [======                        ] 13/60 batches, loss: 0.0853Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.0867Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.0852Epoch 5/15: [========                      ] 16/60 batches, loss: 0.0845Epoch 5/15: [========                      ] 17/60 batches, loss: 0.0846Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.0833Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.0827Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.0849Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.0838Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.0828Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.0859Epoch 5/15: [============                  ] 24/60 batches, loss: 0.0847Epoch 5/15: [============                  ] 25/60 batches, loss: 0.0847Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.0842Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.0834Epoch 5/15: [==============                ] 28/60 batches, loss: 0.0824Epoch 5/15: [==============                ] 29/60 batches, loss: 0.0838Epoch 5/15: [===============               ] 30/60 batches, loss: 0.0841Epoch 5/15: [===============               ] 31/60 batches, loss: 0.0832Epoch 5/15: [================              ] 32/60 batches, loss: 0.0826Epoch 5/15: [================              ] 33/60 batches, loss: 0.0817Epoch 5/15: [=================             ] 34/60 batches, loss: 0.0805Epoch 5/15: [=================             ] 35/60 batches, loss: 0.0801Epoch 5/15: [==================            ] 36/60 batches, loss: 0.0793Epoch 5/15: [==================            ] 37/60 batches, loss: 0.0787Epoch 5/15: [===================           ] 38/60 batches, loss: 0.0778Epoch 5/15: [===================           ] 39/60 batches, loss: 0.0771Epoch 5/15: [====================          ] 40/60 batches, loss: 0.0760Epoch 5/15: [====================          ] 41/60 batches, loss: 0.0757Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.0764Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.0774Epoch 5/15: [======================        ] 44/60 batches, loss: 0.0773Epoch 5/15: [======================        ] 45/60 batches, loss: 0.0766Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.0766Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0757Epoch 5/15: [========================      ] 48/60 batches, loss: 0.0765Epoch 5/15: [========================      ] 49/60 batches, loss: 0.0767Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.0767Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0756Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0753Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0746Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0743Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0743Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0735Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0732Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0731Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0724Epoch 5/15: [==============================] 60/60 batches, loss: 0.0717
[2025-05-07 20:15:57,582][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0717
[2025-05-07 20:15:57,929][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1101, Metrics: {'mse': 0.10112638026475906, 'rmse': 0.31800374253262975, 'r2': -1.7404277324676514}
[2025-05-07 20:15:57,929][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1117Epoch 6/15: [=                             ] 2/60 batches, loss: 0.0728Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0774Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0652Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0650Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0663Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0660Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0733Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0727Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0693Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0668Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0652Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0627Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0623Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0623Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0639Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0631Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0644Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0642Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0622Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0614Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0605Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0599Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0596Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0592Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0591Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0592Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0585Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0578Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0578Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0565Epoch 6/15: [================              ] 32/60 batches, loss: 0.0579Epoch 6/15: [================              ] 33/60 batches, loss: 0.0579Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0577Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0568Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0562Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0560Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0564Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0558Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0558Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0554Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0558Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0552Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0563Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0561Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0555Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0556Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0557Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0550Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0554Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0565Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0565Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0563Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0562Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0570Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0566Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0567Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0562Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0571Epoch 6/15: [==============================] 60/60 batches, loss: 0.0566
[2025-05-07 20:15:59,849][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0566
[2025-05-07 20:16:00,177][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0835, Metrics: {'mse': 0.07634367048740387, 'rmse': 0.27630358392066484, 'r2': -1.0688402652740479}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0497Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0763Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0711Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0714Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0718Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0757Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0741Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0711Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0708Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0677Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0637Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0622Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0621Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0606Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0676Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0666Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0650Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0641Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0639Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0637Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0616Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0608Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0615Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0626Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0619Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0610Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0602Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0593Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0596Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0593Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0592Epoch 7/15: [================              ] 32/60 batches, loss: 0.0595Epoch 7/15: [================              ] 33/60 batches, loss: 0.0591Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0583Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0582Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0572Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0561Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0552Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0549Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0549Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0542Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0545Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0540Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0539Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0531Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0532Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0531Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0532Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0536Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0536Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0542Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0538Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0542Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0540Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0537Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0535Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0535Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0534Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0538Epoch 7/15: [==============================] 60/60 batches, loss: 0.0548
[2025-05-07 20:16:02,395][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0548
[2025-05-07 20:16:02,770][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0919, Metrics: {'mse': 0.08424215763807297, 'rmse': 0.2902449958880824, 'r2': -1.282881736755371}
[2025-05-07 20:16:02,771][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0829Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0548Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0567Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0505Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0522Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0521Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0506Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0493Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0479Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0478Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0481Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0463Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0457Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0457Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0443Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0441Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0430Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0414Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0428Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0426Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0418Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0421Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0439Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0441Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0432Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0425Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0441Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0439Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0447Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0440Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0441Epoch 8/15: [================              ] 32/60 batches, loss: 0.0441Epoch 8/15: [================              ] 33/60 batches, loss: 0.0438Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0438Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0435Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0433Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0434Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0438Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0444Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0438Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0442Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0454Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0454Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0454Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0458Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0456Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0452Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0462Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0463Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0464Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0465Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0463Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0463Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0463Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0460Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0459Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0458Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0460Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0463Epoch 8/15: [==============================] 60/60 batches, loss: 0.0462
[2025-05-07 20:16:04,704][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0462
[2025-05-07 20:16:05,020][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.1045, Metrics: {'mse': 0.09592627733945847, 'rmse': 0.3097196754154609, 'r2': -1.5995099544525146}
[2025-05-07 20:16:05,021][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.1341Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0907Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0732Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0749Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0639Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0632Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0584Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0545Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0512Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0492Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0519Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0513Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0486Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0495Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0505Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0488Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0488Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0490Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0482Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0480Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0477Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0477Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0484Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0471Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0476Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0494Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0489Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0497Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0507Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0507Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0508Epoch 9/15: [================              ] 32/60 batches, loss: 0.0500Epoch 9/15: [================              ] 33/60 batches, loss: 0.0493Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0494Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0498Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0496Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0492Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0491Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0486Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0505Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0507Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0503Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0501Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0497Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0499Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0493Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0490Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0485Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0493Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0489Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0491Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0492Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0487Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0482Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0485Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0485Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0479Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0478Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0479Epoch 9/15: [==============================] 60/60 batches, loss: 0.0479
[2025-05-07 20:16:06,921][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0479
[2025-05-07 20:16:07,216][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0849, Metrics: {'mse': 0.07747901976108551, 'rmse': 0.2783505339694636, 'r2': -1.099606990814209}
[2025-05-07 20:16:07,217][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0258Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0366Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0436Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0350Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0348Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0324Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0320Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0344Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0343Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0344Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0323Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0334Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0355Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0355Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0356Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0355Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0347Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0339Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0342Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0342Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0343Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0349Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0357Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0375Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0370Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0380Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0379Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0382Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0379Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0374Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0380Epoch 10/15: [================              ] 32/60 batches, loss: 0.0384Epoch 10/15: [================              ] 33/60 batches, loss: 0.0387Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0391Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0401Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0403Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0399Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0403Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0412Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0413Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0410Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0409Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0404Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0403Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0400Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0414Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0414Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0412Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0410Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0408Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0410Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0410Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0405Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0402Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0401Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0399Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0398Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0394Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0398Epoch 10/15: [==============================] 60/60 batches, loss: 0.0397
[2025-05-07 20:16:09,145][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0397
[2025-05-07 20:16:09,459][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0827, Metrics: {'mse': 0.07574597746133804, 'rmse': 0.27521987112368546, 'r2': -1.0526432991027832}
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0430Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0604Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0484Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0503Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0537Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0521Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0504Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0491Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0481Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0461Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0443Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0463Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0447Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0457Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0485Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0468Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0464Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0462Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0482Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0476Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0469Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0465Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0459Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0450Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0450Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0441Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0433Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0434Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0428Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0431Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0429Epoch 11/15: [================              ] 32/60 batches, loss: 0.0421Epoch 11/15: [================              ] 33/60 batches, loss: 0.0419Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0413Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0406Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0408Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0406Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0402Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0400Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0402Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0401Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0403Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0409Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0408Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0407Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0408Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0403Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0403Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0404Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0403Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0403Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0401Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0398Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0404Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0399Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0401Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0401Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0399Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0397Epoch 11/15: [==============================] 60/60 batches, loss: 0.0397
[2025-05-07 20:16:11,773][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0397
[2025-05-07 20:16:12,127][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0891, Metrics: {'mse': 0.08188465982675552, 'rmse': 0.2861549577182886, 'r2': -1.2189958095550537}
[2025-05-07 20:16:12,128][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0234Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0415Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0429Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0429Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0408Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0441Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0431Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0396Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0391Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0381Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0401Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0415Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0410Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0400Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0382Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0405Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0408Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0418Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0455Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0453Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0450Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0443Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0438Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0437Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0439Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0440Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0447Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0446Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0438Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0438Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0432Epoch 12/15: [================              ] 32/60 batches, loss: 0.0439Epoch 12/15: [================              ] 33/60 batches, loss: 0.0438Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0437Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0433Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0436Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0433Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0434Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0434Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0427Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0427Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0423Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0419Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0423Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0418Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0414Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0421Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0421Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0417Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0411Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0421Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0415Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0414Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0415Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0412Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0409Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0414Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0412Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0409Epoch 12/15: [==============================] 60/60 batches, loss: 0.0413
[2025-05-07 20:16:14,049][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0413
[2025-05-07 20:16:14,351][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0769, Metrics: {'mse': 0.07040806114673615, 'rmse': 0.26534517358854703, 'r2': -0.9079908132553101}
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0182Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0359Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0366Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0344Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0331Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0318Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0319Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0304Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0306Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0304Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0307Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0298Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0320Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0324Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0324Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0322Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0330Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0320Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0335Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0325Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0328Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0352Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0356Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0356Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0365Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0358Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0356Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0352Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0350Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0347Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0346Epoch 13/15: [================              ] 32/60 batches, loss: 0.0346Epoch 13/15: [================              ] 33/60 batches, loss: 0.0359Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0362Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0367Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0366Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0363Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0358Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0354Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0348Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0347Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0347Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0346Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0343Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0347Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0347Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0346Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0345Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0342Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0341Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0339Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0338Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0336Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0335Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0336Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0334Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0332Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0330Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0329Epoch 13/15: [==============================] 60/60 batches, loss: 0.0328
[2025-05-07 20:16:16,656][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0328
[2025-05-07 20:16:16,922][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0777, Metrics: {'mse': 0.07129637897014618, 'rmse': 0.2670138179385969, 'r2': -0.9320634603500366}
[2025-05-07 20:16:16,923][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0204Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0265Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0273Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0282Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0298Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0294Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0300Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0295Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0335Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0340Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0317Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0315Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0318Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0326Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0311Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0307Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0326Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0322Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0326Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0327Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0328Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0323Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0318Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0311Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0309Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0310Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0318Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0311Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0324Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0332Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0328Epoch 14/15: [================              ] 32/60 batches, loss: 0.0322Epoch 14/15: [================              ] 33/60 batches, loss: 0.0323Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0318Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0328Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0328Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0330Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0332Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0339Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0339Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0337Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0340Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0345Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0343Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0343Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0340Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0341Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0336Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0336Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0340Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0335Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0337Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0338Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0336Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0336Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0333Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0329Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0329Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0331Epoch 14/15: [==============================] 60/60 batches, loss: 0.0328
[2025-05-07 20:16:18,828][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0328
[2025-05-07 20:16:19,145][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0720, Metrics: {'mse': 0.06605473160743713, 'rmse': 0.2570111507453269, 'r2': -0.7900198698043823}
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0653Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0498Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0442Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0443Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0418Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0394Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0366Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0366Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0344Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0333Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0345Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0342Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0325Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0318Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0326Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0331Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0328Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0323Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0333Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0323Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0319Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0312Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0306Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0305Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0301Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0301Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0309Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0301Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0301Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0302Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0317Epoch 15/15: [================              ] 32/60 batches, loss: 0.0315Epoch 15/15: [================              ] 33/60 batches, loss: 0.0314Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0312Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0312Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0309Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0305Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0304Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0305Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0303Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0300Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0307Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0307Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0307Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0305Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0307Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0305Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0306Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0307Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0306Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0303Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0307Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0307Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0306Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0304Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0306Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0308Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0309Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0307Epoch 15/15: [==============================] 60/60 batches, loss: 0.0304
[2025-05-07 20:16:21,376][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0304
[2025-05-07 20:16:21,621][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0719, Metrics: {'mse': 0.06601028889417648, 'rmse': 0.25692467552607023, 'r2': -0.7888154983520508}
[2025-05-07 20:16:22,055][src.training.lm_trainer][INFO] - Training completed in 37.41 seconds
[2025-05-07 20:16:22,055][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:16:24,745][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.020879385992884636, 'rmse': 0.14449701032507434, 'r2': -0.1430114507675171}
[2025-05-07 20:16:24,745][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06601028889417648, 'rmse': 0.25692467552607023, 'r2': -0.7888154983520508}
[2025-05-07 20:16:24,745][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.056695107370615005, 'rmse': 0.2381073442181383, 'r2': -1.5537409782409668}
[2025-05-07 20:16:26,381][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/id/id/model.pt
[2025-05-07 20:16:26,383][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▅▄▂▂▁▁▁
wandb:     best_val_mse █▇▅▄▂▂▁▁▁
wandb:      best_val_r2 ▁▂▄▅▇▇███
wandb:    best_val_rmse █▇▆▄▂▂▁▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▄▅▅▆▆▅▆▆▆▇▇▇
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▅▄▄▂▂▃▂▂▂▁▁▁▁
wandb:          val_mse █▇▅▄▄▂▂▃▂▂▂▁▁▁▁
wandb:           val_r2 ▁▂▄▅▅▇▇▆▇▇▇████
wandb:         val_rmse █▇▆▄▄▂▃▄▂▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07195
wandb:     best_val_mse 0.06601
wandb:      best_val_r2 -0.78882
wandb:    best_val_rmse 0.25692
wandb:            epoch 15
wandb:   final_test_mse 0.0567
wandb:    final_test_r2 -1.55374
wandb:  final_test_rmse 0.23811
wandb:  final_train_mse 0.02088
wandb:   final_train_r2 -0.14301
wandb: final_train_rmse 0.1445
wandb:    final_val_mse 0.06601
wandb:     final_val_r2 -0.78882
wandb:   final_val_rmse 0.25692
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03036
wandb:       train_time 37.41453
wandb:         val_loss 0.07195
wandb:          val_mse 0.06601
wandb:           val_r2 -0.78882
wandb:         val_rmse 0.25692
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_201524-ifee6lij
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_201524-ifee6lij/logs
Experiment probe_layer2_avg_links_len_control2_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_avg_links_len_control3_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_control3_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control3/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:17:01,721][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control3/layer2/id
experiment_name: probe_layer2_avg_links_len_control3_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:17:01,721][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:17:01,721][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 20:17:01,721][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:17:01,721][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:17:01,725][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:17:01,725][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 20:17:01,725][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:17:06,075][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:17:08,561][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:17:08,561][src.data.datasets][INFO] - Loading 'control_avg_links_len_seed3' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:17:08,858][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_links_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:17:58 2025).
[2025-05-07 20:17:08,991][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_links_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:17:58 2025).
[2025-05-07 20:17:09,379][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:17:09,386][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:17:09,387][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:17:09,390][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:17:09,528][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:17:09,665][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:17:09,701][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:17:09,703][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:17:09,703][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:17:09,707][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:17:09,844][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:17:09,962][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:17:10,012][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:17:10,013][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:17:10,013][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:17:10,016][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:17:10,016][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:17:10,016][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:17:10,016][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:17:10,016][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:17:10,016][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:17:10,017][src.data.datasets][INFO] -   Mean: 0.1820, Std: 0.1352
[2025-05-07 20:17:10,017][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:17:10,017][src.data.datasets][INFO] - Sample label: 0.04600000008940697
[2025-05-07 20:17:10,017][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:17:10,017][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:17:10,017][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:17:10,017][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:17:10,017][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9170
[2025-05-07 20:17:10,018][src.data.datasets][INFO] -   Mean: 0.2802, Std: 0.1921
[2025-05-07 20:17:10,018][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:17:10,018][src.data.datasets][INFO] - Sample label: 0.1289999932050705
[2025-05-07 20:17:10,018][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:17:10,018][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:17:10,018][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:17:10,018][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:17:10,018][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7260
[2025-05-07 20:17:10,018][src.data.datasets][INFO] -   Mean: 0.2993, Std: 0.1490
[2025-05-07 20:17:10,018][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:17:10,018][src.data.datasets][INFO] - Sample label: 0.22599999606609344
[2025-05-07 20:17:10,019][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:17:10,019][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:17:10,019][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:17:10,019][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 20:17:10,019][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:17:18,563][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:17:18,564][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:17:18,564][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:17:18,564][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:17:18,567][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:17:18,567][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:17:18,567][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:17:18,567][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:17:18,568][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:17:18,568][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:17:18,569][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.3824Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4518Epoch 1/15: [=                             ] 3/60 batches, loss: 0.4682Epoch 1/15: [==                            ] 4/60 batches, loss: 0.4845Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4556Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4255Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4593Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4700Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4814Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4810Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4652Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4588Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4424Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4351Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4292Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4297Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4317Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4399Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4236Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4160Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4074Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4035Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.3943Epoch 1/15: [============                  ] 24/60 batches, loss: 0.3869Epoch 1/15: [============                  ] 25/60 batches, loss: 0.3820Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.3780Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3702Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3656Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3668Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3712Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3632Epoch 1/15: [================              ] 32/60 batches, loss: 0.3576Epoch 1/15: [================              ] 33/60 batches, loss: 0.3521Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3498Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3473Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3451Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3382Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3339Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3311Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3300Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3285Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3276Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3233Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3225Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3190Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3153Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3130Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3085Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3081Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3049Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3039Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3031Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3004Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.2999Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.2977Epoch 1/15: [============================  ] 56/60 batches, loss: 0.2981Epoch 1/15: [============================  ] 57/60 batches, loss: 0.2992Epoch 1/15: [============================= ] 58/60 batches, loss: 0.2977Epoch 1/15: [============================= ] 59/60 batches, loss: 0.2951Epoch 1/15: [==============================] 60/60 batches, loss: 0.2926
[2025-05-07 20:17:24,931][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2926
[2025-05-07 20:17:25,225][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1585, Metrics: {'mse': 0.1471455693244934, 'rmse': 0.383595580428781, 'r2': -2.9875035285949707}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.2775Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2228Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2018Epoch 2/15: [==                            ] 4/60 batches, loss: 0.1925Epoch 2/15: [==                            ] 5/60 batches, loss: 0.1708Epoch 2/15: [===                           ] 6/60 batches, loss: 0.1763Epoch 2/15: [===                           ] 7/60 batches, loss: 0.1747Epoch 2/15: [====                          ] 8/60 batches, loss: 0.1838Epoch 2/15: [====                          ] 9/60 batches, loss: 0.1832Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.1760Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.1767Epoch 2/15: [======                        ] 12/60 batches, loss: 0.1727Epoch 2/15: [======                        ] 13/60 batches, loss: 0.1798Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.1815Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.1744Epoch 2/15: [========                      ] 16/60 batches, loss: 0.1738Epoch 2/15: [========                      ] 17/60 batches, loss: 0.1766Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.1736Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.1735Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1730Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1691Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1692Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1676Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1636Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1661Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1674Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1648Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1636Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1624Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1614Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1606Epoch 2/15: [================              ] 32/60 batches, loss: 0.1590Epoch 2/15: [================              ] 33/60 batches, loss: 0.1576Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1588Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1592Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1575Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1577Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1570Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1569Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1589Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1584Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1570Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1563Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1573Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1568Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1557Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1543Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1531Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1537Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1529Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1519Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1519Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1514Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1505Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1504Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1492Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1484Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1479Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1467Epoch 2/15: [==============================] 60/60 batches, loss: 0.1453
[2025-05-07 20:17:27,491][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1453
[2025-05-07 20:17:27,813][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1397, Metrics: {'mse': 0.1291879117488861, 'rmse': 0.35942719951178725, 'r2': -2.500868558883667}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.0670Epoch 3/15: [=                             ] 2/60 batches, loss: 0.0995Epoch 3/15: [=                             ] 3/60 batches, loss: 0.0898Epoch 3/15: [==                            ] 4/60 batches, loss: 0.0898Epoch 3/15: [==                            ] 5/60 batches, loss: 0.0920Epoch 3/15: [===                           ] 6/60 batches, loss: 0.0950Epoch 3/15: [===                           ] 7/60 batches, loss: 0.0933Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1034Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1068Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1016Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1030Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1054Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1074Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1088Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1097Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1120Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1134Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1095Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1064Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1047Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1016Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1009Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.0992Epoch 3/15: [============                  ] 24/60 batches, loss: 0.0983Epoch 3/15: [============                  ] 25/60 batches, loss: 0.0989Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1004Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.0990Epoch 3/15: [==============                ] 28/60 batches, loss: 0.0991Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1012Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1000Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1011Epoch 3/15: [================              ] 32/60 batches, loss: 0.1008Epoch 3/15: [================              ] 33/60 batches, loss: 0.1040Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1027Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1017Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1022Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1009Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1006Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1008Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1002Epoch 3/15: [====================          ] 41/60 batches, loss: 0.0995Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.0993Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1006Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1006Epoch 3/15: [======================        ] 45/60 batches, loss: 0.0995Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.0986Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.0988Epoch 3/15: [========================      ] 48/60 batches, loss: 0.0984Epoch 3/15: [========================      ] 49/60 batches, loss: 0.0995Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.0993Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.0991Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.0988Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.0981Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1001Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1000Epoch 3/15: [============================  ] 56/60 batches, loss: 0.0989Epoch 3/15: [============================  ] 57/60 batches, loss: 0.0995Epoch 3/15: [============================= ] 58/60 batches, loss: 0.0985Epoch 3/15: [============================= ] 59/60 batches, loss: 0.0978Epoch 3/15: [==============================] 60/60 batches, loss: 0.0998
[2025-05-07 20:17:30,097][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0998
[2025-05-07 20:17:30,355][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1275, Metrics: {'mse': 0.11735519766807556, 'rmse': 0.34257144899725017, 'r2': -2.180213212966919}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0605Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0723Epoch 4/15: [=                             ] 3/60 batches, loss: 0.0704Epoch 4/15: [==                            ] 4/60 batches, loss: 0.0960Epoch 4/15: [==                            ] 5/60 batches, loss: 0.0942Epoch 4/15: [===                           ] 6/60 batches, loss: 0.0908Epoch 4/15: [===                           ] 7/60 batches, loss: 0.0867Epoch 4/15: [====                          ] 8/60 batches, loss: 0.0859Epoch 4/15: [====                          ] 9/60 batches, loss: 0.0822Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.0769Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.0779Epoch 4/15: [======                        ] 12/60 batches, loss: 0.0769Epoch 4/15: [======                        ] 13/60 batches, loss: 0.0794Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.0790Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.0792Epoch 4/15: [========                      ] 16/60 batches, loss: 0.0830Epoch 4/15: [========                      ] 17/60 batches, loss: 0.0839Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.0829Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.0816Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.0862Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.0873Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.0875Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.0882Epoch 4/15: [============                  ] 24/60 batches, loss: 0.0903Epoch 4/15: [============                  ] 25/60 batches, loss: 0.0885Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.0908Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.0895Epoch 4/15: [==============                ] 28/60 batches, loss: 0.0900Epoch 4/15: [==============                ] 29/60 batches, loss: 0.0884Epoch 4/15: [===============               ] 30/60 batches, loss: 0.0880Epoch 4/15: [===============               ] 31/60 batches, loss: 0.0877Epoch 4/15: [================              ] 32/60 batches, loss: 0.0869Epoch 4/15: [================              ] 33/60 batches, loss: 0.0872Epoch 4/15: [=================             ] 34/60 batches, loss: 0.0868Epoch 4/15: [=================             ] 35/60 batches, loss: 0.0853Epoch 4/15: [==================            ] 36/60 batches, loss: 0.0850Epoch 4/15: [==================            ] 37/60 batches, loss: 0.0851Epoch 4/15: [===================           ] 38/60 batches, loss: 0.0850Epoch 4/15: [===================           ] 39/60 batches, loss: 0.0842Epoch 4/15: [====================          ] 40/60 batches, loss: 0.0831Epoch 4/15: [====================          ] 41/60 batches, loss: 0.0837Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.0844Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.0857Epoch 4/15: [======================        ] 44/60 batches, loss: 0.0849Epoch 4/15: [======================        ] 45/60 batches, loss: 0.0849Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.0841Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.0829Epoch 4/15: [========================      ] 48/60 batches, loss: 0.0822Epoch 4/15: [========================      ] 49/60 batches, loss: 0.0817Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.0818Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.0818Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.0839Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.0833Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.0836Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.0839Epoch 4/15: [============================  ] 56/60 batches, loss: 0.0843Epoch 4/15: [============================  ] 57/60 batches, loss: 0.0839Epoch 4/15: [============================= ] 58/60 batches, loss: 0.0837Epoch 4/15: [============================= ] 59/60 batches, loss: 0.0836Epoch 4/15: [==============================] 60/60 batches, loss: 0.0830
[2025-05-07 20:17:32,577][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0830
[2025-05-07 20:17:32,879][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1111, Metrics: {'mse': 0.10181072354316711, 'rmse': 0.3190779270698102, 'r2': -1.7589728832244873}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.0465Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0648Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0778Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0856Epoch 5/15: [==                            ] 5/60 batches, loss: 0.0931Epoch 5/15: [===                           ] 6/60 batches, loss: 0.0869Epoch 5/15: [===                           ] 7/60 batches, loss: 0.0873Epoch 5/15: [====                          ] 8/60 batches, loss: 0.0861Epoch 5/15: [====                          ] 9/60 batches, loss: 0.0839Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.0874Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.0876Epoch 5/15: [======                        ] 12/60 batches, loss: 0.0906Epoch 5/15: [======                        ] 13/60 batches, loss: 0.0896Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.0901Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.0878Epoch 5/15: [========                      ] 16/60 batches, loss: 0.0914Epoch 5/15: [========                      ] 17/60 batches, loss: 0.0919Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.0907Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.0927Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.0932Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.0926Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.0907Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.0903Epoch 5/15: [============                  ] 24/60 batches, loss: 0.0884Epoch 5/15: [============                  ] 25/60 batches, loss: 0.0872Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.0879Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.0862Epoch 5/15: [==============                ] 28/60 batches, loss: 0.0855Epoch 5/15: [==============                ] 29/60 batches, loss: 0.0881Epoch 5/15: [===============               ] 30/60 batches, loss: 0.0879Epoch 5/15: [===============               ] 31/60 batches, loss: 0.0874Epoch 5/15: [================              ] 32/60 batches, loss: 0.0862Epoch 5/15: [================              ] 33/60 batches, loss: 0.0848Epoch 5/15: [=================             ] 34/60 batches, loss: 0.0835Epoch 5/15: [=================             ] 35/60 batches, loss: 0.0831Epoch 5/15: [==================            ] 36/60 batches, loss: 0.0822Epoch 5/15: [==================            ] 37/60 batches, loss: 0.0811Epoch 5/15: [===================           ] 38/60 batches, loss: 0.0813Epoch 5/15: [===================           ] 39/60 batches, loss: 0.0808Epoch 5/15: [====================          ] 40/60 batches, loss: 0.0804Epoch 5/15: [====================          ] 41/60 batches, loss: 0.0798Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.0802Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.0797Epoch 5/15: [======================        ] 44/60 batches, loss: 0.0791Epoch 5/15: [======================        ] 45/60 batches, loss: 0.0789Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.0787Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0775Epoch 5/15: [========================      ] 48/60 batches, loss: 0.0771Epoch 5/15: [========================      ] 49/60 batches, loss: 0.0774Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.0773Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0763Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0755Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0755Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0750Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0744Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0739Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0735Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0740Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0737Epoch 5/15: [==============================] 60/60 batches, loss: 0.0729
[2025-05-07 20:17:35,126][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0729
[2025-05-07 20:17:35,385][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1012, Metrics: {'mse': 0.09262353181838989, 'rmse': 0.30434114381461785, 'r2': -1.5100088119506836}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1187Epoch 6/15: [=                             ] 2/60 batches, loss: 0.0880Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0864Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0723Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0714Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0706Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0663Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0716Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0725Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0705Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0694Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0673Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0669Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0667Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0699Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0709Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0699Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0691Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0675Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0659Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0655Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0668Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0666Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0663Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0656Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0654Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0656Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0645Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0644Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0644Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0633Epoch 6/15: [================              ] 32/60 batches, loss: 0.0625Epoch 6/15: [================              ] 33/60 batches, loss: 0.0634Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0630Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0626Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0623Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0627Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0626Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0619Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0629Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0620Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0623Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0613Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0610Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0608Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0600Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0606Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0607Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0602Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0596Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0608Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0607Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0613Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0611Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0611Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0606Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0604Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0604Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0608Epoch 6/15: [==============================] 60/60 batches, loss: 0.0601
[2025-05-07 20:17:37,600][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0601
[2025-05-07 20:17:37,913][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0907, Metrics: {'mse': 0.08278664201498032, 'rmse': 0.2877266793590409, 'r2': -1.243438482284546}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0224Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0458Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0523Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0613Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0683Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0632Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0675Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0635Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0649Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0602Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0577Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0561Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0564Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0556Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0551Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0549Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0546Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0557Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0557Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0549Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0539Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0530Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0548Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0544Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0536Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0531Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0528Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0516Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0518Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0514Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0518Epoch 7/15: [================              ] 32/60 batches, loss: 0.0513Epoch 7/15: [================              ] 33/60 batches, loss: 0.0517Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0511Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0507Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0514Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0505Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0503Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0501Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0502Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0499Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0506Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0503Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0506Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0505Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0509Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0512Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0512Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0519Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0520Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0523Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0518Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0514Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0513Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0512Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0510Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0512Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0509Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0511Epoch 7/15: [==============================] 60/60 batches, loss: 0.0516
[2025-05-07 20:17:40,186][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0516
[2025-05-07 20:17:40,488][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0861, Metrics: {'mse': 0.07853968441486359, 'rmse': 0.2802493254494354, 'r2': -1.128350019454956}
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0403Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0397Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0477Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0440Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0424Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0442Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0434Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0407Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0394Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0413Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0415Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0401Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0391Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0404Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0413Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0412Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0417Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0419Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0423Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0420Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0420Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0422Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0446Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0443Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0442Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0436Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0443Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0435Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0431Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0425Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0420Epoch 8/15: [================              ] 32/60 batches, loss: 0.0420Epoch 8/15: [================              ] 33/60 batches, loss: 0.0415Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0424Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0420Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0432Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0430Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0434Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0433Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0430Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0432Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0428Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0431Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0430Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0430Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0428Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0426Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0430Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0432Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0430Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0435Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0437Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0436Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0442Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0442Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0441Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0436Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0438Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0437Epoch 8/15: [==============================] 60/60 batches, loss: 0.0438
[2025-05-07 20:17:42,829][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0438
[2025-05-07 20:17:43,124][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0926, Metrics: {'mse': 0.08476443588733673, 'rmse': 0.29114332533536935, 'r2': -1.297034740447998}
[2025-05-07 20:17:43,125][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.0577Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0414Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0542Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0535Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0498Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0464Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0506Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0484Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0463Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0476Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0473Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0456Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0439Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0446Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0443Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0434Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0445Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0471Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0457Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0449Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0454Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0458Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0459Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0462Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0463Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0464Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0464Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0461Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0460Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0466Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0463Epoch 9/15: [================              ] 32/60 batches, loss: 0.0465Epoch 9/15: [================              ] 33/60 batches, loss: 0.0459Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0460Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0462Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0463Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0476Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0472Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0479Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0496Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0496Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0500Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0496Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0491Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0489Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0484Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0484Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0479Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0478Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0475Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0475Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0479Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0476Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0470Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0466Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0468Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0463Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0469Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0474Epoch 9/15: [==============================] 60/60 batches, loss: 0.0478
[2025-05-07 20:17:45,076][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0478
[2025-05-07 20:17:45,410][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0859, Metrics: {'mse': 0.0783306211233139, 'rmse': 0.2798760817278138, 'r2': -1.1226847171783447}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0334Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0331Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0364Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0349Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0364Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0362Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0362Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0369Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0436Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0418Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0403Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0413Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0406Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0393Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0415Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0411Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0397Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0400Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0400Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0412Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0399Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0418Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0417Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0416Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0412Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0414Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0422Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0422Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0420Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0417Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0426Epoch 10/15: [================              ] 32/60 batches, loss: 0.0424Epoch 10/15: [================              ] 33/60 batches, loss: 0.0425Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0422Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0421Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0418Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0413Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0409Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0405Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0405Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0406Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0403Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0399Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0401Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0396Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0393Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0392Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0393Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0393Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0400Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0401Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0401Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0398Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0394Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0396Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0395Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0396Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0393Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0390Epoch 10/15: [==============================] 60/60 batches, loss: 0.0391
[2025-05-07 20:17:47,658][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0391
[2025-05-07 20:17:48,001][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0760, Metrics: {'mse': 0.06922516971826553, 'rmse': 0.2631067648660246, 'r2': -0.8759356737136841}
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0268Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0420Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0427Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0497Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0553Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0526Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0519Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0545Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0547Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0512Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0502Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0537Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0510Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0498Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0491Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0498Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0504Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0505Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0508Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0495Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0492Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0484Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0470Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0458Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0474Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0469Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0461Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0453Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0445Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0448Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0449Epoch 11/15: [================              ] 32/60 batches, loss: 0.0441Epoch 11/15: [================              ] 33/60 batches, loss: 0.0439Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0436Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0429Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0426Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0420Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0413Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0410Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0416Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0413Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0407Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0408Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0407Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0405Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0401Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0395Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0392Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0389Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0386Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0385Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0388Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0384Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0384Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0382Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0389Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0386Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0383Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0379Epoch 11/15: [==============================] 60/60 batches, loss: 0.0377
[2025-05-07 20:17:50,294][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0377
[2025-05-07 20:17:50,592][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0906, Metrics: {'mse': 0.08309739083051682, 'rmse': 0.2882661805181399, 'r2': -1.2518596649169922}
[2025-05-07 20:17:50,593][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0223Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0449Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0382Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0383Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0491Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0467Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0433Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0410Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0383Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0365Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0371Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0381Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0397Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0399Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0407Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0412Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0402Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0402Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0399Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0410Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0399Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0409Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0412Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0425Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0428Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0423Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0420Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0434Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0439Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0439Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0435Epoch 12/15: [================              ] 32/60 batches, loss: 0.0444Epoch 12/15: [================              ] 33/60 batches, loss: 0.0445Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0448Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0441Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0432Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0427Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0424Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0419Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0415Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0412Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0413Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0409Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0408Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0405Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0406Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0402Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0401Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0399Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0397Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0398Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0394Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0391Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0388Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0383Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0379Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0378Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0377Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0384Epoch 12/15: [==============================] 60/60 batches, loss: 0.0381
[2025-05-07 20:17:52,517][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0381
[2025-05-07 20:17:52,796][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0911, Metrics: {'mse': 0.08375820517539978, 'rmse': 0.28941009860645805, 'r2': -1.2697668075561523}
[2025-05-07 20:17:52,796][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0237Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0358Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0513Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0444Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0475Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0422Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0392Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0383Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0375Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0369Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0349Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0364Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0378Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0381Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0368Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0367Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0366Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0360Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0366Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0373Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0370Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0372Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0369Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0370Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0383Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0373Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0385Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0387Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0387Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0390Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0389Epoch 13/15: [================              ] 32/60 batches, loss: 0.0384Epoch 13/15: [================              ] 33/60 batches, loss: 0.0397Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0393Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0399Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0397Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0395Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0389Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0390Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0388Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0387Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0387Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0383Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0380Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0384Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0384Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0380Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0376Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0377Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0374Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0374Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0372Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0378Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0376Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0377Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0373Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0374Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0372Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0371Epoch 13/15: [==============================] 60/60 batches, loss: 0.0370
[2025-05-07 20:17:54,685][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0370
[2025-05-07 20:17:54,967][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0758, Metrics: {'mse': 0.06936735659837723, 'rmse': 0.26337683383011734, 'r2': -0.8797887563705444}
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0209Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0250Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0253Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0263Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0284Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0274Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0301Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0297Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0283Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0289Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0289Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0282Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0285Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0289Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0285Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0275Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0281Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0291Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0297Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0295Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0296Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0299Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0301Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0294Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0294Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0305Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0307Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0301Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0301Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0305Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0304Epoch 14/15: [================              ] 32/60 batches, loss: 0.0311Epoch 14/15: [================              ] 33/60 batches, loss: 0.0310Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0305Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0303Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0313Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0314Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0312Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0309Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0305Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0304Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0303Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0302Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0307Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0305Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0306Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0304Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0303Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0306Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0312Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0312Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0317Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0315Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0320Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0318Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0317Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0317Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0314Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0318Epoch 14/15: [==============================] 60/60 batches, loss: 0.0328
[2025-05-07 20:17:57,271][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0328
[2025-05-07 20:17:57,544][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0688, Metrics: {'mse': 0.06299899518489838, 'rmse': 0.25099600631264707, 'r2': -0.7072123289108276}
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0203Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0425Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0442Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0457Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0415Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0392Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0404Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0411Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0410Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0415Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0418Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0409Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0387Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0374Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0363Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0357Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0370Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0368Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0368Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0359Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0353Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0348Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0350Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0350Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0348Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0346Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0346Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0338Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0337Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0336Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0334Epoch 15/15: [================              ] 32/60 batches, loss: 0.0333Epoch 15/15: [================              ] 33/60 batches, loss: 0.0336Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0331Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0328Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0324Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0330Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0329Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0327Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0325Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0326Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0328Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0327Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0324Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0323Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0324Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0322Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0323Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0321Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0320Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0320Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0319Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0316Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0321Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0329Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0332Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0333Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0334Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0333Epoch 15/15: [==============================] 60/60 batches, loss: 0.0330
[2025-05-07 20:17:59,956][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0330
[2025-05-07 20:18:00,241][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0744, Metrics: {'mse': 0.06821172684431076, 'rmse': 0.26117374838277824, 'r2': -0.8484723567962646}
[2025-05-07 20:18:00,242][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-07 20:18:00,242][src.training.lm_trainer][INFO] - Training completed in 37.97 seconds
[2025-05-07 20:18:00,242][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:18:02,844][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.019980741664767265, 'rmse': 0.14135325134133728, 'r2': -0.09381651878356934}
[2025-05-07 20:18:02,844][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06299899518489838, 'rmse': 0.25099600631264707, 'r2': -0.7072123289108276}
[2025-05-07 20:18:02,844][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.053336240351200104, 'rmse': 0.23094640146839288, 'r2': -1.4024460315704346}
[2025-05-07 20:18:04,541][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control3/layer2/id/id/model.pt
[2025-05-07 20:18:04,544][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▆▄▄▃▂▂▂▂▁
wandb:     best_val_mse █▇▆▄▃▃▂▂▂▂▁
wandb:      best_val_r2 ▁▂▃▅▆▆▇▇▇▇█
wandb:    best_val_rmse █▇▆▅▄▃▃▃▂▂▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▃▅▅▆▆▆▆▆▆▆▆▇
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▆▄▄▃▂▃▂▂▃▃▂▁▁
wandb:          val_mse █▇▆▄▃▃▂▃▂▂▃▃▂▁▁
wandb:           val_r2 ▁▂▃▅▆▆▇▆▇▇▆▆▇██
wandb:         val_rmse █▇▆▅▄▃▃▃▃▂▃▃▂▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06878
wandb:     best_val_mse 0.063
wandb:      best_val_r2 -0.70721
wandb:    best_val_rmse 0.251
wandb:            epoch 15
wandb:   final_test_mse 0.05334
wandb:    final_test_r2 -1.40245
wandb:  final_test_rmse 0.23095
wandb:  final_train_mse 0.01998
wandb:   final_train_r2 -0.09382
wandb: final_train_rmse 0.14135
wandb:    final_val_mse 0.063
wandb:     final_val_r2 -0.70721
wandb:   final_val_rmse 0.251
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03303
wandb:       train_time 37.96851
wandb:         val_loss 0.07443
wandb:          val_mse 0.06821
wandb:           val_r2 -0.84847
wandb:         val_rmse 0.26117
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_201701-js6oy0rm
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_201701-js6oy0rm/logs
Experiment probe_layer2_avg_links_len_control3_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control3/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_avg_max_depth_control1_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_max_depth_control1_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:18:37,082][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer2/id
experiment_name: probe_layer2_avg_max_depth_control1_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:18:37,082][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:18:37,082][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 20:18:37,082][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:18:37,082][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:18:37,087][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:18:37,087][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 20:18:37,087][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:18:40,816][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:18:43,100][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:18:43,101][src.data.datasets][INFO] - Loading 'control_avg_max_depth_seed1' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:18:43,351][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_max_depth_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:19:23 2025).
[2025-05-07 20:18:43,454][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_max_depth_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:19:23 2025).
[2025-05-07 20:18:43,732][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:18:43,739][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:18:43,740][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:18:43,744][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:18:43,853][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:18:43,960][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:18:44,005][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:18:44,006][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:18:44,007][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:18:44,009][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:18:44,112][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:18:44,232][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:18:44,282][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:18:44,283][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:18:44,283][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:18:44,285][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:18:44,286][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:18:44,286][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:18:44,286][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:18:44,286][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:18:44,286][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:18:44,287][src.data.datasets][INFO] -   Mean: 0.2764, Std: 0.1695
[2025-05-07 20:18:44,287][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:18:44,287][src.data.datasets][INFO] - Sample label: 0.25
[2025-05-07 20:18:44,287][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:18:44,287][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:18:44,287][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:18:44,287][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:18:44,287][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:18:44,288][src.data.datasets][INFO] -   Mean: 0.3215, Std: 0.2018
[2025-05-07 20:18:44,288][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:18:44,288][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:18:44,288][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:18:44,288][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:18:44,288][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:18:44,288][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:18:44,288][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:18:44,288][src.data.datasets][INFO] -   Mean: 0.2675, Std: 0.1795
[2025-05-07 20:18:44,288][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:18:44,288][src.data.datasets][INFO] - Sample label: 0.4440000057220459
[2025-05-07 20:18:44,289][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:18:44,289][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:18:44,289][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:18:44,289][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 20:18:44,289][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:18:51,639][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:18:51,640][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:18:51,640][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:18:51,640][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:18:51,643][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:18:51,644][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:18:51,644][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:18:51,644][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:18:51,644][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:18:51,645][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:18:51,645][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.3211Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4105Epoch 1/15: [=                             ] 3/60 batches, loss: 0.4199Epoch 1/15: [==                            ] 4/60 batches, loss: 0.4084Epoch 1/15: [==                            ] 5/60 batches, loss: 0.3898Epoch 1/15: [===                           ] 6/60 batches, loss: 0.3873Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4308Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4450Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4653Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4696Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4518Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4502Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4359Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4304Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4251Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4230Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4156Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4284Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4137Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4070Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4006Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4031Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.3965Epoch 1/15: [============                  ] 24/60 batches, loss: 0.3933Epoch 1/15: [============                  ] 25/60 batches, loss: 0.3920Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.3880Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3832Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3792Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3797Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3855Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3788Epoch 1/15: [================              ] 32/60 batches, loss: 0.3715Epoch 1/15: [================              ] 33/60 batches, loss: 0.3654Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3644Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3606Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3579Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3525Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3483Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3450Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3395Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3386Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3380Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3340Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3324Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3310Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3273Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3258Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3215Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3190Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3150Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3137Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3121Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3095Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3101Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3085Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3096Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3112Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3096Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3073Epoch 1/15: [==============================] 60/60 batches, loss: 0.3047
[2025-05-07 20:18:58,671][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3047
[2025-05-07 20:18:58,984][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1077, Metrics: {'mse': 0.10272785276174545, 'rmse': 0.3205118605632956, 'r2': -1.5216233730316162}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.4124Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2816Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2484Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2320Epoch 2/15: [==                            ] 5/60 batches, loss: 0.2013Epoch 2/15: [===                           ] 6/60 batches, loss: 0.2153Epoch 2/15: [===                           ] 7/60 batches, loss: 0.2021Epoch 2/15: [====                          ] 8/60 batches, loss: 0.2164Epoch 2/15: [====                          ] 9/60 batches, loss: 0.2163Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.2116Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.2068Epoch 2/15: [======                        ] 12/60 batches, loss: 0.2043Epoch 2/15: [======                        ] 13/60 batches, loss: 0.2072Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.2076Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.2001Epoch 2/15: [========                      ] 16/60 batches, loss: 0.1979Epoch 2/15: [========                      ] 17/60 batches, loss: 0.1957Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.1905Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.1909Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1902Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1890Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1906Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1907Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1869Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1901Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1882Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1879Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1881Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1860Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1837Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1825Epoch 2/15: [================              ] 32/60 batches, loss: 0.1800Epoch 2/15: [================              ] 33/60 batches, loss: 0.1777Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1786Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1791Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1775Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1766Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1773Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1777Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1806Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1806Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1783Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1763Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1773Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1757Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1746Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1738Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1727Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1719Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1706Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1682Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1671Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1674Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1669Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1664Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1643Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1625Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1618Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1608Epoch 2/15: [==============================] 60/60 batches, loss: 0.1594
[2025-05-07 20:19:01,308][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1594
[2025-05-07 20:19:01,587][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0905, Metrics: {'mse': 0.08566129952669144, 'rmse': 0.29267951675286646, 'r2': -1.1026968955993652}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.0877Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1142Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1048Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1151Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1142Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1158Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1272Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1350Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1294Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1228Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1228Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1195Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1260Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1234Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1204Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1233Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1251Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1212Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1203Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1186Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1177Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1166Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1144Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1127Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1125Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1141Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1145Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1143Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1153Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1136Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1138Epoch 3/15: [================              ] 32/60 batches, loss: 0.1148Epoch 3/15: [================              ] 33/60 batches, loss: 0.1178Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1178Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1170Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1169Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1168Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1159Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1154Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1147Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1150Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1144Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1146Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1146Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1130Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1116Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1118Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1114Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1118Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1116Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1117Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1122Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1114Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1125Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1124Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1114Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1104Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1094Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1092Epoch 3/15: [==============================] 60/60 batches, loss: 0.1104
[2025-05-07 20:19:03,923][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1104
[2025-05-07 20:19:04,295][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1093, Metrics: {'mse': 0.10402484983205795, 'rmse': 0.32252883565978707, 'r2': -1.5534603595733643}
[2025-05-07 20:19:04,296][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0313Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0735Epoch 4/15: [=                             ] 3/60 batches, loss: 0.0713Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1093Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1079Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1027Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1112Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1110Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1062Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.1001Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.0976Epoch 4/15: [======                        ] 12/60 batches, loss: 0.0978Epoch 4/15: [======                        ] 13/60 batches, loss: 0.0958Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.0926Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.0941Epoch 4/15: [========                      ] 16/60 batches, loss: 0.0974Epoch 4/15: [========                      ] 17/60 batches, loss: 0.0984Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.0957Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.0950Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.0972Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1037Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1045Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1034Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1049Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1036Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1040Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1045Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1052Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1053Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1031Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1019Epoch 4/15: [================              ] 32/60 batches, loss: 0.1011Epoch 4/15: [================              ] 33/60 batches, loss: 0.1001Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1003Epoch 4/15: [=================             ] 35/60 batches, loss: 0.0997Epoch 4/15: [==================            ] 36/60 batches, loss: 0.0990Epoch 4/15: [==================            ] 37/60 batches, loss: 0.0987Epoch 4/15: [===================           ] 38/60 batches, loss: 0.0986Epoch 4/15: [===================           ] 39/60 batches, loss: 0.0974Epoch 4/15: [====================          ] 40/60 batches, loss: 0.0967Epoch 4/15: [====================          ] 41/60 batches, loss: 0.0980Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.0996Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.0987Epoch 4/15: [======================        ] 44/60 batches, loss: 0.0987Epoch 4/15: [======================        ] 45/60 batches, loss: 0.0986Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.0976Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.0965Epoch 4/15: [========================      ] 48/60 batches, loss: 0.0969Epoch 4/15: [========================      ] 49/60 batches, loss: 0.0976Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.0971Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.0968Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.0968Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.0962Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.0960Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.0956Epoch 4/15: [============================  ] 56/60 batches, loss: 0.0962Epoch 4/15: [============================  ] 57/60 batches, loss: 0.0962Epoch 4/15: [============================= ] 58/60 batches, loss: 0.0958Epoch 4/15: [============================= ] 59/60 batches, loss: 0.0955Epoch 4/15: [==============================] 60/60 batches, loss: 0.0943
[2025-05-07 20:19:06,273][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0943
[2025-05-07 20:19:06,559][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0824, Metrics: {'mse': 0.07833538204431534, 'rmse': 0.2798845870074223, 'r2': -0.922870397567749}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.0335Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0634Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0794Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0828Epoch 5/15: [==                            ] 5/60 batches, loss: 0.0920Epoch 5/15: [===                           ] 6/60 batches, loss: 0.0992Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1003Epoch 5/15: [====                          ] 8/60 batches, loss: 0.1061Epoch 5/15: [====                          ] 9/60 batches, loss: 0.1008Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.1041Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1068Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1056Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1028Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1036Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.0997Epoch 5/15: [========                      ] 16/60 batches, loss: 0.0972Epoch 5/15: [========                      ] 17/60 batches, loss: 0.0966Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.0946Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.0948Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.0952Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.0930Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.0906Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.0905Epoch 5/15: [============                  ] 24/60 batches, loss: 0.0901Epoch 5/15: [============                  ] 25/60 batches, loss: 0.0896Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.0894Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.0880Epoch 5/15: [==============                ] 28/60 batches, loss: 0.0875Epoch 5/15: [==============                ] 29/60 batches, loss: 0.0893Epoch 5/15: [===============               ] 30/60 batches, loss: 0.0912Epoch 5/15: [===============               ] 31/60 batches, loss: 0.0912Epoch 5/15: [================              ] 32/60 batches, loss: 0.0904Epoch 5/15: [================              ] 33/60 batches, loss: 0.0899Epoch 5/15: [=================             ] 34/60 batches, loss: 0.0921Epoch 5/15: [=================             ] 35/60 batches, loss: 0.0927Epoch 5/15: [==================            ] 36/60 batches, loss: 0.0918Epoch 5/15: [==================            ] 37/60 batches, loss: 0.0923Epoch 5/15: [===================           ] 38/60 batches, loss: 0.0926Epoch 5/15: [===================           ] 39/60 batches, loss: 0.0923Epoch 5/15: [====================          ] 40/60 batches, loss: 0.0919Epoch 5/15: [====================          ] 41/60 batches, loss: 0.0909Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.0913Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.0923Epoch 5/15: [======================        ] 44/60 batches, loss: 0.0912Epoch 5/15: [======================        ] 45/60 batches, loss: 0.0919Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.0930Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0921Epoch 5/15: [========================      ] 48/60 batches, loss: 0.0921Epoch 5/15: [========================      ] 49/60 batches, loss: 0.0915Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.0920Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0911Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0914Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0910Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0899Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0889Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0884Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0874Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0874Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0877Epoch 5/15: [==============================] 60/60 batches, loss: 0.0867
[2025-05-07 20:19:08,852][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0867
[2025-05-07 20:19:09,141][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0867, Metrics: {'mse': 0.08255767822265625, 'rmse': 0.28732851968201184, 'r2': -1.0265135765075684}
[2025-05-07 20:19:09,141][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1175Epoch 6/15: [=                             ] 2/60 batches, loss: 0.1209Epoch 6/15: [=                             ] 3/60 batches, loss: 0.1072Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0886Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0853Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0870Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0864Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0966Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0957Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0895Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0881Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0861Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0855Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0847Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0834Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0838Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0816Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0789Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0778Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0770Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0755Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0755Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0762Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0755Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0774Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0776Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0771Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0765Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0747Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0745Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0726Epoch 6/15: [================              ] 32/60 batches, loss: 0.0728Epoch 6/15: [================              ] 33/60 batches, loss: 0.0737Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0730Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0721Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0723Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0724Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0723Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0714Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0721Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0712Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0715Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0716Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0717Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0721Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0713Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0717Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0718Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0711Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0713Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0735Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0730Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0734Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0730Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0735Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0738Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0738Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0730Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0727Epoch 6/15: [==============================] 60/60 batches, loss: 0.0723
[2025-05-07 20:19:11,015][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0723
[2025-05-07 20:19:11,286][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0649, Metrics: {'mse': 0.06177344545722008, 'rmse': 0.2485426431363843, 'r2': -0.5163304805755615}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0707Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0830Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0926Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0831Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0815Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0744Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0724Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0691Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0724Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0671Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0658Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0702Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0737Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0729Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0725Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0725Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0696Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0694Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0696Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0688Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0676Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0688Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0707Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0708Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0687Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0672Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0658Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0655Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0664Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0668Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0662Epoch 7/15: [================              ] 32/60 batches, loss: 0.0653Epoch 7/15: [================              ] 33/60 batches, loss: 0.0659Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0647Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0640Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0653Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0650Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0656Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0647Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0641Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0634Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0647Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0640Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0637Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0631Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0630Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0643Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0650Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0655Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0656Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0662Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0668Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0660Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0661Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0665Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0666Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0665Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0660Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0659Epoch 7/15: [==============================] 60/60 batches, loss: 0.0663
[2025-05-07 20:19:13,576][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0663
[2025-05-07 20:19:13,931][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0649, Metrics: {'mse': 0.06185833737254143, 'rmse': 0.24871336388007265, 'r2': -0.5184142589569092}
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0458Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0558Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0566Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0552Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0513Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0537Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0501Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0576Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0552Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0568Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0540Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0556Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0531Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0520Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0509Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0516Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0542Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0532Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0541Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0551Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0550Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0554Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0555Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0563Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0563Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0562Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0569Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0582Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0573Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0572Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0566Epoch 8/15: [================              ] 32/60 batches, loss: 0.0563Epoch 8/15: [================              ] 33/60 batches, loss: 0.0553Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0547Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0542Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0556Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0557Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0578Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0580Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0581Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0593Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0590Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0587Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0595Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0592Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0592Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0587Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0590Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0590Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0586Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0588Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0585Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0586Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0582Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0580Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0579Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0579Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0592Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0587Epoch 8/15: [==============================] 60/60 batches, loss: 0.0587
[2025-05-07 20:19:16,217][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0587
[2025-05-07 20:19:16,517][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0724, Metrics: {'mse': 0.06907357275485992, 'rmse': 0.2628185167655809, 'r2': -0.6955239772796631}
[2025-05-07 20:19:16,518][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.1059Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0900Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0690Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0575Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0546Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0519Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0496Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0515Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0505Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0492Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0501Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0503Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0540Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0547Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0545Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0542Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0538Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0548Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0538Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0553Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0546Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0596Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0596Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0600Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0619Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0610Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0622Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0632Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0620Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0621Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0618Epoch 9/15: [================              ] 32/60 batches, loss: 0.0609Epoch 9/15: [================              ] 33/60 batches, loss: 0.0605Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0595Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0593Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0595Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0595Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0594Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0588Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0594Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0592Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0590Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0594Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0589Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0586Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0586Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0583Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0579Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0578Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0581Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0585Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0586Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0579Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0572Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0566Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0573Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0578Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0584Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0583Epoch 9/15: [==============================] 60/60 batches, loss: 0.0578
[2025-05-07 20:19:18,426][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0578
[2025-05-07 20:19:18,781][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0633, Metrics: {'mse': 0.060418784618377686, 'rmse': 0.24580232834205962, 'r2': -0.48307812213897705}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0698Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0497Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0588Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0591Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0639Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0593Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0580Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0609Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0574Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0570Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0534Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0512Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0502Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0502Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0490Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0479Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0478Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0476Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0464Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0465Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0449Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0455Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0445Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0445Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0450Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0461Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0461Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0473Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0470Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0471Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0468Epoch 10/15: [================              ] 32/60 batches, loss: 0.0483Epoch 10/15: [================              ] 33/60 batches, loss: 0.0497Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0495Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0496Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0494Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0490Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0489Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0496Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0497Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0498Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0500Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0497Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0494Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0495Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0494Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0496Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0495Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0499Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0500Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0497Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0495Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0490Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0496Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0496Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0493Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0500Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0498Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0497Epoch 10/15: [==============================] 60/60 batches, loss: 0.0493
[2025-05-07 20:19:21,045][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0493
[2025-05-07 20:19:21,346][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0732, Metrics: {'mse': 0.07017870992422104, 'rmse': 0.2649126458367381, 'r2': -0.7226513624191284}
[2025-05-07 20:19:21,346][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0367Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0417Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0470Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0410Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0384Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0446Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0482Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0470Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0459Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0454Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0467Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0474Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0453Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0450Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0480Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0470Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0469Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0478Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0483Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0475Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0486Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0480Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0478Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0468Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0463Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0457Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0460Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0455Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0470Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0479Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0478Epoch 11/15: [================              ] 32/60 batches, loss: 0.0470Epoch 11/15: [================              ] 33/60 batches, loss: 0.0467Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0464Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0462Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0469Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0471Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0474Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0474Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0470Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0473Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0472Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0475Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0471Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0470Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0466Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0470Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0471Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0465Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0462Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0462Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0463Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0459Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0470Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0470Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0474Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0474Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0468Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0467Epoch 11/15: [==============================] 60/60 batches, loss: 0.0476
[2025-05-07 20:19:23,224][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0476
[2025-05-07 20:19:23,499][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0656, Metrics: {'mse': 0.06273284554481506, 'rmse': 0.2504652581593205, 'r2': -0.5398803949356079}
[2025-05-07 20:19:23,500][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0323Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0461Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0533Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0768Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0690Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0650Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0603Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0547Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0561Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0610Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0607Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0597Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0585Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0565Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0552Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0570Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0566Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0561Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0548Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0549Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0540Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0541Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0537Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0530Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0536Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0536Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0531Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0542Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0538Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0540Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0535Epoch 12/15: [================              ] 32/60 batches, loss: 0.0532Epoch 12/15: [================              ] 33/60 batches, loss: 0.0525Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0530Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0526Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0520Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0515Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0511Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0516Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0513Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0513Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0509Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0509Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0507Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0504Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0502Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0501Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0502Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0501Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0495Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0497Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0499Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0498Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0509Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0511Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0509Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0508Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0502Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0500Epoch 12/15: [==============================] 60/60 batches, loss: 0.0501
[2025-05-07 20:19:25,368][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0501
[2025-05-07 20:19:25,647][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0568, Metrics: {'mse': 0.05431469529867172, 'rmse': 0.2330551336029132, 'r2': -0.3332432508468628}
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0538Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0677Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0578Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0509Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0530Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0484Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0460Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0425Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0433Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0506Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0497Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0477Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0495Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0495Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0475Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0472Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0456Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0453Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0457Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0453Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0478Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0473Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0468Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0476Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0489Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0484Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0481Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0482Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0489Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0501Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0501Epoch 13/15: [================              ] 32/60 batches, loss: 0.0497Epoch 13/15: [================              ] 33/60 batches, loss: 0.0487Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0486Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0490Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0489Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0483Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0478Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0476Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0473Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0473Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0480Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0479Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0478Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0480Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0480Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0475Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0473Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0474Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0476Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0480Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0475Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0470Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0465Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0463Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0463Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0460Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0455Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0457Epoch 13/15: [==============================] 60/60 batches, loss: 0.0456
[2025-05-07 20:19:27,978][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0456
[2025-05-07 20:19:28,245][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0714, Metrics: {'mse': 0.06849013268947601, 'rmse': 0.2617061953593686, 'r2': -0.6812026500701904}
[2025-05-07 20:19:28,245][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0358Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0376Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0382Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0371Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0360Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0344Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0429Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0415Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0444Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0442Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0465Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0443Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0437Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0455Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0454Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0439Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0437Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0435Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0442Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0464Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0475Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0477Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0486Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0485Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0478Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0471Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0477Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0468Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0465Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0461Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0458Epoch 14/15: [================              ] 32/60 batches, loss: 0.0456Epoch 14/15: [================              ] 33/60 batches, loss: 0.0455Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0455Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0458Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0454Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0461Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0475Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0480Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0474Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0467Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0465Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0464Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0460Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0460Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0458Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0455Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0456Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0456Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0474Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0473Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0476Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0473Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0471Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0467Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0467Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0463Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0462Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0460Epoch 14/15: [==============================] 60/60 batches, loss: 0.0454
[2025-05-07 20:19:30,155][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0454
[2025-05-07 20:19:30,422][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0522, Metrics: {'mse': 0.05003248155117035, 'rmse': 0.2236794169144098, 'r2': -0.22812926769256592}
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0347Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0338Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0329Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0337Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0408Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0438Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0482Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0513Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0493Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0488Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0471Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0455Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0439Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0440Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0440Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0427Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0417Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0415Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0423Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0419Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0437Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0429Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0428Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0430Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0430Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0453Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0458Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0453Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0457Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0470Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0477Epoch 15/15: [================              ] 32/60 batches, loss: 0.0468Epoch 15/15: [================              ] 33/60 batches, loss: 0.0468Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0462Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0459Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0462Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0463Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0463Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0460Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0461Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0465Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0461Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0454Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0450Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0446Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0449Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0449Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0445Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0441Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0440Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0439Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0436Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0433Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0427Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0425Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0426Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0429Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0428Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0432Epoch 15/15: [==============================] 60/60 batches, loss: 0.0427
[2025-05-07 20:19:32,706][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0427
[2025-05-07 20:19:32,982][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0610, Metrics: {'mse': 0.05855778232216835, 'rmse': 0.24198715321720768, 'r2': -0.437396764755249}
[2025-05-07 20:19:32,983][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-07 20:19:32,983][src.training.lm_trainer][INFO] - Training completed in 36.96 seconds
[2025-05-07 20:19:32,983][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:19:35,540][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.028976259753108025, 'rmse': 0.1702241456230814, 'r2': -0.008097052574157715}
[2025-05-07 20:19:35,541][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05003248155117035, 'rmse': 0.2236794169144098, 'r2': -0.22812926769256592}
[2025-05-07 20:19:35,541][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.0375031977891922, 'rmse': 0.19365742379054876, 'r2': -0.16411030292510986}
[2025-05-07 20:19:37,193][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer2/id/id/model.pt
[2025-05-07 20:19:37,194][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▃▃▂▂▁
wandb:     best_val_mse █▆▅▃▃▂▂▁
wandb:      best_val_r2 ▁▃▄▆▆▇▇█
wandb:    best_val_rmse █▆▅▃▃▃▂▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▁▄▄▆▆▅▆▅▅▆▅▆
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆█▅▅▃▃▃▂▄▃▂▃▁▂
wandb:          val_mse █▆█▅▅▃▃▃▂▄▃▂▃▁▂
wandb:           val_r2 ▁▃▁▄▄▆▆▆▇▅▆▇▆█▇
wandb:         val_rmse █▆█▅▆▃▃▄▃▄▃▂▄▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05219
wandb:     best_val_mse 0.05003
wandb:      best_val_r2 -0.22813
wandb:    best_val_rmse 0.22368
wandb:            epoch 15
wandb:   final_test_mse 0.0375
wandb:    final_test_r2 -0.16411
wandb:  final_test_rmse 0.19366
wandb:  final_train_mse 0.02898
wandb:   final_train_r2 -0.0081
wandb: final_train_rmse 0.17022
wandb:    final_val_mse 0.05003
wandb:     final_val_r2 -0.22813
wandb:   final_val_rmse 0.22368
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04271
wandb:       train_time 36.95985
wandb:         val_loss 0.06099
wandb:          val_mse 0.05856
wandb:           val_r2 -0.4374
wandb:         val_rmse 0.24199
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_201837-4zciyh8y
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_201837-4zciyh8y/logs
Experiment probe_layer2_avg_max_depth_control1_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_avg_max_depth_control2_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_max_depth_control2_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:20:07,993][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer2/id
experiment_name: probe_layer2_avg_max_depth_control2_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:20:07,993][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:20:07,993][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 20:20:07,993][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:20:07,993][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:20:07,997][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:20:07,997][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 20:20:07,998][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:20:12,096][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:20:14,478][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:20:14,478][src.data.datasets][INFO] - Loading 'control_avg_max_depth_seed2' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:20:14,765][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_max_depth_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:21:04 2025).
[2025-05-07 20:20:14,894][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_max_depth_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:21:04 2025).
[2025-05-07 20:20:15,238][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:20:15,245][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:20:15,246][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:20:15,250][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:20:15,390][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:20:15,543][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:20:15,582][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:20:15,583][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:20:15,584][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:20:15,587][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:20:15,704][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:20:15,836][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:20:15,881][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:20:15,882][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:20:15,883][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:20:15,887][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:20:15,888][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:20:15,888][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:20:15,888][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:20:15,888][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:20:15,888][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:20:15,889][src.data.datasets][INFO] -   Mean: 0.2764, Std: 0.1695
[2025-05-07 20:20:15,889][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:20:15,889][src.data.datasets][INFO] - Sample label: 0.11100000143051147
[2025-05-07 20:20:15,889][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:20:15,889][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:20:15,889][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:20:15,889][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:20:15,889][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:20:15,890][src.data.datasets][INFO] -   Mean: 0.3215, Std: 0.2018
[2025-05-07 20:20:15,890][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:20:15,890][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:20:15,890][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:20:15,890][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:20:15,890][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:20:15,890][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:20:15,890][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:20:15,890][src.data.datasets][INFO] -   Mean: 0.2675, Std: 0.1795
[2025-05-07 20:20:15,890][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:20:15,890][src.data.datasets][INFO] - Sample label: 0.4440000057220459
[2025-05-07 20:20:15,891][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:20:15,891][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:20:15,891][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:20:15,891][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 20:20:15,891][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:20:24,565][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:20:24,566][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:20:24,567][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:20:24,567][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:20:24,570][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:20:24,570][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:20:24,570][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:20:24,570][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:20:24,571][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:20:24,571][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:20:24,571][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.2742Epoch 1/15: [=                             ] 2/60 batches, loss: 0.3900Epoch 1/15: [=                             ] 3/60 batches, loss: 0.4435Epoch 1/15: [==                            ] 4/60 batches, loss: 0.4533Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4376Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4101Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4532Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4634Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4953Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4884Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4768Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4660Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4499Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4455Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4383Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4371Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4282Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4398Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4246Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4137Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4083Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4109Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4013Epoch 1/15: [============                  ] 24/60 batches, loss: 0.3963Epoch 1/15: [============                  ] 25/60 batches, loss: 0.3968Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.3918Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3843Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3785Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3797Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3825Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3760Epoch 1/15: [================              ] 32/60 batches, loss: 0.3704Epoch 1/15: [================              ] 33/60 batches, loss: 0.3653Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3623Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3587Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3570Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3495Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3444Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3388Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3374Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3372Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3352Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3319Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3314Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3274Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3235Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3226Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3181Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3163Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3125Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3113Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3103Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3065Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3077Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3059Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3074Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3100Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3082Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3059Epoch 1/15: [==============================] 60/60 batches, loss: 0.3036
[2025-05-07 20:20:30,749][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3036
[2025-05-07 20:20:31,052][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1041, Metrics: {'mse': 0.09940773993730545, 'rmse': 0.31528992996495375, 'r2': -1.4401259422302246}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.3243Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2378Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2182Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2081Epoch 2/15: [==                            ] 5/60 batches, loss: 0.1778Epoch 2/15: [===                           ] 6/60 batches, loss: 0.1916Epoch 2/15: [===                           ] 7/60 batches, loss: 0.1933Epoch 2/15: [====                          ] 8/60 batches, loss: 0.2054Epoch 2/15: [====                          ] 9/60 batches, loss: 0.2037Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.1954Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.1933Epoch 2/15: [======                        ] 12/60 batches, loss: 0.1920Epoch 2/15: [======                        ] 13/60 batches, loss: 0.2001Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.2010Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.1974Epoch 2/15: [========                      ] 16/60 batches, loss: 0.1961Epoch 2/15: [========                      ] 17/60 batches, loss: 0.1950Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.1897Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.1893Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1896Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1894Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1921Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1889Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1851Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1896Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1864Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1826Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1807Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1788Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1772Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1762Epoch 2/15: [================              ] 32/60 batches, loss: 0.1757Epoch 2/15: [================              ] 33/60 batches, loss: 0.1744Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1747Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1762Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1741Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1733Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1719Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1710Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1734Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1751Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1752Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1733Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1744Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1735Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1738Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1736Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1722Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1732Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1722Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1712Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1710Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1700Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1692Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1682Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1668Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1661Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1660Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1648Epoch 2/15: [==============================] 60/60 batches, loss: 0.1632
[2025-05-07 20:20:33,310][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1632
[2025-05-07 20:20:33,570][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1045, Metrics: {'mse': 0.0992591604590416, 'rmse': 0.31505421828479235, 'r2': -1.436478614807129}
[2025-05-07 20:20:33,571][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.1222Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1184Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1251Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1390Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1330Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1324Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1326Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1370Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1315Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1246Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1233Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1280Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1297Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1281Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1268Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1267Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1253Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1216Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1196Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1186Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1166Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1161Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1137Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1132Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1151Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1148Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1135Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1106Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1102Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1091Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1100Epoch 3/15: [================              ] 32/60 batches, loss: 0.1098Epoch 3/15: [================              ] 33/60 batches, loss: 0.1128Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1114Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1106Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1113Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1102Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1089Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1094Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1076Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1069Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1070Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1092Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1093Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1085Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1070Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1070Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1061Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1064Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1066Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1058Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1064Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1058Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1077Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1067Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1058Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1048Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1040Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1037Epoch 3/15: [==============================] 60/60 batches, loss: 0.1049
[2025-05-07 20:20:35,461][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1049
[2025-05-07 20:20:35,772][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1060, Metrics: {'mse': 0.10129934549331665, 'rmse': 0.318275581050945, 'r2': -1.486558437347412}
[2025-05-07 20:20:35,773][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0458Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0751Epoch 4/15: [=                             ] 3/60 batches, loss: 0.0667Epoch 4/15: [==                            ] 4/60 batches, loss: 0.0956Epoch 4/15: [==                            ] 5/60 batches, loss: 0.0937Epoch 4/15: [===                           ] 6/60 batches, loss: 0.0848Epoch 4/15: [===                           ] 7/60 batches, loss: 0.0886Epoch 4/15: [====                          ] 8/60 batches, loss: 0.0901Epoch 4/15: [====                          ] 9/60 batches, loss: 0.0847Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.0777Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.0788Epoch 4/15: [======                        ] 12/60 batches, loss: 0.0804Epoch 4/15: [======                        ] 13/60 batches, loss: 0.0832Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.0819Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.0823Epoch 4/15: [========                      ] 16/60 batches, loss: 0.0831Epoch 4/15: [========                      ] 17/60 batches, loss: 0.0850Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.0854Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.0833Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.0886Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.0913Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.0927Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.0914Epoch 4/15: [============                  ] 24/60 batches, loss: 0.0941Epoch 4/15: [============                  ] 25/60 batches, loss: 0.0927Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.0931Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.0925Epoch 4/15: [==============                ] 28/60 batches, loss: 0.0945Epoch 4/15: [==============                ] 29/60 batches, loss: 0.0941Epoch 4/15: [===============               ] 30/60 batches, loss: 0.0937Epoch 4/15: [===============               ] 31/60 batches, loss: 0.0941Epoch 4/15: [================              ] 32/60 batches, loss: 0.0936Epoch 4/15: [================              ] 33/60 batches, loss: 0.0933Epoch 4/15: [=================             ] 34/60 batches, loss: 0.0935Epoch 4/15: [=================             ] 35/60 batches, loss: 0.0925Epoch 4/15: [==================            ] 36/60 batches, loss: 0.0916Epoch 4/15: [==================            ] 37/60 batches, loss: 0.0911Epoch 4/15: [===================           ] 38/60 batches, loss: 0.0921Epoch 4/15: [===================           ] 39/60 batches, loss: 0.0933Epoch 4/15: [====================          ] 40/60 batches, loss: 0.0922Epoch 4/15: [====================          ] 41/60 batches, loss: 0.0926Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.0937Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.0935Epoch 4/15: [======================        ] 44/60 batches, loss: 0.0923Epoch 4/15: [======================        ] 45/60 batches, loss: 0.0916Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.0909Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.0897Epoch 4/15: [========================      ] 48/60 batches, loss: 0.0902Epoch 4/15: [========================      ] 49/60 batches, loss: 0.0898Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.0893Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.0898Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.0904Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.0895Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.0897Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.0902Epoch 4/15: [============================  ] 56/60 batches, loss: 0.0900Epoch 4/15: [============================  ] 57/60 batches, loss: 0.0895Epoch 4/15: [============================= ] 58/60 batches, loss: 0.0893Epoch 4/15: [============================= ] 59/60 batches, loss: 0.0889Epoch 4/15: [==============================] 60/60 batches, loss: 0.0880
[2025-05-07 20:20:37,660][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0880
[2025-05-07 20:20:37,999][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0811, Metrics: {'mse': 0.07711836695671082, 'rmse': 0.27770193905824786, 'r2': -0.8929967880249023}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.0793Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0886Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0852Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0911Epoch 5/15: [==                            ] 5/60 batches, loss: 0.1018Epoch 5/15: [===                           ] 6/60 batches, loss: 0.0995Epoch 5/15: [===                           ] 7/60 batches, loss: 0.0946Epoch 5/15: [====                          ] 8/60 batches, loss: 0.0920Epoch 5/15: [====                          ] 9/60 batches, loss: 0.0917Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.0933Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1011Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1016Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1005Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1016Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.0997Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1032Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1010Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1002Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1033Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1075Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1068Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1049Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1061Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1055Epoch 5/15: [============                  ] 25/60 batches, loss: 0.1051Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1055Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.1039Epoch 5/15: [==============                ] 28/60 batches, loss: 0.1018Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1035Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1027Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1011Epoch 5/15: [================              ] 32/60 batches, loss: 0.0997Epoch 5/15: [================              ] 33/60 batches, loss: 0.0981Epoch 5/15: [=================             ] 34/60 batches, loss: 0.0962Epoch 5/15: [=================             ] 35/60 batches, loss: 0.0952Epoch 5/15: [==================            ] 36/60 batches, loss: 0.0944Epoch 5/15: [==================            ] 37/60 batches, loss: 0.0944Epoch 5/15: [===================           ] 38/60 batches, loss: 0.0939Epoch 5/15: [===================           ] 39/60 batches, loss: 0.0940Epoch 5/15: [====================          ] 40/60 batches, loss: 0.0929Epoch 5/15: [====================          ] 41/60 batches, loss: 0.0928Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.0922Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.0922Epoch 5/15: [======================        ] 44/60 batches, loss: 0.0920Epoch 5/15: [======================        ] 45/60 batches, loss: 0.0921Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.0916Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0909Epoch 5/15: [========================      ] 48/60 batches, loss: 0.0912Epoch 5/15: [========================      ] 49/60 batches, loss: 0.0899Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.0902Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0899Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0903Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0911Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0903Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0900Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0889Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0884Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0892Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0896Epoch 5/15: [==============================] 60/60 batches, loss: 0.0884
[2025-05-07 20:20:40,332][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0884
[2025-05-07 20:20:40,657][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0787, Metrics: {'mse': 0.07499055564403534, 'rmse': 0.2738440352537103, 'r2': -0.840766191482544}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.0810Epoch 6/15: [=                             ] 2/60 batches, loss: 0.0684Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0715Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0655Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0597Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0583Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0597Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0700Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0719Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0746Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0732Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0718Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0692Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0678Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0679Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0736Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0722Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0701Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0686Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0690Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0700Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0712Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0702Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0693Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0731Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0737Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0731Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0724Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0708Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0700Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0701Epoch 6/15: [================              ] 32/60 batches, loss: 0.0708Epoch 6/15: [================              ] 33/60 batches, loss: 0.0722Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0716Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0727Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0729Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0732Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0730Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0724Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0723Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0714Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0717Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0708Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0712Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0719Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0709Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0706Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0699Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0689Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0688Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0705Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0706Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0699Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0703Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0700Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0697Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0696Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0692Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0699Epoch 6/15: [==============================] 60/60 batches, loss: 0.0695
[2025-05-07 20:20:42,942][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0695
[2025-05-07 20:20:43,289][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0647, Metrics: {'mse': 0.06155814602971077, 'rmse': 0.24810914136667914, 'r2': -0.5110455751419067}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0480Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0822Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0797Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0815Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0841Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0842Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0887Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0852Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0878Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0817Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0784Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0773Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0772Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0743Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0776Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0756Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0727Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0714Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0708Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0700Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0696Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0690Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0697Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0713Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0709Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0695Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0693Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0679Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0681Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0683Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0694Epoch 7/15: [================              ] 32/60 batches, loss: 0.0692Epoch 7/15: [================              ] 33/60 batches, loss: 0.0686Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0680Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0682Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0675Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0677Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0666Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0668Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0664Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0660Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0672Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0663Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0661Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0661Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0664Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0665Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0668Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0670Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0668Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0666Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0660Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0656Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0657Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0662Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0661Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0661Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0660Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0658Epoch 7/15: [==============================] 60/60 batches, loss: 0.0659
[2025-05-07 20:20:45,561][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0659
[2025-05-07 20:20:45,869][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0725, Metrics: {'mse': 0.06912907212972641, 'rmse': 0.26292408054365507, 'r2': -0.6968863010406494}
[2025-05-07 20:20:45,869][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0377Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0280Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0356Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0389Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0412Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0396Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0420Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0419Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0417Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0439Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0437Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0467Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0503Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0534Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0518Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0515Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0515Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0504Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0493Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0502Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0508Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0513Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0530Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0538Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0543Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0533Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0536Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0543Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0542Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0536Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0537Epoch 8/15: [================              ] 32/60 batches, loss: 0.0528Epoch 8/15: [================              ] 33/60 batches, loss: 0.0530Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0534Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0531Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0529Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0530Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0537Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0536Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0551Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0550Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0558Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0564Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0562Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0559Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0560Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0557Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0563Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0560Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0566Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0576Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0575Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0578Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0576Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0575Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0574Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0573Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0579Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0578Epoch 8/15: [==============================] 60/60 batches, loss: 0.0573
[2025-05-07 20:20:47,795][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0573
[2025-05-07 20:20:48,084][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0702, Metrics: {'mse': 0.06671897321939468, 'rmse': 0.25830016109053183, 'r2': -0.6377266645431519}
[2025-05-07 20:20:48,085][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.0802Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0980Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0771Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0902Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0870Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0796Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0827Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0803Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0754Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0723Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0719Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0710Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0690Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0682Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0685Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0661Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0646Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0636Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0617Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0622Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0624Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0621Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0617Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0599Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0598Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0591Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0595Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0596Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0587Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0583Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0579Epoch 9/15: [================              ] 32/60 batches, loss: 0.0576Epoch 9/15: [================              ] 33/60 batches, loss: 0.0573Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0571Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0586Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0582Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0586Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0585Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0583Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0588Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0585Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0592Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0590Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0587Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0580Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0578Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0576Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0568Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0572Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0570Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0570Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0579Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0577Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0571Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0570Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0575Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0568Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0567Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0567Epoch 9/15: [==============================] 60/60 batches, loss: 0.0565
[2025-05-07 20:20:50,025][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0565
[2025-05-07 20:20:50,304][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0625, Metrics: {'mse': 0.05945070460438728, 'rmse': 0.24382515170586336, 'r2': -0.45931506156921387}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0452Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0622Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0662Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0591Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0543Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0509Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0471Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0459Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0455Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0473Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0478Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0477Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0487Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0477Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0476Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0480Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0472Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0477Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0498Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0533Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0523Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0542Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0541Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0536Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0529Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0537Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0542Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0545Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0547Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0547Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0543Epoch 10/15: [================              ] 32/60 batches, loss: 0.0546Epoch 10/15: [================              ] 33/60 batches, loss: 0.0549Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0544Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0554Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0555Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0549Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0553Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0566Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0582Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0581Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0576Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0574Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0570Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0563Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0558Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0556Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0552Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0557Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0556Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0555Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0552Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0550Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0545Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0543Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0538Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0538Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0537Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0536Epoch 10/15: [==============================] 60/60 batches, loss: 0.0532
[2025-05-07 20:20:52,658][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0532
[2025-05-07 20:20:52,949][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0708, Metrics: {'mse': 0.06756702065467834, 'rmse': 0.2599365704449421, 'r2': -0.6585433483123779}
[2025-05-07 20:20:52,949][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0378Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0489Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0569Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0533Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0548Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0517Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0521Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0553Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0543Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0522Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0506Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0506Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0492Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0484Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0480Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0471Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0491Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0487Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0503Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0493Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0495Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0506Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0507Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0502Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0520Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0513Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0523Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0523Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0520Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0531Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0528Epoch 11/15: [================              ] 32/60 batches, loss: 0.0519Epoch 11/15: [================              ] 33/60 batches, loss: 0.0516Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0514Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0508Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0502Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0502Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0504Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0499Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0502Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0502Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0505Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0507Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0507Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0507Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0511Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0513Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0511Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0505Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0506Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0503Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0500Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0498Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0501Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0499Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0501Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0503Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0500Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0496Epoch 11/15: [==============================] 60/60 batches, loss: 0.0492
[2025-05-07 20:20:54,809][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0492
[2025-05-07 20:20:55,112][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0634, Metrics: {'mse': 0.06057768315076828, 'rmse': 0.24612534032636355, 'r2': -0.48697853088378906}
[2025-05-07 20:20:55,113][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0495Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0787Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0686Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0662Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0617Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0592Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0547Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0512Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0509Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0495Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0491Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0530Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0526Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0504Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0508Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0514Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0525Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0522Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0539Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0566Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0550Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0558Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0544Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0554Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0557Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0548Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0550Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0553Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0545Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0543Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0536Epoch 12/15: [================              ] 32/60 batches, loss: 0.0539Epoch 12/15: [================              ] 33/60 batches, loss: 0.0535Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0529Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0525Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0533Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0532Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0532Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0529Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0524Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0520Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0512Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0510Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0510Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0511Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0512Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0515Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0511Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0513Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0508Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0508Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0503Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0506Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0521Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0521Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0515Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0515Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0512Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0514Epoch 12/15: [==============================] 60/60 batches, loss: 0.0512
[2025-05-07 20:20:57,010][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0512
[2025-05-07 20:20:57,272][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0567, Metrics: {'mse': 0.054231539368629456, 'rmse': 0.2328766612793765, 'r2': -0.33120203018188477}
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0366Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0374Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0400Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0416Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0442Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0401Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0414Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0393Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0383Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0374Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0372Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0376Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0406Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0400Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0426Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0411Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0420Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0406Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0408Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0407Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0425Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0438Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0439Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0446Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0444Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0436Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0454Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0446Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0449Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0449Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0447Epoch 13/15: [================              ] 32/60 batches, loss: 0.0454Epoch 13/15: [================              ] 33/60 batches, loss: 0.0451Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0451Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0453Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0447Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0443Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0441Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0441Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0435Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0441Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0437Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0434Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0433Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0440Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0441Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0439Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0442Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0438Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0434Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0436Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0434Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0438Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0438Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0442Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0439Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0440Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0439Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0438Epoch 13/15: [==============================] 60/60 batches, loss: 0.0436
[2025-05-07 20:20:59,553][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0436
[2025-05-07 20:20:59,834][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0565, Metrics: {'mse': 0.05408810079097748, 'rmse': 0.23256848623787676, 'r2': -0.32768118381500244}
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0469Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0406Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0400Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0404Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0458Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0543Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0530Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0485Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0466Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0493Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0461Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0440Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0433Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0441Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0431Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0425Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0427Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0444Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0440Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0441Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0430Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0416Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0419Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0415Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0406Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0404Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0429Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0419Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0432Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0458Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0465Epoch 14/15: [================              ] 32/60 batches, loss: 0.0466Epoch 14/15: [================              ] 33/60 batches, loss: 0.0468Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0460Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0460Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0464Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0462Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0462Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0457Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0456Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0454Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0456Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0453Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0452Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0451Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0448Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0447Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0449Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0456Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0457Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0453Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0452Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0453Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0453Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0450Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0452Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0447Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0446Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0447Epoch 14/15: [==============================] 60/60 batches, loss: 0.0448
[2025-05-07 20:21:02,126][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0448
[2025-05-07 20:21:02,476][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0614, Metrics: {'mse': 0.05892159044742584, 'rmse': 0.24273769885913032, 'r2': -0.44632697105407715}
[2025-05-07 20:21:02,477][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.1046Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0709Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0661Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0589Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0561Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0503Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0499Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0474Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0469Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0456Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0483Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0465Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0447Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0432Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0456Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0458Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0467Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0467Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0463Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0449Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0448Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0445Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0437Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0428Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0420Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0447Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0445Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0440Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0435Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0434Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0428Epoch 15/15: [================              ] 32/60 batches, loss: 0.0428Epoch 15/15: [================              ] 33/60 batches, loss: 0.0432Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0436Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0430Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0444Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0443Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0439Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0439Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0432Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0431Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0436Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0433Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0432Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0437Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0437Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0439Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0438Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0435Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0433Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0430Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0427Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0426Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0425Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0424Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0426Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0426Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0424Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0423Epoch 15/15: [==============================] 60/60 batches, loss: 0.0425
[2025-05-07 20:21:04,410][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0425
[2025-05-07 20:21:04,727][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0650, Metrics: {'mse': 0.062476709485054016, 'rmse': 0.24995341462971457, 'r2': -0.5335931777954102}
[2025-05-07 20:21:04,728][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
[2025-05-07 20:21:04,728][src.training.lm_trainer][INFO] - Training completed in 36.65 seconds
[2025-05-07 20:21:04,728][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:21:07,385][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.030308226123452187, 'rmse': 0.1740925791739906, 'r2': -0.05443680286407471}
[2025-05-07 20:21:07,385][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05408810079097748, 'rmse': 0.23256848623787676, 'r2': -0.32768118381500244}
[2025-05-07 20:21:07,385][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.038818761706352234, 'rmse': 0.19702477434665985, 'r2': -0.20494568347930908}
[2025-05-07 20:21:09,025][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer2/id/id/model.pt
[2025-05-07 20:21:09,026][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▄▂▂▁▁
wandb:     best_val_mse █▅▄▂▂▁▁
wandb:      best_val_r2 ▁▄▅▇▇██
wandb:    best_val_rmse █▅▄▂▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▁▁▄▄▅▅▅▆▅▅▆▆▆
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ███▄▄▂▃▃▂▃▂▁▁▂▂
wandb:          val_mse ███▄▄▂▃▃▂▃▂▁▁▂▂
wandb:           val_r2 ▁▁▁▅▅▇▆▆▇▆▇██▇▇
wandb:         val_rmse ███▅▄▂▃▃▂▃▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0565
wandb:     best_val_mse 0.05409
wandb:      best_val_r2 -0.32768
wandb:    best_val_rmse 0.23257
wandb:            epoch 15
wandb:   final_test_mse 0.03882
wandb:    final_test_r2 -0.20495
wandb:  final_test_rmse 0.19702
wandb:  final_train_mse 0.03031
wandb:   final_train_r2 -0.05444
wandb: final_train_rmse 0.17409
wandb:    final_val_mse 0.05409
wandb:     final_val_r2 -0.32768
wandb:   final_val_rmse 0.23257
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04249
wandb:       train_time 36.65208
wandb:         val_loss 0.06501
wandb:          val_mse 0.06248
wandb:           val_r2 -0.53359
wandb:         val_rmse 0.24995
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_202008-56xd3um7
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_202008-56xd3um7/logs
Experiment probe_layer2_avg_max_depth_control2_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_avg_max_depth_control3_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_max_depth_control3_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:21:40,965][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer2/id
experiment_name: probe_layer2_avg_max_depth_control3_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:21:40,965][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:21:40,965][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 20:21:40,965][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:21:40,965][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:21:40,970][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:21:40,970][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 20:21:40,970][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:21:44,575][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:21:46,824][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:21:46,824][src.data.datasets][INFO] - Loading 'control_avg_max_depth_seed3' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:21:47,039][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_max_depth_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:22:13 2025).
[2025-05-07 20:21:47,125][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_max_depth_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:22:13 2025).
[2025-05-07 20:21:47,441][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:21:47,448][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:21:47,448][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:21:47,450][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:21:47,498][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:21:47,639][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:21:47,666][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:21:47,667][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:21:47,667][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:21:47,670][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:21:47,739][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:21:47,861][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:21:47,889][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:21:47,890][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:21:47,890][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:21:47,892][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:21:47,893][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:21:47,893][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:21:47,893][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:21:47,893][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:21:47,893][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:21:47,893][src.data.datasets][INFO] -   Mean: 0.2764, Std: 0.1695
[2025-05-07 20:21:47,893][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:21:47,893][src.data.datasets][INFO] - Sample label: 0.11100000143051147
[2025-05-07 20:21:47,893][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:21:47,894][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:21:47,894][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:21:47,894][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:21:47,894][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:21:47,894][src.data.datasets][INFO] -   Mean: 0.3215, Std: 0.2018
[2025-05-07 20:21:47,894][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:21:47,894][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:21:47,894][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:21:47,894][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:21:47,894][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:21:47,895][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:21:47,895][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:21:47,895][src.data.datasets][INFO] -   Mean: 0.2675, Std: 0.1795
[2025-05-07 20:21:47,895][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:21:47,895][src.data.datasets][INFO] - Sample label: 0.4440000057220459
[2025-05-07 20:21:47,895][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:21:47,895][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:21:47,895][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:21:47,896][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 20:21:47,896][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:21:55,526][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:21:55,527][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:21:55,527][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:21:55,527][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:21:55,530][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:21:55,530][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:21:55,530][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:21:55,531][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:21:55,531][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:21:55,532][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:21:55,532][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.3368Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4135Epoch 1/15: [=                             ] 3/60 batches, loss: 0.4366Epoch 1/15: [==                            ] 4/60 batches, loss: 0.4474Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4238Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4014Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4336Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4307Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4501Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4423Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4314Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4312Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4158Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4070Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4075Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4107Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4155Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4225Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4117Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4009Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.3971Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.3939Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.3869Epoch 1/15: [============                  ] 24/60 batches, loss: 0.3811Epoch 1/15: [============                  ] 25/60 batches, loss: 0.3789Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.3775Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3710Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3654Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3675Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3726Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3670Epoch 1/15: [================              ] 32/60 batches, loss: 0.3616Epoch 1/15: [================              ] 33/60 batches, loss: 0.3568Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3569Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3531Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3518Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3458Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3422Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3419Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3386Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3380Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3375Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3333Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3337Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3305Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3262Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3248Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3202Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3197Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3160Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3147Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3121Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3100Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3108Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3094Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3075Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3095Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3079Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3060Epoch 1/15: [==============================] 60/60 batches, loss: 0.3049
[2025-05-07 20:22:02,583][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3049
[2025-05-07 20:22:02,889][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1158, Metrics: {'mse': 0.1106589287519455, 'rmse': 0.332654368304319, 'r2': -1.7163047790527344}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.3242Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2356Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2064Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2055Epoch 2/15: [==                            ] 5/60 batches, loss: 0.1827Epoch 2/15: [===                           ] 6/60 batches, loss: 0.1890Epoch 2/15: [===                           ] 7/60 batches, loss: 0.1980Epoch 2/15: [====                          ] 8/60 batches, loss: 0.1958Epoch 2/15: [====                          ] 9/60 batches, loss: 0.1967Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.1868Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.1842Epoch 2/15: [======                        ] 12/60 batches, loss: 0.1812Epoch 2/15: [======                        ] 13/60 batches, loss: 0.1900Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.1964Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.1910Epoch 2/15: [========                      ] 16/60 batches, loss: 0.1909Epoch 2/15: [========                      ] 17/60 batches, loss: 0.1907Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.1887Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.1896Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1877Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1841Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1843Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1818Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1775Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1787Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1784Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1762Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1730Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1712Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1701Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1689Epoch 2/15: [================              ] 32/60 batches, loss: 0.1661Epoch 2/15: [================              ] 33/60 batches, loss: 0.1650Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1672Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1684Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1671Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1662Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1659Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1656Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1676Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1680Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1666Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1654Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1661Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1650Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1632Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1612Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1615Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1617Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1615Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1611Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1607Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1602Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1593Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1585Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1567Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1567Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1557Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1547Epoch 2/15: [==============================] 60/60 batches, loss: 0.1536
[2025-05-07 20:22:05,165][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1536
[2025-05-07 20:22:05,426][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1104, Metrics: {'mse': 0.10494603216648102, 'rmse': 0.3239537500423186, 'r2': -1.5760724544525146}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.0886Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1107Epoch 3/15: [=                             ] 3/60 batches, loss: 0.0965Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1056Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1092Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1144Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1128Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1210Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1215Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1198Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1195Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1207Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1206Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1218Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1196Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1205Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1227Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1194Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1168Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1175Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1154Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1142Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1118Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1114Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1107Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1115Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1092Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1088Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1102Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1079Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1085Epoch 3/15: [================              ] 32/60 batches, loss: 0.1084Epoch 3/15: [================              ] 33/60 batches, loss: 0.1106Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1093Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1078Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1070Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1063Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1060Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1077Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1072Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1073Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1085Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1092Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1088Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1079Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1073Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1083Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1087Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1094Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1092Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1087Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1089Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1087Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1118Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1111Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1106Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1107Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1096Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1088Epoch 3/15: [==============================] 60/60 batches, loss: 0.1103
[2025-05-07 20:22:07,740][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1103
[2025-05-07 20:22:08,058][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0969, Metrics: {'mse': 0.09256788343191147, 'rmse': 0.30424970572198007, 'r2': -1.2722303867340088}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0475Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0756Epoch 4/15: [=                             ] 3/60 batches, loss: 0.0658Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1083Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1072Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1011Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1020Epoch 4/15: [====                          ] 8/60 batches, loss: 0.0964Epoch 4/15: [====                          ] 9/60 batches, loss: 0.0968Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.0893Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.0947Epoch 4/15: [======                        ] 12/60 batches, loss: 0.0963Epoch 4/15: [======                        ] 13/60 batches, loss: 0.0998Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.0981Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.0974Epoch 4/15: [========                      ] 16/60 batches, loss: 0.0996Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1017Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1022Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1000Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1013Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1024Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1014Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1017Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1029Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1017Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1021Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1008Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1028Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1023Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1017Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1013Epoch 4/15: [================              ] 32/60 batches, loss: 0.0993Epoch 4/15: [================              ] 33/60 batches, loss: 0.1002Epoch 4/15: [=================             ] 34/60 batches, loss: 0.0993Epoch 4/15: [=================             ] 35/60 batches, loss: 0.0995Epoch 4/15: [==================            ] 36/60 batches, loss: 0.0991Epoch 4/15: [==================            ] 37/60 batches, loss: 0.0983Epoch 4/15: [===================           ] 38/60 batches, loss: 0.0981Epoch 4/15: [===================           ] 39/60 batches, loss: 0.0968Epoch 4/15: [====================          ] 40/60 batches, loss: 0.0962Epoch 4/15: [====================          ] 41/60 batches, loss: 0.0967Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.0967Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.0961Epoch 4/15: [======================        ] 44/60 batches, loss: 0.0950Epoch 4/15: [======================        ] 45/60 batches, loss: 0.0950Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.0937Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.0922Epoch 4/15: [========================      ] 48/60 batches, loss: 0.0919Epoch 4/15: [========================      ] 49/60 batches, loss: 0.0913Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.0912Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.0912Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.0922Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.0925Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.0926Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.0921Epoch 4/15: [============================  ] 56/60 batches, loss: 0.0924Epoch 4/15: [============================  ] 57/60 batches, loss: 0.0924Epoch 4/15: [============================= ] 58/60 batches, loss: 0.0920Epoch 4/15: [============================= ] 59/60 batches, loss: 0.0915Epoch 4/15: [==============================] 60/60 batches, loss: 0.0914
[2025-05-07 20:22:10,316][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0914
[2025-05-07 20:22:10,651][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1015, Metrics: {'mse': 0.0975150540471077, 'rmse': 0.3122740047572127, 'r2': -1.3936667442321777}
[2025-05-07 20:22:10,651][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.0483Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0740Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0807Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0856Epoch 5/15: [==                            ] 5/60 batches, loss: 0.0879Epoch 5/15: [===                           ] 6/60 batches, loss: 0.0875Epoch 5/15: [===                           ] 7/60 batches, loss: 0.0875Epoch 5/15: [====                          ] 8/60 batches, loss: 0.0880Epoch 5/15: [====                          ] 9/60 batches, loss: 0.0849Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.0875Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.0919Epoch 5/15: [======                        ] 12/60 batches, loss: 0.0979Epoch 5/15: [======                        ] 13/60 batches, loss: 0.0964Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1027Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1006Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1014Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1013Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.0999Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.0997Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1005Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.0997Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.0986Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.0979Epoch 5/15: [============                  ] 24/60 batches, loss: 0.0977Epoch 5/15: [============                  ] 25/60 batches, loss: 0.0960Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.0949Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.0939Epoch 5/15: [==============                ] 28/60 batches, loss: 0.0923Epoch 5/15: [==============                ] 29/60 batches, loss: 0.0951Epoch 5/15: [===============               ] 30/60 batches, loss: 0.0955Epoch 5/15: [===============               ] 31/60 batches, loss: 0.0953Epoch 5/15: [================              ] 32/60 batches, loss: 0.0938Epoch 5/15: [================              ] 33/60 batches, loss: 0.0921Epoch 5/15: [=================             ] 34/60 batches, loss: 0.0903Epoch 5/15: [=================             ] 35/60 batches, loss: 0.0892Epoch 5/15: [==================            ] 36/60 batches, loss: 0.0884Epoch 5/15: [==================            ] 37/60 batches, loss: 0.0874Epoch 5/15: [===================           ] 38/60 batches, loss: 0.0875Epoch 5/15: [===================           ] 39/60 batches, loss: 0.0880Epoch 5/15: [====================          ] 40/60 batches, loss: 0.0871Epoch 5/15: [====================          ] 41/60 batches, loss: 0.0879Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.0895Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.0905Epoch 5/15: [======================        ] 44/60 batches, loss: 0.0895Epoch 5/15: [======================        ] 45/60 batches, loss: 0.0892Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.0888Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0873Epoch 5/15: [========================      ] 48/60 batches, loss: 0.0878Epoch 5/15: [========================      ] 49/60 batches, loss: 0.0872Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.0870Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0860Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0859Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0859Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0859Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0854Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0856Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0854Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0859Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0848Epoch 5/15: [==============================] 60/60 batches, loss: 0.0847
[2025-05-07 20:22:12,564][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0847
[2025-05-07 20:22:12,896][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0762, Metrics: {'mse': 0.07305232435464859, 'rmse': 0.2702819349395157, 'r2': -0.7931890487670898}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1131Epoch 6/15: [=                             ] 2/60 batches, loss: 0.1005Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0949Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0773Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0747Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0761Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0738Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0818Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0808Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0765Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0754Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0728Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0718Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0754Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0753Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0776Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0755Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0740Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0722Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0711Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0699Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0699Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0689Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0687Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0705Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0716Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0714Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0703Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0703Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0695Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0685Epoch 6/15: [================              ] 32/60 batches, loss: 0.0691Epoch 6/15: [================              ] 33/60 batches, loss: 0.0711Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0711Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0710Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0706Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0708Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0707Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0708Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0708Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0719Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0714Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0704Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0699Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0708Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0698Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0696Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0690Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0685Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0682Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0688Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0685Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0690Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0684Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0687Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0682Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0679Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0681Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0679Epoch 6/15: [==============================] 60/60 batches, loss: 0.0679
[2025-05-07 20:22:15,130][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0679
[2025-05-07 20:22:15,497][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0739, Metrics: {'mse': 0.07075291126966476, 'rmse': 0.26599419405254837, 'r2': -0.7367461919784546}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0252Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0887Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0912Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0894Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0915Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0867Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0832Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0804Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0782Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0767Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0738Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0714Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0720Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0706Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0732Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0740Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0733Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0732Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0744Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0728Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0707Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0689Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0698Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0704Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0698Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0685Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0699Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0684Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0687Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0687Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0691Epoch 7/15: [================              ] 32/60 batches, loss: 0.0683Epoch 7/15: [================              ] 33/60 batches, loss: 0.0696Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0687Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0697Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0696Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0685Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0682Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0673Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0679Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0679Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0685Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0679Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0673Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0668Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0670Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0666Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0663Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0663Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0662Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0664Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0671Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0663Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0663Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0656Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0657Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0658Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0652Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0657Epoch 7/15: [==============================] 60/60 batches, loss: 0.0668
[2025-05-07 20:22:17,765][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0668
[2025-05-07 20:22:18,062][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0728, Metrics: {'mse': 0.06974411010742188, 'rmse': 0.2640911019088335, 'r2': -0.7119834423065186}
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0595Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0510Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0526Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0505Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0545Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0552Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0524Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0521Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0505Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0493Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0487Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0483Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0473Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0491Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0491Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0483Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0493Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0479Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0475Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0475Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0467Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0461Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0466Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0469Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0465Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0461Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0461Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0458Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0472Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0470Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0483Epoch 8/15: [================              ] 32/60 batches, loss: 0.0482Epoch 8/15: [================              ] 33/60 batches, loss: 0.0491Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0494Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0489Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0495Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0510Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0516Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0524Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0527Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0534Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0527Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0545Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0552Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0551Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0544Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0541Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0548Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0546Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0542Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0552Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0555Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0551Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0549Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0551Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0547Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0544Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0557Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0558Epoch 8/15: [==============================] 60/60 batches, loss: 0.0562
[2025-05-07 20:22:20,416][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0562
[2025-05-07 20:22:20,754][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0623, Metrics: {'mse': 0.05967969447374344, 'rmse': 0.24429427843022325, 'r2': -0.46493589878082275}
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.1000Epoch 9/15: [=                             ] 2/60 batches, loss: 0.1032Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0775Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0646Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0612Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0582Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0588Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0628Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0600Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0577Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0571Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0559Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0543Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0568Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0622Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0604Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0599Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0601Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0598Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0587Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0583Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0580Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0580Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0597Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0596Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0597Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0605Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0610Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0617Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0614Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0624Epoch 9/15: [================              ] 32/60 batches, loss: 0.0625Epoch 9/15: [================              ] 33/60 batches, loss: 0.0621Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0616Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0611Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0611Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0609Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0616Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0627Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0636Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0628Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0624Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0618Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0620Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0615Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0607Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0606Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0610Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0613Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0611Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0607Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0606Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0598Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0593Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0596Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0606Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0604Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0600Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0604Epoch 9/15: [==============================] 60/60 batches, loss: 0.0602
[2025-05-07 20:22:23,046][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0602
[2025-05-07 20:22:23,422][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0605, Metrics: {'mse': 0.05797627940773964, 'rmse': 0.2407826393404218, 'r2': -0.423122763633728}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0381Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0562Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0499Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0510Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0517Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0483Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0457Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0505Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0481Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0518Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0490Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0480Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0482Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0493Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0524Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0521Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0513Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0498Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0503Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0503Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0508Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0516Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0512Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0514Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0505Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0506Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0521Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0517Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0520Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0517Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0515Epoch 10/15: [================              ] 32/60 batches, loss: 0.0513Epoch 10/15: [================              ] 33/60 batches, loss: 0.0522Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0516Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0517Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0509Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0507Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0504Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0516Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0510Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0529Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0524Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0529Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0540Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0535Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0539Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0536Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0538Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0536Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0537Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0547Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0549Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0545Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0537Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0534Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0538Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0538Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0532Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0534Epoch 10/15: [==============================] 60/60 batches, loss: 0.0531
[2025-05-07 20:22:25,728][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0531
[2025-05-07 20:22:26,108][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0573, Metrics: {'mse': 0.05482206866145134, 'rmse': 0.23414112979451376, 'r2': -0.3456975221633911}
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0433Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0569Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0482Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0536Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0623Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0598Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0580Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0592Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0607Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0615Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0595Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0601Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0575Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0555Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0544Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0541Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0526Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0547Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0566Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0555Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0561Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0561Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0549Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0539Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0557Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0547Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0546Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0536Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0530Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0525Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0520Epoch 11/15: [================              ] 32/60 batches, loss: 0.0530Epoch 11/15: [================              ] 33/60 batches, loss: 0.0526Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0530Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0522Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0517Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0526Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0518Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0514Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0520Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0513Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0507Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0504Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0497Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0493Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0491Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0485Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0490Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0496Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0495Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0500Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0497Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0495Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0502Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0498Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0496Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0491Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0487Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0484Epoch 11/15: [==============================] 60/60 batches, loss: 0.0483
[2025-05-07 20:22:28,407][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0483
[2025-05-07 20:22:28,702][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0639, Metrics: {'mse': 0.06127455458045006, 'rmse': 0.24753697618830617, 'r2': -0.5040843486785889}
[2025-05-07 20:22:28,702][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0594Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0503Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0439Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0487Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0509Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0543Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0509Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0503Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0469Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0466Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0468Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0462Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0456Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0445Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0447Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0465Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0452Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0452Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0469Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0470Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0454Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0459Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0454Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0451Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0460Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0459Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0454Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0456Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0470Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0469Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0472Epoch 12/15: [================              ] 32/60 batches, loss: 0.0473Epoch 12/15: [================              ] 33/60 batches, loss: 0.0485Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0482Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0487Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0483Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0477Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0497Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0501Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0499Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0507Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0501Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0497Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0500Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0496Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0493Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0491Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0494Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0491Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0486Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0487Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0491Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0493Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0502Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0505Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0500Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0502Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0501Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0502Epoch 12/15: [==============================] 60/60 batches, loss: 0.0499
[2025-05-07 20:22:30,646][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0499
[2025-05-07 20:22:30,960][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0557, Metrics: {'mse': 0.05334668979048729, 'rmse': 0.2309690234435936, 'r2': -0.30948197841644287}
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0645Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0523Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0511Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0501Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0479Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0495Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0482Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0480Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0473Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0460Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0431Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0453Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0468Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0472Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0480Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0472Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0467Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0465Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0472Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0472Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0463Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0458Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0458Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0479Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0490Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0480Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0479Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0482Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0476Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0483Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0476Epoch 13/15: [================              ] 32/60 batches, loss: 0.0474Epoch 13/15: [================              ] 33/60 batches, loss: 0.0481Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0479Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0477Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0479Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0472Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0467Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0468Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0466Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0464Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0471Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0485Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0484Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0482Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0487Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0485Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0482Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0478Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0476Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0476Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0473Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0470Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0465Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0472Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0471Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0471Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0476Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0472Epoch 13/15: [==============================] 60/60 batches, loss: 0.0469
[2025-05-07 20:22:33,233][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0469
[2025-05-07 20:22:33,587][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0618, Metrics: {'mse': 0.059417493641376495, 'rmse': 0.24375703813711, 'r2': -0.4584997892379761}
[2025-05-07 20:22:33,588][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0466Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0487Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0661Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0563Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0564Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0660Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0637Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0628Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0641Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0605Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0581Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0558Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0546Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0543Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0524Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0516Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0499Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0494Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0497Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0504Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0491Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0498Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0489Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0491Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0494Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0485Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0482Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0472Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0474Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0469Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0461Epoch 14/15: [================              ] 32/60 batches, loss: 0.0459Epoch 14/15: [================              ] 33/60 batches, loss: 0.0459Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0454Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0451Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0449Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0456Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0451Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0457Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0451Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0457Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0454Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0448Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0443Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0438Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0433Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0433Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0434Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0434Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0433Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0435Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0436Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0430Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0431Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0428Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0428Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0428Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0426Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0427Epoch 14/15: [==============================] 60/60 batches, loss: 0.0422
[2025-05-07 20:22:35,538][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0422
[2025-05-07 20:22:35,853][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0669, Metrics: {'mse': 0.06439533829689026, 'rmse': 0.25376236580094036, 'r2': -0.5806891918182373}
[2025-05-07 20:22:35,854][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0230Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0337Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0399Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0424Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0428Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0424Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0422Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0417Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0427Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0444Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0442Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0448Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0448Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0456Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0452Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0449Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0464Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0471Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0475Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0465Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0453Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0454Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0453Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0454Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0444Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0445Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0441Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0450Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0448Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0448Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0445Epoch 15/15: [================              ] 32/60 batches, loss: 0.0440Epoch 15/15: [================              ] 33/60 batches, loss: 0.0433Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0428Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0419Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0417Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0415Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0411Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0408Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0403Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0399Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0399Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0396Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0394Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0390Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0387Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0384Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0385Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0389Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0387Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0395Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0394Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0401Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0396Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0397Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0400Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0409Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0409Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0415Epoch 15/15: [==============================] 60/60 batches, loss: 0.0415
[2025-05-07 20:22:37,791][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0415
[2025-05-07 20:22:38,089][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0584, Metrics: {'mse': 0.056108295917510986, 'rmse': 0.23687189769474762, 'r2': -0.3772701025009155}
[2025-05-07 20:22:38,090][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
[2025-05-07 20:22:38,090][src.training.lm_trainer][INFO] - Training completed in 38.32 seconds
[2025-05-07 20:22:38,090][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:22:40,679][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.029920922592282295, 'rmse': 0.1729766533156492, 'r2': -0.0409623384475708}
[2025-05-07 20:22:40,680][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05334668979048729, 'rmse': 0.2309690234435936, 'r2': -0.30948197841644287}
[2025-05-07 20:22:40,680][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.038508154451847076, 'rmse': 0.19623494707071693, 'r2': -0.19530439376831055}
[2025-05-07 20:22:42,322][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer2/id/id/model.pt
[2025-05-07 20:22:42,324][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▆▃▃▃▂▂▁▁
wandb:     best_val_mse █▇▆▃▃▃▂▂▁▁
wandb:      best_val_r2 ▁▂▃▆▆▆▇▇██
wandb:    best_val_rmse █▇▆▄▃▃▂▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▃▃▅▅▅▆▆▆▆▆▆▆
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▆▆▃▃▃▂▂▁▂▁▂▂▁
wandb:          val_mse █▇▆▆▃▃▃▂▂▁▂▁▂▂▁
wandb:           val_r2 ▁▂▃▃▆▆▆▇▇█▇█▇▇█
wandb:         val_rmse █▇▆▇▄▃▃▂▂▁▂▁▂▃▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05568
wandb:     best_val_mse 0.05335
wandb:      best_val_r2 -0.30948
wandb:    best_val_rmse 0.23097
wandb:            epoch 15
wandb:   final_test_mse 0.03851
wandb:    final_test_r2 -0.1953
wandb:  final_test_rmse 0.19623
wandb:  final_train_mse 0.02992
wandb:   final_train_r2 -0.04096
wandb: final_train_rmse 0.17298
wandb:    final_val_mse 0.05335
wandb:     final_val_r2 -0.30948
wandb:   final_val_rmse 0.23097
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04147
wandb:       train_time 38.31627
wandb:         val_loss 0.05842
wandb:          val_mse 0.05611
wandb:           val_r2 -0.37727
wandb:         val_rmse 0.23687
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_202141-rffgby51
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_202141-rffgby51/logs
Experiment probe_layer2_avg_max_depth_control3_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_avg_subordinate_chain_len_control1_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_subordinate_chain_len_control1_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_subordinate_chain_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:23:15,517][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer2/id
experiment_name: probe_layer2_avg_subordinate_chain_len_control1_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:23:15,517][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:23:15,517][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 20:23:15,517][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:23:15,517][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:23:15,521][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:23:15,522][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 20:23:15,522][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:23:20,108][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:23:22,495][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:23:22,495][src.data.datasets][INFO] - Loading 'control_avg_subordinate_chain_len_seed1' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:23:22,759][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:23:31 2025).
[2025-05-07 20:23:22,884][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:23:31 2025).
[2025-05-07 20:23:23,371][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:23:23,379][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:23:23,380][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:23:23,395][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:23:23,529][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:23:23,659][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:23:23,701][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:23:23,702][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:23:23,702][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:23:23,705][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:23:23,827][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:23:23,980][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:23:24,015][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:23:24,017][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:23:24,017][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:23:24,022][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:23:24,023][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:23:24,023][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 20:23:24,023][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 20:23:24,023][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 20:23:24,023][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:23:24,024][src.data.datasets][INFO] -   Mean: 0.0833, Std: 0.1932
[2025-05-07 20:23:24,024][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:23:24,024][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:23:24,024][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:23:24,024][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 20:23:24,024][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 20:23:24,024][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 20:23:24,024][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:23:24,024][src.data.datasets][INFO] -   Mean: 0.1851, Std: 0.2231
[2025-05-07 20:23:24,024][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:23:24,025][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:23:24,025][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:23:24,025][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 20:23:24,025][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 20:23:24,025][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 20:23:24,025][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:23:24,025][src.data.datasets][INFO] -   Mean: 0.2145, Std: 0.2290
[2025-05-07 20:23:24,025][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:23:24,025][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 20:23:24,025][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:23:24,025][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:23:24,026][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:23:24,026][__main__][INFO] - Using model type: lm_probe for submetric avg_subordinate_chain_len
[2025-05-07 20:23:24,026][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:23:32,462][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:23:32,463][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:23:32,463][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:23:32,463][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:23:32,466][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:23:32,467][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:23:32,467][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:23:32,467][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:23:32,467][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:23:32,468][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:23:32,468][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.4445Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4462Epoch 1/15: [=                             ] 3/60 batches, loss: 0.5021Epoch 1/15: [==                            ] 4/60 batches, loss: 0.4872Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4370Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4243Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4548Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4613Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4856Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4796Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4580Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4470Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4335Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4294Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4237Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4231Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4150Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4262Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4222Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4152Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4097Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4179Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4078Epoch 1/15: [============                  ] 24/60 batches, loss: 0.4015Epoch 1/15: [============                  ] 25/60 batches, loss: 0.3992Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.4003Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3944Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3921Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3930Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3914Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3820Epoch 1/15: [================              ] 32/60 batches, loss: 0.3756Epoch 1/15: [================              ] 33/60 batches, loss: 0.3721Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3698Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3677Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3662Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3582Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3534Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3518Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3503Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3504Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3473Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3441Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3446Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3421Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3391Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3374Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3348Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3325Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3286Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3276Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3277Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3251Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3255Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3240Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3239Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3248Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3229Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3217Epoch 1/15: [==============================] 60/60 batches, loss: 0.3214
[2025-05-07 20:23:39,110][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3214
[2025-05-07 20:23:39,433][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1757, Metrics: {'mse': 0.16779111325740814, 'rmse': 0.40962313564715574, 'r2': -2.3701512813568115}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.3446Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2674Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2291Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2460Epoch 2/15: [==                            ] 5/60 batches, loss: 0.2093Epoch 2/15: [===                           ] 6/60 batches, loss: 0.2117Epoch 2/15: [===                           ] 7/60 batches, loss: 0.2031Epoch 2/15: [====                          ] 8/60 batches, loss: 0.2120Epoch 2/15: [====                          ] 9/60 batches, loss: 0.2094Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.2200Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.2199Epoch 2/15: [======                        ] 12/60 batches, loss: 0.2169Epoch 2/15: [======                        ] 13/60 batches, loss: 0.2203Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.2184Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.2100Epoch 2/15: [========                      ] 16/60 batches, loss: 0.2093Epoch 2/15: [========                      ] 17/60 batches, loss: 0.2078Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.2035Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.2015Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1992Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1937Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1969Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1961Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1905Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1950Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1930Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1916Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1891Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1859Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1840Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1843Epoch 2/15: [================              ] 32/60 batches, loss: 0.1814Epoch 2/15: [================              ] 33/60 batches, loss: 0.1793Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1809Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1827Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1831Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1823Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1819Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1828Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1832Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1824Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1818Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1814Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1832Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1814Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1796Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1813Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1802Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1801Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1793Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1779Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1775Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1769Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1766Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1758Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1747Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1738Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1727Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1716Epoch 2/15: [==============================] 60/60 batches, loss: 0.1701
[2025-05-07 20:23:41,796][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1701
[2025-05-07 20:23:42,075][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1655, Metrics: {'mse': 0.15657883882522583, 'rmse': 0.39570044076956223, 'r2': -2.1449482440948486}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.0760Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1011Epoch 3/15: [=                             ] 3/60 batches, loss: 0.0978Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1138Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1074Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1131Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1165Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1247Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1201Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1153Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1094Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1243Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1270Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1285Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1304Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1314Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1299Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1261Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1239Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1244Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1235Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1222Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1199Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1187Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1227Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1233Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1258Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1251Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1260Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1248Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1263Epoch 3/15: [================              ] 32/60 batches, loss: 0.1247Epoch 3/15: [================              ] 33/60 batches, loss: 0.1264Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1256Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1251Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1252Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1231Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1219Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1219Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1218Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1206Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1214Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1217Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1221Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1207Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1199Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1196Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1198Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1196Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1200Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1194Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1203Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1192Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1196Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1192Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1182Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1176Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1170Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1164Epoch 3/15: [==============================] 60/60 batches, loss: 0.1189
[2025-05-07 20:23:44,365][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1189
[2025-05-07 20:23:44,638][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1516, Metrics: {'mse': 0.1435159295797348, 'rmse': 0.37883496351278717, 'r2': -1.8825745582580566}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0451Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0754Epoch 4/15: [=                             ] 3/60 batches, loss: 0.0886Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1209Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1161Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1274Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1274Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1198Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1183Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.1103Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1059Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1065Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1054Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.1019Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.0987Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1050Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1071Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1077Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1050Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1074Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1109Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1105Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1099Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1135Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1121Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1137Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1133Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1131Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1128Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1116Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1108Epoch 4/15: [================              ] 32/60 batches, loss: 0.1098Epoch 4/15: [================              ] 33/60 batches, loss: 0.1107Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1112Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1103Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1085Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1068Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1077Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1063Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1054Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1057Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1062Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1054Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1046Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1053Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1051Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.1037Epoch 4/15: [========================      ] 48/60 batches, loss: 0.1043Epoch 4/15: [========================      ] 49/60 batches, loss: 0.1044Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.1036Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.1030Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1032Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.1030Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1022Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1022Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1032Epoch 4/15: [============================  ] 57/60 batches, loss: 0.1031Epoch 4/15: [============================= ] 58/60 batches, loss: 0.1025Epoch 4/15: [============================= ] 59/60 batches, loss: 0.1024Epoch 4/15: [==============================] 60/60 batches, loss: 0.1015
[2025-05-07 20:23:46,849][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1015
[2025-05-07 20:23:47,115][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1442, Metrics: {'mse': 0.13670577108860016, 'rmse': 0.3697374353356719, 'r2': -1.7457897663116455}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.0354Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0640Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0860Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0931Epoch 5/15: [==                            ] 5/60 batches, loss: 0.1019Epoch 5/15: [===                           ] 6/60 batches, loss: 0.0996Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1002Epoch 5/15: [====                          ] 8/60 batches, loss: 0.0977Epoch 5/15: [====                          ] 9/60 batches, loss: 0.0919Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.1007Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1021Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1046Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1031Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1044Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1037Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1052Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1051Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1021Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.0998Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1006Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.0981Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.0958Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.0970Epoch 5/15: [============                  ] 24/60 batches, loss: 0.0979Epoch 5/15: [============                  ] 25/60 batches, loss: 0.0981Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1012Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.0994Epoch 5/15: [==============                ] 28/60 batches, loss: 0.1011Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1046Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1052Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1047Epoch 5/15: [================              ] 32/60 batches, loss: 0.1038Epoch 5/15: [================              ] 33/60 batches, loss: 0.1037Epoch 5/15: [=================             ] 34/60 batches, loss: 0.1030Epoch 5/15: [=================             ] 35/60 batches, loss: 0.1036Epoch 5/15: [==================            ] 36/60 batches, loss: 0.1026Epoch 5/15: [==================            ] 37/60 batches, loss: 0.1019Epoch 5/15: [===================           ] 38/60 batches, loss: 0.1018Epoch 5/15: [===================           ] 39/60 batches, loss: 0.1018Epoch 5/15: [====================          ] 40/60 batches, loss: 0.1010Epoch 5/15: [====================          ] 41/60 batches, loss: 0.1020Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.1018Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.1016Epoch 5/15: [======================        ] 44/60 batches, loss: 0.1013Epoch 5/15: [======================        ] 45/60 batches, loss: 0.1020Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.1042Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.1035Epoch 5/15: [========================      ] 48/60 batches, loss: 0.1037Epoch 5/15: [========================      ] 49/60 batches, loss: 0.1033Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.1032Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.1025Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.1017Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.1010Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0998Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0995Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0983Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0971Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0965Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0963Epoch 5/15: [==============================] 60/60 batches, loss: 0.0957
[2025-05-07 20:23:49,327][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0957
[2025-05-07 20:23:49,577][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1359, Metrics: {'mse': 0.12900467216968536, 'rmse': 0.35917220406051104, 'r2': -1.5911102294921875}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1638Epoch 6/15: [=                             ] 2/60 batches, loss: 0.1274Epoch 6/15: [=                             ] 3/60 batches, loss: 0.1258Epoch 6/15: [==                            ] 4/60 batches, loss: 0.1076Epoch 6/15: [==                            ] 5/60 batches, loss: 0.1124Epoch 6/15: [===                           ] 6/60 batches, loss: 0.1090Epoch 6/15: [===                           ] 7/60 batches, loss: 0.1041Epoch 6/15: [====                          ] 8/60 batches, loss: 0.1108Epoch 6/15: [====                          ] 9/60 batches, loss: 0.1084Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.1015Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0977Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0951Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0956Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0928Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0903Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0972Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0942Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0915Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0893Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0873Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0875Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0863Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0865Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0865Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0849Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0878Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0868Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0847Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0832Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0828Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0823Epoch 6/15: [================              ] 32/60 batches, loss: 0.0815Epoch 6/15: [================              ] 33/60 batches, loss: 0.0820Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0817Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0817Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0808Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0807Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0804Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0793Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0787Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0772Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0780Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0773Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0785Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0775Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0769Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0782Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0800Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0797Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0796Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0805Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0800Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0798Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0802Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0798Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0796Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0795Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0789Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0787Epoch 6/15: [==============================] 60/60 batches, loss: 0.0780
[2025-05-07 20:23:51,821][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0780
[2025-05-07 20:23:52,099][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.1061, Metrics: {'mse': 0.10094781965017319, 'rmse': 0.3177228661116055, 'r2': -1.0275771617889404}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0398Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0975Epoch 7/15: [=                             ] 3/60 batches, loss: 0.1033Epoch 7/15: [==                            ] 4/60 batches, loss: 0.1018Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0904Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0854Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0886Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0822Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0865Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0855Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0845Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0842Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0854Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0833Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0822Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0833Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0830Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0849Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0844Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0843Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0828Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0808Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0817Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0811Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0809Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0797Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0807Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0796Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0808Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0798Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0793Epoch 7/15: [================              ] 32/60 batches, loss: 0.0789Epoch 7/15: [================              ] 33/60 batches, loss: 0.0779Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0769Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0753Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0765Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0767Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0767Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0762Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0765Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0756Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0770Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0756Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0753Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0750Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0753Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0777Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0782Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0787Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0781Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0777Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0777Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0773Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0768Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0766Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0761Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0764Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0757Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0756Epoch 7/15: [==============================] 60/60 batches, loss: 0.0769
[2025-05-07 20:23:54,361][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0769
[2025-05-07 20:23:54,648][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1072, Metrics: {'mse': 0.10214674472808838, 'rmse': 0.3196040436666726, 'r2': -1.0516581535339355}
[2025-05-07 20:23:54,649][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0535Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0631Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0598Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0631Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0722Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0709Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0676Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0693Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0659Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0710Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0748Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0731Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0690Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0691Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0670Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0686Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0680Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0659Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0669Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0662Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0652Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0662Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0671Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0692Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0687Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0678Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0686Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0688Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0686Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0673Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0667Epoch 8/15: [================              ] 32/60 batches, loss: 0.0661Epoch 8/15: [================              ] 33/60 batches, loss: 0.0663Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0672Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0665Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0678Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0673Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0676Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0677Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0664Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0670Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0680Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0672Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0674Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0672Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0665Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0670Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0671Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0667Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0667Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0673Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0669Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0668Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0666Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0661Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0656Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0657Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0659Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0657Epoch 8/15: [==============================] 60/60 batches, loss: 0.0661
[2025-05-07 20:23:56,508][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0661
[2025-05-07 20:23:56,762][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.1063, Metrics: {'mse': 0.10080119967460632, 'rmse': 0.31749204663204766, 'r2': -1.024632215499878}
[2025-05-07 20:23:56,763][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.1410Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0865Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0830Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0795Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0729Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0748Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0715Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0694Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0685Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0736Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0747Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0721Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0743Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0733Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0735Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0710Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0728Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0730Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0711Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0708Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0693Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0693Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0688Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0671Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0667Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0669Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0674Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0682Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0691Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0678Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0671Epoch 9/15: [================              ] 32/60 batches, loss: 0.0671Epoch 9/15: [================              ] 33/60 batches, loss: 0.0673Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0675Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0669Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0666Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0682Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0678Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0674Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0715Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0707Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0698Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0695Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0687Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0679Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0668Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0673Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0671Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0674Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0666Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0664Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0665Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0661Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0652Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0653Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0658Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0658Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0658Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0653Epoch 9/15: [==============================] 60/60 batches, loss: 0.0645
[2025-05-07 20:23:58,650][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0645
[2025-05-07 20:23:58,902][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.1022, Metrics: {'mse': 0.09695747494697571, 'rmse': 0.3113799527056546, 'r2': -0.9474295377731323}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0385Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0494Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0578Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0617Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0640Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0629Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0600Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0562Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0552Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0559Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0534Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0513Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0508Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0517Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0544Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0565Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0556Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0558Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0552Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0557Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0552Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0578Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0571Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0571Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0572Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0571Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0569Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0586Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0579Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0590Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0593Epoch 10/15: [================              ] 32/60 batches, loss: 0.0615Epoch 10/15: [================              ] 33/60 batches, loss: 0.0612Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0612Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0612Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0607Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0605Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0602Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0597Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0595Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0602Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0624Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0635Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0630Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0629Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0630Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0636Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0634Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0634Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0629Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0629Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0628Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0619Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0615Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0609Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0603Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0609Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0614Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0612Epoch 10/15: [==============================] 60/60 batches, loss: 0.0605
[2025-05-07 20:24:01,223][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0605
[2025-05-07 20:24:01,528][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0974, Metrics: {'mse': 0.09251800924539566, 'rmse': 0.3041677320910219, 'r2': -0.8582611083984375}
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.1126Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0753Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0634Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0611Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0572Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0613Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0609Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0597Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0558Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0601Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0630Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0640Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0623Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0625Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0617Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0591Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0589Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0622Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0638Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0640Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0646Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0643Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0640Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0633Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0620Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0613Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0605Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0594Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0587Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0580Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0581Epoch 11/15: [================              ] 32/60 batches, loss: 0.0576Epoch 11/15: [================              ] 33/60 batches, loss: 0.0564Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0572Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0564Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0579Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0581Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0573Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0567Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0570Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0563Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0563Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0554Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0558Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0564Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0562Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0568Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0567Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0576Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0571Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0565Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0581Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0582Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0580Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0575Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0578Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0577Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0573Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0572Epoch 11/15: [==============================] 60/60 batches, loss: 0.0572
[2025-05-07 20:24:03,767][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0572
[2025-05-07 20:24:04,042][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0919, Metrics: {'mse': 0.08725877106189728, 'rmse': 0.2953959564074926, 'r2': -0.7526271343231201}
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0525Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0728Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0684Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0741Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0726Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0699Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0725Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0695Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0670Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0637Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0673Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0652Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0662Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0668Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0668Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0664Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0649Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0654Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0645Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0668Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0651Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0643Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0631Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0644Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0654Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0639Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0640Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0642Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0643Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0650Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0646Epoch 12/15: [================              ] 32/60 batches, loss: 0.0644Epoch 12/15: [================              ] 33/60 batches, loss: 0.0643Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0635Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0626Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0622Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0612Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0614Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0616Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0605Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0600Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0601Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0598Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0599Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0591Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0581Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0574Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0569Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0579Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0593Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0588Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0581Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0582Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0583Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0588Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0596Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0595Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0595Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0592Epoch 12/15: [==============================] 60/60 batches, loss: 0.0591
[2025-05-07 20:24:06,328][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0591
[2025-05-07 20:24:06,654][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0852, Metrics: {'mse': 0.08104399591684341, 'rmse': 0.28468227186961154, 'r2': -0.6278009414672852}
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0326Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0332Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0397Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0556Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0512Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0485Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0462Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0497Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0491Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0493Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0473Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0499Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0496Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0494Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0488Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0482Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0468Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0476Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0502Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0498Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0502Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0499Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0503Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0504Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0502Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0484Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0484Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0486Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0485Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0489Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0481Epoch 13/15: [================              ] 32/60 batches, loss: 0.0476Epoch 13/15: [================              ] 33/60 batches, loss: 0.0480Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0481Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0488Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0495Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0490Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0481Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0478Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0486Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0488Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0508Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0500Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0517Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0526Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0525Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0528Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0526Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0534Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0531Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0533Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0532Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0534Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0542Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0543Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0536Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0537Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0546Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0544Epoch 13/15: [==============================] 60/60 batches, loss: 0.0540
[2025-05-07 20:24:08,908][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0540
[2025-05-07 20:24:09,238][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0796, Metrics: {'mse': 0.0759783387184143, 'rmse': 0.27564168537870737, 'r2': -0.5260552167892456}
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0490Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0508Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0489Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0511Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0464Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0484Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0456Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0490Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0534Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0512Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0510Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0514Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0517Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0508Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0488Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0471Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0464Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0463Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0489Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0500Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0492Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0521Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0539Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0537Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0529Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0533Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0537Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0544Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0535Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0540Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0536Epoch 14/15: [================              ] 32/60 batches, loss: 0.0532Epoch 14/15: [================              ] 33/60 batches, loss: 0.0529Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0526Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0525Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0527Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0523Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0522Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0524Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0526Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0520Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0519Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0521Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0522Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0523Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0524Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0530Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0535Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0562Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0565Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0558Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0558Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0555Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0553Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0549Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0545Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0544Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0541Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0535Epoch 14/15: [==============================] 60/60 batches, loss: 0.0530
[2025-05-07 20:24:11,545][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0530
[2025-05-07 20:24:11,870][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0880, Metrics: {'mse': 0.08395632356405258, 'rmse': 0.2897521761161641, 'r2': -0.6862961053848267}
[2025-05-07 20:24:11,871][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0806Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0866Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0800Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0721Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0686Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0650Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0619Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0601Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0613Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0589Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0557Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0550Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0529Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0509Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0495Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0506Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0502Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0505Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0514Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0506Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0549Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0560Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0549Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0545Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0541Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0538Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0551Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0542Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0538Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0544Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0539Epoch 15/15: [================              ] 32/60 batches, loss: 0.0535Epoch 15/15: [================              ] 33/60 batches, loss: 0.0527Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0535Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0545Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0546Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0556Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0549Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0543Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0539Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0552Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0542Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0538Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0534Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0537Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0536Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0539Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0540Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0535Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0538Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0537Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0537Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0535Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0536Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0538Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0539Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0540Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0544Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0538Epoch 15/15: [==============================] 60/60 batches, loss: 0.0532
[2025-05-07 20:24:13,765][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0532
[2025-05-07 20:24:14,042][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0847, Metrics: {'mse': 0.08090423047542572, 'rmse': 0.2844366897490999, 'r2': -0.6249936819076538}
[2025-05-07 20:24:14,043][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
[2025-05-07 20:24:14,043][src.training.lm_trainer][INFO] - Training completed in 37.76 seconds
[2025-05-07 20:24:14,043][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:24:16,685][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03771071881055832, 'rmse': 0.19419247876928272, 'r2': -0.009853363037109375}
[2025-05-07 20:24:16,686][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.0759783387184143, 'rmse': 0.27564168537870737, 'r2': -0.5260552167892456}
[2025-05-07 20:24:16,686][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.08909633010625839, 'rmse': 0.29849008376537134, 'r2': -0.6993064880371094}
[2025-05-07 20:24:18,328][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer2/id/id/model.pt
[2025-05-07 20:24:18,329][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▆▆▅▃▃▂▂▁▁
wandb:     best_val_mse █▇▆▆▅▃▃▂▂▁▁
wandb:      best_val_r2 ▁▂▃▃▄▆▆▇▇██
wandb:    best_val_rmse █▇▆▆▅▃▃▂▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▃▃▄▅▅▅▆▆▆▆▇▆
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▆▆▅▃▃▃▃▂▂▁▁▂▁
wandb:          val_mse █▇▆▆▅▃▃▃▃▂▂▁▁▂▁
wandb:           val_r2 ▁▂▃▃▄▆▆▆▆▇▇██▇█
wandb:         val_rmse █▇▆▆▅▃▃▃▃▂▂▁▁▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07965
wandb:     best_val_mse 0.07598
wandb:      best_val_r2 -0.52606
wandb:    best_val_rmse 0.27564
wandb:            epoch 15
wandb:   final_test_mse 0.0891
wandb:    final_test_r2 -0.69931
wandb:  final_test_rmse 0.29849
wandb:  final_train_mse 0.03771
wandb:   final_train_r2 -0.00985
wandb: final_train_rmse 0.19419
wandb:    final_val_mse 0.07598
wandb:     final_val_r2 -0.52606
wandb:   final_val_rmse 0.27564
wandb:    learning_rate 0.0001
wandb:       train_loss 0.05319
wandb:       train_time 37.75908
wandb:         val_loss 0.08475
wandb:          val_mse 0.0809
wandb:           val_r2 -0.62499
wandb:         val_rmse 0.28444
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_202315-jmy9otjx
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_202315-jmy9otjx/logs
Experiment probe_layer2_avg_subordinate_chain_len_control1_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_avg_subordinate_chain_len_control2_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_subordinate_chain_len_control2_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=avg_subordinate_chain_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:24:52,411][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer2/id
experiment_name: probe_layer2_avg_subordinate_chain_len_control2_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:24:52,411][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:24:52,411][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 20:24:52,412][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:24:52,412][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:24:52,417][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:24:52,417][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 20:24:52,417][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:24:56,115][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:24:58,431][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:24:58,432][src.data.datasets][INFO] - Loading 'control_avg_subordinate_chain_len_seed2' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:24:58,600][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:25:20 2025).
[2025-05-07 20:24:58,689][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:25:20 2025).
[2025-05-07 20:24:58,966][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:24:58,973][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:24:58,973][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:24:58,977][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:24:59,056][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:24:59,192][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:24:59,245][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:24:59,246][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:24:59,246][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:24:59,251][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:24:59,332][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:24:59,428][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:24:59,464][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:24:59,465][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:24:59,466][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:24:59,468][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:24:59,468][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:24:59,468][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 20:24:59,468][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 20:24:59,468][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 20:24:59,469][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:24:59,469][src.data.datasets][INFO] -   Mean: 0.0833, Std: 0.1932
[2025-05-07 20:24:59,469][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:24:59,469][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:24:59,469][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:24:59,469][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 20:24:59,469][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 20:24:59,469][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 20:24:59,469][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:24:59,470][src.data.datasets][INFO] -   Mean: 0.1851, Std: 0.2231
[2025-05-07 20:24:59,470][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:24:59,470][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:24:59,470][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:24:59,470][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 20:24:59,470][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 20:24:59,470][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 20:24:59,470][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:24:59,470][src.data.datasets][INFO] -   Mean: 0.2145, Std: 0.2290
[2025-05-07 20:24:59,471][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:24:59,471][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 20:24:59,471][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:24:59,471][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:24:59,471][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:24:59,471][__main__][INFO] - Using model type: lm_probe for submetric avg_subordinate_chain_len
[2025-05-07 20:24:59,471][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:25:06,891][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:25:06,892][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:25:06,892][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:25:06,892][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:25:06,895][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:25:06,896][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:25:06,896][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:25:06,896][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:25:06,896][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:25:06,897][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:25:06,897][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.3457Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4699Epoch 1/15: [=                             ] 3/60 batches, loss: 0.5082Epoch 1/15: [==                            ] 4/60 batches, loss: 0.5027Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4617Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4273Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4751Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4738Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4867Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4866Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4755Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4687Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4555Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4514Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4544Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4507Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4505Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4598Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4472Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4388Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4340Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4352Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4227Epoch 1/15: [============                  ] 24/60 batches, loss: 0.4207Epoch 1/15: [============                  ] 25/60 batches, loss: 0.4171Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.4183Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.4089Epoch 1/15: [==============                ] 28/60 batches, loss: 0.4057Epoch 1/15: [==============                ] 29/60 batches, loss: 0.4047Epoch 1/15: [===============               ] 30/60 batches, loss: 0.4057Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3961Epoch 1/15: [================              ] 32/60 batches, loss: 0.3891Epoch 1/15: [================              ] 33/60 batches, loss: 0.3842Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3809Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3784Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3787Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3712Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3657Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3611Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3610Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3594Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3572Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3529Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3528Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3498Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3456Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3449Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3396Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3376Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3362Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3351Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3345Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3310Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3299Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3286Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3306Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3315Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3291Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3278Epoch 1/15: [==============================] 60/60 batches, loss: 0.3249
[2025-05-07 20:25:12,756][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3249
[2025-05-07 20:25:13,001][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1549, Metrics: {'mse': 0.14850984513759613, 'rmse': 0.3853697511969461, 'r2': -1.982879400253296}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.3151Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2563Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2201Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2210Epoch 2/15: [==                            ] 5/60 batches, loss: 0.1870Epoch 2/15: [===                           ] 6/60 batches, loss: 0.2019Epoch 2/15: [===                           ] 7/60 batches, loss: 0.1983Epoch 2/15: [====                          ] 8/60 batches, loss: 0.2003Epoch 2/15: [====                          ] 9/60 batches, loss: 0.1934Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.1895Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.1878Epoch 2/15: [======                        ] 12/60 batches, loss: 0.1842Epoch 2/15: [======                        ] 13/60 batches, loss: 0.1913Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.1906Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.1835Epoch 2/15: [========                      ] 16/60 batches, loss: 0.1817Epoch 2/15: [========                      ] 17/60 batches, loss: 0.1852Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.1831Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.1826Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1846Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1859Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1872Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1876Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1843Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1876Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1863Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1849Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1829Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1792Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1791Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1797Epoch 2/15: [================              ] 32/60 batches, loss: 0.1766Epoch 2/15: [================              ] 33/60 batches, loss: 0.1756Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1783Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1790Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1772Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1754Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1753Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1748Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1775Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1794Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1789Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1766Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1804Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1812Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1804Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1820Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1812Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1808Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1793Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1788Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1782Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1777Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1766Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1761Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1745Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1744Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1741Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1724Epoch 2/15: [==============================] 60/60 batches, loss: 0.1702
[2025-05-07 20:25:15,264][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1702
[2025-05-07 20:25:15,494][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1329, Metrics: {'mse': 0.12636734545230865, 'rmse': 0.35548184968055496, 'r2': -1.5381383895874023}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.0862Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1454Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1295Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1358Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1351Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1418Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1347Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1386Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1453Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1393Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1389Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1430Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1471Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1437Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1410Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1374Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1383Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1393Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1366Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1377Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1393Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1414Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1377Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1358Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1353Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1355Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1338Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1307Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1308Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1293Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1291Epoch 3/15: [================              ] 32/60 batches, loss: 0.1279Epoch 3/15: [================              ] 33/60 batches, loss: 0.1297Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1274Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1256Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1270Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1254Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1233Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1234Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1221Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1210Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1219Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1219Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1209Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1192Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1188Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1187Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1175Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1183Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1218Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1213Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1216Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1207Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1226Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1229Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1218Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1207Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1204Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1194Epoch 3/15: [==============================] 60/60 batches, loss: 0.1221
[2025-05-07 20:25:17,793][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1221
[2025-05-07 20:25:18,134][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1224, Metrics: {'mse': 0.11614090949296951, 'rmse': 0.34079452679432737, 'r2': -1.3327364921569824}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0737Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0901Epoch 4/15: [=                             ] 3/60 batches, loss: 0.0928Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1234Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1177Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1107Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1134Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1175Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1111Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.1048Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1009Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1014Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1035Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.1004Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.0996Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1058Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1067Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1035Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1036Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1063Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1070Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1056Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1058Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1066Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1055Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1069Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1058Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1058Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1048Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1039Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1034Epoch 4/15: [================              ] 32/60 batches, loss: 0.1032Epoch 4/15: [================              ] 33/60 batches, loss: 0.1027Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1045Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1034Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1018Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1014Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1012Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1009Epoch 4/15: [====================          ] 40/60 batches, loss: 0.0993Epoch 4/15: [====================          ] 41/60 batches, loss: 0.0998Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1009Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1022Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1012Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1028Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1020Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.1007Epoch 4/15: [========================      ] 48/60 batches, loss: 0.1012Epoch 4/15: [========================      ] 49/60 batches, loss: 0.1009Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.1001Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.0996Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1000Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.0997Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1002Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1001Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1009Epoch 4/15: [============================  ] 57/60 batches, loss: 0.1010Epoch 4/15: [============================= ] 58/60 batches, loss: 0.1009Epoch 4/15: [============================= ] 59/60 batches, loss: 0.1005Epoch 4/15: [==============================] 60/60 batches, loss: 0.1009
[2025-05-07 20:25:20,435][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1009
[2025-05-07 20:25:20,753][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1274, Metrics: {'mse': 0.12059429287910461, 'rmse': 0.3472668899839209, 'r2': -1.4221842288970947}
[2025-05-07 20:25:20,754][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.0492Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0648Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0702Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0793Epoch 5/15: [==                            ] 5/60 batches, loss: 0.0985Epoch 5/15: [===                           ] 6/60 batches, loss: 0.0960Epoch 5/15: [===                           ] 7/60 batches, loss: 0.0951Epoch 5/15: [====                          ] 8/60 batches, loss: 0.0956Epoch 5/15: [====                          ] 9/60 batches, loss: 0.0906Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.0965Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1005Epoch 5/15: [======                        ] 12/60 batches, loss: 0.0985Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1010Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1032Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1032Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1052Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1058Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1028Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1038Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1054Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1040Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1031Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1027Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1020Epoch 5/15: [============                  ] 25/60 batches, loss: 0.1004Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1016Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.1012Epoch 5/15: [==============                ] 28/60 batches, loss: 0.0999Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1007Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1007Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1008Epoch 5/15: [================              ] 32/60 batches, loss: 0.0995Epoch 5/15: [================              ] 33/60 batches, loss: 0.0975Epoch 5/15: [=================             ] 34/60 batches, loss: 0.0971Epoch 5/15: [=================             ] 35/60 batches, loss: 0.0969Epoch 5/15: [==================            ] 36/60 batches, loss: 0.0958Epoch 5/15: [==================            ] 37/60 batches, loss: 0.0958Epoch 5/15: [===================           ] 38/60 batches, loss: 0.0957Epoch 5/15: [===================           ] 39/60 batches, loss: 0.0956Epoch 5/15: [====================          ] 40/60 batches, loss: 0.0950Epoch 5/15: [====================          ] 41/60 batches, loss: 0.0942Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.0943Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.0937Epoch 5/15: [======================        ] 44/60 batches, loss: 0.0932Epoch 5/15: [======================        ] 45/60 batches, loss: 0.0922Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.0918Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0929Epoch 5/15: [========================      ] 48/60 batches, loss: 0.0921Epoch 5/15: [========================      ] 49/60 batches, loss: 0.0909Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.0922Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0916Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0914Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0912Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0921Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0922Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0918Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0918Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0948Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0945Epoch 5/15: [==============================] 60/60 batches, loss: 0.0938
[2025-05-07 20:25:22,611][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0938
[2025-05-07 20:25:22,967][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1060, Metrics: {'mse': 0.10041037946939468, 'rmse': 0.3168759685892805, 'r2': -1.016782522201538}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1124Epoch 6/15: [=                             ] 2/60 batches, loss: 0.1372Epoch 6/15: [=                             ] 3/60 batches, loss: 0.1208Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0995Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0875Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0897Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0926Epoch 6/15: [====                          ] 8/60 batches, loss: 0.1005Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0959Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0957Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0934Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0917Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0921Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0930Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0895Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0916Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0902Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0876Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0865Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0848Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0841Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0860Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0854Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0845Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0851Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0865Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0860Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0840Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0828Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0820Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0809Epoch 6/15: [================              ] 32/60 batches, loss: 0.0805Epoch 6/15: [================              ] 33/60 batches, loss: 0.0807Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0823Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0820Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0820Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0844Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0833Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0835Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0824Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0813Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0818Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0806Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0804Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0817Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0810Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0805Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0801Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0790Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0782Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0788Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0788Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0784Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0783Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0790Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0785Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0787Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0780Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0785Epoch 6/15: [==============================] 60/60 batches, loss: 0.0778
[2025-05-07 20:25:25,259][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0778
[2025-05-07 20:25:25,557][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.1010, Metrics: {'mse': 0.09575269371271133, 'rmse': 0.30943932153608295, 'r2': -0.9232310056686401}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0383Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0681Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0719Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0746Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0874Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0917Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0910Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0994Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0937Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0881Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0851Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0856Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0845Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0850Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0861Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0854Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0841Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0849Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0864Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0845Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0837Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0832Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0842Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0860Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0853Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0851Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0839Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0819Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0808Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0806Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0820Epoch 7/15: [================              ] 32/60 batches, loss: 0.0809Epoch 7/15: [================              ] 33/60 batches, loss: 0.0797Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0783Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0784Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0778Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0772Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0762Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0755Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0759Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0764Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0770Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0761Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0761Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0765Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0762Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0759Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0767Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0764Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0764Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0758Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0751Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0747Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0753Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0761Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0757Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0754Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0744Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0744Epoch 7/15: [==============================] 60/60 batches, loss: 0.0742
[2025-05-07 20:25:27,865][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0742
[2025-05-07 20:25:28,227][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1206, Metrics: {'mse': 0.11416694521903992, 'rmse': 0.33788599441089584, 'r2': -1.293088674545288}
[2025-05-07 20:25:28,228][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0472Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0469Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0415Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0546Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0662Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0781Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0730Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0726Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0738Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0776Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0755Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0748Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0727Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0707Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0684Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0681Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0671Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0657Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0656Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0679Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0669Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0655Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0653Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0661Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0653Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0641Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0644Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0642Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0635Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0635Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0623Epoch 8/15: [================              ] 32/60 batches, loss: 0.0622Epoch 8/15: [================              ] 33/60 batches, loss: 0.0639Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0630Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0622Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0639Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0643Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0640Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0650Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0647Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0656Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0673Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0682Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0696Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0702Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0703Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0699Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0704Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0704Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0702Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0703Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0701Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0700Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0697Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0695Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0693Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0688Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0690Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0684Epoch 8/15: [==============================] 60/60 batches, loss: 0.0683
[2025-05-07 20:25:30,156][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0683
[2025-05-07 20:25:30,531][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.1070, Metrics: {'mse': 0.10114127397537231, 'rmse': 0.31802715917885427, 'r2': -1.0314626693725586}
[2025-05-07 20:25:30,531][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.1028Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0810Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0768Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0663Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0672Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0627Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0630Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0585Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0578Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0651Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0644Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0639Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0645Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0634Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0618Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0665Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0644Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0643Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0653Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0656Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0646Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0647Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0644Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0646Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0661Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0657Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0676Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0678Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0672Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0666Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0661Epoch 9/15: [================              ] 32/60 batches, loss: 0.0659Epoch 9/15: [================              ] 33/60 batches, loss: 0.0649Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0645Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0649Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0649Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0660Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0653Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0670Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0674Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0671Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0668Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0678Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0679Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0669Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0667Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0669Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0659Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0666Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0663Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0671Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0684Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0678Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0673Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0668Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0674Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0671Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0670Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0663Epoch 9/15: [==============================] 60/60 batches, loss: 0.0660
[2025-05-07 20:25:32,442][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0660
[2025-05-07 20:25:32,749][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0859, Metrics: {'mse': 0.08139211684465408, 'rmse': 0.2852930367966489, 'r2': -0.6347930431365967}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0509Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0618Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0587Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0546Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0555Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0511Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0550Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0506Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0529Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0587Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0566Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0546Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0576Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0552Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0577Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0593Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0584Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0584Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0581Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0592Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0584Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0603Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0588Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0605Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0601Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0606Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0637Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0651Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0667Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0661Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0666Epoch 10/15: [================              ] 32/60 batches, loss: 0.0673Epoch 10/15: [================              ] 33/60 batches, loss: 0.0666Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0659Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0654Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0672Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0661Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0654Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0665Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0659Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0672Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0670Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0667Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0672Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0671Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0670Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0662Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0672Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0679Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0675Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0670Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0665Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0662Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0663Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0661Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0663Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0660Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0656Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0652Epoch 10/15: [==============================] 60/60 batches, loss: 0.0648
[2025-05-07 20:25:35,035][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0648
[2025-05-07 20:25:35,421][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0903, Metrics: {'mse': 0.0856798067688942, 'rmse': 0.2927111319524664, 'r2': -0.7209130525588989}
[2025-05-07 20:25:35,421][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0455Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0731Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0588Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0585Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0733Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0690Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0762Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0754Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0766Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0737Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0744Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0724Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0709Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0697Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0681Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0651Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0646Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0676Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0687Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0672Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0683Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0680Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0672Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0669Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0666Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0651Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0664Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0666Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0657Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0650Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0644Epoch 11/15: [================              ] 32/60 batches, loss: 0.0634Epoch 11/15: [================              ] 33/60 batches, loss: 0.0625Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0621Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0609Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0621Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0621Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0613Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0607Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0607Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0610Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0607Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0602Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0594Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0585Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0590Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0594Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0587Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0586Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0591Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0587Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0593Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0596Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0595Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0598Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0603Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0599Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0595Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0595Epoch 11/15: [==============================] 60/60 batches, loss: 0.0593
[2025-05-07 20:25:37,324][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0593
[2025-05-07 20:25:37,638][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0878, Metrics: {'mse': 0.08335904031991959, 'rmse': 0.28871965696834634, 'r2': -0.6742995977401733}
[2025-05-07 20:25:37,639][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0699Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0647Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0544Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0480Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0458Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0465Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0450Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0526Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0511Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0485Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0483Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0534Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0527Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0517Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0584Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0622Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0633Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0658Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0660Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0677Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0653Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0641Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0633Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0638Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0643Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0646Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0647Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0637Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0624Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0621Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0625Epoch 12/15: [================              ] 32/60 batches, loss: 0.0628Epoch 12/15: [================              ] 33/60 batches, loss: 0.0621Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0610Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0604Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0602Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0595Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0597Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0597Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0601Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0592Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0588Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0600Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0594Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0603Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0605Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0595Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0588Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0588Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0587Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0585Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0580Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0581Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0579Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0580Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0572Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0569Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0564Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0567Epoch 12/15: [==============================] 60/60 batches, loss: 0.0584
[2025-05-07 20:25:39,605][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0584
[2025-05-07 20:25:39,949][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0920, Metrics: {'mse': 0.08733801543712616, 'rmse': 0.29553005843251573, 'r2': -0.7542188167572021}
[2025-05-07 20:25:39,950][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0828Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0898Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0709Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0679Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0671Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0620Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0636Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0631Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0607Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0613Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0579Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0594Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0614Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0616Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0581Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0562Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0547Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0561Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0562Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0548Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0545Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0534Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0528Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0518Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0515Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0519Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0514Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0510Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0507Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0517Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0506Epoch 13/15: [================              ] 32/60 batches, loss: 0.0518Epoch 13/15: [================              ] 33/60 batches, loss: 0.0527Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0523Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0532Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0531Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0527Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0527Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0548Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0541Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0541Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0538Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0536Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0542Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0538Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0538Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0532Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0530Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0530Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0538Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0535Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0543Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0544Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0539Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0542Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0543Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0548Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0545Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0541Epoch 13/15: [==============================] 60/60 batches, loss: 0.0539
[2025-05-07 20:25:41,916][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0539
[2025-05-07 20:25:42,229][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0995, Metrics: {'mse': 0.09457269310951233, 'rmse': 0.30752673560117066, 'r2': -0.8995301723480225}
[2025-05-07 20:25:42,231][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 20:25:42,231][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 13
[2025-05-07 20:25:42,231][src.training.lm_trainer][INFO] - Training completed in 32.24 seconds
[2025-05-07 20:25:42,231][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:25:44,997][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03846760839223862, 'rmse': 0.19613160987520248, 'r2': -0.03012216091156006}
[2025-05-07 20:25:44,998][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.08139211684465408, 'rmse': 0.2852930367966489, 'r2': -0.6347930431365967}
[2025-05-07 20:25:44,998][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.09550528228282928, 'rmse': 0.3090392892219843, 'r2': -0.821542501449585}
[2025-05-07 20:25:46,684][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer2/id/id/model.pt
[2025-05-07 20:25:46,685][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▃▃▁
wandb:     best_val_mse █▆▅▃▂▁
wandb:      best_val_r2 ▁▃▄▆▇█
wandb:    best_val_rmse █▆▅▃▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▄▃▅▅▄▅▆▆▆▆
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▅▅▃▃▅▃▁▁▁▂▂
wandb:          val_mse █▆▅▅▃▂▄▃▁▁▁▂▂
wandb:           val_r2 ▁▃▄▄▆▇▅▆███▇▇
wandb:         val_rmse █▆▅▅▃▃▅▃▁▂▁▂▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.08586
wandb:     best_val_mse 0.08139
wandb:      best_val_r2 -0.63479
wandb:    best_val_rmse 0.28529
wandb: early_stop_epoch 13
wandb:            epoch 13
wandb:   final_test_mse 0.09551
wandb:    final_test_r2 -0.82154
wandb:  final_test_rmse 0.30904
wandb:  final_train_mse 0.03847
wandb:   final_train_r2 -0.03012
wandb: final_train_rmse 0.19613
wandb:    final_val_mse 0.08139
wandb:     final_val_r2 -0.63479
wandb:   final_val_rmse 0.28529
wandb:    learning_rate 0.0001
wandb:       train_loss 0.05387
wandb:       train_time 32.23763
wandb:         val_loss 0.09949
wandb:          val_mse 0.09457
wandb:           val_r2 -0.89953
wandb:         val_rmse 0.30753
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_202452-q03tvfuv
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_202452-q03tvfuv/logs
Experiment probe_layer2_avg_subordinate_chain_len_control2_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_avg_subordinate_chain_len_control3_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_subordinate_chain_len_control3_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control3/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=avg_subordinate_chain_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:26:16,467][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control3/layer2/id
experiment_name: probe_layer2_avg_subordinate_chain_len_control3_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:26:16,468][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:26:16,468][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 20:26:16,468][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:26:16,468][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:26:16,472][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:26:16,472][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 20:26:16,472][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:26:20,088][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:26:22,442][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:26:22,443][src.data.datasets][INFO] - Loading 'control_avg_subordinate_chain_len_seed3' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:26:22,700][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:27:11 2025).
[2025-05-07 20:26:22,805][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:27:11 2025).
[2025-05-07 20:26:23,068][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:26:23,075][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:26:23,075][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:26:23,079][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:26:23,177][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:26:23,272][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:26:23,307][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:26:23,308][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:26:23,309][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:26:23,311][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:26:23,412][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:26:23,532][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:26:23,558][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:26:23,559][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:26:23,559][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:26:23,562][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:26:23,563][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:26:23,563][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 20:26:23,563][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 20:26:23,563][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 20:26:23,563][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:26:23,563][src.data.datasets][INFO] -   Mean: 0.0833, Std: 0.1932
[2025-05-07 20:26:23,563][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:26:23,563][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:26:23,564][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:26:23,564][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 20:26:23,564][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 20:26:23,564][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 20:26:23,564][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:26:23,564][src.data.datasets][INFO] -   Mean: 0.1851, Std: 0.2231
[2025-05-07 20:26:23,564][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:26:23,564][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:26:23,564][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:26:23,564][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 20:26:23,565][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 20:26:23,565][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 20:26:23,565][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:26:23,565][src.data.datasets][INFO] -   Mean: 0.2145, Std: 0.2290
[2025-05-07 20:26:23,565][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:26:23,565][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 20:26:23,565][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:26:23,565][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:26:23,565][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:26:23,566][__main__][INFO] - Using model type: lm_probe for submetric avg_subordinate_chain_len
[2025-05-07 20:26:23,566][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:26:30,703][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:26:30,704][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:26:30,704][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:26:30,704][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:26:30,707][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:26:30,708][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:26:30,708][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:26:30,708][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:26:30,708][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:26:30,709][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:26:30,709][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.4006Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4587Epoch 1/15: [=                             ] 3/60 batches, loss: 0.5056Epoch 1/15: [==                            ] 4/60 batches, loss: 0.5067Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4685Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4403Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4738Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4762Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4845Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4843Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4671Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4627Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4463Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4430Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4418Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4454Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4448Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4608Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4465Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4386Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4332Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4344Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4242Epoch 1/15: [============                  ] 24/60 batches, loss: 0.4177Epoch 1/15: [============                  ] 25/60 batches, loss: 0.4148Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.4093Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.4024Epoch 1/15: [==============                ] 28/60 batches, loss: 0.4048Epoch 1/15: [==============                ] 29/60 batches, loss: 0.4031Epoch 1/15: [===============               ] 30/60 batches, loss: 0.4011Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3955Epoch 1/15: [================              ] 32/60 batches, loss: 0.3920Epoch 1/15: [================              ] 33/60 batches, loss: 0.3870Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3838Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3797Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3790Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3723Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3663Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3625Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3588Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3580Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3564Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3525Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3533Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3504Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3459Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3459Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3411Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3382Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3337Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3323Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3317Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3296Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3300Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3284Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3281Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3279Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3271Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3255Epoch 1/15: [==============================] 60/60 batches, loss: 0.3226
[2025-05-07 20:26:37,388][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3226
[2025-05-07 20:26:37,677][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1891, Metrics: {'mse': 0.18099310994148254, 'rmse': 0.4254328500967956, 'r2': -2.6353185176849365}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.3720Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2806Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2515Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2373Epoch 2/15: [==                            ] 5/60 batches, loss: 0.2048Epoch 2/15: [===                           ] 6/60 batches, loss: 0.2059Epoch 2/15: [===                           ] 7/60 batches, loss: 0.2149Epoch 2/15: [====                          ] 8/60 batches, loss: 0.2151Epoch 2/15: [====                          ] 9/60 batches, loss: 0.2178Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.2091Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.2092Epoch 2/15: [======                        ] 12/60 batches, loss: 0.2051Epoch 2/15: [======                        ] 13/60 batches, loss: 0.2106Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.2176Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.2078Epoch 2/15: [========                      ] 16/60 batches, loss: 0.2093Epoch 2/15: [========                      ] 17/60 batches, loss: 0.2097Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.2020Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.2007Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1984Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1940Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1931Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1940Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1891Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1943Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1923Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1905Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1885Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1844Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1826Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1808Epoch 2/15: [================              ] 32/60 batches, loss: 0.1782Epoch 2/15: [================              ] 33/60 batches, loss: 0.1762Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1786Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1777Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1759Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1767Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1782Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1803Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1833Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1860Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1847Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1849Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1847Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1827Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1806Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1780Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1778Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1778Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1769Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1786Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1785Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1785Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1773Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1760Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1743Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1744Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1734Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1724Epoch 2/15: [==============================] 60/60 batches, loss: 0.1708
[2025-05-07 20:26:39,979][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1708
[2025-05-07 20:26:40,223][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1568, Metrics: {'mse': 0.1492207646369934, 'rmse': 0.38629103618514554, 'r2': -1.9971582889556885}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.0787Epoch 3/15: [=                             ] 2/60 batches, loss: 0.0866Epoch 3/15: [=                             ] 3/60 batches, loss: 0.0835Epoch 3/15: [==                            ] 4/60 batches, loss: 0.0979Epoch 3/15: [==                            ] 5/60 batches, loss: 0.0985Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1182Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1194Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1270Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1255Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1243Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1206Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1237Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1346Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1321Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1319Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1336Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1333Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1310Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1280Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1308Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1310Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1292Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1272Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1235Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1223Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1258Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1256Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1231Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1256Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1248Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1274Epoch 3/15: [================              ] 32/60 batches, loss: 0.1264Epoch 3/15: [================              ] 33/60 batches, loss: 0.1280Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1261Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1243Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1270Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1257Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1231Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1245Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1239Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1244Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1254Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1248Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1243Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1234Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1235Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1229Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1245Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1247Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1258Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1263Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1259Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1244Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1263Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1252Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1244Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1234Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1226Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1227Epoch 3/15: [==============================] 60/60 batches, loss: 0.1244
[2025-05-07 20:26:42,539][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1244
[2025-05-07 20:26:42,837][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1438, Metrics: {'mse': 0.13641750812530518, 'rmse': 0.3693474084453621, 'r2': -1.7400000095367432}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.1179Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0904Epoch 4/15: [=                             ] 3/60 batches, loss: 0.1015Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1213Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1308Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1239Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1320Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1295Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1237Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.1175Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1146Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1122Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1109Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.1092Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.1062Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1087Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1099Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1125Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1142Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1189Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1203Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1231Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1219Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1236Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1222Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1213Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1190Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1182Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1182Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1160Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1160Epoch 4/15: [================              ] 32/60 batches, loss: 0.1149Epoch 4/15: [================              ] 33/60 batches, loss: 0.1141Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1153Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1143Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1127Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1113Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1102Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1087Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1082Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1074Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1068Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1062Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1050Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1050Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1043Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.1031Epoch 4/15: [========================      ] 48/60 batches, loss: 0.1044Epoch 4/15: [========================      ] 49/60 batches, loss: 0.1036Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.1028Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.1030Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1046Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.1043Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1048Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1043Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1039Epoch 4/15: [============================  ] 57/60 batches, loss: 0.1030Epoch 4/15: [============================= ] 58/60 batches, loss: 0.1021Epoch 4/15: [============================= ] 59/60 batches, loss: 0.1014Epoch 4/15: [==============================] 60/60 batches, loss: 0.1005
[2025-05-07 20:26:45,070][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1005
[2025-05-07 20:26:45,319][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1467, Metrics: {'mse': 0.13938011229038239, 'rmse': 0.37333645989962244, 'r2': -1.7995049953460693}
[2025-05-07 20:26:45,320][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.0954Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0984Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0962Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0818Epoch 5/15: [==                            ] 5/60 batches, loss: 0.0923Epoch 5/15: [===                           ] 6/60 batches, loss: 0.0926Epoch 5/15: [===                           ] 7/60 batches, loss: 0.0893Epoch 5/15: [====                          ] 8/60 batches, loss: 0.0943Epoch 5/15: [====                          ] 9/60 batches, loss: 0.0911Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.0961Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.0954Epoch 5/15: [======                        ] 12/60 batches, loss: 0.0959Epoch 5/15: [======                        ] 13/60 batches, loss: 0.0979Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1049Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1027Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1037Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1021Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1013Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1006Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1014Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1016Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1007Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1013Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1002Epoch 5/15: [============                  ] 25/60 batches, loss: 0.0999Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.0991Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.0969Epoch 5/15: [==============                ] 28/60 batches, loss: 0.0964Epoch 5/15: [==============                ] 29/60 batches, loss: 0.0967Epoch 5/15: [===============               ] 30/60 batches, loss: 0.0969Epoch 5/15: [===============               ] 31/60 batches, loss: 0.0964Epoch 5/15: [================              ] 32/60 batches, loss: 0.0948Epoch 5/15: [================              ] 33/60 batches, loss: 0.0936Epoch 5/15: [=================             ] 34/60 batches, loss: 0.0935Epoch 5/15: [=================             ] 35/60 batches, loss: 0.0924Epoch 5/15: [==================            ] 36/60 batches, loss: 0.0921Epoch 5/15: [==================            ] 37/60 batches, loss: 0.0948Epoch 5/15: [===================           ] 38/60 batches, loss: 0.0945Epoch 5/15: [===================           ] 39/60 batches, loss: 0.0940Epoch 5/15: [====================          ] 40/60 batches, loss: 0.0929Epoch 5/15: [====================          ] 41/60 batches, loss: 0.0937Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.0939Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.0941Epoch 5/15: [======================        ] 44/60 batches, loss: 0.0941Epoch 5/15: [======================        ] 45/60 batches, loss: 0.0934Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.0940Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0932Epoch 5/15: [========================      ] 48/60 batches, loss: 0.0946Epoch 5/15: [========================      ] 49/60 batches, loss: 0.0940Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.0936Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0935Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0929Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0935Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0934Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0942Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0937Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0938Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0936Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0928Epoch 5/15: [==============================] 60/60 batches, loss: 0.0940
[2025-05-07 20:26:47,168][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0940
[2025-05-07 20:26:47,452][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1072, Metrics: {'mse': 0.10192128270864487, 'rmse': 0.3192511279676939, 'r2': -1.0471293926239014}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.0680Epoch 6/15: [=                             ] 2/60 batches, loss: 0.0758Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0740Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0636Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0639Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0647Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0648Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0756Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0740Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0691Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0676Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0649Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0647Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0683Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0696Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0697Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0707Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0712Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0727Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0720Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0700Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0705Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0720Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0716Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0750Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0756Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0766Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0762Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0769Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0770Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0764Epoch 6/15: [================              ] 32/60 batches, loss: 0.0761Epoch 6/15: [================              ] 33/60 batches, loss: 0.0775Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0793Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0805Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0815Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0811Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0809Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0807Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0805Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0798Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0811Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0805Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0804Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0810Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0819Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0817Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0809Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0803Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0791Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0801Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0794Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0790Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0792Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0798Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0794Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0797Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0795Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0805Epoch 6/15: [==============================] 60/60 batches, loss: 0.0802
[2025-05-07 20:26:49,695][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0802
[2025-05-07 20:26:49,949][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.1179, Metrics: {'mse': 0.11176608502864838, 'rmse': 0.33431435061727216, 'r2': -1.2448663711547852}
[2025-05-07 20:26:49,949][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0418Epoch 7/15: [=                             ] 2/60 batches, loss: 0.1111Epoch 7/15: [=                             ] 3/60 batches, loss: 0.1011Epoch 7/15: [==                            ] 4/60 batches, loss: 0.1033Epoch 7/15: [==                            ] 5/60 batches, loss: 0.1011Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0955Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0882Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0843Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0845Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0788Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0777Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0764Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0731Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0714Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0701Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0730Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0724Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0762Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0773Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0793Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0789Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0788Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0774Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0771Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0757Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0762Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0765Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0753Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0753Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0746Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0743Epoch 7/15: [================              ] 32/60 batches, loss: 0.0736Epoch 7/15: [================              ] 33/60 batches, loss: 0.0726Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0723Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0739Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0730Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0729Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0721Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0717Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0713Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0709Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0714Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0713Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0708Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0703Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0716Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0716Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0711Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0708Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0713Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0718Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0725Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0731Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0722Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0730Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0727Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0736Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0735Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0734Epoch 7/15: [==============================] 60/60 batches, loss: 0.0750
[2025-05-07 20:26:51,816][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0750
[2025-05-07 20:26:52,140][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1058, Metrics: {'mse': 0.10064765810966492, 'rmse': 0.31725015068501533, 'r2': -1.0215482711791992}
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0664Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0556Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0544Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0568Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0667Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0688Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0813Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0780Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0754Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0819Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0803Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0787Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0775Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0779Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0758Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0756Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0755Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0745Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0750Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0751Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0734Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0739Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0726Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0721Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0703Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0694Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0700Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0700Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0697Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0720Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0722Epoch 8/15: [================              ] 32/60 batches, loss: 0.0709Epoch 8/15: [================              ] 33/60 batches, loss: 0.0719Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0709Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0705Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0703Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0705Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0715Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0711Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0704Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0719Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0708Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0702Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0716Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0709Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0702Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0709Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0710Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0708Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0705Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0719Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0715Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0709Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0706Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0700Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0697Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0690Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0693Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0687Epoch 8/15: [==============================] 60/60 batches, loss: 0.0680
[2025-05-07 20:26:54,423][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0680
[2025-05-07 20:26:54,750][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.1039, Metrics: {'mse': 0.09864670038223267, 'rmse': 0.314080722716681, 'r2': -0.9813582897186279}
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.1136Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0877Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0816Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0695Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0664Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0717Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0661Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0611Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0606Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0672Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0656Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0661Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0657Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0650Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0678Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0667Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0655Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0703Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0688Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0716Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0704Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0694Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0698Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0701Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0698Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0731Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0743Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0742Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0762Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0764Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0760Epoch 9/15: [================              ] 32/60 batches, loss: 0.0748Epoch 9/15: [================              ] 33/60 batches, loss: 0.0736Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0722Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0712Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0704Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0697Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0708Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0703Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0703Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0694Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0696Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0686Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0691Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0685Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0674Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0668Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0667Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0667Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0670Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0662Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0665Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0658Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0657Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0657Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0667Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0664Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0676Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0669Epoch 9/15: [==============================] 60/60 batches, loss: 0.0671
[2025-05-07 20:26:56,975][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0671
[2025-05-07 20:26:57,286][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0814, Metrics: {'mse': 0.07749651372432709, 'rmse': 0.27838195653513015, 'r2': -0.5565483570098877}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0450Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0558Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0582Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0605Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0601Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0579Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0571Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0605Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0606Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0625Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0637Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0693Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0685Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0697Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0701Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0694Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0678Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0653Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0638Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0654Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0650Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0642Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0631Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0631Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0623Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0621Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0622Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0638Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0641Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0644Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0636Epoch 10/15: [================              ] 32/60 batches, loss: 0.0634Epoch 10/15: [================              ] 33/60 batches, loss: 0.0625Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0623Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0626Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0623Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0614Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0607Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0613Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0613Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0610Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0612Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0615Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0622Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0620Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0618Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0615Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0618Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0619Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0619Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0631Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0626Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0618Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0621Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0615Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0620Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0617Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0616Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0614Epoch 10/15: [==============================] 60/60 batches, loss: 0.0611
[2025-05-07 20:26:59,542][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0611
[2025-05-07 20:26:59,804][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0925, Metrics: {'mse': 0.08790162950754166, 'rmse': 0.29648208969099915, 'r2': -0.765539288520813}
[2025-05-07 20:26:59,804][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0458Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0552Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0490Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0452Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0477Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0524Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0555Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0576Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0567Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0574Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0545Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0552Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0538Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0584Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0558Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0538Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0533Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0551Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0594Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0595Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0615Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0612Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0596Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0601Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0593Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0583Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0580Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0590Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0591Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0583Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0583Epoch 11/15: [================              ] 32/60 batches, loss: 0.0580Epoch 11/15: [================              ] 33/60 batches, loss: 0.0579Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0579Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0573Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0575Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0578Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0575Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0588Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0592Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0592Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0583Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0591Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0586Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0583Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0574Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0568Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0565Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0573Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0573Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0570Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0568Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0569Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0581Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0579Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0584Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0592Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0586Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0590Epoch 11/15: [==============================] 60/60 batches, loss: 0.0595
[2025-05-07 20:27:01,688][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0595
[2025-05-07 20:27:01,947][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.1128, Metrics: {'mse': 0.10716455429792404, 'rmse': 0.3273599766280601, 'r2': -1.1524426937103271}
[2025-05-07 20:27:01,948][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0463Epoch 12/15: [=                             ] 2/60 batches, loss: 0.1007Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0796Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0756Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0635Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0570Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0570Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0539Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0501Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0503Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0537Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0521Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0519Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0507Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0530Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0546Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0546Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0539Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0558Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0570Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0563Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0557Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0542Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0538Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0561Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0556Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0556Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0555Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0559Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0555Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0567Epoch 12/15: [================              ] 32/60 batches, loss: 0.0568Epoch 12/15: [================              ] 33/60 batches, loss: 0.0570Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0578Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0569Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0573Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0569Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0579Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0576Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0572Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0581Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0572Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0577Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0581Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0576Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0568Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0560Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0553Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0551Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0545Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0547Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0553Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0551Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0549Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0549Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0550Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0555Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0559Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0556Epoch 12/15: [==============================] 60/60 batches, loss: 0.0557
[2025-05-07 20:27:03,818][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0557
[2025-05-07 20:27:04,088][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0880, Metrics: {'mse': 0.08397059887647629, 'rmse': 0.2897768087278143, 'r2': -0.6865829229354858}
[2025-05-07 20:27:04,089][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0306Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0537Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0486Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0563Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0554Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0515Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0523Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0503Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0521Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0507Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0499Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0528Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0528Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0527Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0551Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0583Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0570Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0556Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0565Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0559Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0547Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0539Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0533Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0537Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0551Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0554Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0558Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0546Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0540Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0530Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0532Epoch 13/15: [================              ] 32/60 batches, loss: 0.0523Epoch 13/15: [================              ] 33/60 batches, loss: 0.0518Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0528Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0523Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0517Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0515Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0521Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0517Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0508Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0507Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0514Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0520Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0514Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0514Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0513Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0510Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0508Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0506Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0508Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0516Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0521Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0529Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0530Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0532Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0537Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0540Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0543Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0538Epoch 13/15: [==============================] 60/60 batches, loss: 0.0533
[2025-05-07 20:27:05,983][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0533
[2025-05-07 20:27:06,240][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0793, Metrics: {'mse': 0.0758446529507637, 'rmse': 0.27539907942976805, 'r2': -0.5233700275421143}
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0699Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0489Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0500Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0492Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0505Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0471Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0553Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0550Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0604Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0577Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0554Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0543Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0554Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0560Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0582Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0578Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0574Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0572Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0568Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0565Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0565Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0577Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0559Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0552Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0552Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0546Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0546Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0546Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0542Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0532Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0533Epoch 14/15: [================              ] 32/60 batches, loss: 0.0524Epoch 14/15: [================              ] 33/60 batches, loss: 0.0519Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0519Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0524Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0521Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0549Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0550Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0552Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0545Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0537Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0534Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0529Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0524Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0523Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0521Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0517Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0512Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0521Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0521Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0521Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0516Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0512Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0509Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0510Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0508Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0507Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0503Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0503Epoch 14/15: [==============================] 60/60 batches, loss: 0.0497
[2025-05-07 20:27:08,561][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0497
[2025-05-07 20:27:08,832][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0951, Metrics: {'mse': 0.09071359038352966, 'rmse': 0.30118696914629234, 'r2': -0.8220185041427612}
[2025-05-07 20:27:08,833][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0223Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0487Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0430Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0648Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0650Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0595Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0612Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0558Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0546Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0531Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0505Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0547Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0531Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0514Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0497Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0480Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0470Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0470Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0481Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0466Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0457Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0457Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0452Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0471Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0464Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0464Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0469Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0462Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0462Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0457Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0455Epoch 15/15: [================              ] 32/60 batches, loss: 0.0466Epoch 15/15: [================              ] 33/60 batches, loss: 0.0467Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0461Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0474Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0471Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0473Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0466Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0464Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0465Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0457Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0474Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0469Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0467Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0466Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0467Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0467Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0468Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0469Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0466Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0464Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0466Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0465Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0466Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0461Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0472Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0471Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0472Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0475Epoch 15/15: [==============================] 60/60 batches, loss: 0.0477
[2025-05-07 20:27:10,702][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0477
[2025-05-07 20:27:11,010][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0838, Metrics: {'mse': 0.07993561029434204, 'rmse': 0.2827288635678042, 'r2': -0.6055386066436768}
[2025-05-07 20:27:11,010][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
[2025-05-07 20:27:11,010][src.training.lm_trainer][INFO] - Training completed in 36.40 seconds
[2025-05-07 20:27:11,010][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:27:13,533][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.038041096180677414, 'rmse': 0.1950412678913809, 'r2': -0.018700480461120605}
[2025-05-07 20:27:13,534][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.0758446529507637, 'rmse': 0.27539907942976805, 'r2': -0.5233700275421143}
[2025-05-07 20:27:13,534][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.08893829584121704, 'rmse': 0.29822524346744533, 'r2': -0.6962924003601074}
[2025-05-07 20:27:15,212][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control3/layer2/id/id/model.pt
[2025-05-07 20:27:15,213][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▃▃▃▁▁
wandb:     best_val_mse █▆▅▃▃▃▁▁
wandb:      best_val_r2 ▁▃▄▆▆▆██
wandb:    best_val_rmse █▆▅▃▃▃▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▄▄▆▅▆▆▇▆▆▆▇▆
wandb:       train_loss █▄▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▅▅▃▃▃▃▁▂▃▂▁▂▁
wandb:          val_mse █▆▅▅▃▃▃▃▁▂▃▂▁▂▁
wandb:           val_r2 ▁▃▄▄▆▆▆▆█▇▆▇█▇█
wandb:         val_rmse █▆▅▆▃▄▃▃▁▂▃▂▁▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07928
wandb:     best_val_mse 0.07584
wandb:      best_val_r2 -0.52337
wandb:    best_val_rmse 0.2754
wandb:            epoch 15
wandb:   final_test_mse 0.08894
wandb:    final_test_r2 -0.69629
wandb:  final_test_rmse 0.29823
wandb:  final_train_mse 0.03804
wandb:   final_train_r2 -0.0187
wandb: final_train_rmse 0.19504
wandb:    final_val_mse 0.07584
wandb:     final_val_r2 -0.52337
wandb:   final_val_rmse 0.2754
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04775
wandb:       train_time 36.40187
wandb:         val_loss 0.08379
wandb:          val_mse 0.07994
wandb:           val_r2 -0.60554
wandb:         val_rmse 0.28273
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_202616-cvg4m1nh
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_202616-cvg4m1nh/logs
Experiment probe_layer2_avg_subordinate_chain_len_control3_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control3/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_avg_verb_edges_control1_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_verb_edges_control1_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control1/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_verb_edges"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:27:41,717][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control1/layer2/id
experiment_name: probe_layer2_avg_verb_edges_control1_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:27:41,718][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:27:41,718][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 20:27:41,718][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:27:41,718][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:27:41,722][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:27:41,722][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 20:27:41,723][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:27:44,641][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:27:47,119][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:27:47,119][src.data.datasets][INFO] - Loading 'control_avg_verb_edges_seed1' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:27:47,222][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_verb_edges_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:29:02 2025).
[2025-05-07 20:27:47,286][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_verb_edges_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:29:02 2025).
[2025-05-07 20:27:47,466][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:27:47,473][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:27:47,474][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:27:47,475][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:27:47,546][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:27:47,601][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:27:47,629][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:27:47,630][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:27:47,630][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:27:47,632][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:27:47,692][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:27:47,768][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:27:47,794][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:27:47,795][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:27:47,796][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:27:47,797][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:27:47,798][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:27:47,798][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 20:27:47,798][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 20:27:47,798][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 20:27:47,798][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:27:47,798][src.data.datasets][INFO] -   Mean: 0.3916, Std: 0.3139
[2025-05-07 20:27:47,798][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:27:47,798][src.data.datasets][INFO] - Sample label: 0.75
[2025-05-07 20:27:47,799][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:27:47,799][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 20:27:47,799][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 20:27:47,799][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 20:27:47,799][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-05-07 20:27:47,799][src.data.datasets][INFO] -   Mean: 0.3834, Std: 0.2455
[2025-05-07 20:27:47,799][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:27:47,799][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:27:47,799][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:27:47,799][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 20:27:47,799][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 20:27:47,800][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 20:27:47,800][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:27:47,800][src.data.datasets][INFO] -   Mean: 0.4230, Std: 0.2527
[2025-05-07 20:27:47,800][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:27:47,800][src.data.datasets][INFO] - Sample label: 0.5830000042915344
[2025-05-07 20:27:47,800][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:27:47,800][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:27:47,800][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:27:47,801][__main__][INFO] - Using model type: lm_probe for submetric avg_verb_edges
[2025-05-07 20:27:47,801][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:27:53,928][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:27:53,928][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:27:53,929][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:27:53,929][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:27:53,932][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:27:53,932][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:27:53,932][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:27:53,932][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:27:53,933][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:27:53,933][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:27:53,933][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.4427Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4841Epoch 1/15: [=                             ] 3/60 batches, loss: 0.5329Epoch 1/15: [==                            ] 4/60 batches, loss: 0.5037Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4682Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4299Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4778Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4739Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4843Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.5013Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4969Epoch 1/15: [======                        ] 12/60 batches, loss: 0.5074Epoch 1/15: [======                        ] 13/60 batches, loss: 0.5052Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4938Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4855Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4887Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4798Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4863Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4712Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4585Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4488Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4476Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4402Epoch 1/15: [============                  ] 24/60 batches, loss: 0.4385Epoch 1/15: [============                  ] 25/60 batches, loss: 0.4358Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.4338Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.4299Epoch 1/15: [==============                ] 28/60 batches, loss: 0.4316Epoch 1/15: [==============                ] 29/60 batches, loss: 0.4404Epoch 1/15: [===============               ] 30/60 batches, loss: 0.4465Epoch 1/15: [===============               ] 31/60 batches, loss: 0.4418Epoch 1/15: [================              ] 32/60 batches, loss: 0.4394Epoch 1/15: [================              ] 33/60 batches, loss: 0.4342Epoch 1/15: [=================             ] 34/60 batches, loss: 0.4295Epoch 1/15: [=================             ] 35/60 batches, loss: 0.4256Epoch 1/15: [==================            ] 36/60 batches, loss: 0.4226Epoch 1/15: [==================            ] 37/60 batches, loss: 0.4171Epoch 1/15: [===================           ] 38/60 batches, loss: 0.4106Epoch 1/15: [===================           ] 39/60 batches, loss: 0.4046Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3993Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3997Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3986Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3950Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3955Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3941Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3886Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3878Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3847Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3833Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3813Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3802Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3780Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3751Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3748Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3714Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3739Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3765Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3755Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3727Epoch 1/15: [==============================] 60/60 batches, loss: 0.3686
[2025-05-07 20:27:59,761][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3686
[2025-05-07 20:28:00,070][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1127, Metrics: {'mse': 0.11833332479000092, 'rmse': 0.3439961115913971, 'r2': -0.9628778696060181}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.3717Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2805Epoch 2/15: [=                             ] 3/60 batches, loss: 0.3012Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2942Epoch 2/15: [==                            ] 5/60 batches, loss: 0.2732Epoch 2/15: [===                           ] 6/60 batches, loss: 0.3069Epoch 2/15: [===                           ] 7/60 batches, loss: 0.3167Epoch 2/15: [====                          ] 8/60 batches, loss: 0.3208Epoch 2/15: [====                          ] 9/60 batches, loss: 0.3159Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.3150Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.3062Epoch 2/15: [======                        ] 12/60 batches, loss: 0.3050Epoch 2/15: [======                        ] 13/60 batches, loss: 0.3082Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.3089Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.2981Epoch 2/15: [========                      ] 16/60 batches, loss: 0.2915Epoch 2/15: [========                      ] 17/60 batches, loss: 0.2889Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.2864Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.2896Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.2871Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.2811Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.2782Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.2821Epoch 2/15: [============                  ] 24/60 batches, loss: 0.2778Epoch 2/15: [============                  ] 25/60 batches, loss: 0.2804Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.2780Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.2772Epoch 2/15: [==============                ] 28/60 batches, loss: 0.2771Epoch 2/15: [==============                ] 29/60 batches, loss: 0.2738Epoch 2/15: [===============               ] 30/60 batches, loss: 0.2724Epoch 2/15: [===============               ] 31/60 batches, loss: 0.2695Epoch 2/15: [================              ] 32/60 batches, loss: 0.2637Epoch 2/15: [================              ] 33/60 batches, loss: 0.2614Epoch 2/15: [=================             ] 34/60 batches, loss: 0.2632Epoch 2/15: [=================             ] 35/60 batches, loss: 0.2632Epoch 2/15: [==================            ] 36/60 batches, loss: 0.2646Epoch 2/15: [==================            ] 37/60 batches, loss: 0.2646Epoch 2/15: [===================           ] 38/60 batches, loss: 0.2625Epoch 2/15: [===================           ] 39/60 batches, loss: 0.2608Epoch 2/15: [====================          ] 40/60 batches, loss: 0.2610Epoch 2/15: [====================          ] 41/60 batches, loss: 0.2623Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.2599Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.2588Epoch 2/15: [======================        ] 44/60 batches, loss: 0.2600Epoch 2/15: [======================        ] 45/60 batches, loss: 0.2595Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.2577Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.2549Epoch 2/15: [========================      ] 48/60 batches, loss: 0.2539Epoch 2/15: [========================      ] 49/60 batches, loss: 0.2555Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.2565Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.2540Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.2529Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.2514Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.2501Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.2494Epoch 2/15: [============================  ] 56/60 batches, loss: 0.2493Epoch 2/15: [============================  ] 57/60 batches, loss: 0.2489Epoch 2/15: [============================= ] 58/60 batches, loss: 0.2491Epoch 2/15: [============================= ] 59/60 batches, loss: 0.2470Epoch 2/15: [==============================] 60/60 batches, loss: 0.2443
[2025-05-07 20:28:02,315][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.2443
[2025-05-07 20:28:02,624][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1110, Metrics: {'mse': 0.11605008691549301, 'rmse': 0.3406612495067395, 'r2': -0.9250041246414185}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.1186Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1571Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1479Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1741Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1626Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1748Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1761Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1744Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1714Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1749Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1747Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1693Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1776Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1780Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1793Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1807Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1824Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1839Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1823Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1779Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1771Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1721Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1714Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1716Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1732Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1752Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1745Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1738Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1756Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1741Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1746Epoch 3/15: [================              ] 32/60 batches, loss: 0.1730Epoch 3/15: [================              ] 33/60 batches, loss: 0.1750Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1726Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1708Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1737Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1757Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1742Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1751Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1731Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1728Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1740Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1766Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1774Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1767Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1741Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1739Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1730Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1736Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1749Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1764Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1769Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1749Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1786Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1771Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1765Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1764Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1762Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1765Epoch 3/15: [==============================] 60/60 batches, loss: 0.1782
[2025-05-07 20:28:04,893][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1782
[2025-05-07 20:28:05,159][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1106, Metrics: {'mse': 0.11507967114448547, 'rmse': 0.33923394751186897, 'r2': -0.9089071750640869}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.1561Epoch 4/15: [=                             ] 2/60 batches, loss: 0.1599Epoch 4/15: [=                             ] 3/60 batches, loss: 0.1723Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1988Epoch 4/15: [==                            ] 5/60 batches, loss: 0.2030Epoch 4/15: [===                           ] 6/60 batches, loss: 0.2057Epoch 4/15: [===                           ] 7/60 batches, loss: 0.2124Epoch 4/15: [====                          ] 8/60 batches, loss: 0.2089Epoch 4/15: [====                          ] 9/60 batches, loss: 0.2098Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.2014Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.2019Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1985Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1979Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.2066Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.1988Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1988Epoch 4/15: [========                      ] 17/60 batches, loss: 0.2069Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.2013Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1978Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1995Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.2031Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.2044Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.2036Epoch 4/15: [============                  ] 24/60 batches, loss: 0.2061Epoch 4/15: [============                  ] 25/60 batches, loss: 0.2042Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.2057Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.2036Epoch 4/15: [==============                ] 28/60 batches, loss: 0.2015Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1990Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1959Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1936Epoch 4/15: [================              ] 32/60 batches, loss: 0.1929Epoch 4/15: [================              ] 33/60 batches, loss: 0.1905Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1883Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1891Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1876Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1866Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1873Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1869Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1842Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1835Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1835Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1830Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1814Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1812Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1796Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.1783Epoch 4/15: [========================      ] 48/60 batches, loss: 0.1782Epoch 4/15: [========================      ] 49/60 batches, loss: 0.1783Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.1774Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.1767Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1772Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.1769Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1758Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1741Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1734Epoch 4/15: [============================  ] 57/60 batches, loss: 0.1726Epoch 4/15: [============================= ] 58/60 batches, loss: 0.1730Epoch 4/15: [============================= ] 59/60 batches, loss: 0.1713Epoch 4/15: [==============================] 60/60 batches, loss: 0.1714
[2025-05-07 20:28:07,371][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1714
[2025-05-07 20:28:07,619][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0946, Metrics: {'mse': 0.0994010716676712, 'rmse': 0.31527935496583215, 'r2': -0.648835301399231}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.2236Epoch 5/15: [=                             ] 2/60 batches, loss: 0.1875Epoch 5/15: [=                             ] 3/60 batches, loss: 0.1824Epoch 5/15: [==                            ] 4/60 batches, loss: 0.1534Epoch 5/15: [==                            ] 5/60 batches, loss: 0.1862Epoch 5/15: [===                           ] 6/60 batches, loss: 0.1931Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1848Epoch 5/15: [====                          ] 8/60 batches, loss: 0.1906Epoch 5/15: [====                          ] 9/60 batches, loss: 0.1855Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.1820Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1843Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1820Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1774Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1802Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1769Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1786Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1745Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1749Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1718Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1751Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1746Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1720Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1740Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1734Epoch 5/15: [============                  ] 25/60 batches, loss: 0.1730Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1751Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.1720Epoch 5/15: [==============                ] 28/60 batches, loss: 0.1691Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1675Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1670Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1676Epoch 5/15: [================              ] 32/60 batches, loss: 0.1679Epoch 5/15: [================              ] 33/60 batches, loss: 0.1666Epoch 5/15: [=================             ] 34/60 batches, loss: 0.1669Epoch 5/15: [=================             ] 35/60 batches, loss: 0.1692Epoch 5/15: [==================            ] 36/60 batches, loss: 0.1679Epoch 5/15: [==================            ] 37/60 batches, loss: 0.1681Epoch 5/15: [===================           ] 38/60 batches, loss: 0.1677Epoch 5/15: [===================           ] 39/60 batches, loss: 0.1671Epoch 5/15: [====================          ] 40/60 batches, loss: 0.1653Epoch 5/15: [====================          ] 41/60 batches, loss: 0.1646Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.1645Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.1645Epoch 5/15: [======================        ] 44/60 batches, loss: 0.1640Epoch 5/15: [======================        ] 45/60 batches, loss: 0.1635Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.1633Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.1623Epoch 5/15: [========================      ] 48/60 batches, loss: 0.1623Epoch 5/15: [========================      ] 49/60 batches, loss: 0.1619Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.1643Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.1642Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.1643Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.1654Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.1654Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.1650Epoch 5/15: [============================  ] 56/60 batches, loss: 0.1637Epoch 5/15: [============================  ] 57/60 batches, loss: 0.1632Epoch 5/15: [============================= ] 58/60 batches, loss: 0.1636Epoch 5/15: [============================= ] 59/60 batches, loss: 0.1627Epoch 5/15: [==============================] 60/60 batches, loss: 0.1611
[2025-05-07 20:28:09,825][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1611
[2025-05-07 20:28:10,123][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0907, Metrics: {'mse': 0.09515360742807388, 'rmse': 0.30846978365485633, 'r2': -0.5783796310424805}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1325Epoch 6/15: [=                             ] 2/60 batches, loss: 0.1337Epoch 6/15: [=                             ] 3/60 batches, loss: 0.1349Epoch 6/15: [==                            ] 4/60 batches, loss: 0.1305Epoch 6/15: [==                            ] 5/60 batches, loss: 0.1241Epoch 6/15: [===                           ] 6/60 batches, loss: 0.1274Epoch 6/15: [===                           ] 7/60 batches, loss: 0.1318Epoch 6/15: [====                          ] 8/60 batches, loss: 0.1455Epoch 6/15: [====                          ] 9/60 batches, loss: 0.1530Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.1516Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.1449Epoch 6/15: [======                        ] 12/60 batches, loss: 0.1415Epoch 6/15: [======                        ] 13/60 batches, loss: 0.1440Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.1572Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.1548Epoch 6/15: [========                      ] 16/60 batches, loss: 0.1542Epoch 6/15: [========                      ] 17/60 batches, loss: 0.1501Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.1505Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.1471Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.1450Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.1417Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.1416Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.1447Epoch 6/15: [============                  ] 24/60 batches, loss: 0.1432Epoch 6/15: [============                  ] 25/60 batches, loss: 0.1455Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.1468Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.1460Epoch 6/15: [==============                ] 28/60 batches, loss: 0.1453Epoch 6/15: [==============                ] 29/60 batches, loss: 0.1443Epoch 6/15: [===============               ] 30/60 batches, loss: 0.1450Epoch 6/15: [===============               ] 31/60 batches, loss: 0.1450Epoch 6/15: [================              ] 32/60 batches, loss: 0.1467Epoch 6/15: [================              ] 33/60 batches, loss: 0.1480Epoch 6/15: [=================             ] 34/60 batches, loss: 0.1471Epoch 6/15: [=================             ] 35/60 batches, loss: 0.1448Epoch 6/15: [==================            ] 36/60 batches, loss: 0.1456Epoch 6/15: [==================            ] 37/60 batches, loss: 0.1446Epoch 6/15: [===================           ] 38/60 batches, loss: 0.1440Epoch 6/15: [===================           ] 39/60 batches, loss: 0.1438Epoch 6/15: [====================          ] 40/60 batches, loss: 0.1442Epoch 6/15: [====================          ] 41/60 batches, loss: 0.1434Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.1431Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.1420Epoch 6/15: [======================        ] 44/60 batches, loss: 0.1415Epoch 6/15: [======================        ] 45/60 batches, loss: 0.1404Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.1402Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.1409Epoch 6/15: [========================      ] 48/60 batches, loss: 0.1408Epoch 6/15: [========================      ] 49/60 batches, loss: 0.1405Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.1408Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.1425Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.1428Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.1427Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.1436Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.1431Epoch 6/15: [============================  ] 56/60 batches, loss: 0.1424Epoch 6/15: [============================  ] 57/60 batches, loss: 0.1422Epoch 6/15: [============================= ] 58/60 batches, loss: 0.1419Epoch 6/15: [============================= ] 59/60 batches, loss: 0.1407Epoch 6/15: [==============================] 60/60 batches, loss: 0.1416
[2025-05-07 20:28:12,394][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1416
[2025-05-07 20:28:12,685][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0769, Metrics: {'mse': 0.08135971426963806, 'rmse': 0.28523624291039534, 'r2': -0.34957075119018555}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0648Epoch 7/15: [=                             ] 2/60 batches, loss: 0.1359Epoch 7/15: [=                             ] 3/60 batches, loss: 0.1252Epoch 7/15: [==                            ] 4/60 batches, loss: 0.1362Epoch 7/15: [==                            ] 5/60 batches, loss: 0.1423Epoch 7/15: [===                           ] 6/60 batches, loss: 0.1325Epoch 7/15: [===                           ] 7/60 batches, loss: 0.1264Epoch 7/15: [====                          ] 8/60 batches, loss: 0.1267Epoch 7/15: [====                          ] 9/60 batches, loss: 0.1245Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.1217Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.1271Epoch 7/15: [======                        ] 12/60 batches, loss: 0.1308Epoch 7/15: [======                        ] 13/60 batches, loss: 0.1334Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.1355Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.1329Epoch 7/15: [========                      ] 16/60 batches, loss: 0.1375Epoch 7/15: [========                      ] 17/60 batches, loss: 0.1380Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.1428Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.1425Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.1443Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.1411Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.1462Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.1443Epoch 7/15: [============                  ] 24/60 batches, loss: 0.1460Epoch 7/15: [============                  ] 25/60 batches, loss: 0.1490Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.1487Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.1463Epoch 7/15: [==============                ] 28/60 batches, loss: 0.1449Epoch 7/15: [==============                ] 29/60 batches, loss: 0.1439Epoch 7/15: [===============               ] 30/60 batches, loss: 0.1454Epoch 7/15: [===============               ] 31/60 batches, loss: 0.1444Epoch 7/15: [================              ] 32/60 batches, loss: 0.1447Epoch 7/15: [================              ] 33/60 batches, loss: 0.1440Epoch 7/15: [=================             ] 34/60 batches, loss: 0.1437Epoch 7/15: [=================             ] 35/60 batches, loss: 0.1453Epoch 7/15: [==================            ] 36/60 batches, loss: 0.1447Epoch 7/15: [==================            ] 37/60 batches, loss: 0.1433Epoch 7/15: [===================           ] 38/60 batches, loss: 0.1429Epoch 7/15: [===================           ] 39/60 batches, loss: 0.1422Epoch 7/15: [====================          ] 40/60 batches, loss: 0.1435Epoch 7/15: [====================          ] 41/60 batches, loss: 0.1428Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.1420Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.1420Epoch 7/15: [======================        ] 44/60 batches, loss: 0.1432Epoch 7/15: [======================        ] 45/60 batches, loss: 0.1419Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.1422Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.1427Epoch 7/15: [========================      ] 48/60 batches, loss: 0.1438Epoch 7/15: [========================      ] 49/60 batches, loss: 0.1447Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.1443Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.1450Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.1454Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.1461Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.1475Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.1477Epoch 7/15: [============================  ] 56/60 batches, loss: 0.1476Epoch 7/15: [============================  ] 57/60 batches, loss: 0.1477Epoch 7/15: [============================= ] 58/60 batches, loss: 0.1478Epoch 7/15: [============================= ] 59/60 batches, loss: 0.1491Epoch 7/15: [==============================] 60/60 batches, loss: 0.1497
[2025-05-07 20:28:14,902][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.1497
[2025-05-07 20:28:15,156][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0755, Metrics: {'mse': 0.08003924787044525, 'rmse': 0.28291208505549076, 'r2': -0.327667236328125}
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.1823Epoch 8/15: [=                             ] 2/60 batches, loss: 0.1363Epoch 8/15: [=                             ] 3/60 batches, loss: 0.1418Epoch 8/15: [==                            ] 4/60 batches, loss: 0.1249Epoch 8/15: [==                            ] 5/60 batches, loss: 0.1348Epoch 8/15: [===                           ] 6/60 batches, loss: 0.1428Epoch 8/15: [===                           ] 7/60 batches, loss: 0.1381Epoch 8/15: [====                          ] 8/60 batches, loss: 0.1335Epoch 8/15: [====                          ] 9/60 batches, loss: 0.1343Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.1408Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.1381Epoch 8/15: [======                        ] 12/60 batches, loss: 0.1387Epoch 8/15: [======                        ] 13/60 batches, loss: 0.1423Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.1409Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.1381Epoch 8/15: [========                      ] 16/60 batches, loss: 0.1350Epoch 8/15: [========                      ] 17/60 batches, loss: 0.1354Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.1365Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.1370Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.1349Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.1353Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.1336Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.1330Epoch 8/15: [============                  ] 24/60 batches, loss: 0.1320Epoch 8/15: [============                  ] 25/60 batches, loss: 0.1348Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.1322Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.1331Epoch 8/15: [==============                ] 28/60 batches, loss: 0.1341Epoch 8/15: [==============                ] 29/60 batches, loss: 0.1324Epoch 8/15: [===============               ] 30/60 batches, loss: 0.1335Epoch 8/15: [===============               ] 31/60 batches, loss: 0.1339Epoch 8/15: [================              ] 32/60 batches, loss: 0.1341Epoch 8/15: [================              ] 33/60 batches, loss: 0.1328Epoch 8/15: [=================             ] 34/60 batches, loss: 0.1311Epoch 8/15: [=================             ] 35/60 batches, loss: 0.1316Epoch 8/15: [==================            ] 36/60 batches, loss: 0.1309Epoch 8/15: [==================            ] 37/60 batches, loss: 0.1318Epoch 8/15: [===================           ] 38/60 batches, loss: 0.1339Epoch 8/15: [===================           ] 39/60 batches, loss: 0.1331Epoch 8/15: [====================          ] 40/60 batches, loss: 0.1324Epoch 8/15: [====================          ] 41/60 batches, loss: 0.1328Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.1303Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.1319Epoch 8/15: [======================        ] 44/60 batches, loss: 0.1319Epoch 8/15: [======================        ] 45/60 batches, loss: 0.1319Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.1316Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.1314Epoch 8/15: [========================      ] 48/60 batches, loss: 0.1324Epoch 8/15: [========================      ] 49/60 batches, loss: 0.1333Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.1328Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.1329Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.1335Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.1334Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.1331Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.1337Epoch 8/15: [============================  ] 56/60 batches, loss: 0.1343Epoch 8/15: [============================  ] 57/60 batches, loss: 0.1332Epoch 8/15: [============================= ] 58/60 batches, loss: 0.1334Epoch 8/15: [============================= ] 59/60 batches, loss: 0.1333Epoch 8/15: [==============================] 60/60 batches, loss: 0.1330
[2025-05-07 20:28:17,468][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.1330
[2025-05-07 20:28:17,750][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0773, Metrics: {'mse': 0.08192615956068039, 'rmse': 0.28622746122739584, 'r2': -0.3589667081832886}
[2025-05-07 20:28:17,750][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.1678Epoch 9/15: [=                             ] 2/60 batches, loss: 0.1477Epoch 9/15: [=                             ] 3/60 batches, loss: 0.1396Epoch 9/15: [==                            ] 4/60 batches, loss: 0.1362Epoch 9/15: [==                            ] 5/60 batches, loss: 0.1506Epoch 9/15: [===                           ] 6/60 batches, loss: 0.1362Epoch 9/15: [===                           ] 7/60 batches, loss: 0.1293Epoch 9/15: [====                          ] 8/60 batches, loss: 0.1375Epoch 9/15: [====                          ] 9/60 batches, loss: 0.1350Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.1286Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.1331Epoch 9/15: [======                        ] 12/60 batches, loss: 0.1281Epoch 9/15: [======                        ] 13/60 batches, loss: 0.1263Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.1335Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.1323Epoch 9/15: [========                      ] 16/60 batches, loss: 0.1318Epoch 9/15: [========                      ] 17/60 batches, loss: 0.1322Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.1294Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.1318Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.1330Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.1326Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.1325Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.1321Epoch 9/15: [============                  ] 24/60 batches, loss: 0.1324Epoch 9/15: [============                  ] 25/60 batches, loss: 0.1345Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.1366Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.1369Epoch 9/15: [==============                ] 28/60 batches, loss: 0.1382Epoch 9/15: [==============                ] 29/60 batches, loss: 0.1382Epoch 9/15: [===============               ] 30/60 batches, loss: 0.1400Epoch 9/15: [===============               ] 31/60 batches, loss: 0.1406Epoch 9/15: [================              ] 32/60 batches, loss: 0.1390Epoch 9/15: [================              ] 33/60 batches, loss: 0.1390Epoch 9/15: [=================             ] 34/60 batches, loss: 0.1388Epoch 9/15: [=================             ] 35/60 batches, loss: 0.1395Epoch 9/15: [==================            ] 36/60 batches, loss: 0.1395Epoch 9/15: [==================            ] 37/60 batches, loss: 0.1393Epoch 9/15: [===================           ] 38/60 batches, loss: 0.1383Epoch 9/15: [===================           ] 39/60 batches, loss: 0.1374Epoch 9/15: [====================          ] 40/60 batches, loss: 0.1357Epoch 9/15: [====================          ] 41/60 batches, loss: 0.1357Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.1360Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.1351Epoch 9/15: [======================        ] 44/60 batches, loss: 0.1355Epoch 9/15: [======================        ] 45/60 batches, loss: 0.1355Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.1345Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.1340Epoch 9/15: [========================      ] 48/60 batches, loss: 0.1331Epoch 9/15: [========================      ] 49/60 batches, loss: 0.1335Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.1337Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.1338Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.1341Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.1333Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.1327Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.1331Epoch 9/15: [============================  ] 56/60 batches, loss: 0.1326Epoch 9/15: [============================  ] 57/60 batches, loss: 0.1320Epoch 9/15: [============================= ] 58/60 batches, loss: 0.1316Epoch 9/15: [============================= ] 59/60 batches, loss: 0.1307Epoch 9/15: [==============================] 60/60 batches, loss: 0.1303
[2025-05-07 20:28:19,651][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.1303
[2025-05-07 20:28:19,945][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0616, Metrics: {'mse': 0.06565887480974197, 'rmse': 0.25623987747761273, 'r2': -0.08912980556488037}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.1542Epoch 10/15: [=                             ] 2/60 batches, loss: 0.1770Epoch 10/15: [=                             ] 3/60 batches, loss: 0.1344Epoch 10/15: [==                            ] 4/60 batches, loss: 0.1362Epoch 10/15: [==                            ] 5/60 batches, loss: 0.1267Epoch 10/15: [===                           ] 6/60 batches, loss: 0.1240Epoch 10/15: [===                           ] 7/60 batches, loss: 0.1261Epoch 10/15: [====                          ] 8/60 batches, loss: 0.1296Epoch 10/15: [====                          ] 9/60 batches, loss: 0.1318Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.1279Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.1244Epoch 10/15: [======                        ] 12/60 batches, loss: 0.1219Epoch 10/15: [======                        ] 13/60 batches, loss: 0.1298Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.1296Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.1310Epoch 10/15: [========                      ] 16/60 batches, loss: 0.1274Epoch 10/15: [========                      ] 17/60 batches, loss: 0.1298Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.1315Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.1313Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.1291Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.1276Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.1253Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.1242Epoch 10/15: [============                  ] 24/60 batches, loss: 0.1262Epoch 10/15: [============                  ] 25/60 batches, loss: 0.1264Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.1269Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.1281Epoch 10/15: [==============                ] 28/60 batches, loss: 0.1298Epoch 10/15: [==============                ] 29/60 batches, loss: 0.1287Epoch 10/15: [===============               ] 30/60 batches, loss: 0.1305Epoch 10/15: [===============               ] 31/60 batches, loss: 0.1308Epoch 10/15: [================              ] 32/60 batches, loss: 0.1307Epoch 10/15: [================              ] 33/60 batches, loss: 0.1316Epoch 10/15: [=================             ] 34/60 batches, loss: 0.1315Epoch 10/15: [=================             ] 35/60 batches, loss: 0.1325Epoch 10/15: [==================            ] 36/60 batches, loss: 0.1320Epoch 10/15: [==================            ] 37/60 batches, loss: 0.1315Epoch 10/15: [===================           ] 38/60 batches, loss: 0.1304Epoch 10/15: [===================           ] 39/60 batches, loss: 0.1299Epoch 10/15: [====================          ] 40/60 batches, loss: 0.1299Epoch 10/15: [====================          ] 41/60 batches, loss: 0.1289Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.1281Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.1282Epoch 10/15: [======================        ] 44/60 batches, loss: 0.1281Epoch 10/15: [======================        ] 45/60 batches, loss: 0.1279Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.1267Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.1281Epoch 10/15: [========================      ] 48/60 batches, loss: 0.1280Epoch 10/15: [========================      ] 49/60 batches, loss: 0.1279Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.1275Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.1273Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.1275Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.1275Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.1267Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.1266Epoch 10/15: [============================  ] 56/60 batches, loss: 0.1265Epoch 10/15: [============================  ] 57/60 batches, loss: 0.1262Epoch 10/15: [============================= ] 58/60 batches, loss: 0.1257Epoch 10/15: [============================= ] 59/60 batches, loss: 0.1255Epoch 10/15: [==============================] 60/60 batches, loss: 0.1247
[2025-05-07 20:28:22,179][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.1247
[2025-05-07 20:28:22,454][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0590, Metrics: {'mse': 0.06277391314506531, 'rmse': 0.2505472273745317, 'r2': -0.04127490520477295}
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.1251Epoch 11/15: [=                             ] 2/60 batches, loss: 0.1215Epoch 11/15: [=                             ] 3/60 batches, loss: 0.1297Epoch 11/15: [==                            ] 4/60 batches, loss: 0.1227Epoch 11/15: [==                            ] 5/60 batches, loss: 0.1125Epoch 11/15: [===                           ] 6/60 batches, loss: 0.1133Epoch 11/15: [===                           ] 7/60 batches, loss: 0.1285Epoch 11/15: [====                          ] 8/60 batches, loss: 0.1296Epoch 11/15: [====                          ] 9/60 batches, loss: 0.1336Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.1358Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.1361Epoch 11/15: [======                        ] 12/60 batches, loss: 0.1333Epoch 11/15: [======                        ] 13/60 batches, loss: 0.1316Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.1374Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.1390Epoch 11/15: [========                      ] 16/60 batches, loss: 0.1378Epoch 11/15: [========                      ] 17/60 batches, loss: 0.1338Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.1316Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.1310Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.1294Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.1285Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.1252Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.1234Epoch 11/15: [============                  ] 24/60 batches, loss: 0.1241Epoch 11/15: [============                  ] 25/60 batches, loss: 0.1241Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.1236Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.1225Epoch 11/15: [==============                ] 28/60 batches, loss: 0.1207Epoch 11/15: [==============                ] 29/60 batches, loss: 0.1212Epoch 11/15: [===============               ] 30/60 batches, loss: 0.1230Epoch 11/15: [===============               ] 31/60 batches, loss: 0.1217Epoch 11/15: [================              ] 32/60 batches, loss: 0.1197Epoch 11/15: [================              ] 33/60 batches, loss: 0.1211Epoch 11/15: [=================             ] 34/60 batches, loss: 0.1213Epoch 11/15: [=================             ] 35/60 batches, loss: 0.1216Epoch 11/15: [==================            ] 36/60 batches, loss: 0.1207Epoch 11/15: [==================            ] 37/60 batches, loss: 0.1207Epoch 11/15: [===================           ] 38/60 batches, loss: 0.1204Epoch 11/15: [===================           ] 39/60 batches, loss: 0.1207Epoch 11/15: [====================          ] 40/60 batches, loss: 0.1223Epoch 11/15: [====================          ] 41/60 batches, loss: 0.1227Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.1222Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.1228Epoch 11/15: [======================        ] 44/60 batches, loss: 0.1220Epoch 11/15: [======================        ] 45/60 batches, loss: 0.1212Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.1207Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.1205Epoch 11/15: [========================      ] 48/60 batches, loss: 0.1201Epoch 11/15: [========================      ] 49/60 batches, loss: 0.1207Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.1205Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.1203Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.1200Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.1192Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.1198Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.1200Epoch 11/15: [============================  ] 56/60 batches, loss: 0.1219Epoch 11/15: [============================  ] 57/60 batches, loss: 0.1222Epoch 11/15: [============================= ] 58/60 batches, loss: 0.1222Epoch 11/15: [============================= ] 59/60 batches, loss: 0.1231Epoch 11/15: [==============================] 60/60 batches, loss: 0.1225
[2025-05-07 20:28:24,688][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.1225
[2025-05-07 20:28:25,023][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0696, Metrics: {'mse': 0.07424313575029373, 'rmse': 0.2724759360939856, 'r2': -0.23152291774749756}
[2025-05-07 20:28:25,023][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.1467Epoch 12/15: [=                             ] 2/60 batches, loss: 0.1537Epoch 12/15: [=                             ] 3/60 batches, loss: 0.1263Epoch 12/15: [==                            ] 4/60 batches, loss: 0.1248Epoch 12/15: [==                            ] 5/60 batches, loss: 0.1223Epoch 12/15: [===                           ] 6/60 batches, loss: 0.1237Epoch 12/15: [===                           ] 7/60 batches, loss: 0.1174Epoch 12/15: [====                          ] 8/60 batches, loss: 0.1181Epoch 12/15: [====                          ] 9/60 batches, loss: 0.1193Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.1184Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.1161Epoch 12/15: [======                        ] 12/60 batches, loss: 0.1180Epoch 12/15: [======                        ] 13/60 batches, loss: 0.1191Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.1235Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.1249Epoch 12/15: [========                      ] 16/60 batches, loss: 0.1283Epoch 12/15: [========                      ] 17/60 batches, loss: 0.1278Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.1282Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.1261Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.1271Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.1256Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.1230Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.1216Epoch 12/15: [============                  ] 24/60 batches, loss: 0.1212Epoch 12/15: [============                  ] 25/60 batches, loss: 0.1237Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.1239Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.1230Epoch 12/15: [==============                ] 28/60 batches, loss: 0.1228Epoch 12/15: [==============                ] 29/60 batches, loss: 0.1227Epoch 12/15: [===============               ] 30/60 batches, loss: 0.1248Epoch 12/15: [===============               ] 31/60 batches, loss: 0.1251Epoch 12/15: [================              ] 32/60 batches, loss: 0.1259Epoch 12/15: [================              ] 33/60 batches, loss: 0.1247Epoch 12/15: [=================             ] 34/60 batches, loss: 0.1244Epoch 12/15: [=================             ] 35/60 batches, loss: 0.1241Epoch 12/15: [==================            ] 36/60 batches, loss: 0.1237Epoch 12/15: [==================            ] 37/60 batches, loss: 0.1228Epoch 12/15: [===================           ] 38/60 batches, loss: 0.1211Epoch 12/15: [===================           ] 39/60 batches, loss: 0.1209Epoch 12/15: [====================          ] 40/60 batches, loss: 0.1221Epoch 12/15: [====================          ] 41/60 batches, loss: 0.1238Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.1229Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.1229Epoch 12/15: [======================        ] 44/60 batches, loss: 0.1232Epoch 12/15: [======================        ] 45/60 batches, loss: 0.1240Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.1244Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.1246Epoch 12/15: [========================      ] 48/60 batches, loss: 0.1242Epoch 12/15: [========================      ] 49/60 batches, loss: 0.1239Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.1236Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.1235Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.1240Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.1243Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.1246Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.1247Epoch 12/15: [============================  ] 56/60 batches, loss: 0.1242Epoch 12/15: [============================  ] 57/60 batches, loss: 0.1241Epoch 12/15: [============================= ] 58/60 batches, loss: 0.1236Epoch 12/15: [============================= ] 59/60 batches, loss: 0.1233Epoch 12/15: [==============================] 60/60 batches, loss: 0.1243
[2025-05-07 20:28:26,922][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.1243
[2025-05-07 20:28:27,236][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0638, Metrics: {'mse': 0.06816726177930832, 'rmse': 0.26108860905697956, 'r2': -0.1307382583618164}
[2025-05-07 20:28:27,236][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.1366Epoch 13/15: [=                             ] 2/60 batches, loss: 0.1268Epoch 13/15: [=                             ] 3/60 batches, loss: 0.1220Epoch 13/15: [==                            ] 4/60 batches, loss: 0.1114Epoch 13/15: [==                            ] 5/60 batches, loss: 0.1114Epoch 13/15: [===                           ] 6/60 batches, loss: 0.1116Epoch 13/15: [===                           ] 7/60 batches, loss: 0.1079Epoch 13/15: [====                          ] 8/60 batches, loss: 0.1043Epoch 13/15: [====                          ] 9/60 batches, loss: 0.1021Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.1012Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.1010Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0986Epoch 13/15: [======                        ] 13/60 batches, loss: 0.1053Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.1105Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.1076Epoch 13/15: [========                      ] 16/60 batches, loss: 0.1076Epoch 13/15: [========                      ] 17/60 batches, loss: 0.1088Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.1093Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.1091Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.1096Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.1086Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.1089Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.1097Epoch 13/15: [============                  ] 24/60 batches, loss: 0.1122Epoch 13/15: [============                  ] 25/60 batches, loss: 0.1122Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.1124Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.1135Epoch 13/15: [==============                ] 28/60 batches, loss: 0.1140Epoch 13/15: [==============                ] 29/60 batches, loss: 0.1139Epoch 13/15: [===============               ] 30/60 batches, loss: 0.1140Epoch 13/15: [===============               ] 31/60 batches, loss: 0.1146Epoch 13/15: [================              ] 32/60 batches, loss: 0.1143Epoch 13/15: [================              ] 33/60 batches, loss: 0.1135Epoch 13/15: [=================             ] 34/60 batches, loss: 0.1148Epoch 13/15: [=================             ] 35/60 batches, loss: 0.1141Epoch 13/15: [==================            ] 36/60 batches, loss: 0.1143Epoch 13/15: [==================            ] 37/60 batches, loss: 0.1161Epoch 13/15: [===================           ] 38/60 batches, loss: 0.1168Epoch 13/15: [===================           ] 39/60 batches, loss: 0.1164Epoch 13/15: [====================          ] 40/60 batches, loss: 0.1158Epoch 13/15: [====================          ] 41/60 batches, loss: 0.1169Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.1177Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.1179Epoch 13/15: [======================        ] 44/60 batches, loss: 0.1182Epoch 13/15: [======================        ] 45/60 batches, loss: 0.1181Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.1188Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.1178Epoch 13/15: [========================      ] 48/60 batches, loss: 0.1184Epoch 13/15: [========================      ] 49/60 batches, loss: 0.1175Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.1172Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.1187Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.1186Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.1183Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.1185Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.1183Epoch 13/15: [============================  ] 56/60 batches, loss: 0.1182Epoch 13/15: [============================  ] 57/60 batches, loss: 0.1186Epoch 13/15: [============================= ] 58/60 batches, loss: 0.1181Epoch 13/15: [============================= ] 59/60 batches, loss: 0.1179Epoch 13/15: [==============================] 60/60 batches, loss: 0.1179
[2025-05-07 20:28:29,117][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.1179
[2025-05-07 20:28:29,401][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0615, Metrics: {'mse': 0.06579729169607162, 'rmse': 0.2565098276793145, 'r2': -0.09142577648162842}
[2025-05-07 20:28:29,401][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.1247Epoch 14/15: [=                             ] 2/60 batches, loss: 0.1422Epoch 14/15: [=                             ] 3/60 batches, loss: 0.1324Epoch 14/15: [==                            ] 4/60 batches, loss: 0.1305Epoch 14/15: [==                            ] 5/60 batches, loss: 0.1335Epoch 14/15: [===                           ] 6/60 batches, loss: 0.1363Epoch 14/15: [===                           ] 7/60 batches, loss: 0.1368Epoch 14/15: [====                          ] 8/60 batches, loss: 0.1372Epoch 14/15: [====                          ] 9/60 batches, loss: 0.1342Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.1311Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.1318Epoch 14/15: [======                        ] 12/60 batches, loss: 0.1281Epoch 14/15: [======                        ] 13/60 batches, loss: 0.1258Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.1262Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.1242Epoch 14/15: [========                      ] 16/60 batches, loss: 0.1243Epoch 14/15: [========                      ] 17/60 batches, loss: 0.1231Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.1258Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.1236Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.1242Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.1235Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.1228Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.1211Epoch 14/15: [============                  ] 24/60 batches, loss: 0.1200Epoch 14/15: [============                  ] 25/60 batches, loss: 0.1196Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.1187Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.1196Epoch 14/15: [==============                ] 28/60 batches, loss: 0.1193Epoch 14/15: [==============                ] 29/60 batches, loss: 0.1188Epoch 14/15: [===============               ] 30/60 batches, loss: 0.1195Epoch 14/15: [===============               ] 31/60 batches, loss: 0.1195Epoch 14/15: [================              ] 32/60 batches, loss: 0.1192Epoch 14/15: [================              ] 33/60 batches, loss: 0.1198Epoch 14/15: [=================             ] 34/60 batches, loss: 0.1192Epoch 14/15: [=================             ] 35/60 batches, loss: 0.1185Epoch 14/15: [==================            ] 36/60 batches, loss: 0.1182Epoch 14/15: [==================            ] 37/60 batches, loss: 0.1183Epoch 14/15: [===================           ] 38/60 batches, loss: 0.1167Epoch 14/15: [===================           ] 39/60 batches, loss: 0.1183Epoch 14/15: [====================          ] 40/60 batches, loss: 0.1178Epoch 14/15: [====================          ] 41/60 batches, loss: 0.1186Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.1179Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.1175Epoch 14/15: [======================        ] 44/60 batches, loss: 0.1189Epoch 14/15: [======================        ] 45/60 batches, loss: 0.1180Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.1193Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.1182Epoch 14/15: [========================      ] 48/60 batches, loss: 0.1188Epoch 14/15: [========================      ] 49/60 batches, loss: 0.1192Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.1191Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.1188Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.1187Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.1184Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.1180Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.1184Epoch 14/15: [============================  ] 56/60 batches, loss: 0.1185Epoch 14/15: [============================  ] 57/60 batches, loss: 0.1181Epoch 14/15: [============================= ] 58/60 batches, loss: 0.1188Epoch 14/15: [============================= ] 59/60 batches, loss: 0.1182Epoch 14/15: [==============================] 60/60 batches, loss: 0.1181
[2025-05-07 20:28:31,310][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.1181
[2025-05-07 20:28:31,575][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0577, Metrics: {'mse': 0.061602167785167694, 'rmse': 0.24819784000906955, 'r2': -0.02183842658996582}
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0700Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0867Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0904Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0902Epoch 15/15: [==                            ] 5/60 batches, loss: 0.1010Epoch 15/15: [===                           ] 6/60 batches, loss: 0.1037Epoch 15/15: [===                           ] 7/60 batches, loss: 0.1055Epoch 15/15: [====                          ] 8/60 batches, loss: 0.1088Epoch 15/15: [====                          ] 9/60 batches, loss: 0.1097Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.1151Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.1150Epoch 15/15: [======                        ] 12/60 batches, loss: 0.1190Epoch 15/15: [======                        ] 13/60 batches, loss: 0.1199Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.1175Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.1170Epoch 15/15: [========                      ] 16/60 batches, loss: 0.1147Epoch 15/15: [========                      ] 17/60 batches, loss: 0.1147Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.1157Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.1147Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.1144Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.1141Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.1130Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.1104Epoch 15/15: [============                  ] 24/60 batches, loss: 0.1129Epoch 15/15: [============                  ] 25/60 batches, loss: 0.1136Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.1155Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.1175Epoch 15/15: [==============                ] 28/60 batches, loss: 0.1173Epoch 15/15: [==============                ] 29/60 batches, loss: 0.1174Epoch 15/15: [===============               ] 30/60 batches, loss: 0.1168Epoch 15/15: [===============               ] 31/60 batches, loss: 0.1152Epoch 15/15: [================              ] 32/60 batches, loss: 0.1154Epoch 15/15: [================              ] 33/60 batches, loss: 0.1144Epoch 15/15: [=================             ] 34/60 batches, loss: 0.1137Epoch 15/15: [=================             ] 35/60 batches, loss: 0.1127Epoch 15/15: [==================            ] 36/60 batches, loss: 0.1127Epoch 15/15: [==================            ] 37/60 batches, loss: 0.1121Epoch 15/15: [===================           ] 38/60 batches, loss: 0.1126Epoch 15/15: [===================           ] 39/60 batches, loss: 0.1128Epoch 15/15: [====================          ] 40/60 batches, loss: 0.1131Epoch 15/15: [====================          ] 41/60 batches, loss: 0.1129Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.1130Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.1142Epoch 15/15: [======================        ] 44/60 batches, loss: 0.1132Epoch 15/15: [======================        ] 45/60 batches, loss: 0.1120Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.1135Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.1135Epoch 15/15: [========================      ] 48/60 batches, loss: 0.1138Epoch 15/15: [========================      ] 49/60 batches, loss: 0.1143Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.1132Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.1133Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.1128Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.1127Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.1120Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.1108Epoch 15/15: [============================  ] 56/60 batches, loss: 0.1114Epoch 15/15: [============================  ] 57/60 batches, loss: 0.1114Epoch 15/15: [============================= ] 58/60 batches, loss: 0.1112Epoch 15/15: [============================= ] 59/60 batches, loss: 0.1118Epoch 15/15: [==============================] 60/60 batches, loss: 0.1133
[2025-05-07 20:28:33,931][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.1133
[2025-05-07 20:28:34,383][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0715, Metrics: {'mse': 0.07651400566101074, 'rmse': 0.27661165134717436, 'r2': -0.2691915035247803}
[2025-05-07 20:28:34,384][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-07 20:28:34,384][src.training.lm_trainer][INFO] - Training completed in 37.45 seconds
[2025-05-07 20:28:34,384][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:28:37,230][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.09752615541219711, 'rmse': 0.31229177929013296, 'r2': 0.01005542278289795}
[2025-05-07 20:28:37,231][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.061602167785167694, 'rmse': 0.24819784000906955, 'r2': -0.02183842658996582}
[2025-05-07 20:28:37,231][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06952373683452606, 'rmse': 0.2636735421587196, 'r2': -0.08851778507232666}
[2025-05-07 20:28:38,871][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control1/layer2/id/id/model.pt
[2025-05-07 20:28:38,873][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ███▆▅▃▃▁▁▁
wandb:     best_val_mse ███▆▅▃▃▂▁▁
wandb:      best_val_r2 ▁▁▁▃▄▆▆▇██
wandb:    best_val_rmse ███▆▅▄▄▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▁▁▃▃▅▅▅▆▆▅▆▆▆
wandb:       train_loss █▅▃▃▂▂▂▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ███▆▅▃▃▃▁▁▃▂▁▁▃
wandb:          val_mse ███▆▅▃▃▄▂▁▃▂▂▁▃
wandb:           val_r2 ▁▁▁▃▄▆▆▅▇█▆▇▇█▆
wandb:         val_rmse ███▆▅▄▄▄▂▁▃▂▂▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05769
wandb:     best_val_mse 0.0616
wandb:      best_val_r2 -0.02184
wandb:    best_val_rmse 0.2482
wandb:            epoch 15
wandb:   final_test_mse 0.06952
wandb:    final_test_r2 -0.08852
wandb:  final_test_rmse 0.26367
wandb:  final_train_mse 0.09753
wandb:   final_train_r2 0.01006
wandb: final_train_rmse 0.31229
wandb:    final_val_mse 0.0616
wandb:     final_val_r2 -0.02184
wandb:   final_val_rmse 0.2482
wandb:    learning_rate 0.0001
wandb:       train_loss 0.11325
wandb:       train_time 37.45045
wandb:         val_loss 0.07152
wandb:          val_mse 0.07651
wandb:           val_r2 -0.26919
wandb:         val_rmse 0.27661
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_202741-ghwawzo5
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_202741-ghwawzo5/logs
Experiment probe_layer2_avg_verb_edges_control1_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control1/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_avg_verb_edges_control2_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_verb_edges_control2_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control2/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=avg_verb_edges"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:29:07,989][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control2/layer2/id
experiment_name: probe_layer2_avg_verb_edges_control2_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:29:07,989][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:29:07,989][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 20:29:07,989][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:29:07,989][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:29:07,993][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:29:07,993][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 20:29:07,994][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:29:11,537][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:29:13,858][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:29:13,859][src.data.datasets][INFO] - Loading 'control_avg_verb_edges_seed2' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:29:14,064][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_verb_edges_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:30:51 2025).
[2025-05-07 20:29:14,142][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_verb_edges_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:30:51 2025).
[2025-05-07 20:29:14,439][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:29:14,446][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:29:14,447][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:29:14,448][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:29:14,530][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:29:14,636][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:29:14,660][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:29:14,661][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:29:14,662][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:29:14,665][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:29:14,767][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:29:14,862][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:29:14,886][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:29:14,888][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:29:14,888][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:29:14,891][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:29:14,891][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:29:14,892][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 20:29:14,892][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 20:29:14,892][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 20:29:14,892][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:29:14,892][src.data.datasets][INFO] -   Mean: 0.3916, Std: 0.3139
[2025-05-07 20:29:14,892][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:29:14,892][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:29:14,893][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:29:14,893][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 20:29:14,893][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 20:29:14,893][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 20:29:14,893][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-05-07 20:29:14,893][src.data.datasets][INFO] -   Mean: 0.3834, Std: 0.2455
[2025-05-07 20:29:14,893][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:29:14,893][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:29:14,893][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:29:14,893][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 20:29:14,894][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 20:29:14,894][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 20:29:14,894][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:29:14,894][src.data.datasets][INFO] -   Mean: 0.4230, Std: 0.2527
[2025-05-07 20:29:14,894][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:29:14,894][src.data.datasets][INFO] - Sample label: 0.5830000042915344
[2025-05-07 20:29:14,894][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:29:14,894][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:29:14,895][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:29:14,895][__main__][INFO] - Using model type: lm_probe for submetric avg_verb_edges
[2025-05-07 20:29:14,895][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:29:22,130][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:29:22,131][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:29:22,131][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:29:22,132][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:29:22,134][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:29:22,135][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:29:22,135][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:29:22,135][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:29:22,135][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:29:22,136][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:29:22,136][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.4392Epoch 1/15: [=                             ] 2/60 batches, loss: 0.5499Epoch 1/15: [=                             ] 3/60 batches, loss: 0.5581Epoch 1/15: [==                            ] 4/60 batches, loss: 0.5681Epoch 1/15: [==                            ] 5/60 batches, loss: 0.5341Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4961Epoch 1/15: [===                           ] 7/60 batches, loss: 0.5176Epoch 1/15: [====                          ] 8/60 batches, loss: 0.5294Epoch 1/15: [====                          ] 9/60 batches, loss: 0.5298Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.5346Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.5190Epoch 1/15: [======                        ] 12/60 batches, loss: 0.5254Epoch 1/15: [======                        ] 13/60 batches, loss: 0.5079Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.5041Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4960Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4911Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4867Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4877Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4726Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4591Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4619Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4662Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4613Epoch 1/15: [============                  ] 24/60 batches, loss: 0.4587Epoch 1/15: [============                  ] 25/60 batches, loss: 0.4523Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.4506Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.4486Epoch 1/15: [==============                ] 28/60 batches, loss: 0.4440Epoch 1/15: [==============                ] 29/60 batches, loss: 0.4467Epoch 1/15: [===============               ] 30/60 batches, loss: 0.4520Epoch 1/15: [===============               ] 31/60 batches, loss: 0.4481Epoch 1/15: [================              ] 32/60 batches, loss: 0.4433Epoch 1/15: [================              ] 33/60 batches, loss: 0.4378Epoch 1/15: [=================             ] 34/60 batches, loss: 0.4356Epoch 1/15: [=================             ] 35/60 batches, loss: 0.4344Epoch 1/15: [==================            ] 36/60 batches, loss: 0.4313Epoch 1/15: [==================            ] 37/60 batches, loss: 0.4238Epoch 1/15: [===================           ] 38/60 batches, loss: 0.4209Epoch 1/15: [===================           ] 39/60 batches, loss: 0.4165Epoch 1/15: [====================          ] 40/60 batches, loss: 0.4122Epoch 1/15: [====================          ] 41/60 batches, loss: 0.4112Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.4071Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.4055Epoch 1/15: [======================        ] 44/60 batches, loss: 0.4041Epoch 1/15: [======================        ] 45/60 batches, loss: 0.4014Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3974Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3951Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3903Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3882Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3851Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3851Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3882Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3853Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3871Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3873Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3901Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3904Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3888Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3868Epoch 1/15: [==============================] 60/60 batches, loss: 0.3845
[2025-05-07 20:29:28,101][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3845
[2025-05-07 20:29:28,333][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1184, Metrics: {'mse': 0.12394605576992035, 'rmse': 0.35205973324127876, 'r2': -1.0559802055358887}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.2366Epoch 2/15: [=                             ] 2/60 batches, loss: 0.1919Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2027Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2219Epoch 2/15: [==                            ] 5/60 batches, loss: 0.2241Epoch 2/15: [===                           ] 6/60 batches, loss: 0.2457Epoch 2/15: [===                           ] 7/60 batches, loss: 0.2512Epoch 2/15: [====                          ] 8/60 batches, loss: 0.2528Epoch 2/15: [====                          ] 9/60 batches, loss: 0.2558Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.2501Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.2503Epoch 2/15: [======                        ] 12/60 batches, loss: 0.2472Epoch 2/15: [======                        ] 13/60 batches, loss: 0.2454Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.2529Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.2495Epoch 2/15: [========                      ] 16/60 batches, loss: 0.2538Epoch 2/15: [========                      ] 17/60 batches, loss: 0.2558Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.2595Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.2564Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.2610Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.2554Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.2570Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.2515Epoch 2/15: [============                  ] 24/60 batches, loss: 0.2480Epoch 2/15: [============                  ] 25/60 batches, loss: 0.2477Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.2466Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.2403Epoch 2/15: [==============                ] 28/60 batches, loss: 0.2392Epoch 2/15: [==============                ] 29/60 batches, loss: 0.2354Epoch 2/15: [===============               ] 30/60 batches, loss: 0.2324Epoch 2/15: [===============               ] 31/60 batches, loss: 0.2337Epoch 2/15: [================              ] 32/60 batches, loss: 0.2346Epoch 2/15: [================              ] 33/60 batches, loss: 0.2319Epoch 2/15: [=================             ] 34/60 batches, loss: 0.2339Epoch 2/15: [=================             ] 35/60 batches, loss: 0.2391Epoch 2/15: [==================            ] 36/60 batches, loss: 0.2386Epoch 2/15: [==================            ] 37/60 batches, loss: 0.2384Epoch 2/15: [===================           ] 38/60 batches, loss: 0.2370Epoch 2/15: [===================           ] 39/60 batches, loss: 0.2391Epoch 2/15: [====================          ] 40/60 batches, loss: 0.2431Epoch 2/15: [====================          ] 41/60 batches, loss: 0.2474Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.2461Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.2426Epoch 2/15: [======================        ] 44/60 batches, loss: 0.2449Epoch 2/15: [======================        ] 45/60 batches, loss: 0.2405Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.2392Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.2380Epoch 2/15: [========================      ] 48/60 batches, loss: 0.2385Epoch 2/15: [========================      ] 49/60 batches, loss: 0.2396Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.2390Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.2391Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.2370Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.2353Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.2326Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.2320Epoch 2/15: [============================  ] 56/60 batches, loss: 0.2309Epoch 2/15: [============================  ] 57/60 batches, loss: 0.2301Epoch 2/15: [============================= ] 58/60 batches, loss: 0.2294Epoch 2/15: [============================= ] 59/60 batches, loss: 0.2289Epoch 2/15: [==============================] 60/60 batches, loss: 0.2259
[2025-05-07 20:29:30,577][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.2259
[2025-05-07 20:29:30,841][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1423, Metrics: {'mse': 0.14756658673286438, 'rmse': 0.3841439661544411, 'r2': -1.4477903842926025}
[2025-05-07 20:29:30,842][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.2665Epoch 3/15: [=                             ] 2/60 batches, loss: 0.2734Epoch 3/15: [=                             ] 3/60 batches, loss: 0.2451Epoch 3/15: [==                            ] 4/60 batches, loss: 0.2448Epoch 3/15: [==                            ] 5/60 batches, loss: 0.2241Epoch 3/15: [===                           ] 6/60 batches, loss: 0.2120Epoch 3/15: [===                           ] 7/60 batches, loss: 0.2047Epoch 3/15: [====                          ] 8/60 batches, loss: 0.2176Epoch 3/15: [====                          ] 9/60 batches, loss: 0.2127Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.2046Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.2000Epoch 3/15: [======                        ] 12/60 batches, loss: 0.2015Epoch 3/15: [======                        ] 13/60 batches, loss: 0.2028Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.2022Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1957Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1946Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1955Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1928Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1903Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1885Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1853Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1857Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1834Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1852Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1869Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1882Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1855Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1821Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1832Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1805Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1777Epoch 3/15: [================              ] 32/60 batches, loss: 0.1789Epoch 3/15: [================              ] 33/60 batches, loss: 0.1811Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1802Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1773Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1801Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1801Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1815Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1819Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1801Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1785Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1812Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1806Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1798Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1779Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1779Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1775Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1761Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1746Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1740Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1739Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1754Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1750Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1775Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1773Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1764Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1769Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1756Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1759Epoch 3/15: [==============================] 60/60 batches, loss: 0.1780
[2025-05-07 20:29:32,697][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1780
[2025-05-07 20:29:33,009][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0982, Metrics: {'mse': 0.10258034616708755, 'rmse': 0.3202816669231749, 'r2': -0.7015721797943115}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.1577Epoch 4/15: [=                             ] 2/60 batches, loss: 0.2139Epoch 4/15: [=                             ] 3/60 batches, loss: 0.1769Epoch 4/15: [==                            ] 4/60 batches, loss: 0.2166Epoch 4/15: [==                            ] 5/60 batches, loss: 0.2077Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1972Epoch 4/15: [===                           ] 7/60 batches, loss: 0.2181Epoch 4/15: [====                          ] 8/60 batches, loss: 0.2103Epoch 4/15: [====                          ] 9/60 batches, loss: 0.2090Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.1985Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1929Epoch 4/15: [======                        ] 12/60 batches, loss: 0.2012Epoch 4/15: [======                        ] 13/60 batches, loss: 0.2027Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.2003Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.1953Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1974Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1979Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1984Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1964Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1970Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.2004Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.2012Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.2005Epoch 4/15: [============                  ] 24/60 batches, loss: 0.2009Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1989Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1991Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1956Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1990Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1995Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1983Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1959Epoch 4/15: [================              ] 32/60 batches, loss: 0.1920Epoch 4/15: [================              ] 33/60 batches, loss: 0.1894Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1914Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1906Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1867Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1864Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1858Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1851Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1848Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1832Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1834Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1834Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1822Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1814Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1788Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.1762Epoch 4/15: [========================      ] 48/60 batches, loss: 0.1748Epoch 4/15: [========================      ] 49/60 batches, loss: 0.1750Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.1727Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.1731Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1752Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.1750Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1752Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1745Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1744Epoch 4/15: [============================  ] 57/60 batches, loss: 0.1732Epoch 4/15: [============================= ] 58/60 batches, loss: 0.1736Epoch 4/15: [============================= ] 59/60 batches, loss: 0.1719Epoch 4/15: [==============================] 60/60 batches, loss: 0.1715
[2025-05-07 20:29:35,280][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1715
[2025-05-07 20:29:35,559][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0807, Metrics: {'mse': 0.08515121042728424, 'rmse': 0.29180680325736796, 'r2': -0.41246283054351807}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.1400Epoch 5/15: [=                             ] 2/60 batches, loss: 0.1510Epoch 5/15: [=                             ] 3/60 batches, loss: 0.1707Epoch 5/15: [==                            ] 4/60 batches, loss: 0.2039Epoch 5/15: [==                            ] 5/60 batches, loss: 0.2038Epoch 5/15: [===                           ] 6/60 batches, loss: 0.2011Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1811Epoch 5/15: [====                          ] 8/60 batches, loss: 0.1785Epoch 5/15: [====                          ] 9/60 batches, loss: 0.1754Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.1785Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1775Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1786Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1773Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1797Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1830Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1836Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1835Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1822Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1833Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1844Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1841Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1846Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1889Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1870Epoch 5/15: [============                  ] 25/60 batches, loss: 0.1868Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1871Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.1882Epoch 5/15: [==============                ] 28/60 batches, loss: 0.1853Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1853Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1856Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1839Epoch 5/15: [================              ] 32/60 batches, loss: 0.1818Epoch 5/15: [================              ] 33/60 batches, loss: 0.1805Epoch 5/15: [=================             ] 34/60 batches, loss: 0.1804Epoch 5/15: [=================             ] 35/60 batches, loss: 0.1816Epoch 5/15: [==================            ] 36/60 batches, loss: 0.1807Epoch 5/15: [==================            ] 37/60 batches, loss: 0.1800Epoch 5/15: [===================           ] 38/60 batches, loss: 0.1776Epoch 5/15: [===================           ] 39/60 batches, loss: 0.1776Epoch 5/15: [====================          ] 40/60 batches, loss: 0.1758Epoch 5/15: [====================          ] 41/60 batches, loss: 0.1763Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.1757Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.1758Epoch 5/15: [======================        ] 44/60 batches, loss: 0.1752Epoch 5/15: [======================        ] 45/60 batches, loss: 0.1746Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.1736Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.1710Epoch 5/15: [========================      ] 48/60 batches, loss: 0.1723Epoch 5/15: [========================      ] 49/60 batches, loss: 0.1721Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.1708Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.1694Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.1693Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.1693Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.1694Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.1694Epoch 5/15: [============================  ] 56/60 batches, loss: 0.1686Epoch 5/15: [============================  ] 57/60 batches, loss: 0.1691Epoch 5/15: [============================= ] 58/60 batches, loss: 0.1685Epoch 5/15: [============================= ] 59/60 batches, loss: 0.1672Epoch 5/15: [==============================] 60/60 batches, loss: 0.1660
[2025-05-07 20:29:37,793][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1660
[2025-05-07 20:29:38,118][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0858, Metrics: {'mse': 0.09057697653770447, 'rmse': 0.30096009127076045, 'r2': -0.5024639368057251}
[2025-05-07 20:29:38,119][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1474Epoch 6/15: [=                             ] 2/60 batches, loss: 0.1149Epoch 6/15: [=                             ] 3/60 batches, loss: 0.1534Epoch 6/15: [==                            ] 4/60 batches, loss: 0.1528Epoch 6/15: [==                            ] 5/60 batches, loss: 0.1452Epoch 6/15: [===                           ] 6/60 batches, loss: 0.1450Epoch 6/15: [===                           ] 7/60 batches, loss: 0.1459Epoch 6/15: [====                          ] 8/60 batches, loss: 0.1548Epoch 6/15: [====                          ] 9/60 batches, loss: 0.1474Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.1454Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.1458Epoch 6/15: [======                        ] 12/60 batches, loss: 0.1422Epoch 6/15: [======                        ] 13/60 batches, loss: 0.1455Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.1422Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.1381Epoch 6/15: [========                      ] 16/60 batches, loss: 0.1385Epoch 6/15: [========                      ] 17/60 batches, loss: 0.1382Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.1427Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.1448Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.1428Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.1437Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.1432Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.1411Epoch 6/15: [============                  ] 24/60 batches, loss: 0.1422Epoch 6/15: [============                  ] 25/60 batches, loss: 0.1420Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.1390Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.1388Epoch 6/15: [==============                ] 28/60 batches, loss: 0.1404Epoch 6/15: [==============                ] 29/60 batches, loss: 0.1416Epoch 6/15: [===============               ] 30/60 batches, loss: 0.1450Epoch 6/15: [===============               ] 31/60 batches, loss: 0.1445Epoch 6/15: [================              ] 32/60 batches, loss: 0.1450Epoch 6/15: [================              ] 33/60 batches, loss: 0.1460Epoch 6/15: [=================             ] 34/60 batches, loss: 0.1470Epoch 6/15: [=================             ] 35/60 batches, loss: 0.1459Epoch 6/15: [==================            ] 36/60 batches, loss: 0.1454Epoch 6/15: [==================            ] 37/60 batches, loss: 0.1451Epoch 6/15: [===================           ] 38/60 batches, loss: 0.1454Epoch 6/15: [===================           ] 39/60 batches, loss: 0.1448Epoch 6/15: [====================          ] 40/60 batches, loss: 0.1461Epoch 6/15: [====================          ] 41/60 batches, loss: 0.1446Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.1455Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.1457Epoch 6/15: [======================        ] 44/60 batches, loss: 0.1453Epoch 6/15: [======================        ] 45/60 batches, loss: 0.1454Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.1446Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.1454Epoch 6/15: [========================      ] 48/60 batches, loss: 0.1458Epoch 6/15: [========================      ] 49/60 batches, loss: 0.1447Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.1447Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.1466Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.1469Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.1474Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.1467Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.1472Epoch 6/15: [============================  ] 56/60 batches, loss: 0.1466Epoch 6/15: [============================  ] 57/60 batches, loss: 0.1466Epoch 6/15: [============================= ] 58/60 batches, loss: 0.1466Epoch 6/15: [============================= ] 59/60 batches, loss: 0.1473Epoch 6/15: [==============================] 60/60 batches, loss: 0.1466
[2025-05-07 20:29:40,039][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1466
[2025-05-07 20:29:40,305][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0692, Metrics: {'mse': 0.07358250021934509, 'rmse': 0.2712609448839716, 'r2': -0.22056448459625244}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.1224Epoch 7/15: [=                             ] 2/60 batches, loss: 0.1777Epoch 7/15: [=                             ] 3/60 batches, loss: 0.1505Epoch 7/15: [==                            ] 4/60 batches, loss: 0.1384Epoch 7/15: [==                            ] 5/60 batches, loss: 0.1348Epoch 7/15: [===                           ] 6/60 batches, loss: 0.1338Epoch 7/15: [===                           ] 7/60 batches, loss: 0.1323Epoch 7/15: [====                          ] 8/60 batches, loss: 0.1335Epoch 7/15: [====                          ] 9/60 batches, loss: 0.1470Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.1457Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.1417Epoch 7/15: [======                        ] 12/60 batches, loss: 0.1433Epoch 7/15: [======                        ] 13/60 batches, loss: 0.1446Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.1479Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.1572Epoch 7/15: [========                      ] 16/60 batches, loss: 0.1545Epoch 7/15: [========                      ] 17/60 batches, loss: 0.1521Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.1481Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.1486Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.1489Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.1449Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.1496Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.1527Epoch 7/15: [============                  ] 24/60 batches, loss: 0.1551Epoch 7/15: [============                  ] 25/60 batches, loss: 0.1523Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.1497Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.1465Epoch 7/15: [==============                ] 28/60 batches, loss: 0.1466Epoch 7/15: [==============                ] 29/60 batches, loss: 0.1476Epoch 7/15: [===============               ] 30/60 batches, loss: 0.1461Epoch 7/15: [===============               ] 31/60 batches, loss: 0.1447Epoch 7/15: [================              ] 32/60 batches, loss: 0.1442Epoch 7/15: [================              ] 33/60 batches, loss: 0.1450Epoch 7/15: [=================             ] 34/60 batches, loss: 0.1450Epoch 7/15: [=================             ] 35/60 batches, loss: 0.1449Epoch 7/15: [==================            ] 36/60 batches, loss: 0.1426Epoch 7/15: [==================            ] 37/60 batches, loss: 0.1412Epoch 7/15: [===================           ] 38/60 batches, loss: 0.1399Epoch 7/15: [===================           ] 39/60 batches, loss: 0.1404Epoch 7/15: [====================          ] 40/60 batches, loss: 0.1418Epoch 7/15: [====================          ] 41/60 batches, loss: 0.1403Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.1405Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.1398Epoch 7/15: [======================        ] 44/60 batches, loss: 0.1405Epoch 7/15: [======================        ] 45/60 batches, loss: 0.1403Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.1412Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.1416Epoch 7/15: [========================      ] 48/60 batches, loss: 0.1412Epoch 7/15: [========================      ] 49/60 batches, loss: 0.1418Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.1422Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.1415Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.1407Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.1420Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.1425Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.1422Epoch 7/15: [============================  ] 56/60 batches, loss: 0.1419Epoch 7/15: [============================  ] 57/60 batches, loss: 0.1414Epoch 7/15: [============================= ] 58/60 batches, loss: 0.1415Epoch 7/15: [============================= ] 59/60 batches, loss: 0.1427Epoch 7/15: [==============================] 60/60 batches, loss: 0.1436
[2025-05-07 20:29:42,588][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.1436
[2025-05-07 20:29:42,897][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0833, Metrics: {'mse': 0.08843201398849487, 'rmse': 0.2973752074206841, 'r2': -0.46688389778137207}
[2025-05-07 20:29:42,898][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.1622Epoch 8/15: [=                             ] 2/60 batches, loss: 0.1441Epoch 8/15: [=                             ] 3/60 batches, loss: 0.1341Epoch 8/15: [==                            ] 4/60 batches, loss: 0.1325Epoch 8/15: [==                            ] 5/60 batches, loss: 0.1302Epoch 8/15: [===                           ] 6/60 batches, loss: 0.1413Epoch 8/15: [===                           ] 7/60 batches, loss: 0.1412Epoch 8/15: [====                          ] 8/60 batches, loss: 0.1432Epoch 8/15: [====                          ] 9/60 batches, loss: 0.1376Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.1411Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.1385Epoch 8/15: [======                        ] 12/60 batches, loss: 0.1347Epoch 8/15: [======                        ] 13/60 batches, loss: 0.1342Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.1315Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.1341Epoch 8/15: [========                      ] 16/60 batches, loss: 0.1323Epoch 8/15: [========                      ] 17/60 batches, loss: 0.1314Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.1265Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.1274Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.1264Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.1238Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.1279Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.1283Epoch 8/15: [============                  ] 24/60 batches, loss: 0.1290Epoch 8/15: [============                  ] 25/60 batches, loss: 0.1267Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.1253Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.1269Epoch 8/15: [==============                ] 28/60 batches, loss: 0.1273Epoch 8/15: [==============                ] 29/60 batches, loss: 0.1266Epoch 8/15: [===============               ] 30/60 batches, loss: 0.1270Epoch 8/15: [===============               ] 31/60 batches, loss: 0.1279Epoch 8/15: [================              ] 32/60 batches, loss: 0.1280Epoch 8/15: [================              ] 33/60 batches, loss: 0.1268Epoch 8/15: [=================             ] 34/60 batches, loss: 0.1266Epoch 8/15: [=================             ] 35/60 batches, loss: 0.1259Epoch 8/15: [==================            ] 36/60 batches, loss: 0.1259Epoch 8/15: [==================            ] 37/60 batches, loss: 0.1252Epoch 8/15: [===================           ] 38/60 batches, loss: 0.1270Epoch 8/15: [===================           ] 39/60 batches, loss: 0.1271Epoch 8/15: [====================          ] 40/60 batches, loss: 0.1274Epoch 8/15: [====================          ] 41/60 batches, loss: 0.1281Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.1280Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.1286Epoch 8/15: [======================        ] 44/60 batches, loss: 0.1279Epoch 8/15: [======================        ] 45/60 batches, loss: 0.1290Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.1274Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.1273Epoch 8/15: [========================      ] 48/60 batches, loss: 0.1277Epoch 8/15: [========================      ] 49/60 batches, loss: 0.1275Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.1275Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.1272Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.1266Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.1268Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.1279Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.1285Epoch 8/15: [============================  ] 56/60 batches, loss: 0.1283Epoch 8/15: [============================  ] 57/60 batches, loss: 0.1285Epoch 8/15: [============================= ] 58/60 batches, loss: 0.1273Epoch 8/15: [============================= ] 59/60 batches, loss: 0.1272Epoch 8/15: [==============================] 60/60 batches, loss: 0.1274
[2025-05-07 20:29:44,799][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.1274
[2025-05-07 20:29:45,111][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0734, Metrics: {'mse': 0.07788590341806412, 'rmse': 0.27908046047343427, 'r2': -0.29194819927215576}
[2025-05-07 20:29:45,112][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.1799Epoch 9/15: [=                             ] 2/60 batches, loss: 0.1611Epoch 9/15: [=                             ] 3/60 batches, loss: 0.1420Epoch 9/15: [==                            ] 4/60 batches, loss: 0.1432Epoch 9/15: [==                            ] 5/60 batches, loss: 0.1436Epoch 9/15: [===                           ] 6/60 batches, loss: 0.1386Epoch 9/15: [===                           ] 7/60 batches, loss: 0.1287Epoch 9/15: [====                          ] 8/60 batches, loss: 0.1351Epoch 9/15: [====                          ] 9/60 batches, loss: 0.1296Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.1295Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.1312Epoch 9/15: [======                        ] 12/60 batches, loss: 0.1350Epoch 9/15: [======                        ] 13/60 batches, loss: 0.1318Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.1308Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.1370Epoch 9/15: [========                      ] 16/60 batches, loss: 0.1382Epoch 9/15: [========                      ] 17/60 batches, loss: 0.1379Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.1369Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.1331Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.1315Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.1328Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.1315Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.1322Epoch 9/15: [============                  ] 24/60 batches, loss: 0.1315Epoch 9/15: [============                  ] 25/60 batches, loss: 0.1337Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.1334Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.1324Epoch 9/15: [==============                ] 28/60 batches, loss: 0.1362Epoch 9/15: [==============                ] 29/60 batches, loss: 0.1367Epoch 9/15: [===============               ] 30/60 batches, loss: 0.1372Epoch 9/15: [===============               ] 31/60 batches, loss: 0.1382Epoch 9/15: [================              ] 32/60 batches, loss: 0.1363Epoch 9/15: [================              ] 33/60 batches, loss: 0.1358Epoch 9/15: [=================             ] 34/60 batches, loss: 0.1367Epoch 9/15: [=================             ] 35/60 batches, loss: 0.1402Epoch 9/15: [==================            ] 36/60 batches, loss: 0.1389Epoch 9/15: [==================            ] 37/60 batches, loss: 0.1383Epoch 9/15: [===================           ] 38/60 batches, loss: 0.1381Epoch 9/15: [===================           ] 39/60 batches, loss: 0.1370Epoch 9/15: [====================          ] 40/60 batches, loss: 0.1385Epoch 9/15: [====================          ] 41/60 batches, loss: 0.1392Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.1400Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.1404Epoch 9/15: [======================        ] 44/60 batches, loss: 0.1405Epoch 9/15: [======================        ] 45/60 batches, loss: 0.1405Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.1410Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.1395Epoch 9/15: [========================      ] 48/60 batches, loss: 0.1383Epoch 9/15: [========================      ] 49/60 batches, loss: 0.1402Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.1394Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.1400Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.1401Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.1394Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.1383Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.1384Epoch 9/15: [============================  ] 56/60 batches, loss: 0.1378Epoch 9/15: [============================  ] 57/60 batches, loss: 0.1365Epoch 9/15: [============================= ] 58/60 batches, loss: 0.1364Epoch 9/15: [============================= ] 59/60 batches, loss: 0.1370Epoch 9/15: [==============================] 60/60 batches, loss: 0.1378
[2025-05-07 20:29:47,023][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.1378
[2025-05-07 20:29:47,339][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0689, Metrics: {'mse': 0.07338396459817886, 'rmse': 0.27089474819231707, 'r2': -0.2172713279724121}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0707Epoch 10/15: [=                             ] 2/60 batches, loss: 0.1219Epoch 10/15: [=                             ] 3/60 batches, loss: 0.1389Epoch 10/15: [==                            ] 4/60 batches, loss: 0.1308Epoch 10/15: [==                            ] 5/60 batches, loss: 0.1247Epoch 10/15: [===                           ] 6/60 batches, loss: 0.1207Epoch 10/15: [===                           ] 7/60 batches, loss: 0.1193Epoch 10/15: [====                          ] 8/60 batches, loss: 0.1251Epoch 10/15: [====                          ] 9/60 batches, loss: 0.1296Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.1285Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.1291Epoch 10/15: [======                        ] 12/60 batches, loss: 0.1300Epoch 10/15: [======                        ] 13/60 batches, loss: 0.1263Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.1271Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.1277Epoch 10/15: [========                      ] 16/60 batches, loss: 0.1282Epoch 10/15: [========                      ] 17/60 batches, loss: 0.1307Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.1292Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.1284Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.1275Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.1293Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.1299Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.1286Epoch 10/15: [============                  ] 24/60 batches, loss: 0.1277Epoch 10/15: [============                  ] 25/60 batches, loss: 0.1270Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.1301Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.1282Epoch 10/15: [==============                ] 28/60 batches, loss: 0.1289Epoch 10/15: [==============                ] 29/60 batches, loss: 0.1271Epoch 10/15: [===============               ] 30/60 batches, loss: 0.1256Epoch 10/15: [===============               ] 31/60 batches, loss: 0.1267Epoch 10/15: [================              ] 32/60 batches, loss: 0.1269Epoch 10/15: [================              ] 33/60 batches, loss: 0.1272Epoch 10/15: [=================             ] 34/60 batches, loss: 0.1276Epoch 10/15: [=================             ] 35/60 batches, loss: 0.1294Epoch 10/15: [==================            ] 36/60 batches, loss: 0.1289Epoch 10/15: [==================            ] 37/60 batches, loss: 0.1288Epoch 10/15: [===================           ] 38/60 batches, loss: 0.1291Epoch 10/15: [===================           ] 39/60 batches, loss: 0.1293Epoch 10/15: [====================          ] 40/60 batches, loss: 0.1299Epoch 10/15: [====================          ] 41/60 batches, loss: 0.1292Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.1288Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.1291Epoch 10/15: [======================        ] 44/60 batches, loss: 0.1291Epoch 10/15: [======================        ] 45/60 batches, loss: 0.1287Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.1303Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.1315Epoch 10/15: [========================      ] 48/60 batches, loss: 0.1306Epoch 10/15: [========================      ] 49/60 batches, loss: 0.1297Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.1301Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.1298Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.1288Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.1281Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.1274Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.1273Epoch 10/15: [============================  ] 56/60 batches, loss: 0.1272Epoch 10/15: [============================  ] 57/60 batches, loss: 0.1272Epoch 10/15: [============================= ] 58/60 batches, loss: 0.1277Epoch 10/15: [============================= ] 59/60 batches, loss: 0.1276Epoch 10/15: [==============================] 60/60 batches, loss: 0.1270
[2025-05-07 20:29:49,736][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.1270
[2025-05-07 20:29:50,024][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0751, Metrics: {'mse': 0.0797790139913559, 'rmse': 0.28245179056142644, 'r2': -0.32335054874420166}
[2025-05-07 20:29:50,024][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.1335Epoch 11/15: [=                             ] 2/60 batches, loss: 0.1137Epoch 11/15: [=                             ] 3/60 batches, loss: 0.1148Epoch 11/15: [==                            ] 4/60 batches, loss: 0.1224Epoch 11/15: [==                            ] 5/60 batches, loss: 0.1286Epoch 11/15: [===                           ] 6/60 batches, loss: 0.1284Epoch 11/15: [===                           ] 7/60 batches, loss: 0.1232Epoch 11/15: [====                          ] 8/60 batches, loss: 0.1206Epoch 11/15: [====                          ] 9/60 batches, loss: 0.1206Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.1196Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.1130Epoch 11/15: [======                        ] 12/60 batches, loss: 0.1102Epoch 11/15: [======                        ] 13/60 batches, loss: 0.1071Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.1120Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.1121Epoch 11/15: [========                      ] 16/60 batches, loss: 0.1120Epoch 11/15: [========                      ] 17/60 batches, loss: 0.1133Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.1125Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.1151Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.1136Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.1139Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.1161Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.1147Epoch 11/15: [============                  ] 24/60 batches, loss: 0.1153Epoch 11/15: [============                  ] 25/60 batches, loss: 0.1156Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.1159Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.1167Epoch 11/15: [==============                ] 28/60 batches, loss: 0.1181Epoch 11/15: [==============                ] 29/60 batches, loss: 0.1176Epoch 11/15: [===============               ] 30/60 batches, loss: 0.1177Epoch 11/15: [===============               ] 31/60 batches, loss: 0.1184Epoch 11/15: [================              ] 32/60 batches, loss: 0.1181Epoch 11/15: [================              ] 33/60 batches, loss: 0.1197Epoch 11/15: [=================             ] 34/60 batches, loss: 0.1197Epoch 11/15: [=================             ] 35/60 batches, loss: 0.1189Epoch 11/15: [==================            ] 36/60 batches, loss: 0.1189Epoch 11/15: [==================            ] 37/60 batches, loss: 0.1195Epoch 11/15: [===================           ] 38/60 batches, loss: 0.1205Epoch 11/15: [===================           ] 39/60 batches, loss: 0.1212Epoch 11/15: [====================          ] 40/60 batches, loss: 0.1209Epoch 11/15: [====================          ] 41/60 batches, loss: 0.1205Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.1210Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.1204Epoch 11/15: [======================        ] 44/60 batches, loss: 0.1198Epoch 11/15: [======================        ] 45/60 batches, loss: 0.1210Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.1223Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.1219Epoch 11/15: [========================      ] 48/60 batches, loss: 0.1209Epoch 11/15: [========================      ] 49/60 batches, loss: 0.1208Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.1203Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.1203Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.1196Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.1195Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.1217Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.1208Epoch 11/15: [============================  ] 56/60 batches, loss: 0.1219Epoch 11/15: [============================  ] 57/60 batches, loss: 0.1225Epoch 11/15: [============================= ] 58/60 batches, loss: 0.1225Epoch 11/15: [============================= ] 59/60 batches, loss: 0.1233Epoch 11/15: [==============================] 60/60 batches, loss: 0.1232
[2025-05-07 20:29:51,911][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.1232
[2025-05-07 20:29:52,183][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0646, Metrics: {'mse': 0.06888088583946228, 'rmse': 0.2624516828665084, 'r2': -0.14257562160491943}
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.1034Epoch 12/15: [=                             ] 2/60 batches, loss: 0.1573Epoch 12/15: [=                             ] 3/60 batches, loss: 0.1440Epoch 12/15: [==                            ] 4/60 batches, loss: 0.1419Epoch 12/15: [==                            ] 5/60 batches, loss: 0.1348Epoch 12/15: [===                           ] 6/60 batches, loss: 0.1431Epoch 12/15: [===                           ] 7/60 batches, loss: 0.1438Epoch 12/15: [====                          ] 8/60 batches, loss: 0.1357Epoch 12/15: [====                          ] 9/60 batches, loss: 0.1328Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.1286Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.1327Epoch 12/15: [======                        ] 12/60 batches, loss: 0.1388Epoch 12/15: [======                        ] 13/60 batches, loss: 0.1372Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.1340Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.1307Epoch 12/15: [========                      ] 16/60 batches, loss: 0.1306Epoch 12/15: [========                      ] 17/60 batches, loss: 0.1306Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.1301Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.1339Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.1354Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.1339Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.1339Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.1326Epoch 12/15: [============                  ] 24/60 batches, loss: 0.1315Epoch 12/15: [============                  ] 25/60 batches, loss: 0.1326Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.1335Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.1343Epoch 12/15: [==============                ] 28/60 batches, loss: 0.1341Epoch 12/15: [==============                ] 29/60 batches, loss: 0.1329Epoch 12/15: [===============               ] 30/60 batches, loss: 0.1331Epoch 12/15: [===============               ] 31/60 batches, loss: 0.1313Epoch 12/15: [================              ] 32/60 batches, loss: 0.1313Epoch 12/15: [================              ] 33/60 batches, loss: 0.1317Epoch 12/15: [=================             ] 34/60 batches, loss: 0.1331Epoch 12/15: [=================             ] 35/60 batches, loss: 0.1325Epoch 12/15: [==================            ] 36/60 batches, loss: 0.1327Epoch 12/15: [==================            ] 37/60 batches, loss: 0.1327Epoch 12/15: [===================           ] 38/60 batches, loss: 0.1320Epoch 12/15: [===================           ] 39/60 batches, loss: 0.1310Epoch 12/15: [====================          ] 40/60 batches, loss: 0.1300Epoch 12/15: [====================          ] 41/60 batches, loss: 0.1308Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.1298Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.1293Epoch 12/15: [======================        ] 44/60 batches, loss: 0.1315Epoch 12/15: [======================        ] 45/60 batches, loss: 0.1312Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.1300Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.1299Epoch 12/15: [========================      ] 48/60 batches, loss: 0.1298Epoch 12/15: [========================      ] 49/60 batches, loss: 0.1296Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.1294Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.1310Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.1298Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.1294Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.1303Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.1299Epoch 12/15: [============================  ] 56/60 batches, loss: 0.1297Epoch 12/15: [============================  ] 57/60 batches, loss: 0.1306Epoch 12/15: [============================= ] 58/60 batches, loss: 0.1297Epoch 12/15: [============================= ] 59/60 batches, loss: 0.1291Epoch 12/15: [==============================] 60/60 batches, loss: 0.1286
[2025-05-07 20:29:54,514][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.1286
[2025-05-07 20:29:54,817][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0617, Metrics: {'mse': 0.06588176637887955, 'rmse': 0.2566744365512069, 'r2': -0.09282708168029785}
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0908Epoch 13/15: [=                             ] 2/60 batches, loss: 0.1106Epoch 13/15: [=                             ] 3/60 batches, loss: 0.1014Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0982Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0925Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0985Epoch 13/15: [===                           ] 7/60 batches, loss: 0.1009Epoch 13/15: [====                          ] 8/60 batches, loss: 0.1054Epoch 13/15: [====                          ] 9/60 batches, loss: 0.1067Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.1047Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.1111Epoch 13/15: [======                        ] 12/60 batches, loss: 0.1084Epoch 13/15: [======                        ] 13/60 batches, loss: 0.1085Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.1112Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.1115Epoch 13/15: [========                      ] 16/60 batches, loss: 0.1109Epoch 13/15: [========                      ] 17/60 batches, loss: 0.1147Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.1136Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.1127Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.1132Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.1153Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.1159Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.1147Epoch 13/15: [============                  ] 24/60 batches, loss: 0.1158Epoch 13/15: [============                  ] 25/60 batches, loss: 0.1153Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.1144Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.1126Epoch 13/15: [==============                ] 28/60 batches, loss: 0.1122Epoch 13/15: [==============                ] 29/60 batches, loss: 0.1123Epoch 13/15: [===============               ] 30/60 batches, loss: 0.1114Epoch 13/15: [===============               ] 31/60 batches, loss: 0.1129Epoch 13/15: [================              ] 32/60 batches, loss: 0.1139Epoch 13/15: [================              ] 33/60 batches, loss: 0.1146Epoch 13/15: [=================             ] 34/60 batches, loss: 0.1151Epoch 13/15: [=================             ] 35/60 batches, loss: 0.1154Epoch 13/15: [==================            ] 36/60 batches, loss: 0.1156Epoch 13/15: [==================            ] 37/60 batches, loss: 0.1144Epoch 13/15: [===================           ] 38/60 batches, loss: 0.1142Epoch 13/15: [===================           ] 39/60 batches, loss: 0.1144Epoch 13/15: [====================          ] 40/60 batches, loss: 0.1148Epoch 13/15: [====================          ] 41/60 batches, loss: 0.1160Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.1150Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.1150Epoch 13/15: [======================        ] 44/60 batches, loss: 0.1146Epoch 13/15: [======================        ] 45/60 batches, loss: 0.1154Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.1157Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.1161Epoch 13/15: [========================      ] 48/60 batches, loss: 0.1163Epoch 13/15: [========================      ] 49/60 batches, loss: 0.1167Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.1179Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.1183Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.1182Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.1190Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.1196Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.1203Epoch 13/15: [============================  ] 56/60 batches, loss: 0.1207Epoch 13/15: [============================  ] 57/60 batches, loss: 0.1207Epoch 13/15: [============================= ] 58/60 batches, loss: 0.1206Epoch 13/15: [============================= ] 59/60 batches, loss: 0.1221Epoch 13/15: [==============================] 60/60 batches, loss: 0.1224
[2025-05-07 20:29:57,063][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.1224
[2025-05-07 20:29:57,323][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0671, Metrics: {'mse': 0.07189144939184189, 'rmse': 0.2681258088879955, 'r2': -0.1925138235092163}
[2025-05-07 20:29:57,323][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0890Epoch 14/15: [=                             ] 2/60 batches, loss: 0.1062Epoch 14/15: [=                             ] 3/60 batches, loss: 0.1098Epoch 14/15: [==                            ] 4/60 batches, loss: 0.1135Epoch 14/15: [==                            ] 5/60 batches, loss: 0.1158Epoch 14/15: [===                           ] 6/60 batches, loss: 0.1205Epoch 14/15: [===                           ] 7/60 batches, loss: 0.1170Epoch 14/15: [====                          ] 8/60 batches, loss: 0.1200Epoch 14/15: [====                          ] 9/60 batches, loss: 0.1190Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.1162Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.1167Epoch 14/15: [======                        ] 12/60 batches, loss: 0.1168Epoch 14/15: [======                        ] 13/60 batches, loss: 0.1211Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.1270Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.1257Epoch 14/15: [========                      ] 16/60 batches, loss: 0.1250Epoch 14/15: [========                      ] 17/60 batches, loss: 0.1274Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.1260Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.1250Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.1256Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.1243Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.1232Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.1222Epoch 14/15: [============                  ] 24/60 batches, loss: 0.1218Epoch 14/15: [============                  ] 25/60 batches, loss: 0.1224Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.1220Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.1202Epoch 14/15: [==============                ] 28/60 batches, loss: 0.1193Epoch 14/15: [==============                ] 29/60 batches, loss: 0.1200Epoch 14/15: [===============               ] 30/60 batches, loss: 0.1206Epoch 14/15: [===============               ] 31/60 batches, loss: 0.1189Epoch 14/15: [================              ] 32/60 batches, loss: 0.1174Epoch 14/15: [================              ] 33/60 batches, loss: 0.1173Epoch 14/15: [=================             ] 34/60 batches, loss: 0.1169Epoch 14/15: [=================             ] 35/60 batches, loss: 0.1163Epoch 14/15: [==================            ] 36/60 batches, loss: 0.1164Epoch 14/15: [==================            ] 37/60 batches, loss: 0.1159Epoch 14/15: [===================           ] 38/60 batches, loss: 0.1173Epoch 14/15: [===================           ] 39/60 batches, loss: 0.1168Epoch 14/15: [====================          ] 40/60 batches, loss: 0.1174Epoch 14/15: [====================          ] 41/60 batches, loss: 0.1170Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.1170Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.1175Epoch 14/15: [======================        ] 44/60 batches, loss: 0.1164Epoch 14/15: [======================        ] 45/60 batches, loss: 0.1175Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.1176Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.1182Epoch 14/15: [========================      ] 48/60 batches, loss: 0.1181Epoch 14/15: [========================      ] 49/60 batches, loss: 0.1175Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.1175Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.1163Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.1163Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.1173Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.1167Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.1174Epoch 14/15: [============================  ] 56/60 batches, loss: 0.1164Epoch 14/15: [============================  ] 57/60 batches, loss: 0.1160Epoch 14/15: [============================= ] 58/60 batches, loss: 0.1172Epoch 14/15: [============================= ] 59/60 batches, loss: 0.1170Epoch 14/15: [==============================] 60/60 batches, loss: 0.1168
[2025-05-07 20:29:59,205][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.1168
[2025-05-07 20:29:59,556][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0630, Metrics: {'mse': 0.06769587099552155, 'rmse': 0.2601843019775051, 'r2': -0.12291884422302246}
[2025-05-07 20:29:59,556][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.1457Epoch 15/15: [=                             ] 2/60 batches, loss: 0.1426Epoch 15/15: [=                             ] 3/60 batches, loss: 0.1416Epoch 15/15: [==                            ] 4/60 batches, loss: 0.1285Epoch 15/15: [==                            ] 5/60 batches, loss: 0.1282Epoch 15/15: [===                           ] 6/60 batches, loss: 0.1215Epoch 15/15: [===                           ] 7/60 batches, loss: 0.1158Epoch 15/15: [====                          ] 8/60 batches, loss: 0.1190Epoch 15/15: [====                          ] 9/60 batches, loss: 0.1128Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.1093Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.1104Epoch 15/15: [======                        ] 12/60 batches, loss: 0.1104Epoch 15/15: [======                        ] 13/60 batches, loss: 0.1099Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.1131Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.1205Epoch 15/15: [========                      ] 16/60 batches, loss: 0.1223Epoch 15/15: [========                      ] 17/60 batches, loss: 0.1215Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.1229Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.1231Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.1219Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.1210Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.1201Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.1172Epoch 15/15: [============                  ] 24/60 batches, loss: 0.1188Epoch 15/15: [============                  ] 25/60 batches, loss: 0.1201Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.1183Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.1194Epoch 15/15: [==============                ] 28/60 batches, loss: 0.1176Epoch 15/15: [==============                ] 29/60 batches, loss: 0.1184Epoch 15/15: [===============               ] 30/60 batches, loss: 0.1188Epoch 15/15: [===============               ] 31/60 batches, loss: 0.1196Epoch 15/15: [================              ] 32/60 batches, loss: 0.1187Epoch 15/15: [================              ] 33/60 batches, loss: 0.1164Epoch 15/15: [=================             ] 34/60 batches, loss: 0.1165Epoch 15/15: [=================             ] 35/60 batches, loss: 0.1173Epoch 15/15: [==================            ] 36/60 batches, loss: 0.1185Epoch 15/15: [==================            ] 37/60 batches, loss: 0.1180Epoch 15/15: [===================           ] 38/60 batches, loss: 0.1183Epoch 15/15: [===================           ] 39/60 batches, loss: 0.1184Epoch 15/15: [====================          ] 40/60 batches, loss: 0.1182Epoch 15/15: [====================          ] 41/60 batches, loss: 0.1170Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.1174Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.1175Epoch 15/15: [======================        ] 44/60 batches, loss: 0.1169Epoch 15/15: [======================        ] 45/60 batches, loss: 0.1171Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.1172Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.1183Epoch 15/15: [========================      ] 48/60 batches, loss: 0.1188Epoch 15/15: [========================      ] 49/60 batches, loss: 0.1184Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.1187Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.1186Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.1189Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.1187Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.1193Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.1194Epoch 15/15: [============================  ] 56/60 batches, loss: 0.1198Epoch 15/15: [============================  ] 57/60 batches, loss: 0.1193Epoch 15/15: [============================= ] 58/60 batches, loss: 0.1194Epoch 15/15: [============================= ] 59/60 batches, loss: 0.1188Epoch 15/15: [==============================] 60/60 batches, loss: 0.1190
[2025-05-07 20:30:01,430][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.1190
[2025-05-07 20:30:01,701][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0584, Metrics: {'mse': 0.06230078265070915, 'rmse': 0.24960124729397717, 'r2': -0.033426761627197266}
[2025-05-07 20:30:02,131][src.training.lm_trainer][INFO] - Training completed in 36.75 seconds
[2025-05-07 20:30:02,131][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:30:04,811][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.0980134978890419, 'rmse': 0.31307107482014673, 'r2': 0.0051087141036987305}
[2025-05-07 20:30:04,811][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06230078265070915, 'rmse': 0.24960124729397717, 'r2': -0.033426761627197266}
[2025-05-07 20:30:04,811][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06778217107057571, 'rmse': 0.2603500932793682, 'r2': -0.06125044822692871}
[2025-05-07 20:30:06,481][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control2/layer2/id/id/model.pt
[2025-05-07 20:30:06,483][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▄▂▂▂▁▁
wandb:     best_val_mse █▆▄▂▂▂▁▁
wandb:      best_val_r2 ▁▃▅▇▇▇██
wandb:    best_val_rmse █▆▄▂▂▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▁▅▆▅▆▆▆▆▆▇▇▆▇
wandb:       train_loss █▄▃▂▂▂▂▁▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆█▄▃▃▂▃▂▂▂▂▁▂▁▁
wandb:          val_mse ▆█▄▃▃▂▃▂▂▂▂▁▂▁▁
wandb:           val_r2 ▃▁▅▆▆▇▆▇▇▇▇█▇██
wandb:         val_rmse ▆█▅▃▄▂▃▃▂▃▂▁▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0584
wandb:     best_val_mse 0.0623
wandb:      best_val_r2 -0.03343
wandb:    best_val_rmse 0.2496
wandb:            epoch 15
wandb:   final_test_mse 0.06778
wandb:    final_test_r2 -0.06125
wandb:  final_test_rmse 0.26035
wandb:  final_train_mse 0.09801
wandb:   final_train_r2 0.00511
wandb: final_train_rmse 0.31307
wandb:    final_val_mse 0.0623
wandb:     final_val_r2 -0.03343
wandb:   final_val_rmse 0.2496
wandb:    learning_rate 0.0001
wandb:       train_loss 0.11903
wandb:       train_time 36.75328
wandb:         val_loss 0.0584
wandb:          val_mse 0.0623
wandb:           val_r2 -0.03343
wandb:         val_rmse 0.2496
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_202908-34z49czd
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_202908-34z49czd/logs
Experiment probe_layer2_avg_verb_edges_control2_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control2/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_avg_verb_edges_control3_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_verb_edges_control3_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control3/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=avg_verb_edges"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:30:36,057][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control3/layer2/id
experiment_name: probe_layer2_avg_verb_edges_control3_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:30:36,057][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:30:36,057][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 20:30:36,057][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:30:36,057][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:30:36,062][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:30:36,062][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 20:30:36,062][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:30:39,178][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:30:41,617][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:30:41,617][src.data.datasets][INFO] - Loading 'control_avg_verb_edges_seed3' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:30:41,851][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_verb_edges_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:32:16 2025).
[2025-05-07 20:30:41,962][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_verb_edges_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:32:16 2025).
[2025-05-07 20:30:42,264][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:30:42,271][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:30:42,272][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:30:42,275][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:30:42,380][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:30:42,456][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:30:42,508][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:30:42,509][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:30:42,509][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:30:42,514][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:30:42,603][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:30:42,680][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:30:42,718][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:30:42,719][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:30:42,719][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:30:42,723][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:30:42,724][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:30:42,724][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 20:30:42,724][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 20:30:42,724][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 20:30:42,724][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:30:42,724][src.data.datasets][INFO] -   Mean: 0.3916, Std: 0.3139
[2025-05-07 20:30:42,724][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:30:42,724][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:30:42,725][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:30:42,725][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 20:30:42,725][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 20:30:42,725][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 20:30:42,725][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-05-07 20:30:42,725][src.data.datasets][INFO] -   Mean: 0.3834, Std: 0.2455
[2025-05-07 20:30:42,725][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:30:42,725][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:30:42,725][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:30:42,726][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 20:30:42,726][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 20:30:42,726][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 20:30:42,726][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:30:42,726][src.data.datasets][INFO] -   Mean: 0.4230, Std: 0.2527
[2025-05-07 20:30:42,726][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:30:42,726][src.data.datasets][INFO] - Sample label: 0.5830000042915344
[2025-05-07 20:30:42,726][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:30:42,726][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:30:42,727][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:30:42,727][__main__][INFO] - Using model type: lm_probe for submetric avg_verb_edges
[2025-05-07 20:30:42,727][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:30:50,880][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:30:50,881][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:30:50,881][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:30:50,881][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:30:50,884][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:30:50,885][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:30:50,885][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:30:50,885][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:30:50,885][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:30:50,886][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:30:50,886][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.5486Epoch 1/15: [=                             ] 2/60 batches, loss: 0.6257Epoch 1/15: [=                             ] 3/60 batches, loss: 0.5805Epoch 1/15: [==                            ] 4/60 batches, loss: 0.6381Epoch 1/15: [==                            ] 5/60 batches, loss: 0.6228Epoch 1/15: [===                           ] 6/60 batches, loss: 0.5975Epoch 1/15: [===                           ] 7/60 batches, loss: 0.6157Epoch 1/15: [====                          ] 8/60 batches, loss: 0.6065Epoch 1/15: [====                          ] 9/60 batches, loss: 0.6069Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.6006Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.5834Epoch 1/15: [======                        ] 12/60 batches, loss: 0.5722Epoch 1/15: [======                        ] 13/60 batches, loss: 0.5538Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.5459Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.5367Epoch 1/15: [========                      ] 16/60 batches, loss: 0.5386Epoch 1/15: [========                      ] 17/60 batches, loss: 0.5277Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.5443Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.5263Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.5185Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.5093Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4999Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4897Epoch 1/15: [============                  ] 24/60 batches, loss: 0.4817Epoch 1/15: [============                  ] 25/60 batches, loss: 0.4781Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.4822Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.4784Epoch 1/15: [==============                ] 28/60 batches, loss: 0.4718Epoch 1/15: [==============                ] 29/60 batches, loss: 0.4722Epoch 1/15: [===============               ] 30/60 batches, loss: 0.4762Epoch 1/15: [===============               ] 31/60 batches, loss: 0.4674Epoch 1/15: [================              ] 32/60 batches, loss: 0.4625Epoch 1/15: [================              ] 33/60 batches, loss: 0.4574Epoch 1/15: [=================             ] 34/60 batches, loss: 0.4525Epoch 1/15: [=================             ] 35/60 batches, loss: 0.4499Epoch 1/15: [==================            ] 36/60 batches, loss: 0.4471Epoch 1/15: [==================            ] 37/60 batches, loss: 0.4417Epoch 1/15: [===================           ] 38/60 batches, loss: 0.4376Epoch 1/15: [===================           ] 39/60 batches, loss: 0.4366Epoch 1/15: [====================          ] 40/60 batches, loss: 0.4358Epoch 1/15: [====================          ] 41/60 batches, loss: 0.4343Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.4366Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.4330Epoch 1/15: [======================        ] 44/60 batches, loss: 0.4289Epoch 1/15: [======================        ] 45/60 batches, loss: 0.4248Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.4212Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.4213Epoch 1/15: [========================      ] 48/60 batches, loss: 0.4168Epoch 1/15: [========================      ] 49/60 batches, loss: 0.4181Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.4146Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.4154Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.4152Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.4114Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.4107Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.4065Epoch 1/15: [============================  ] 56/60 batches, loss: 0.4060Epoch 1/15: [============================  ] 57/60 batches, loss: 0.4059Epoch 1/15: [============================= ] 58/60 batches, loss: 0.4039Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3988Epoch 1/15: [==============================] 60/60 batches, loss: 0.3946
[2025-05-07 20:30:57,254][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3946
[2025-05-07 20:30:57,552][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1134, Metrics: {'mse': 0.11906436085700989, 'rmse': 0.3450570400049967, 'r2': -0.9750040769577026}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.2582Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2815Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2945Epoch 2/15: [==                            ] 4/60 batches, loss: 0.3090Epoch 2/15: [==                            ] 5/60 batches, loss: 0.2782Epoch 2/15: [===                           ] 6/60 batches, loss: 0.2818Epoch 2/15: [===                           ] 7/60 batches, loss: 0.2822Epoch 2/15: [====                          ] 8/60 batches, loss: 0.2895Epoch 2/15: [====                          ] 9/60 batches, loss: 0.2758Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.2717Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.2719Epoch 2/15: [======                        ] 12/60 batches, loss: 0.2706Epoch 2/15: [======                        ] 13/60 batches, loss: 0.2746Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.2799Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.2675Epoch 2/15: [========                      ] 16/60 batches, loss: 0.2681Epoch 2/15: [========                      ] 17/60 batches, loss: 0.2704Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.2708Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.2702Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.2707Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.2635Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.2630Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.2568Epoch 2/15: [============                  ] 24/60 batches, loss: 0.2525Epoch 2/15: [============                  ] 25/60 batches, loss: 0.2572Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.2561Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.2559Epoch 2/15: [==============                ] 28/60 batches, loss: 0.2500Epoch 2/15: [==============                ] 29/60 batches, loss: 0.2471Epoch 2/15: [===============               ] 30/60 batches, loss: 0.2472Epoch 2/15: [===============               ] 31/60 batches, loss: 0.2476Epoch 2/15: [================              ] 32/60 batches, loss: 0.2449Epoch 2/15: [================              ] 33/60 batches, loss: 0.2463Epoch 2/15: [=================             ] 34/60 batches, loss: 0.2490Epoch 2/15: [=================             ] 35/60 batches, loss: 0.2486Epoch 2/15: [==================            ] 36/60 batches, loss: 0.2475Epoch 2/15: [==================            ] 37/60 batches, loss: 0.2472Epoch 2/15: [===================           ] 38/60 batches, loss: 0.2484Epoch 2/15: [===================           ] 39/60 batches, loss: 0.2475Epoch 2/15: [====================          ] 40/60 batches, loss: 0.2497Epoch 2/15: [====================          ] 41/60 batches, loss: 0.2483Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.2481Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.2472Epoch 2/15: [======================        ] 44/60 batches, loss: 0.2472Epoch 2/15: [======================        ] 45/60 batches, loss: 0.2470Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.2456Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.2452Epoch 2/15: [========================      ] 48/60 batches, loss: 0.2439Epoch 2/15: [========================      ] 49/60 batches, loss: 0.2435Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.2433Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.2408Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.2408Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.2399Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.2391Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.2398Epoch 2/15: [============================  ] 56/60 batches, loss: 0.2386Epoch 2/15: [============================  ] 57/60 batches, loss: 0.2378Epoch 2/15: [============================= ] 58/60 batches, loss: 0.2382Epoch 2/15: [============================= ] 59/60 batches, loss: 0.2365Epoch 2/15: [==============================] 60/60 batches, loss: 0.2354
[2025-05-07 20:30:59,848][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.2354
[2025-05-07 20:31:00,153][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0975, Metrics: {'mse': 0.10248371958732605, 'rmse': 0.3201307851290251, 'r2': -0.6999694108963013}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.1243Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1692Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1617Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1614Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1621Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1814Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1755Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1768Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1866Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1876Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1911Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1964Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1986Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.2024Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.2094Epoch 3/15: [========                      ] 16/60 batches, loss: 0.2045Epoch 3/15: [========                      ] 17/60 batches, loss: 0.2023Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.2002Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1982Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1972Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1940Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1952Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1942Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1946Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1943Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1954Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1944Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1915Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1932Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1932Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1918Epoch 3/15: [================              ] 32/60 batches, loss: 0.1926Epoch 3/15: [================              ] 33/60 batches, loss: 0.1960Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1949Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1934Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1960Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1936Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1902Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1904Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1896Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1888Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1881Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1883Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1871Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1867Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1858Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1857Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1836Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1872Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1874Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1866Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1874Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1868Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1898Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1896Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1889Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1899Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1886Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1885Epoch 3/15: [==============================] 60/60 batches, loss: 0.1914
[2025-05-07 20:31:02,429][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1914
[2025-05-07 20:31:02,704][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0895, Metrics: {'mse': 0.09453930705785751, 'rmse': 0.3074724492663652, 'r2': -0.5681897401809692}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.2010Epoch 4/15: [=                             ] 2/60 batches, loss: 0.1679Epoch 4/15: [=                             ] 3/60 batches, loss: 0.1581Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1927Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1745Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1666Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1648Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1670Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1695Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.1648Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1713Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1735Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1720Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.1760Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.1829Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1880Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1839Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1802Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1773Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1753Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1782Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1782Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1846Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1873Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1854Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1881Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1846Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1861Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1847Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1833Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1825Epoch 4/15: [================              ] 32/60 batches, loss: 0.1793Epoch 4/15: [================              ] 33/60 batches, loss: 0.1791Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1779Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1768Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1766Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1768Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1787Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1769Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1744Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1749Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1766Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1763Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1757Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1765Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1745Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.1725Epoch 4/15: [========================      ] 48/60 batches, loss: 0.1709Epoch 4/15: [========================      ] 49/60 batches, loss: 0.1710Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.1714Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.1723Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1738Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.1735Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1758Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1767Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1771Epoch 4/15: [============================  ] 57/60 batches, loss: 0.1754Epoch 4/15: [============================= ] 58/60 batches, loss: 0.1749Epoch 4/15: [============================= ] 59/60 batches, loss: 0.1735Epoch 4/15: [==============================] 60/60 batches, loss: 0.1728
[2025-05-07 20:31:04,995][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1728
[2025-05-07 20:31:05,279][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0953, Metrics: {'mse': 0.10037897527217865, 'rmse': 0.316826411891715, 'r2': -0.6650564670562744}
[2025-05-07 20:31:05,279][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.1528Epoch 5/15: [=                             ] 2/60 batches, loss: 0.1382Epoch 5/15: [=                             ] 3/60 batches, loss: 0.1623Epoch 5/15: [==                            ] 4/60 batches, loss: 0.1517Epoch 5/15: [==                            ] 5/60 batches, loss: 0.1720Epoch 5/15: [===                           ] 6/60 batches, loss: 0.1632Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1665Epoch 5/15: [====                          ] 8/60 batches, loss: 0.1583Epoch 5/15: [====                          ] 9/60 batches, loss: 0.1573Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.1650Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1698Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1761Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1770Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1825Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1805Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1784Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1783Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1809Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1795Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1782Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1804Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1782Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1851Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1809Epoch 5/15: [============                  ] 25/60 batches, loss: 0.1800Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1781Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.1762Epoch 5/15: [==============                ] 28/60 batches, loss: 0.1732Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1765Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1762Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1759Epoch 5/15: [================              ] 32/60 batches, loss: 0.1754Epoch 5/15: [================              ] 33/60 batches, loss: 0.1756Epoch 5/15: [=================             ] 34/60 batches, loss: 0.1745Epoch 5/15: [=================             ] 35/60 batches, loss: 0.1741Epoch 5/15: [==================            ] 36/60 batches, loss: 0.1744Epoch 5/15: [==================            ] 37/60 batches, loss: 0.1731Epoch 5/15: [===================           ] 38/60 batches, loss: 0.1732Epoch 5/15: [===================           ] 39/60 batches, loss: 0.1730Epoch 5/15: [====================          ] 40/60 batches, loss: 0.1720Epoch 5/15: [====================          ] 41/60 batches, loss: 0.1701Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.1697Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.1700Epoch 5/15: [======================        ] 44/60 batches, loss: 0.1701Epoch 5/15: [======================        ] 45/60 batches, loss: 0.1712Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.1702Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.1688Epoch 5/15: [========================      ] 48/60 batches, loss: 0.1680Epoch 5/15: [========================      ] 49/60 batches, loss: 0.1681Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.1697Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.1682Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.1678Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.1685Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.1673Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.1677Epoch 5/15: [============================  ] 56/60 batches, loss: 0.1660Epoch 5/15: [============================  ] 57/60 batches, loss: 0.1661Epoch 5/15: [============================= ] 58/60 batches, loss: 0.1669Epoch 5/15: [============================= ] 59/60 batches, loss: 0.1673Epoch 5/15: [==============================] 60/60 batches, loss: 0.1665
[2025-05-07 20:31:07,140][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1665
[2025-05-07 20:31:07,402][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0809, Metrics: {'mse': 0.08575841784477234, 'rmse': 0.2928453821469144, 'r2': -0.4225350618362427}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.2176Epoch 6/15: [=                             ] 2/60 batches, loss: 0.1923Epoch 6/15: [=                             ] 3/60 batches, loss: 0.1873Epoch 6/15: [==                            ] 4/60 batches, loss: 0.1707Epoch 6/15: [==                            ] 5/60 batches, loss: 0.1590Epoch 6/15: [===                           ] 6/60 batches, loss: 0.1617Epoch 6/15: [===                           ] 7/60 batches, loss: 0.1551Epoch 6/15: [====                          ] 8/60 batches, loss: 0.1567Epoch 6/15: [====                          ] 9/60 batches, loss: 0.1672Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.1674Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.1689Epoch 6/15: [======                        ] 12/60 batches, loss: 0.1683Epoch 6/15: [======                        ] 13/60 batches, loss: 0.1662Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.1658Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.1612Epoch 6/15: [========                      ] 16/60 batches, loss: 0.1632Epoch 6/15: [========                      ] 17/60 batches, loss: 0.1610Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.1624Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.1590Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.1561Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.1536Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.1552Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.1567Epoch 6/15: [============                  ] 24/60 batches, loss: 0.1581Epoch 6/15: [============                  ] 25/60 batches, loss: 0.1586Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.1559Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.1554Epoch 6/15: [==============                ] 28/60 batches, loss: 0.1553Epoch 6/15: [==============                ] 29/60 batches, loss: 0.1547Epoch 6/15: [===============               ] 30/60 batches, loss: 0.1537Epoch 6/15: [===============               ] 31/60 batches, loss: 0.1528Epoch 6/15: [================              ] 32/60 batches, loss: 0.1513Epoch 6/15: [================              ] 33/60 batches, loss: 0.1554Epoch 6/15: [=================             ] 34/60 batches, loss: 0.1531Epoch 6/15: [=================             ] 35/60 batches, loss: 0.1518Epoch 6/15: [==================            ] 36/60 batches, loss: 0.1510Epoch 6/15: [==================            ] 37/60 batches, loss: 0.1510Epoch 6/15: [===================           ] 38/60 batches, loss: 0.1521Epoch 6/15: [===================           ] 39/60 batches, loss: 0.1530Epoch 6/15: [====================          ] 40/60 batches, loss: 0.1513Epoch 6/15: [====================          ] 41/60 batches, loss: 0.1499Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.1491Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.1473Epoch 6/15: [======================        ] 44/60 batches, loss: 0.1488Epoch 6/15: [======================        ] 45/60 batches, loss: 0.1494Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.1483Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.1501Epoch 6/15: [========================      ] 48/60 batches, loss: 0.1502Epoch 6/15: [========================      ] 49/60 batches, loss: 0.1500Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.1495Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.1515Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.1507Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.1495Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.1493Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.1498Epoch 6/15: [============================  ] 56/60 batches, loss: 0.1493Epoch 6/15: [============================  ] 57/60 batches, loss: 0.1491Epoch 6/15: [============================= ] 58/60 batches, loss: 0.1488Epoch 6/15: [============================= ] 59/60 batches, loss: 0.1491Epoch 6/15: [==============================] 60/60 batches, loss: 0.1484
[2025-05-07 20:31:09,593][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1484
[2025-05-07 20:31:09,867][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0685, Metrics: {'mse': 0.07290752232074738, 'rmse': 0.27001392986427086, 'r2': -0.20936822891235352}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.1542Epoch 7/15: [=                             ] 2/60 batches, loss: 0.1839Epoch 7/15: [=                             ] 3/60 batches, loss: 0.1893Epoch 7/15: [==                            ] 4/60 batches, loss: 0.1756Epoch 7/15: [==                            ] 5/60 batches, loss: 0.1842Epoch 7/15: [===                           ] 6/60 batches, loss: 0.1729Epoch 7/15: [===                           ] 7/60 batches, loss: 0.1723Epoch 7/15: [====                          ] 8/60 batches, loss: 0.1669Epoch 7/15: [====                          ] 9/60 batches, loss: 0.1615Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.1543Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.1476Epoch 7/15: [======                        ] 12/60 batches, loss: 0.1427Epoch 7/15: [======                        ] 13/60 batches, loss: 0.1399Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.1371Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.1373Epoch 7/15: [========                      ] 16/60 batches, loss: 0.1361Epoch 7/15: [========                      ] 17/60 batches, loss: 0.1364Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.1394Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.1360Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.1352Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.1357Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.1365Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.1381Epoch 7/15: [============                  ] 24/60 batches, loss: 0.1378Epoch 7/15: [============                  ] 25/60 batches, loss: 0.1344Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.1338Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.1332Epoch 7/15: [==============                ] 28/60 batches, loss: 0.1319Epoch 7/15: [==============                ] 29/60 batches, loss: 0.1340Epoch 7/15: [===============               ] 30/60 batches, loss: 0.1338Epoch 7/15: [===============               ] 31/60 batches, loss: 0.1323Epoch 7/15: [================              ] 32/60 batches, loss: 0.1334Epoch 7/15: [================              ] 33/60 batches, loss: 0.1336Epoch 7/15: [=================             ] 34/60 batches, loss: 0.1334Epoch 7/15: [=================             ] 35/60 batches, loss: 0.1320Epoch 7/15: [==================            ] 36/60 batches, loss: 0.1322Epoch 7/15: [==================            ] 37/60 batches, loss: 0.1307Epoch 7/15: [===================           ] 38/60 batches, loss: 0.1321Epoch 7/15: [===================           ] 39/60 batches, loss: 0.1316Epoch 7/15: [====================          ] 40/60 batches, loss: 0.1329Epoch 7/15: [====================          ] 41/60 batches, loss: 0.1328Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.1335Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.1332Epoch 7/15: [======================        ] 44/60 batches, loss: 0.1333Epoch 7/15: [======================        ] 45/60 batches, loss: 0.1325Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.1334Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.1344Epoch 7/15: [========================      ] 48/60 batches, loss: 0.1355Epoch 7/15: [========================      ] 49/60 batches, loss: 0.1354Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.1364Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.1367Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.1361Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.1353Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.1364Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.1362Epoch 7/15: [============================  ] 56/60 batches, loss: 0.1361Epoch 7/15: [============================  ] 57/60 batches, loss: 0.1355Epoch 7/15: [============================= ] 58/60 batches, loss: 0.1342Epoch 7/15: [============================= ] 59/60 batches, loss: 0.1349Epoch 7/15: [==============================] 60/60 batches, loss: 0.1367
[2025-05-07 20:31:12,134][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.1367
[2025-05-07 20:31:12,427][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0698, Metrics: {'mse': 0.07405783236026764, 'rmse': 0.27213568740660904, 'r2': -0.22844922542572021}
[2025-05-07 20:31:12,428][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.1207Epoch 8/15: [=                             ] 2/60 batches, loss: 0.1302Epoch 8/15: [=                             ] 3/60 batches, loss: 0.1599Epoch 8/15: [==                            ] 4/60 batches, loss: 0.1551Epoch 8/15: [==                            ] 5/60 batches, loss: 0.1448Epoch 8/15: [===                           ] 6/60 batches, loss: 0.1320Epoch 8/15: [===                           ] 7/60 batches, loss: 0.1311Epoch 8/15: [====                          ] 8/60 batches, loss: 0.1287Epoch 8/15: [====                          ] 9/60 batches, loss: 0.1314Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.1286Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.1261Epoch 8/15: [======                        ] 12/60 batches, loss: 0.1271Epoch 8/15: [======                        ] 13/60 batches, loss: 0.1239Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.1213Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.1226Epoch 8/15: [========                      ] 16/60 batches, loss: 0.1219Epoch 8/15: [========                      ] 17/60 batches, loss: 0.1231Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.1260Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.1275Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.1271Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.1257Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.1275Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.1314Epoch 8/15: [============                  ] 24/60 batches, loss: 0.1304Epoch 8/15: [============                  ] 25/60 batches, loss: 0.1308Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.1294Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.1304Epoch 8/15: [==============                ] 28/60 batches, loss: 0.1295Epoch 8/15: [==============                ] 29/60 batches, loss: 0.1281Epoch 8/15: [===============               ] 30/60 batches, loss: 0.1262Epoch 8/15: [===============               ] 31/60 batches, loss: 0.1285Epoch 8/15: [================              ] 32/60 batches, loss: 0.1273Epoch 8/15: [================              ] 33/60 batches, loss: 0.1269Epoch 8/15: [=================             ] 34/60 batches, loss: 0.1268Epoch 8/15: [=================             ] 35/60 batches, loss: 0.1273Epoch 8/15: [==================            ] 36/60 batches, loss: 0.1261Epoch 8/15: [==================            ] 37/60 batches, loss: 0.1263Epoch 8/15: [===================           ] 38/60 batches, loss: 0.1280Epoch 8/15: [===================           ] 39/60 batches, loss: 0.1271Epoch 8/15: [====================          ] 40/60 batches, loss: 0.1261Epoch 8/15: [====================          ] 41/60 batches, loss: 0.1272Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.1271Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.1265Epoch 8/15: [======================        ] 44/60 batches, loss: 0.1275Epoch 8/15: [======================        ] 45/60 batches, loss: 0.1276Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.1273Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.1270Epoch 8/15: [========================      ] 48/60 batches, loss: 0.1261Epoch 8/15: [========================      ] 49/60 batches, loss: 0.1260Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.1257Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.1254Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.1249Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.1245Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.1253Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.1254Epoch 8/15: [============================  ] 56/60 batches, loss: 0.1252Epoch 8/15: [============================  ] 57/60 batches, loss: 0.1239Epoch 8/15: [============================= ] 58/60 batches, loss: 0.1242Epoch 8/15: [============================= ] 59/60 batches, loss: 0.1241Epoch 8/15: [==============================] 60/60 batches, loss: 0.1246
[2025-05-07 20:31:14,286][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.1246
[2025-05-07 20:31:14,566][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0679, Metrics: {'mse': 0.07239120453596115, 'rmse': 0.26905613640272386, 'r2': -0.2008037567138672}
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.2447Epoch 9/15: [=                             ] 2/60 batches, loss: 0.1766Epoch 9/15: [=                             ] 3/60 batches, loss: 0.1439Epoch 9/15: [==                            ] 4/60 batches, loss: 0.1475Epoch 9/15: [==                            ] 5/60 batches, loss: 0.1423Epoch 9/15: [===                           ] 6/60 batches, loss: 0.1379Epoch 9/15: [===                           ] 7/60 batches, loss: 0.1452Epoch 9/15: [====                          ] 8/60 batches, loss: 0.1420Epoch 9/15: [====                          ] 9/60 batches, loss: 0.1358Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.1324Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.1308Epoch 9/15: [======                        ] 12/60 batches, loss: 0.1282Epoch 9/15: [======                        ] 13/60 batches, loss: 0.1261Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.1261Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.1242Epoch 9/15: [========                      ] 16/60 batches, loss: 0.1228Epoch 9/15: [========                      ] 17/60 batches, loss: 0.1248Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.1242Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.1273Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.1265Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.1283Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.1279Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.1298Epoch 9/15: [============                  ] 24/60 batches, loss: 0.1278Epoch 9/15: [============                  ] 25/60 batches, loss: 0.1291Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.1296Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.1296Epoch 9/15: [==============                ] 28/60 batches, loss: 0.1276Epoch 9/15: [==============                ] 29/60 batches, loss: 0.1269Epoch 9/15: [===============               ] 30/60 batches, loss: 0.1294Epoch 9/15: [===============               ] 31/60 batches, loss: 0.1286Epoch 9/15: [================              ] 32/60 batches, loss: 0.1302Epoch 9/15: [================              ] 33/60 batches, loss: 0.1310Epoch 9/15: [=================             ] 34/60 batches, loss: 0.1329Epoch 9/15: [=================             ] 35/60 batches, loss: 0.1332Epoch 9/15: [==================            ] 36/60 batches, loss: 0.1338Epoch 9/15: [==================            ] 37/60 batches, loss: 0.1355Epoch 9/15: [===================           ] 38/60 batches, loss: 0.1364Epoch 9/15: [===================           ] 39/60 batches, loss: 0.1379Epoch 9/15: [====================          ] 40/60 batches, loss: 0.1405Epoch 9/15: [====================          ] 41/60 batches, loss: 0.1396Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.1400Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.1386Epoch 9/15: [======================        ] 44/60 batches, loss: 0.1384Epoch 9/15: [======================        ] 45/60 batches, loss: 0.1380Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.1371Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.1374Epoch 9/15: [========================      ] 48/60 batches, loss: 0.1372Epoch 9/15: [========================      ] 49/60 batches, loss: 0.1372Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.1361Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.1367Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.1366Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.1365Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.1353Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.1351Epoch 9/15: [============================  ] 56/60 batches, loss: 0.1365Epoch 9/15: [============================  ] 57/60 batches, loss: 0.1356Epoch 9/15: [============================= ] 58/60 batches, loss: 0.1367Epoch 9/15: [============================= ] 59/60 batches, loss: 0.1375Epoch 9/15: [==============================] 60/60 batches, loss: 0.1370
[2025-05-07 20:31:16,860][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.1370
[2025-05-07 20:31:17,202][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0689, Metrics: {'mse': 0.0734376609325409, 'rmse': 0.270993839288905, 'r2': -0.218161940574646}
[2025-05-07 20:31:17,202][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.1107Epoch 10/15: [=                             ] 2/60 batches, loss: 0.1192Epoch 10/15: [=                             ] 3/60 batches, loss: 0.1139Epoch 10/15: [==                            ] 4/60 batches, loss: 0.1178Epoch 10/15: [==                            ] 5/60 batches, loss: 0.1278Epoch 10/15: [===                           ] 6/60 batches, loss: 0.1266Epoch 10/15: [===                           ] 7/60 batches, loss: 0.1263Epoch 10/15: [====                          ] 8/60 batches, loss: 0.1183Epoch 10/15: [====                          ] 9/60 batches, loss: 0.1201Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.1185Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.1217Epoch 10/15: [======                        ] 12/60 batches, loss: 0.1204Epoch 10/15: [======                        ] 13/60 batches, loss: 0.1208Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.1195Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.1227Epoch 10/15: [========                      ] 16/60 batches, loss: 0.1207Epoch 10/15: [========                      ] 17/60 batches, loss: 0.1211Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.1186Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.1215Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.1222Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.1204Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.1219Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.1215Epoch 10/15: [============                  ] 24/60 batches, loss: 0.1231Epoch 10/15: [============                  ] 25/60 batches, loss: 0.1236Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.1240Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.1261Epoch 10/15: [==============                ] 28/60 batches, loss: 0.1252Epoch 10/15: [==============                ] 29/60 batches, loss: 0.1258Epoch 10/15: [===============               ] 30/60 batches, loss: 0.1254Epoch 10/15: [===============               ] 31/60 batches, loss: 0.1261Epoch 10/15: [================              ] 32/60 batches, loss: 0.1277Epoch 10/15: [================              ] 33/60 batches, loss: 0.1278Epoch 10/15: [=================             ] 34/60 batches, loss: 0.1287Epoch 10/15: [=================             ] 35/60 batches, loss: 0.1289Epoch 10/15: [==================            ] 36/60 batches, loss: 0.1278Epoch 10/15: [==================            ] 37/60 batches, loss: 0.1263Epoch 10/15: [===================           ] 38/60 batches, loss: 0.1248Epoch 10/15: [===================           ] 39/60 batches, loss: 0.1243Epoch 10/15: [====================          ] 40/60 batches, loss: 0.1254Epoch 10/15: [====================          ] 41/60 batches, loss: 0.1259Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.1247Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.1237Epoch 10/15: [======================        ] 44/60 batches, loss: 0.1248Epoch 10/15: [======================        ] 45/60 batches, loss: 0.1242Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.1235Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.1236Epoch 10/15: [========================      ] 48/60 batches, loss: 0.1233Epoch 10/15: [========================      ] 49/60 batches, loss: 0.1228Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.1238Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.1242Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.1248Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.1244Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.1237Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.1244Epoch 10/15: [============================  ] 56/60 batches, loss: 0.1247Epoch 10/15: [============================  ] 57/60 batches, loss: 0.1250Epoch 10/15: [============================= ] 58/60 batches, loss: 0.1241Epoch 10/15: [============================= ] 59/60 batches, loss: 0.1237Epoch 10/15: [==============================] 60/60 batches, loss: 0.1242
[2025-05-07 20:31:19,064][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.1242
[2025-05-07 20:31:19,320][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0689, Metrics: {'mse': 0.07334300875663757, 'rmse': 0.2708191439995289, 'r2': -0.21659183502197266}
[2025-05-07 20:31:19,320][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0827Epoch 11/15: [=                             ] 2/60 batches, loss: 0.1192Epoch 11/15: [=                             ] 3/60 batches, loss: 0.1184Epoch 11/15: [==                            ] 4/60 batches, loss: 0.1290Epoch 11/15: [==                            ] 5/60 batches, loss: 0.1350Epoch 11/15: [===                           ] 6/60 batches, loss: 0.1458Epoch 11/15: [===                           ] 7/60 batches, loss: 0.1458Epoch 11/15: [====                          ] 8/60 batches, loss: 0.1463Epoch 11/15: [====                          ] 9/60 batches, loss: 0.1452Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.1387Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.1400Epoch 11/15: [======                        ] 12/60 batches, loss: 0.1375Epoch 11/15: [======                        ] 13/60 batches, loss: 0.1353Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.1329Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.1359Epoch 11/15: [========                      ] 16/60 batches, loss: 0.1334Epoch 11/15: [========                      ] 17/60 batches, loss: 0.1314Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.1319Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.1311Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.1325Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.1331Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.1306Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.1295Epoch 11/15: [============                  ] 24/60 batches, loss: 0.1285Epoch 11/15: [============                  ] 25/60 batches, loss: 0.1277Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.1291Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.1275Epoch 11/15: [==============                ] 28/60 batches, loss: 0.1266Epoch 11/15: [==============                ] 29/60 batches, loss: 0.1237Epoch 11/15: [===============               ] 30/60 batches, loss: 0.1234Epoch 11/15: [===============               ] 31/60 batches, loss: 0.1235Epoch 11/15: [================              ] 32/60 batches, loss: 0.1241Epoch 11/15: [================              ] 33/60 batches, loss: 0.1235Epoch 11/15: [=================             ] 34/60 batches, loss: 0.1254Epoch 11/15: [=================             ] 35/60 batches, loss: 0.1258Epoch 11/15: [==================            ] 36/60 batches, loss: 0.1256Epoch 11/15: [==================            ] 37/60 batches, loss: 0.1262Epoch 11/15: [===================           ] 38/60 batches, loss: 0.1261Epoch 11/15: [===================           ] 39/60 batches, loss: 0.1258Epoch 11/15: [====================          ] 40/60 batches, loss: 0.1273Epoch 11/15: [====================          ] 41/60 batches, loss: 0.1266Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.1254Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.1252Epoch 11/15: [======================        ] 44/60 batches, loss: 0.1259Epoch 11/15: [======================        ] 45/60 batches, loss: 0.1245Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.1248Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.1243Epoch 11/15: [========================      ] 48/60 batches, loss: 0.1229Epoch 11/15: [========================      ] 49/60 batches, loss: 0.1222Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.1221Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.1221Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.1226Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.1221Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.1220Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.1222Epoch 11/15: [============================  ] 56/60 batches, loss: 0.1227Epoch 11/15: [============================  ] 57/60 batches, loss: 0.1226Epoch 11/15: [============================= ] 58/60 batches, loss: 0.1226Epoch 11/15: [============================= ] 59/60 batches, loss: 0.1219Epoch 11/15: [==============================] 60/60 batches, loss: 0.1221
[2025-05-07 20:31:21,178][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.1221
[2025-05-07 20:31:21,436][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0684, Metrics: {'mse': 0.07264308631420135, 'rmse': 0.2695238140020309, 'r2': -0.20498180389404297}
[2025-05-07 20:31:21,437][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0773Epoch 12/15: [=                             ] 2/60 batches, loss: 0.1169Epoch 12/15: [=                             ] 3/60 batches, loss: 0.1135Epoch 12/15: [==                            ] 4/60 batches, loss: 0.1112Epoch 12/15: [==                            ] 5/60 batches, loss: 0.1202Epoch 12/15: [===                           ] 6/60 batches, loss: 0.1238Epoch 12/15: [===                           ] 7/60 batches, loss: 0.1144Epoch 12/15: [====                          ] 8/60 batches, loss: 0.1138Epoch 12/15: [====                          ] 9/60 batches, loss: 0.1104Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.1069Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.1058Epoch 12/15: [======                        ] 12/60 batches, loss: 0.1089Epoch 12/15: [======                        ] 13/60 batches, loss: 0.1105Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.1112Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.1125Epoch 12/15: [========                      ] 16/60 batches, loss: 0.1110Epoch 12/15: [========                      ] 17/60 batches, loss: 0.1109Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.1133Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.1131Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.1146Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.1152Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.1168Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.1195Epoch 12/15: [============                  ] 24/60 batches, loss: 0.1213Epoch 12/15: [============                  ] 25/60 batches, loss: 0.1212Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.1209Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.1228Epoch 12/15: [==============                ] 28/60 batches, loss: 0.1220Epoch 12/15: [==============                ] 29/60 batches, loss: 0.1219Epoch 12/15: [===============               ] 30/60 batches, loss: 0.1227Epoch 12/15: [===============               ] 31/60 batches, loss: 0.1219Epoch 12/15: [================              ] 32/60 batches, loss: 0.1239Epoch 12/15: [================              ] 33/60 batches, loss: 0.1240Epoch 12/15: [=================             ] 34/60 batches, loss: 0.1229Epoch 12/15: [=================             ] 35/60 batches, loss: 0.1221Epoch 12/15: [==================            ] 36/60 batches, loss: 0.1212Epoch 12/15: [==================            ] 37/60 batches, loss: 0.1207Epoch 12/15: [===================           ] 38/60 batches, loss: 0.1206Epoch 12/15: [===================           ] 39/60 batches, loss: 0.1199Epoch 12/15: [====================          ] 40/60 batches, loss: 0.1192Epoch 12/15: [====================          ] 41/60 batches, loss: 0.1193Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.1201Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.1204Epoch 12/15: [======================        ] 44/60 batches, loss: 0.1197Epoch 12/15: [======================        ] 45/60 batches, loss: 0.1191Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.1199Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.1204Epoch 12/15: [========================      ] 48/60 batches, loss: 0.1203Epoch 12/15: [========================      ] 49/60 batches, loss: 0.1192Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.1195Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.1193Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.1186Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.1184Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.1189Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.1183Epoch 12/15: [============================  ] 56/60 batches, loss: 0.1181Epoch 12/15: [============================  ] 57/60 batches, loss: 0.1180Epoch 12/15: [============================= ] 58/60 batches, loss: 0.1177Epoch 12/15: [============================= ] 59/60 batches, loss: 0.1172Epoch 12/15: [==============================] 60/60 batches, loss: 0.1165
[2025-05-07 20:31:23,300][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.1165
[2025-05-07 20:31:23,573][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0636, Metrics: {'mse': 0.06745211035013199, 'rmse': 0.25971544110840233, 'r2': -0.11887550354003906}
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.1021Epoch 13/15: [=                             ] 2/60 batches, loss: 0.1131Epoch 13/15: [=                             ] 3/60 batches, loss: 0.1063Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0969Epoch 13/15: [==                            ] 5/60 batches, loss: 0.1109Epoch 13/15: [===                           ] 6/60 batches, loss: 0.1041Epoch 13/15: [===                           ] 7/60 batches, loss: 0.1050Epoch 13/15: [====                          ] 8/60 batches, loss: 0.1111Epoch 13/15: [====                          ] 9/60 batches, loss: 0.1170Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.1180Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.1160Epoch 13/15: [======                        ] 12/60 batches, loss: 0.1140Epoch 13/15: [======                        ] 13/60 batches, loss: 0.1164Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.1214Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.1179Epoch 13/15: [========                      ] 16/60 batches, loss: 0.1188Epoch 13/15: [========                      ] 17/60 batches, loss: 0.1178Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.1211Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.1214Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.1217Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.1220Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.1230Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.1240Epoch 13/15: [============                  ] 24/60 batches, loss: 0.1235Epoch 13/15: [============                  ] 25/60 batches, loss: 0.1238Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.1231Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.1224Epoch 13/15: [==============                ] 28/60 batches, loss: 0.1217Epoch 13/15: [==============                ] 29/60 batches, loss: 0.1220Epoch 13/15: [===============               ] 30/60 batches, loss: 0.1216Epoch 13/15: [===============               ] 31/60 batches, loss: 0.1214Epoch 13/15: [================              ] 32/60 batches, loss: 0.1232Epoch 13/15: [================              ] 33/60 batches, loss: 0.1231Epoch 13/15: [=================             ] 34/60 batches, loss: 0.1243Epoch 13/15: [=================             ] 35/60 batches, loss: 0.1262Epoch 13/15: [==================            ] 36/60 batches, loss: 0.1272Epoch 13/15: [==================            ] 37/60 batches, loss: 0.1268Epoch 13/15: [===================           ] 38/60 batches, loss: 0.1262Epoch 13/15: [===================           ] 39/60 batches, loss: 0.1262Epoch 13/15: [====================          ] 40/60 batches, loss: 0.1264Epoch 13/15: [====================          ] 41/60 batches, loss: 0.1272Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.1268Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.1264Epoch 13/15: [======================        ] 44/60 batches, loss: 0.1257Epoch 13/15: [======================        ] 45/60 batches, loss: 0.1247Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.1240Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.1244Epoch 13/15: [========================      ] 48/60 batches, loss: 0.1240Epoch 13/15: [========================      ] 49/60 batches, loss: 0.1241Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.1237Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.1241Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.1238Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.1235Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.1232Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.1232Epoch 13/15: [============================  ] 56/60 batches, loss: 0.1230Epoch 13/15: [============================  ] 57/60 batches, loss: 0.1232Epoch 13/15: [============================= ] 58/60 batches, loss: 0.1225Epoch 13/15: [============================= ] 59/60 batches, loss: 0.1228Epoch 13/15: [==============================] 60/60 batches, loss: 0.1232
[2025-05-07 20:31:25,827][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.1232
[2025-05-07 20:31:26,090][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0681, Metrics: {'mse': 0.07254619151353836, 'rmse': 0.2693440021859376, 'r2': -0.20337450504302979}
[2025-05-07 20:31:26,091][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0927Epoch 14/15: [=                             ] 2/60 batches, loss: 0.1032Epoch 14/15: [=                             ] 3/60 batches, loss: 0.1150Epoch 14/15: [==                            ] 4/60 batches, loss: 0.1181Epoch 14/15: [==                            ] 5/60 batches, loss: 0.1208Epoch 14/15: [===                           ] 6/60 batches, loss: 0.1275Epoch 14/15: [===                           ] 7/60 batches, loss: 0.1359Epoch 14/15: [====                          ] 8/60 batches, loss: 0.1354Epoch 14/15: [====                          ] 9/60 batches, loss: 0.1297Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.1340Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.1311Epoch 14/15: [======                        ] 12/60 batches, loss: 0.1280Epoch 14/15: [======                        ] 13/60 batches, loss: 0.1275Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.1284Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.1292Epoch 14/15: [========                      ] 16/60 batches, loss: 0.1263Epoch 14/15: [========                      ] 17/60 batches, loss: 0.1244Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.1211Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.1222Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.1236Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.1245Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.1234Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.1218Epoch 14/15: [============                  ] 24/60 batches, loss: 0.1196Epoch 14/15: [============                  ] 25/60 batches, loss: 0.1183Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.1183Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.1187Epoch 14/15: [==============                ] 28/60 batches, loss: 0.1184Epoch 14/15: [==============                ] 29/60 batches, loss: 0.1193Epoch 14/15: [===============               ] 30/60 batches, loss: 0.1196Epoch 14/15: [===============               ] 31/60 batches, loss: 0.1194Epoch 14/15: [================              ] 32/60 batches, loss: 0.1213Epoch 14/15: [================              ] 33/60 batches, loss: 0.1200Epoch 14/15: [=================             ] 34/60 batches, loss: 0.1195Epoch 14/15: [=================             ] 35/60 batches, loss: 0.1175Epoch 14/15: [==================            ] 36/60 batches, loss: 0.1176Epoch 14/15: [==================            ] 37/60 batches, loss: 0.1175Epoch 14/15: [===================           ] 38/60 batches, loss: 0.1181Epoch 14/15: [===================           ] 39/60 batches, loss: 0.1180Epoch 14/15: [====================          ] 40/60 batches, loss: 0.1180Epoch 14/15: [====================          ] 41/60 batches, loss: 0.1183Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.1185Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.1190Epoch 14/15: [======================        ] 44/60 batches, loss: 0.1186Epoch 14/15: [======================        ] 45/60 batches, loss: 0.1178Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.1175Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.1167Epoch 14/15: [========================      ] 48/60 batches, loss: 0.1164Epoch 14/15: [========================      ] 49/60 batches, loss: 0.1168Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.1168Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.1158Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.1172Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.1174Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.1177Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.1167Epoch 14/15: [============================  ] 56/60 batches, loss: 0.1158Epoch 14/15: [============================  ] 57/60 batches, loss: 0.1156Epoch 14/15: [============================= ] 58/60 batches, loss: 0.1156Epoch 14/15: [============================= ] 59/60 batches, loss: 0.1158Epoch 14/15: [==============================] 60/60 batches, loss: 0.1161
[2025-05-07 20:31:27,969][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.1161
[2025-05-07 20:31:28,300][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0621, Metrics: {'mse': 0.0662197694182396, 'rmse': 0.2573320217505773, 'r2': -0.0984337329864502}
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.1413Epoch 15/15: [=                             ] 2/60 batches, loss: 0.1294Epoch 15/15: [=                             ] 3/60 batches, loss: 0.1292Epoch 15/15: [==                            ] 4/60 batches, loss: 0.1205Epoch 15/15: [==                            ] 5/60 batches, loss: 0.1092Epoch 15/15: [===                           ] 6/60 batches, loss: 0.1134Epoch 15/15: [===                           ] 7/60 batches, loss: 0.1120Epoch 15/15: [====                          ] 8/60 batches, loss: 0.1117Epoch 15/15: [====                          ] 9/60 batches, loss: 0.1175Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.1256Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.1328Epoch 15/15: [======                        ] 12/60 batches, loss: 0.1377Epoch 15/15: [======                        ] 13/60 batches, loss: 0.1347Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.1294Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.1270Epoch 15/15: [========                      ] 16/60 batches, loss: 0.1264Epoch 15/15: [========                      ] 17/60 batches, loss: 0.1246Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.1241Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.1204Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.1190Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.1199Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.1204Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.1212Epoch 15/15: [============                  ] 24/60 batches, loss: 0.1202Epoch 15/15: [============                  ] 25/60 batches, loss: 0.1205Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.1188Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.1190Epoch 15/15: [==============                ] 28/60 batches, loss: 0.1198Epoch 15/15: [==============                ] 29/60 batches, loss: 0.1191Epoch 15/15: [===============               ] 30/60 batches, loss: 0.1196Epoch 15/15: [===============               ] 31/60 batches, loss: 0.1180Epoch 15/15: [================              ] 32/60 batches, loss: 0.1169Epoch 15/15: [================              ] 33/60 batches, loss: 0.1169Epoch 15/15: [=================             ] 34/60 batches, loss: 0.1164Epoch 15/15: [=================             ] 35/60 batches, loss: 0.1161Epoch 15/15: [==================            ] 36/60 batches, loss: 0.1148Epoch 15/15: [==================            ] 37/60 batches, loss: 0.1145Epoch 15/15: [===================           ] 38/60 batches, loss: 0.1144Epoch 15/15: [===================           ] 39/60 batches, loss: 0.1140Epoch 15/15: [====================          ] 40/60 batches, loss: 0.1146Epoch 15/15: [====================          ] 41/60 batches, loss: 0.1157Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.1150Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.1146Epoch 15/15: [======================        ] 44/60 batches, loss: 0.1150Epoch 15/15: [======================        ] 45/60 batches, loss: 0.1150Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.1139Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.1136Epoch 15/15: [========================      ] 48/60 batches, loss: 0.1135Epoch 15/15: [========================      ] 49/60 batches, loss: 0.1138Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.1131Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.1134Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.1133Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.1136Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.1138Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.1138Epoch 15/15: [============================  ] 56/60 batches, loss: 0.1138Epoch 15/15: [============================  ] 57/60 batches, loss: 0.1134Epoch 15/15: [============================= ] 58/60 batches, loss: 0.1138Epoch 15/15: [============================= ] 59/60 batches, loss: 0.1138Epoch 15/15: [==============================] 60/60 batches, loss: 0.1140
[2025-05-07 20:31:30,751][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.1140
[2025-05-07 20:31:31,077][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0686, Metrics: {'mse': 0.07313039898872375, 'rmse': 0.2704263282092255, 'r2': -0.2130652666091919}
[2025-05-07 20:31:31,078][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-07 20:31:31,078][src.training.lm_trainer][INFO] - Training completed in 36.64 seconds
[2025-05-07 20:31:31,078][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:31:33,640][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.09859465062618256, 'rmse': 0.3139978513082256, 'r2': -0.0007903575897216797}
[2025-05-07 20:31:33,641][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.0662197694182396, 'rmse': 0.2573320217505773, 'r2': -0.0984337329864502}
[2025-05-07 20:31:33,641][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.07473739981651306, 'rmse': 0.2733814181990302, 'r2': -0.17014694213867188}
[2025-05-07 20:31:35,341][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control3/layer2/id/id/model.pt
[2025-05-07 20:31:35,343][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▄▂▂▁▁
wandb:     best_val_mse █▆▅▄▂▂▁▁
wandb:      best_val_r2 ▁▃▄▅▇▇██
wandb:    best_val_rmse █▆▅▄▂▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▄▃▄▅▅▅▅▅▅▆▅▆
wandb:       train_loss █▄▃▂▂▂▂▁▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▅▆▄▂▂▂▂▂▂▁▂▁▂
wandb:          val_mse █▆▅▆▄▂▂▂▂▂▂▁▂▁▂
wandb:           val_r2 ▁▃▄▃▅▇▇▇▇▇▇█▇█▇
wandb:         val_rmse █▆▅▆▄▂▂▂▂▂▂▁▂▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06205
wandb:     best_val_mse 0.06622
wandb:      best_val_r2 -0.09843
wandb:    best_val_rmse 0.25733
wandb:            epoch 15
wandb:   final_test_mse 0.07474
wandb:    final_test_r2 -0.17015
wandb:  final_test_rmse 0.27338
wandb:  final_train_mse 0.09859
wandb:   final_train_r2 -0.00079
wandb: final_train_rmse 0.314
wandb:    final_val_mse 0.06622
wandb:     final_val_r2 -0.09843
wandb:   final_val_rmse 0.25733
wandb:    learning_rate 0.0001
wandb:       train_loss 0.11399
wandb:       train_time 36.63597
wandb:         val_loss 0.06856
wandb:          val_mse 0.07313
wandb:           val_r2 -0.21307
wandb:         val_rmse 0.27043
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_203036-i3mzmfq3
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_203036-i3mzmfq3/logs
Experiment probe_layer2_avg_verb_edges_control3_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control3/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_lexical_density_control1_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_lexical_density_control1_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control1/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=lexical_density"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:32:00,811][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control1/layer2/id
experiment_name: probe_layer2_lexical_density_control1_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: lexical_density
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:32:00,812][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:32:00,812][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 20:32:00,812][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:32:00,812][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:32:00,816][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:32:00,816][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 20:32:00,817][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:32:03,457][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'lexical_density'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:32:05,909][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:32:05,909][src.data.datasets][INFO] - Loading 'control_lexical_density_seed1' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:32:06,145][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_lexical_density_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:47:48 2025).
[2025-05-07 20:32:06,209][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_lexical_density_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:47:48 2025).
[2025-05-07 20:32:06,375][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:32:06,382][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:32:06,382][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:32:06,384][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:32:06,470][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:32:06,534][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:32:06,578][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:32:06,579][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:32:06,579][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:32:06,581][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:32:06,643][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:32:06,733][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:32:06,746][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:32:06,747][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:32:06,747][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:32:06,748][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:32:06,749][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:32:06,750][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 20:32:06,750][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 20:32:06,750][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 20:32:06,750][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:32:06,750][src.data.datasets][INFO] -   Mean: 0.6348, Std: 0.2113
[2025-05-07 20:32:06,750][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:32:06,750][src.data.datasets][INFO] - Sample label: 0.6669999957084656
[2025-05-07 20:32:06,751][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:32:06,751][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 20:32:06,751][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 20:32:06,751][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 20:32:06,751][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:32:06,751][src.data.datasets][INFO] -   Mean: 0.5853, Std: 0.2230
[2025-05-07 20:32:06,751][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:32:06,751][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:32:06,751][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:32:06,751][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 20:32:06,752][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 20:32:06,752][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 20:32:06,752][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:32:06,752][src.data.datasets][INFO] -   Mean: 0.5191, Std: 0.2023
[2025-05-07 20:32:06,752][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:32:06,752][src.data.datasets][INFO] - Sample label: 0.5289999842643738
[2025-05-07 20:32:06,752][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:32:06,752][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:32:06,753][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:32:06,753][__main__][INFO] - Using model type: lm_probe for submetric lexical_density
[2025-05-07 20:32:06,753][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:32:12,778][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:32:12,779][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:32:12,779][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:32:12,779][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:32:12,782][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:32:12,783][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:32:12,783][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:32:12,783][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:32:12,783][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:32:12,784][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:32:12,784][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.4013Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4650Epoch 1/15: [=                             ] 3/60 batches, loss: 0.5062Epoch 1/15: [==                            ] 4/60 batches, loss: 0.5110Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4717Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4458Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4752Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4687Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4884Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4915Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4768Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4700Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4532Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4540Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4572Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4504Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4458Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4581Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4419Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4297Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4203Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4149Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4055Epoch 1/15: [============                  ] 24/60 batches, loss: 0.4009Epoch 1/15: [============                  ] 25/60 batches, loss: 0.4008Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.3955Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3917Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3888Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3902Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3939Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3897Epoch 1/15: [================              ] 32/60 batches, loss: 0.3844Epoch 1/15: [================              ] 33/60 batches, loss: 0.3799Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3777Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3769Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3769Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3703Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3650Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3636Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3615Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3586Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3558Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3558Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3582Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3543Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3494Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3485Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3430Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3418Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3369Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3354Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3336Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3294Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3289Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3264Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3263Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3270Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3251Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3222Epoch 1/15: [==============================] 60/60 batches, loss: 0.3195
[2025-05-07 20:32:18,233][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3195
[2025-05-07 20:32:18,481][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0593, Metrics: {'mse': 0.060254573822021484, 'rmse': 0.245468070880963, 'r2': -0.21203351020812988}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.2763Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2657Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2350Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2273Epoch 2/15: [==                            ] 5/60 batches, loss: 0.2007Epoch 2/15: [===                           ] 6/60 batches, loss: 0.2042Epoch 2/15: [===                           ] 7/60 batches, loss: 0.2109Epoch 2/15: [====                          ] 8/60 batches, loss: 0.2221Epoch 2/15: [====                          ] 9/60 batches, loss: 0.2254Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.2194Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.2121Epoch 2/15: [======                        ] 12/60 batches, loss: 0.2092Epoch 2/15: [======                        ] 13/60 batches, loss: 0.2202Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.2255Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.2200Epoch 2/15: [========                      ] 16/60 batches, loss: 0.2191Epoch 2/15: [========                      ] 17/60 batches, loss: 0.2219Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.2170Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.2123Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.2127Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.2078Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.2111Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.2100Epoch 2/15: [============                  ] 24/60 batches, loss: 0.2070Epoch 2/15: [============                  ] 25/60 batches, loss: 0.2121Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.2105Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.2070Epoch 2/15: [==============                ] 28/60 batches, loss: 0.2068Epoch 2/15: [==============                ] 29/60 batches, loss: 0.2043Epoch 2/15: [===============               ] 30/60 batches, loss: 0.2026Epoch 2/15: [===============               ] 31/60 batches, loss: 0.2014Epoch 2/15: [================              ] 32/60 batches, loss: 0.2003Epoch 2/15: [================              ] 33/60 batches, loss: 0.1975Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1993Epoch 2/15: [=================             ] 35/60 batches, loss: 0.2013Epoch 2/15: [==================            ] 36/60 batches, loss: 0.2018Epoch 2/15: [==================            ] 37/60 batches, loss: 0.2016Epoch 2/15: [===================           ] 38/60 batches, loss: 0.2044Epoch 2/15: [===================           ] 39/60 batches, loss: 0.2033Epoch 2/15: [====================          ] 40/60 batches, loss: 0.2031Epoch 2/15: [====================          ] 41/60 batches, loss: 0.2019Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.2015Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.2011Epoch 2/15: [======================        ] 44/60 batches, loss: 0.2021Epoch 2/15: [======================        ] 45/60 batches, loss: 0.2017Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.2014Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1991Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1991Epoch 2/15: [========================      ] 49/60 batches, loss: 0.2010Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.2017Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1999Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1996Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1995Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1979Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1967Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1953Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1954Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1950Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1951Epoch 2/15: [==============================] 60/60 batches, loss: 0.1932
[2025-05-07 20:32:20,737][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1932
[2025-05-07 20:32:20,993][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0650, Metrics: {'mse': 0.06556959450244904, 'rmse': 0.25606560585609506, 'r2': -0.3189462423324585}
[2025-05-07 20:32:20,994][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.0904Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1488Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1421Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1569Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1576Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1437Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1464Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1474Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1424Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1405Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1368Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1418Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1469Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1464Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1465Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1439Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1452Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1443Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1463Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1467Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1456Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1416Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1391Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1373Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1369Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1363Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1353Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1322Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1350Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1320Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1326Epoch 3/15: [================              ] 32/60 batches, loss: 0.1331Epoch 3/15: [================              ] 33/60 batches, loss: 0.1347Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1337Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1320Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1343Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1352Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1355Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1377Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1383Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1374Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1365Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1375Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1366Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1357Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1344Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1364Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1363Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1372Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1383Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1371Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1376Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1369Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1401Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1393Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1387Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1387Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1382Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1378Epoch 3/15: [==============================] 60/60 batches, loss: 0.1379
[2025-05-07 20:32:22,864][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1379
[2025-05-07 20:32:23,146][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0643, Metrics: {'mse': 0.06476476043462753, 'rmse': 0.25448921477073944, 'r2': -0.30275678634643555}
[2025-05-07 20:32:23,146][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0811Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0875Epoch 4/15: [=                             ] 3/60 batches, loss: 0.0835Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1177Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1185Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1154Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1167Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1141Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1054Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.1003Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1049Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1084Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1092Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.1073Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.1042Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1069Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1069Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1047Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1067Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1106Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1133Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1137Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1131Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1156Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1144Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1143Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1137Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1141Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1133Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1140Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1123Epoch 4/15: [================              ] 32/60 batches, loss: 0.1127Epoch 4/15: [================              ] 33/60 batches, loss: 0.1121Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1124Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1114Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1104Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1095Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1084Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1076Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1072Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1084Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1091Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1088Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1080Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1080Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1077Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.1068Epoch 4/15: [========================      ] 48/60 batches, loss: 0.1061Epoch 4/15: [========================      ] 49/60 batches, loss: 0.1052Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.1045Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.1054Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1072Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.1068Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1078Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1077Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1074Epoch 4/15: [============================  ] 57/60 batches, loss: 0.1067Epoch 4/15: [============================= ] 58/60 batches, loss: 0.1060Epoch 4/15: [============================= ] 59/60 batches, loss: 0.1057Epoch 4/15: [==============================] 60/60 batches, loss: 0.1053
[2025-05-07 20:32:25,035][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1053
[2025-05-07 20:32:25,293][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0556, Metrics: {'mse': 0.05677308514714241, 'rmse': 0.23827103295856678, 'r2': -0.14200258255004883}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.1141Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0923Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0895Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0970Epoch 5/15: [==                            ] 5/60 batches, loss: 0.1103Epoch 5/15: [===                           ] 6/60 batches, loss: 0.1108Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1110Epoch 5/15: [====                          ] 8/60 batches, loss: 0.1084Epoch 5/15: [====                          ] 9/60 batches, loss: 0.1037Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.1052Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1107Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1126Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1119Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1140Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1132Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1131Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1116Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1114Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1108Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1158Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1145Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1136Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1149Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1132Epoch 5/15: [============                  ] 25/60 batches, loss: 0.1109Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1096Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.1094Epoch 5/15: [==============                ] 28/60 batches, loss: 0.1093Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1095Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1090Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1095Epoch 5/15: [================              ] 32/60 batches, loss: 0.1076Epoch 5/15: [================              ] 33/60 batches, loss: 0.1051Epoch 5/15: [=================             ] 34/60 batches, loss: 0.1050Epoch 5/15: [=================             ] 35/60 batches, loss: 0.1058Epoch 5/15: [==================            ] 36/60 batches, loss: 0.1046Epoch 5/15: [==================            ] 37/60 batches, loss: 0.1040Epoch 5/15: [===================           ] 38/60 batches, loss: 0.1046Epoch 5/15: [===================           ] 39/60 batches, loss: 0.1055Epoch 5/15: [====================          ] 40/60 batches, loss: 0.1049Epoch 5/15: [====================          ] 41/60 batches, loss: 0.1039Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.1042Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.1037Epoch 5/15: [======================        ] 44/60 batches, loss: 0.1024Epoch 5/15: [======================        ] 45/60 batches, loss: 0.1024Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.1021Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.1015Epoch 5/15: [========================      ] 48/60 batches, loss: 0.1015Epoch 5/15: [========================      ] 49/60 batches, loss: 0.1011Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.1009Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0997Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0992Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0987Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0980Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0979Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0979Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0983Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0989Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0984Epoch 5/15: [==============================] 60/60 batches, loss: 0.0974
[2025-05-07 20:32:27,582][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0974
[2025-05-07 20:32:27,845][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0507, Metrics: {'mse': 0.052467282861471176, 'rmse': 0.22905737897188813, 'r2': -0.05539047718048096}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.0966Epoch 6/15: [=                             ] 2/60 batches, loss: 0.0816Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0752Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0782Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0752Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0891Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0895Epoch 6/15: [====                          ] 8/60 batches, loss: 0.1032Epoch 6/15: [====                          ] 9/60 batches, loss: 0.1042Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.1005Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.1026Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0980Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0982Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.1006Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0990Epoch 6/15: [========                      ] 16/60 batches, loss: 0.1035Epoch 6/15: [========                      ] 17/60 batches, loss: 0.1043Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.1069Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.1062Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.1027Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.1030Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.1029Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.1022Epoch 6/15: [============                  ] 24/60 batches, loss: 0.1031Epoch 6/15: [============                  ] 25/60 batches, loss: 0.1054Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.1035Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.1057Epoch 6/15: [==============                ] 28/60 batches, loss: 0.1035Epoch 6/15: [==============                ] 29/60 batches, loss: 0.1047Epoch 6/15: [===============               ] 30/60 batches, loss: 0.1043Epoch 6/15: [===============               ] 31/60 batches, loss: 0.1022Epoch 6/15: [================              ] 32/60 batches, loss: 0.1009Epoch 6/15: [================              ] 33/60 batches, loss: 0.1014Epoch 6/15: [=================             ] 34/60 batches, loss: 0.1005Epoch 6/15: [=================             ] 35/60 batches, loss: 0.1009Epoch 6/15: [==================            ] 36/60 batches, loss: 0.1014Epoch 6/15: [==================            ] 37/60 batches, loss: 0.1006Epoch 6/15: [===================           ] 38/60 batches, loss: 0.1018Epoch 6/15: [===================           ] 39/60 batches, loss: 0.1010Epoch 6/15: [====================          ] 40/60 batches, loss: 0.1003Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0985Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0982Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0974Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0970Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0972Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0965Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0970Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0960Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0950Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0942Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0952Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0950Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0944Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0943Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0953Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0941Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0935Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0926Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0924Epoch 6/15: [==============================] 60/60 batches, loss: 0.0921
[2025-05-07 20:32:30,046][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0921
[2025-05-07 20:32:30,312][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0480, Metrics: {'mse': 0.04977118968963623, 'rmse': 0.2230945756616154, 'r2': -0.001157999038696289}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0645Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0816Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0839Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0727Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0736Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0678Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0685Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0682Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0662Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0685Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0691Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0703Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0705Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0714Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0738Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0748Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0730Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0765Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0787Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0829Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0855Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0872Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0884Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0903Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0895Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0885Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0890Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0884Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0877Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0894Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0881Epoch 7/15: [================              ] 32/60 batches, loss: 0.0871Epoch 7/15: [================              ] 33/60 batches, loss: 0.0870Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0863Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0870Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0855Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0851Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0853Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0845Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0851Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0845Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0858Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0850Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0845Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0853Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0853Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0854Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0845Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0848Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0843Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0840Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0841Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0837Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0834Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0839Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0842Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0842Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0843Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0841Epoch 7/15: [==============================] 60/60 batches, loss: 0.0843
[2025-05-07 20:32:32,530][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0843
[2025-05-07 20:32:32,784][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0467, Metrics: {'mse': 0.04852083697915077, 'rmse': 0.22027445829952863, 'r2': 0.02399313449859619}
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0361Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0685Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0599Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0581Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0654Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0726Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0692Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0710Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0696Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0745Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0733Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0723Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0694Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0732Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0732Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0780Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0781Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0781Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0780Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0792Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0768Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0779Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0780Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0790Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0781Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0767Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0787Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0775Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0783Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0773Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0757Epoch 8/15: [================              ] 32/60 batches, loss: 0.0770Epoch 8/15: [================              ] 33/60 batches, loss: 0.0766Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0758Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0759Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0757Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0760Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0756Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0748Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0753Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0761Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0776Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0772Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0785Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0791Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0789Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0790Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0787Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0787Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0783Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0783Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0791Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0793Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0788Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0794Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0794Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0789Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0809Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0809Epoch 8/15: [==============================] 60/60 batches, loss: 0.0803
[2025-05-07 20:32:35,080][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0803
[2025-05-07 20:32:35,346][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0469, Metrics: {'mse': 0.048834577202796936, 'rmse': 0.22098546830684804, 'r2': 0.017682194709777832}
[2025-05-07 20:32:35,346][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.0992Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0837Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0728Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0669Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0680Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0726Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0685Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0664Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0691Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0736Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0754Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0777Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0753Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0747Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0773Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0771Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0778Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0773Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0777Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0781Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0786Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0797Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0800Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0794Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0784Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0780Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0777Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0803Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0805Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0802Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0813Epoch 9/15: [================              ] 32/60 batches, loss: 0.0801Epoch 9/15: [================              ] 33/60 batches, loss: 0.0793Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0787Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0785Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0786Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0783Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0783Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0778Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0777Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0777Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0784Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0777Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0778Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0774Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0765Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0762Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0755Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0763Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0761Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0760Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0763Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0757Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0751Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0747Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0743Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0737Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0735Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0735Epoch 9/15: [==============================] 60/60 batches, loss: 0.0731
[2025-05-07 20:32:37,255][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0731
[2025-05-07 20:32:37,522][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0460, Metrics: {'mse': 0.04796086251735687, 'rmse': 0.21899968611246198, 'r2': 0.035257160663604736}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0689Epoch 10/15: [=                             ] 2/60 batches, loss: 0.1028Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0944Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0832Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0849Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0781Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0716Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0677Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0692Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0707Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0687Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0712Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0731Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0733Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0717Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0714Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0707Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0710Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0712Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0731Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0730Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0724Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0725Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0733Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0739Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0731Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0738Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0734Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0733Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0734Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0745Epoch 10/15: [================              ] 32/60 batches, loss: 0.0752Epoch 10/15: [================              ] 33/60 batches, loss: 0.0756Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0759Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0750Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0747Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0744Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0734Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0731Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0724Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0720Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0715Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0718Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0712Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0708Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0707Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0702Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0701Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0694Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0694Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0697Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0692Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0692Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0695Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0699Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0696Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0699Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0692Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0692Epoch 10/15: [==============================] 60/60 batches, loss: 0.0688
[2025-05-07 20:32:39,760][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0688
[2025-05-07 20:32:40,042][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0459, Metrics: {'mse': 0.047928400337696075, 'rmse': 0.21892555889547496, 'r2': 0.03591012954711914}
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0355Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0805Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0813Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0806Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0792Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0769Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0734Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0747Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0719Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0713Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0691Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0713Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0716Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0687Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0696Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0687Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0665Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0676Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0695Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0687Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0694Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0712Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0711Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0709Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0723Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0717Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0705Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0721Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0709Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0711Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0705Epoch 11/15: [================              ] 32/60 batches, loss: 0.0705Epoch 11/15: [================              ] 33/60 batches, loss: 0.0697Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0697Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0688Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0680Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0683Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0680Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0677Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0675Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0678Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0675Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0671Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0669Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0675Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0678Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0679Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0673Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0667Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0664Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0673Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0671Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0673Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0671Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0666Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0669Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0667Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0661Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0666Epoch 11/15: [==============================] 60/60 batches, loss: 0.0666
[2025-05-07 20:32:42,288][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0666
[2025-05-07 20:32:42,564][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0533, Metrics: {'mse': 0.054437145590782166, 'rmse': 0.23331769240840303, 'r2': -0.09501469135284424}
[2025-05-07 20:32:42,565][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0685Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0965Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0763Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0769Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0739Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0734Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0672Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0645Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0659Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0624Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0634Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0634Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0634Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0601Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0603Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0603Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0579Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0576Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0599Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0613Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0599Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0584Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0599Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0607Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0617Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0626Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0616Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0614Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0636Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0640Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0635Epoch 12/15: [================              ] 32/60 batches, loss: 0.0651Epoch 12/15: [================              ] 33/60 batches, loss: 0.0653Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0654Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0654Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0645Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0640Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0651Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0651Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0648Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0652Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0647Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0652Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0657Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0649Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0663Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0664Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0664Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0667Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0667Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0671Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0672Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0666Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0664Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0663Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0656Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0652Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0650Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0643Epoch 12/15: [==============================] 60/60 batches, loss: 0.0644
[2025-05-07 20:32:44,429][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0644
[2025-05-07 20:32:44,711][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0469, Metrics: {'mse': 0.04855446517467499, 'rmse': 0.22035077756766594, 'r2': 0.023316681385040283}
[2025-05-07 20:32:44,712][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0441Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0601Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0623Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0586Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0567Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0548Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0546Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0535Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0567Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0565Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0558Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0578Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0571Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0602Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0600Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0600Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0580Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0564Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0542Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0541Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0544Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0540Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0549Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0545Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0557Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0548Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0553Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0548Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0553Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0573Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0588Epoch 13/15: [================              ] 32/60 batches, loss: 0.0591Epoch 13/15: [================              ] 33/60 batches, loss: 0.0588Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0584Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0588Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0590Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0587Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0584Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0584Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0582Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0575Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0575Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0584Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0587Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0589Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0592Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0588Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0581Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0581Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0588Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0584Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0582Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0595Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0598Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0597Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0607Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0619Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0619Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0619Epoch 13/15: [==============================] 60/60 batches, loss: 0.0621
[2025-05-07 20:32:46,615][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0621
[2025-05-07 20:32:46,882][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0499, Metrics: {'mse': 0.05128483474254608, 'rmse': 0.2264615524598957, 'r2': -0.03160536289215088}
[2025-05-07 20:32:46,883][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0377Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0646Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0639Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0638Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0684Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0670Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0699Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0679Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0688Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0694Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0658Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0632Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0643Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0633Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0616Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0625Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0633Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0648Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0659Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0670Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0666Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0650Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0642Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0647Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0631Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0628Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0625Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0618Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0623Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0629Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0621Epoch 14/15: [================              ] 32/60 batches, loss: 0.0617Epoch 14/15: [================              ] 33/60 batches, loss: 0.0624Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0625Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0619Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0613Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0611Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0618Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0621Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0616Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0613Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0614Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0608Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0614Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0606Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0609Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0610Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0608Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0613Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0609Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0608Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0609Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0608Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0607Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0607Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0607Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0602Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0596Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0594Epoch 14/15: [==============================] 60/60 batches, loss: 0.0593
[2025-05-07 20:32:48,756][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0593
[2025-05-07 20:32:49,040][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0533, Metrics: {'mse': 0.05444902181625366, 'rmse': 0.23334314178105528, 'r2': -0.095253586769104}
[2025-05-07 20:32:49,040][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 20:32:49,040][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 14
[2025-05-07 20:32:49,040][src.training.lm_trainer][INFO] - Training completed in 33.56 seconds
[2025-05-07 20:32:49,041][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:32:51,627][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.04604894295334816, 'rmse': 0.21459017441007908, 'r2': -0.031221985816955566}
[2025-05-07 20:32:51,627][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.047928400337696075, 'rmse': 0.21892555889547496, 'r2': 0.03591012954711914}
[2025-05-07 20:32:51,627][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04344950243830681, 'rmse': 0.20844544235436477, 'r2': -0.06183326244354248}
[2025-05-07 20:32:53,303][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control1/layer2/id/id/model.pt
[2025-05-07 20:32:53,304][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▃▂▁▁▁
wandb:     best_val_mse █▆▄▂▁▁▁
wandb:      best_val_r2 ▁▃▅▇███
wandb:    best_val_rmse █▆▄▂▁▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▁▃▄▄▄▄▅▅▃▄▄
wandb:       train_loss █▅▃▂▂▂▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆██▅▃▂▁▁▁▁▄▁▂▄
wandb:          val_mse ▆██▅▃▂▁▁▁▁▄▁▂▄
wandb:           val_r2 ▃▁▁▄▆▇████▅█▇▅
wandb:         val_rmse ▆██▅▃▂▁▁▁▁▄▁▂▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04595
wandb:     best_val_mse 0.04793
wandb:      best_val_r2 0.03591
wandb:    best_val_rmse 0.21893
wandb: early_stop_epoch 14
wandb:            epoch 14
wandb:   final_test_mse 0.04345
wandb:    final_test_r2 -0.06183
wandb:  final_test_rmse 0.20845
wandb:  final_train_mse 0.04605
wandb:   final_train_r2 -0.03122
wandb: final_train_rmse 0.21459
wandb:    final_val_mse 0.04793
wandb:     final_val_r2 0.03591
wandb:   final_val_rmse 0.21893
wandb:    learning_rate 0.0001
wandb:       train_loss 0.05932
wandb:       train_time 33.56095
wandb:         val_loss 0.05331
wandb:          val_mse 0.05445
wandb:           val_r2 -0.09525
wandb:         val_rmse 0.23334
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_203200-nq5qiigo
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_203200-nq5qiigo/logs
Experiment probe_layer2_lexical_density_control1_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control1/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_lexical_density_control2_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_lexical_density_control2_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control2/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=lexical_density"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:33:16,206][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control2/layer2/id
experiment_name: probe_layer2_lexical_density_control2_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: lexical_density
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:33:16,206][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:33:16,206][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 20:33:16,207][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:33:16,207][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:33:16,211][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:33:16,211][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 20:33:16,211][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:33:18,694][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'lexical_density'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:33:21,000][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:33:21,000][src.data.datasets][INFO] - Loading 'control_lexical_density_seed2' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:33:21,117][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_lexical_density_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:48:37 2025).
[2025-05-07 20:33:21,189][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_lexical_density_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:48:37 2025).
[2025-05-07 20:33:21,514][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:33:21,521][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:33:21,521][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:33:21,522][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:33:21,558][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:33:21,612][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:33:21,641][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:33:21,642][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:33:21,643][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:33:21,643][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:33:21,684][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:33:21,727][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:33:21,759][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:33:21,761][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:33:21,761][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:33:21,762][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:33:21,763][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:33:21,763][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 20:33:21,763][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 20:33:21,763][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 20:33:21,763][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:33:21,763][src.data.datasets][INFO] -   Mean: 0.6348, Std: 0.2113
[2025-05-07 20:33:21,763][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:33:21,764][src.data.datasets][INFO] - Sample label: 0.7329999804496765
[2025-05-07 20:33:21,764][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:33:21,764][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 20:33:21,764][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 20:33:21,764][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 20:33:21,764][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:33:21,764][src.data.datasets][INFO] -   Mean: 0.5853, Std: 0.2230
[2025-05-07 20:33:21,764][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:33:21,764][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:33:21,765][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:33:21,765][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 20:33:21,765][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 20:33:21,765][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 20:33:21,765][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:33:21,765][src.data.datasets][INFO] -   Mean: 0.5191, Std: 0.2023
[2025-05-07 20:33:21,765][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:33:21,765][src.data.datasets][INFO] - Sample label: 0.5289999842643738
[2025-05-07 20:33:21,765][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:33:21,765][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:33:21,766][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:33:21,766][__main__][INFO] - Using model type: lm_probe for submetric lexical_density
[2025-05-07 20:33:21,766][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:33:27,838][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:33:27,839][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:33:27,839][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:33:27,839][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:33:27,842][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:33:27,842][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:33:27,842][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:33:27,843][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:33:27,843][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:33:27,844][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:33:27,844][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.5507Epoch 1/15: [=                             ] 2/60 batches, loss: 0.5608Epoch 1/15: [=                             ] 3/60 batches, loss: 0.5555Epoch 1/15: [==                            ] 4/60 batches, loss: 0.5446Epoch 1/15: [==                            ] 5/60 batches, loss: 0.5077Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4824Epoch 1/15: [===                           ] 7/60 batches, loss: 0.5200Epoch 1/15: [====                          ] 8/60 batches, loss: 0.5091Epoch 1/15: [====                          ] 9/60 batches, loss: 0.5412Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.5403Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.5212Epoch 1/15: [======                        ] 12/60 batches, loss: 0.5232Epoch 1/15: [======                        ] 13/60 batches, loss: 0.5052Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.5023Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4961Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4855Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4780Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4832Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4671Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4564Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4465Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4442Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4328Epoch 1/15: [============                  ] 24/60 batches, loss: 0.4254Epoch 1/15: [============                  ] 25/60 batches, loss: 0.4202Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.4165Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.4066Epoch 1/15: [==============                ] 28/60 batches, loss: 0.4009Epoch 1/15: [==============                ] 29/60 batches, loss: 0.4001Epoch 1/15: [===============               ] 30/60 batches, loss: 0.4038Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3967Epoch 1/15: [================              ] 32/60 batches, loss: 0.3923Epoch 1/15: [================              ] 33/60 batches, loss: 0.3863Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3837Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3803Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3798Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3738Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3695Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3652Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3618Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3595Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3591Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3563Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3584Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3559Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3526Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3513Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3469Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3444Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3411Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3416Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3402Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3384Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3386Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3361Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3347Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3342Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3319Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3298Epoch 1/15: [==============================] 60/60 batches, loss: 0.3272
[2025-05-07 20:33:33,208][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3272
[2025-05-07 20:33:33,470][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0546, Metrics: {'mse': 0.05618871748447418, 'rmse': 0.2370415944185201, 'r2': -0.1302478313446045}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.3703Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2691Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2498Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2300Epoch 2/15: [==                            ] 5/60 batches, loss: 0.2059Epoch 2/15: [===                           ] 6/60 batches, loss: 0.2060Epoch 2/15: [===                           ] 7/60 batches, loss: 0.2025Epoch 2/15: [====                          ] 8/60 batches, loss: 0.2059Epoch 2/15: [====                          ] 9/60 batches, loss: 0.2060Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.1949Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.2047Epoch 2/15: [======                        ] 12/60 batches, loss: 0.2014Epoch 2/15: [======                        ] 13/60 batches, loss: 0.2070Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.2186Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.2134Epoch 2/15: [========                      ] 16/60 batches, loss: 0.2133Epoch 2/15: [========                      ] 17/60 batches, loss: 0.2201Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.2205Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.2173Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.2174Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.2158Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.2137Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.2103Epoch 2/15: [============                  ] 24/60 batches, loss: 0.2072Epoch 2/15: [============                  ] 25/60 batches, loss: 0.2066Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.2070Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.2043Epoch 2/15: [==============                ] 28/60 batches, loss: 0.2033Epoch 2/15: [==============                ] 29/60 batches, loss: 0.2017Epoch 2/15: [===============               ] 30/60 batches, loss: 0.2009Epoch 2/15: [===============               ] 31/60 batches, loss: 0.2003Epoch 2/15: [================              ] 32/60 batches, loss: 0.1967Epoch 2/15: [================              ] 33/60 batches, loss: 0.1936Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1962Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1967Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1965Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1974Epoch 2/15: [===================           ] 38/60 batches, loss: 0.2002Epoch 2/15: [===================           ] 39/60 batches, loss: 0.2019Epoch 2/15: [====================          ] 40/60 batches, loss: 0.2026Epoch 2/15: [====================          ] 41/60 batches, loss: 0.2038Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.2022Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.2013Epoch 2/15: [======================        ] 44/60 batches, loss: 0.2008Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1994Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1985Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1963Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1973Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1976Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1970Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1960Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1958Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1965Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1948Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1934Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1915Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1912Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1899Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1896Epoch 2/15: [==============================] 60/60 batches, loss: 0.1884
[2025-05-07 20:33:35,724][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1884
[2025-05-07 20:33:36,019][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0534, Metrics: {'mse': 0.0547570139169693, 'rmse': 0.2340021664792215, 'r2': -0.1014488935470581}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.0943Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1786Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1747Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1827Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1689Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1665Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1690Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1728Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1653Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1576Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1542Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1526Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1576Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1595Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1607Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1593Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1619Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1571Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1526Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1547Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1527Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1495Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1480Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1496Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1481Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1481Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1448Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1427Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1460Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1445Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1444Epoch 3/15: [================              ] 32/60 batches, loss: 0.1428Epoch 3/15: [================              ] 33/60 batches, loss: 0.1438Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1425Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1409Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1446Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1426Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1407Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1399Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1394Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1380Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1385Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1379Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1384Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1367Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1356Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1353Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1354Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1366Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1359Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1371Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1373Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1366Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1382Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1373Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1370Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1371Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1363Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1355Epoch 3/15: [==============================] 60/60 batches, loss: 0.1356
[2025-05-07 20:33:38,289][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1356
[2025-05-07 20:33:38,564][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0613, Metrics: {'mse': 0.06203269958496094, 'rmse': 0.24906364565098804, 'r2': -0.2478008270263672}
[2025-05-07 20:33:38,565][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0762Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0833Epoch 4/15: [=                             ] 3/60 batches, loss: 0.1009Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1393Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1315Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1307Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1425Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1338Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1288Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.1209Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1186Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1156Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1161Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.1144Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.1128Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1128Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1159Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1164Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1187Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1180Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1168Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1153Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1169Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1194Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1200Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1202Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1210Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1212Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1219Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1223Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1207Epoch 4/15: [================              ] 32/60 batches, loss: 0.1193Epoch 4/15: [================              ] 33/60 batches, loss: 0.1197Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1184Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1168Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1156Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1160Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1158Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1163Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1153Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1144Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1142Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1163Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1150Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1143Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1129Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.1126Epoch 4/15: [========================      ] 48/60 batches, loss: 0.1122Epoch 4/15: [========================      ] 49/60 batches, loss: 0.1122Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.1119Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.1119Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1127Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.1136Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1133Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1128Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1120Epoch 4/15: [============================  ] 57/60 batches, loss: 0.1108Epoch 4/15: [============================= ] 58/60 batches, loss: 0.1100Epoch 4/15: [============================= ] 59/60 batches, loss: 0.1097Epoch 4/15: [==============================] 60/60 batches, loss: 0.1085
[2025-05-07 20:33:40,411][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1085
[2025-05-07 20:33:40,680][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0592, Metrics: {'mse': 0.06038669869303703, 'rmse': 0.24573705193364112, 'r2': -0.214691162109375}
[2025-05-07 20:33:40,681][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.1174Epoch 5/15: [=                             ] 2/60 batches, loss: 0.1130Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0980Epoch 5/15: [==                            ] 4/60 batches, loss: 0.1048Epoch 5/15: [==                            ] 5/60 batches, loss: 0.1080Epoch 5/15: [===                           ] 6/60 batches, loss: 0.1062Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1096Epoch 5/15: [====                          ] 8/60 batches, loss: 0.1057Epoch 5/15: [====                          ] 9/60 batches, loss: 0.1059Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.1138Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1088Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1112Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1141Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1163Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1127Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1123Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1123Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1129Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1103Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1101Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1090Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1087Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1114Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1121Epoch 5/15: [============                  ] 25/60 batches, loss: 0.1107Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1090Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.1092Epoch 5/15: [==============                ] 28/60 batches, loss: 0.1078Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1099Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1091Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1095Epoch 5/15: [================              ] 32/60 batches, loss: 0.1091Epoch 5/15: [================              ] 33/60 batches, loss: 0.1076Epoch 5/15: [=================             ] 34/60 batches, loss: 0.1072Epoch 5/15: [=================             ] 35/60 batches, loss: 0.1080Epoch 5/15: [==================            ] 36/60 batches, loss: 0.1074Epoch 5/15: [==================            ] 37/60 batches, loss: 0.1068Epoch 5/15: [===================           ] 38/60 batches, loss: 0.1061Epoch 5/15: [===================           ] 39/60 batches, loss: 0.1054Epoch 5/15: [====================          ] 40/60 batches, loss: 0.1044Epoch 5/15: [====================          ] 41/60 batches, loss: 0.1042Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.1044Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.1040Epoch 5/15: [======================        ] 44/60 batches, loss: 0.1030Epoch 5/15: [======================        ] 45/60 batches, loss: 0.1030Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.1035Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.1027Epoch 5/15: [========================      ] 48/60 batches, loss: 0.1023Epoch 5/15: [========================      ] 49/60 batches, loss: 0.1018Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.1011Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.1010Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.1005Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0997Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0998Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0995Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0989Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0995Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0998Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0988Epoch 5/15: [==============================] 60/60 batches, loss: 0.0997
[2025-05-07 20:33:42,545][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0997
[2025-05-07 20:33:42,881][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0507, Metrics: {'mse': 0.05246753245592117, 'rmse': 0.22905792380077397, 'r2': -0.05539548397064209}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1449Epoch 6/15: [=                             ] 2/60 batches, loss: 0.0990Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0991Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0805Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0842Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0845Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0901Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0969Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0985Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.1020Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.1049Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0999Epoch 6/15: [======                        ] 13/60 batches, loss: 0.1039Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.1064Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.1059Epoch 6/15: [========                      ] 16/60 batches, loss: 0.1068Epoch 6/15: [========                      ] 17/60 batches, loss: 0.1035Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.1024Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.1024Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0999Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0985Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0975Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.1014Epoch 6/15: [============                  ] 24/60 batches, loss: 0.1011Epoch 6/15: [============                  ] 25/60 batches, loss: 0.1005Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.1000Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.1004Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0984Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0988Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0984Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0970Epoch 6/15: [================              ] 32/60 batches, loss: 0.0964Epoch 6/15: [================              ] 33/60 batches, loss: 0.0971Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0978Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0988Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0975Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0973Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0979Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0970Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0971Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0968Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0969Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0961Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0954Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0948Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0942Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0948Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0942Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0939Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0937Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0936Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0927Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0921Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0919Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0925Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0919Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0918Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0913Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0913Epoch 6/15: [==============================] 60/60 batches, loss: 0.0915
[2025-05-07 20:33:45,072][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0915
[2025-05-07 20:33:45,330][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0513, Metrics: {'mse': 0.052869170904159546, 'rmse': 0.22993297045913086, 'r2': -0.06347453594207764}
[2025-05-07 20:33:45,331][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0804Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0973Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0898Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0996Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0907Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0977Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0941Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0895Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0899Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0931Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0914Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0923Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0895Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0887Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0906Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0911Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0917Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0957Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0933Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0935Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0902Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0893Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0905Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0924Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0899Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0900Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0914Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0907Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0906Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0897Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0896Epoch 7/15: [================              ] 32/60 batches, loss: 0.0888Epoch 7/15: [================              ] 33/60 batches, loss: 0.0887Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0890Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0890Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0885Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0881Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0892Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0883Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0875Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0866Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0871Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0864Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0871Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0861Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0864Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0863Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0856Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0862Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0857Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0856Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0858Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0854Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0854Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0851Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0849Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0842Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0832Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0832Epoch 7/15: [==============================] 60/60 batches, loss: 0.0845
[2025-05-07 20:33:47,197][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0845
[2025-05-07 20:33:47,481][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0507, Metrics: {'mse': 0.05216521769762039, 'rmse': 0.228397061490774, 'r2': -0.04931437969207764}
[2025-05-07 20:33:47,482][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0770Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0793Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0750Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0637Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0660Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0668Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0692Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0694Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0656Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0658Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0665Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0656Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0634Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0650Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0626Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0650Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0662Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0664Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0668Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0681Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0673Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0672Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0685Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0694Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0679Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0674Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0676Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0662Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0672Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0698Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0698Epoch 8/15: [================              ] 32/60 batches, loss: 0.0697Epoch 8/15: [================              ] 33/60 batches, loss: 0.0698Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0706Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0720Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0715Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0713Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0718Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0709Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0707Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0714Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0712Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0708Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0717Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0712Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0726Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0729Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0744Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0751Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0744Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0750Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0752Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0753Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0751Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0750Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0750Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0752Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0755Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0752Epoch 8/15: [==============================] 60/60 batches, loss: 0.0752
[2025-05-07 20:33:49,343][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0752
[2025-05-07 20:33:49,591][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0509, Metrics: {'mse': 0.05249802768230438, 'rmse': 0.22912448075730446, 'r2': -0.05600893497467041}
[2025-05-07 20:33:49,592][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.1212Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0771Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0652Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0641Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0661Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0630Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0612Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0605Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0630Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0643Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0629Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0634Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0648Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0655Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0648Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0659Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0682Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0677Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0690Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0691Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0729Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0723Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0720Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0733Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0734Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0735Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0740Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0748Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0748Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0757Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0751Epoch 9/15: [================              ] 32/60 batches, loss: 0.0743Epoch 9/15: [================              ] 33/60 batches, loss: 0.0738Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0732Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0734Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0732Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0733Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0733Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0757Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0769Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0779Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0774Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0765Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0765Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0765Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0761Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0762Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0751Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0755Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0748Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0751Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0766Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0765Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0757Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0750Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0748Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0743Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0749Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0750Epoch 9/15: [==============================] 60/60 batches, loss: 0.0755
[2025-05-07 20:33:51,456][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0755
[2025-05-07 20:33:51,740][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0484, Metrics: {'mse': 0.050079409033060074, 'rmse': 0.2237842913009313, 'r2': -0.00735783576965332}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0564Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0492Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0500Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0532Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0634Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0620Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0629Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0663Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0680Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0674Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0650Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0668Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0668Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0646Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0650Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0662Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0665Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0651Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0653Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0669Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0662Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0673Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0667Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0672Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0674Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0681Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0691Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0689Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0690Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0682Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0697Epoch 10/15: [================              ] 32/60 batches, loss: 0.0705Epoch 10/15: [================              ] 33/60 batches, loss: 0.0702Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0701Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0704Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0707Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0701Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0699Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0708Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0711Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0704Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0708Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0699Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0697Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0701Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0705Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0700Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0703Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0698Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0693Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0692Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0692Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0689Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0682Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0683Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0682Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0682Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0678Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0687Epoch 10/15: [==============================] 60/60 batches, loss: 0.0691
[2025-05-07 20:33:54,021][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0691
[2025-05-07 20:33:54,277][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0469, Metrics: {'mse': 0.048756226897239685, 'rmse': 0.22080812235341274, 'r2': 0.01925814151763916}
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0710Epoch 11/15: [=                             ] 2/60 batches, loss: 0.1343Epoch 11/15: [=                             ] 3/60 batches, loss: 0.1018Epoch 11/15: [==                            ] 4/60 batches, loss: 0.1002Epoch 11/15: [==                            ] 5/60 batches, loss: 0.1026Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0941Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0938Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0910Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0859Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0835Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0795Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0790Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0755Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0757Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0742Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0746Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0728Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0749Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0756Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0740Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0753Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0748Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0742Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0734Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0735Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0720Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0713Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0704Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0694Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0682Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0683Epoch 11/15: [================              ] 32/60 batches, loss: 0.0674Epoch 11/15: [================              ] 33/60 batches, loss: 0.0662Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0661Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0656Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0658Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0661Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0655Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0658Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0660Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0654Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0654Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0654Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0651Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0655Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0648Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0650Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0646Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0646Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0640Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0637Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0640Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0646Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0652Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0651Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0653Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0653Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0652Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0655Epoch 11/15: [==============================] 60/60 batches, loss: 0.0653
[2025-05-07 20:33:56,537][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0653
[2025-05-07 20:33:56,798][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0514, Metrics: {'mse': 0.05277245491743088, 'rmse': 0.22972256074976807, 'r2': -0.06152904033660889}
[2025-05-07 20:33:56,799][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0483Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0615Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0665Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0598Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0611Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0596Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0574Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0602Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0595Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0566Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0612Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0652Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0646Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0635Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0642Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0631Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0649Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0637Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0663Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0703Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0689Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0686Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0676Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0662Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0676Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0673Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0670Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0679Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0682Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0685Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0680Epoch 12/15: [================              ] 32/60 batches, loss: 0.0679Epoch 12/15: [================              ] 33/60 batches, loss: 0.0679Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0684Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0684Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0680Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0684Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0679Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0674Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0666Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0669Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0671Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0673Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0679Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0681Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0681Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0680Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0680Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0678Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0680Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0686Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0683Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0684Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0687Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0683Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0683Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0685Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0688Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0687Epoch 12/15: [==============================] 60/60 batches, loss: 0.0687
[2025-05-07 20:33:58,672][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0687
[2025-05-07 20:33:58,934][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0512, Metrics: {'mse': 0.0526493564248085, 'rmse': 0.22945447571317606, 'r2': -0.05905294418334961}
[2025-05-07 20:33:58,935][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0777Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0895Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0792Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0691Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0650Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0638Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0668Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0627Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0654Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0646Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0644Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0636Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0640Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0643Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0661Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0654Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0651Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0643Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0637Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0634Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0631Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0635Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0634Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0645Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0649Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0643Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0633Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0634Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0642Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0635Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0632Epoch 13/15: [================              ] 32/60 batches, loss: 0.0627Epoch 13/15: [================              ] 33/60 batches, loss: 0.0620Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0620Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0623Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0637Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0646Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0640Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0635Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0635Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0626Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0624Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0623Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0629Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0627Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0628Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0629Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0632Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0628Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0624Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0621Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0621Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0619Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0620Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0624Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0618Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0617Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0625Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0620Epoch 13/15: [==============================] 60/60 batches, loss: 0.0619
[2025-05-07 20:34:00,782][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0619
[2025-05-07 20:34:01,035][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0489, Metrics: {'mse': 0.05037614330649376, 'rmse': 0.2244463038378974, 'r2': -0.013326764106750488}
[2025-05-07 20:34:01,036][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0576Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0613Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0605Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0530Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0535Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0553Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0578Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0565Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0573Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0557Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0545Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0537Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0529Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0531Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0515Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0527Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0540Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0547Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0546Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0546Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0561Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0571Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0577Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0587Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0586Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0575Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0584Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0588Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0576Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0576Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0584Epoch 14/15: [================              ] 32/60 batches, loss: 0.0584Epoch 14/15: [================              ] 33/60 batches, loss: 0.0591Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0584Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0579Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0581Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0579Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0583Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0588Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0586Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0583Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0577Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0570Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0572Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0573Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0576Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0570Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0570Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0575Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0580Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0579Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0585Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0584Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0589Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0590Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0587Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0583Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0591Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0589Epoch 14/15: [==============================] 60/60 batches, loss: 0.0587
[2025-05-07 20:34:02,890][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0587
[2025-05-07 20:34:03,156][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0533, Metrics: {'mse': 0.05446783825755119, 'rmse': 0.2333834575490542, 'r2': -0.09563207626342773}
[2025-05-07 20:34:03,157][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 20:34:03,157][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 14
[2025-05-07 20:34:03,157][src.training.lm_trainer][INFO] - Training completed in 32.65 seconds
[2025-05-07 20:34:03,157][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:34:05,705][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.04548681154847145, 'rmse': 0.21327637362931565, 'r2': -0.018633484840393066}
[2025-05-07 20:34:05,705][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.048756226897239685, 'rmse': 0.22080812235341274, 'r2': 0.01925814151763916}
[2025-05-07 20:34:05,705][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.045623425394296646, 'rmse': 0.21359640772797805, 'r2': -0.11496031284332275}
[2025-05-07 20:34:07,392][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control2/layer2/id/id/model.pt
[2025-05-07 20:34:07,400][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▄▂▁
wandb:     best_val_mse █▇▄▂▁
wandb:      best_val_r2 ▁▂▅▇█
wandb:    best_val_rmse █▇▅▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▃▁▁▃▃▃▃▄▄▃▃▄
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▅▄█▇▃▃▃▃▂▁▃▃▂▄
wandb:          val_mse ▅▄█▇▃▃▃▃▂▁▃▃▂▄
wandb:           val_r2 ▄▅▁▂▆▆▆▆▇█▆▆▇▅
wandb:         val_rmse ▅▄█▇▃▃▃▃▂▁▃▃▂▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04687
wandb:     best_val_mse 0.04876
wandb:      best_val_r2 0.01926
wandb:    best_val_rmse 0.22081
wandb: early_stop_epoch 14
wandb:            epoch 14
wandb:   final_test_mse 0.04562
wandb:    final_test_r2 -0.11496
wandb:  final_test_rmse 0.2136
wandb:  final_train_mse 0.04549
wandb:   final_train_r2 -0.01863
wandb: final_train_rmse 0.21328
wandb:    final_val_mse 0.04876
wandb:     final_val_r2 0.01926
wandb:   final_val_rmse 0.22081
wandb:    learning_rate 0.0001
wandb:       train_loss 0.05871
wandb:       train_time 32.65096
wandb:         val_loss 0.05329
wandb:          val_mse 0.05447
wandb:           val_r2 -0.09563
wandb:         val_rmse 0.23338
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_203316-sd64dcid
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_203316-sd64dcid/logs
Experiment probe_layer2_lexical_density_control2_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control2/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_lexical_density_control3_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_lexical_density_control3_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control3/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=lexical_density"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:34:28,842][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control3/layer2/id
experiment_name: probe_layer2_lexical_density_control3_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: lexical_density
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:34:28,842][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:34:28,842][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 20:34:28,842][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:34:28,842][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:34:28,847][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:34:28,847][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 20:34:28,847][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:34:31,486][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'lexical_density'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:34:33,773][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:34:33,773][src.data.datasets][INFO] - Loading 'control_lexical_density_seed3' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:34:33,937][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_lexical_density_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:49:28 2025).
[2025-05-07 20:34:34,052][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_lexical_density_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:49:28 2025).
[2025-05-07 20:34:34,280][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:34:34,287][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:34:34,288][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:34:34,289][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:34:34,362][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:34:34,473][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:34:34,537][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:34:34,538][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:34:34,538][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:34:34,539][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:34:34,665][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:34:34,779][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:34:34,792][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:34:34,793][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:34:34,794][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:34:34,794][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:34:34,795][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:34:34,795][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 20:34:34,795][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 20:34:34,795][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 20:34:34,795][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:34:34,795][src.data.datasets][INFO] -   Mean: 0.6348, Std: 0.2113
[2025-05-07 20:34:34,796][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:34:34,796][src.data.datasets][INFO] - Sample label: 0.7329999804496765
[2025-05-07 20:34:34,796][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:34:34,796][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 20:34:34,796][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 20:34:34,796][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 20:34:34,796][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:34:34,796][src.data.datasets][INFO] -   Mean: 0.5853, Std: 0.2230
[2025-05-07 20:34:34,796][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:34:34,796][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:34:34,797][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:34:34,797][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 20:34:34,797][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 20:34:34,797][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 20:34:34,797][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:34:34,797][src.data.datasets][INFO] -   Mean: 0.5191, Std: 0.2023
[2025-05-07 20:34:34,797][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:34:34,797][src.data.datasets][INFO] - Sample label: 0.5289999842643738
[2025-05-07 20:34:34,797][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:34:34,797][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:34:34,798][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:34:34,798][__main__][INFO] - Using model type: lm_probe for submetric lexical_density
[2025-05-07 20:34:34,798][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:34:40,991][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:34:40,991][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:34:40,991][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:34:40,992][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:34:40,994][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:34:40,995][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:34:40,995][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:34:40,995][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:34:40,995][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:34:40,996][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:34:40,996][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.3839Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4581Epoch 1/15: [=                             ] 3/60 batches, loss: 0.4754Epoch 1/15: [==                            ] 4/60 batches, loss: 0.4784Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4450Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4402Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4943Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4856Epoch 1/15: [====                          ] 9/60 batches, loss: 0.5017Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.5137Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4943Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4802Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4651Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4567Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4602Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4506Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4416Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4519Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4328Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4230Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4163Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4149Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4077Epoch 1/15: [============                  ] 24/60 batches, loss: 0.4056Epoch 1/15: [============                  ] 25/60 batches, loss: 0.4015Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.3986Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3911Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3873Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3869Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3920Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3887Epoch 1/15: [================              ] 32/60 batches, loss: 0.3833Epoch 1/15: [================              ] 33/60 batches, loss: 0.3787Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3765Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3752Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3714Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3633Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3587Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3545Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3526Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3513Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3502Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3475Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3474Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3444Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3407Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3405Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3386Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3386Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3342Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3324Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3322Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3278Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3285Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3268Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3298Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3328Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3323Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3305Epoch 1/15: [==============================] 60/60 batches, loss: 0.3264
[2025-05-07 20:34:46,154][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3264
[2025-05-07 20:34:46,388][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0563, Metrics: {'mse': 0.057490501552820206, 'rmse': 0.23977176971616196, 'r2': -0.1564335823059082}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.4091Epoch 2/15: [=                             ] 2/60 batches, loss: 0.3011Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2438Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2282Epoch 2/15: [==                            ] 5/60 batches, loss: 0.2010Epoch 2/15: [===                           ] 6/60 batches, loss: 0.2127Epoch 2/15: [===                           ] 7/60 batches, loss: 0.2115Epoch 2/15: [====                          ] 8/60 batches, loss: 0.2182Epoch 2/15: [====                          ] 9/60 batches, loss: 0.2110Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.2119Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.2122Epoch 2/15: [======                        ] 12/60 batches, loss: 0.2131Epoch 2/15: [======                        ] 13/60 batches, loss: 0.2261Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.2280Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.2219Epoch 2/15: [========                      ] 16/60 batches, loss: 0.2168Epoch 2/15: [========                      ] 17/60 batches, loss: 0.2197Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.2207Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.2172Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.2174Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.2157Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.2158Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.2168Epoch 2/15: [============                  ] 24/60 batches, loss: 0.2161Epoch 2/15: [============                  ] 25/60 batches, loss: 0.2187Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.2167Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.2137Epoch 2/15: [==============                ] 28/60 batches, loss: 0.2116Epoch 2/15: [==============                ] 29/60 batches, loss: 0.2106Epoch 2/15: [===============               ] 30/60 batches, loss: 0.2080Epoch 2/15: [===============               ] 31/60 batches, loss: 0.2085Epoch 2/15: [================              ] 32/60 batches, loss: 0.2073Epoch 2/15: [================              ] 33/60 batches, loss: 0.2045Epoch 2/15: [=================             ] 34/60 batches, loss: 0.2036Epoch 2/15: [=================             ] 35/60 batches, loss: 0.2053Epoch 2/15: [==================            ] 36/60 batches, loss: 0.2057Epoch 2/15: [==================            ] 37/60 batches, loss: 0.2068Epoch 2/15: [===================           ] 38/60 batches, loss: 0.2072Epoch 2/15: [===================           ] 39/60 batches, loss: 0.2064Epoch 2/15: [====================          ] 40/60 batches, loss: 0.2102Epoch 2/15: [====================          ] 41/60 batches, loss: 0.2096Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.2096Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.2083Epoch 2/15: [======================        ] 44/60 batches, loss: 0.2093Epoch 2/15: [======================        ] 45/60 batches, loss: 0.2086Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.2076Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.2059Epoch 2/15: [========================      ] 48/60 batches, loss: 0.2053Epoch 2/15: [========================      ] 49/60 batches, loss: 0.2044Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.2031Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.2023Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.2000Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1995Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1991Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1981Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1965Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1966Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1977Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1964Epoch 2/15: [==============================] 60/60 batches, loss: 0.1956
[2025-05-07 20:34:48,622][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1956
[2025-05-07 20:34:48,875][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0527, Metrics: {'mse': 0.0540783628821373, 'rmse': 0.2325475497229272, 'r2': -0.08779776096343994}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.1258Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1710Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1534Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1712Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1615Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1570Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1589Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1663Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1594Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1521Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1482Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1537Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1523Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1518Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1497Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1474Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1529Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1507Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1450Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1504Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1499Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1472Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1454Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1460Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1438Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1444Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1416Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1409Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1432Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1419Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1417Epoch 3/15: [================              ] 32/60 batches, loss: 0.1416Epoch 3/15: [================              ] 33/60 batches, loss: 0.1446Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1439Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1419Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1435Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1421Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1407Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1411Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1417Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1400Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1405Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1399Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1395Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1385Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1379Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1377Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1366Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1379Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1372Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1368Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1371Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1358Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1375Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1375Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1366Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1363Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1358Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1351Epoch 3/15: [==============================] 60/60 batches, loss: 0.1355
[2025-05-07 20:34:51,132][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1355
[2025-05-07 20:34:51,391][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0621, Metrics: {'mse': 0.06281139701604843, 'rmse': 0.2506220202138041, 'r2': -0.26346445083618164}
[2025-05-07 20:34:51,392][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0543Epoch 4/15: [=                             ] 2/60 batches, loss: 0.1173Epoch 4/15: [=                             ] 3/60 batches, loss: 0.1085Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1252Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1143Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1135Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1220Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1167Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1092Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.1032Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1005Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1006Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1045Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.1047Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.1069Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1075Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1091Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1052Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1053Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1072Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1089Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1106Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1108Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1129Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1109Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1125Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1143Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1150Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1130Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1128Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1126Epoch 4/15: [================              ] 32/60 batches, loss: 0.1116Epoch 4/15: [================              ] 33/60 batches, loss: 0.1114Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1111Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1102Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1090Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1096Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1091Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1099Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1095Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1098Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1123Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1127Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1116Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1108Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1102Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.1108Epoch 4/15: [========================      ] 48/60 batches, loss: 0.1109Epoch 4/15: [========================      ] 49/60 batches, loss: 0.1111Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.1103Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.1106Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1103Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.1092Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1097Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1092Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1097Epoch 4/15: [============================  ] 57/60 batches, loss: 0.1089Epoch 4/15: [============================= ] 58/60 batches, loss: 0.1092Epoch 4/15: [============================= ] 59/60 batches, loss: 0.1091Epoch 4/15: [==============================] 60/60 batches, loss: 0.1090
[2025-05-07 20:34:53,232][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1090
[2025-05-07 20:34:53,491][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0647, Metrics: {'mse': 0.06550846993923187, 'rmse': 0.25594622470204925, 'r2': -0.31771671772003174}
[2025-05-07 20:34:53,492][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.0703Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0718Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0790Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0930Epoch 5/15: [==                            ] 5/60 batches, loss: 0.0988Epoch 5/15: [===                           ] 6/60 batches, loss: 0.0966Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1013Epoch 5/15: [====                          ] 8/60 batches, loss: 0.0970Epoch 5/15: [====                          ] 9/60 batches, loss: 0.1044Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.1044Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1063Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1071Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1083Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1078Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1045Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1060Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1065Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1073Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1095Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1113Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1102Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1087Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1103Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1127Epoch 5/15: [============                  ] 25/60 batches, loss: 0.1109Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1105Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.1085Epoch 5/15: [==============                ] 28/60 batches, loss: 0.1070Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1084Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1087Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1088Epoch 5/15: [================              ] 32/60 batches, loss: 0.1073Epoch 5/15: [================              ] 33/60 batches, loss: 0.1060Epoch 5/15: [=================             ] 34/60 batches, loss: 0.1042Epoch 5/15: [=================             ] 35/60 batches, loss: 0.1058Epoch 5/15: [==================            ] 36/60 batches, loss: 0.1058Epoch 5/15: [==================            ] 37/60 batches, loss: 0.1061Epoch 5/15: [===================           ] 38/60 batches, loss: 0.1067Epoch 5/15: [===================           ] 39/60 batches, loss: 0.1054Epoch 5/15: [====================          ] 40/60 batches, loss: 0.1038Epoch 5/15: [====================          ] 41/60 batches, loss: 0.1035Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.1035Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.1036Epoch 5/15: [======================        ] 44/60 batches, loss: 0.1024Epoch 5/15: [======================        ] 45/60 batches, loss: 0.1023Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.1025Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.1027Epoch 5/15: [========================      ] 48/60 batches, loss: 0.1024Epoch 5/15: [========================      ] 49/60 batches, loss: 0.1012Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.1017Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.1010Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.1012Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.1016Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.1006Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.1007Epoch 5/15: [============================  ] 56/60 batches, loss: 0.1005Epoch 5/15: [============================  ] 57/60 batches, loss: 0.1007Epoch 5/15: [============================= ] 58/60 batches, loss: 0.1019Epoch 5/15: [============================= ] 59/60 batches, loss: 0.1020Epoch 5/15: [==============================] 60/60 batches, loss: 0.1013
[2025-05-07 20:34:55,345][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1013
[2025-05-07 20:34:55,605][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0576, Metrics: {'mse': 0.0588354654610157, 'rmse': 0.2425602305841081, 'r2': -0.1834878921508789}
[2025-05-07 20:34:55,606][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1168Epoch 6/15: [=                             ] 2/60 batches, loss: 0.1125Epoch 6/15: [=                             ] 3/60 batches, loss: 0.1200Epoch 6/15: [==                            ] 4/60 batches, loss: 0.1035Epoch 6/15: [==                            ] 5/60 batches, loss: 0.1057Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0999Epoch 6/15: [===                           ] 7/60 batches, loss: 0.1093Epoch 6/15: [====                          ] 8/60 batches, loss: 0.1216Epoch 6/15: [====                          ] 9/60 batches, loss: 0.1146Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.1144Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.1110Epoch 6/15: [======                        ] 12/60 batches, loss: 0.1077Epoch 6/15: [======                        ] 13/60 batches, loss: 0.1067Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.1029Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0986Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0999Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0991Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.1010Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0995Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0979Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0995Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0979Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0986Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0991Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0983Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0975Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0968Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0969Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0974Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0959Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0946Epoch 6/15: [================              ] 32/60 batches, loss: 0.0939Epoch 6/15: [================              ] 33/60 batches, loss: 0.0959Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0958Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0957Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0951Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0952Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0949Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0937Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0931Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0930Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0930Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0929Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0948Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0947Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0941Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0945Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0937Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0937Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0934Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0947Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0939Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0936Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0932Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0937Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0929Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0931Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0924Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0922Epoch 6/15: [==============================] 60/60 batches, loss: 0.0915
[2025-05-07 20:34:57,476][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0915
[2025-05-07 20:34:57,743][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0520, Metrics: {'mse': 0.05333062633872032, 'rmse': 0.23093424678622337, 'r2': -0.07275676727294922}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0626Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0888Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0875Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0851Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0841Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0746Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0788Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0789Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0801Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0804Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0783Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0792Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0791Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0763Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0769Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0766Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0754Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0744Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0742Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0725Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0703Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0716Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0768Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0801Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0785Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0787Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0782Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0782Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0786Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0800Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0796Epoch 7/15: [================              ] 32/60 batches, loss: 0.0807Epoch 7/15: [================              ] 33/60 batches, loss: 0.0817Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0807Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0797Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0816Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0816Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0811Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0807Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0801Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0805Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0818Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0819Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0815Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0815Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0829Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0834Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0829Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0844Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0843Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0845Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0837Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0836Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0834Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0842Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0842Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0848Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0852Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0856Epoch 7/15: [==============================] 60/60 batches, loss: 0.0857
[2025-05-07 20:34:59,987][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0857
[2025-05-07 20:35:00,257][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0587, Metrics: {'mse': 0.05943916738033295, 'rmse': 0.24380149175165633, 'r2': -0.19563138484954834}
[2025-05-07 20:35:00,258][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0474Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0577Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0685Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0724Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0800Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0810Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0831Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0807Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0798Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0757Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0748Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0743Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0725Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0721Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0719Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0728Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0712Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0705Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0700Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0724Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0718Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0717Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0720Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0716Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0724Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0705Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0713Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0702Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0704Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0709Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0715Epoch 8/15: [================              ] 32/60 batches, loss: 0.0727Epoch 8/15: [================              ] 33/60 batches, loss: 0.0718Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0709Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0714Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0712Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0712Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0719Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0716Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0720Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0734Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0737Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0746Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0744Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0747Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0749Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0748Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0757Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0763Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0771Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0769Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0772Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0774Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0769Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0776Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0773Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0769Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0780Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0777Epoch 8/15: [==============================] 60/60 batches, loss: 0.0782
[2025-05-07 20:35:02,156][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0782
[2025-05-07 20:35:02,411][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0493, Metrics: {'mse': 0.050666920840740204, 'rmse': 0.22509313814672408, 'r2': -0.01917588710784912}
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.0888Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0643Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0662Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0643Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0647Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0680Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0655Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0631Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0644Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0689Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0703Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0704Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0706Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0699Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0691Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0702Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0714Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0705Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0702Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0734Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0738Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0733Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0730Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0729Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0723Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0730Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0745Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0744Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0741Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0748Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0749Epoch 9/15: [================              ] 32/60 batches, loss: 0.0739Epoch 9/15: [================              ] 33/60 batches, loss: 0.0734Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0729Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0740Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0743Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0732Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0730Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0733Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0745Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0752Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0752Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0748Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0745Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0739Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0743Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0735Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0728Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0745Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0745Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0744Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0737Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0732Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0733Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0728Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0729Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0736Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0734Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0731Epoch 9/15: [==============================] 60/60 batches, loss: 0.0732
[2025-05-07 20:35:04,714][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0732
[2025-05-07 20:35:04,969][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0509, Metrics: {'mse': 0.05219168961048126, 'rmse': 0.22845500565862256, 'r2': -0.04984688758850098}
[2025-05-07 20:35:04,969][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0657Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0740Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0706Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0677Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0646Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0636Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0606Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0593Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0621Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0601Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0603Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0628Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0660Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0679Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0674Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0669Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0655Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0656Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0655Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0645Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0663Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0673Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0680Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0694Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0697Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0707Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0711Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0730Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0721Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0712Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0714Epoch 10/15: [================              ] 32/60 batches, loss: 0.0725Epoch 10/15: [================              ] 33/60 batches, loss: 0.0729Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0737Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0737Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0741Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0733Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0728Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0733Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0727Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0720Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0725Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0714Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0707Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0709Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0711Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0710Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0707Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0705Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0704Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0704Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0706Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0705Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0710Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0712Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0705Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0710Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0712Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0717Epoch 10/15: [==============================] 60/60 batches, loss: 0.0719
[2025-05-07 20:35:06,869][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0719
[2025-05-07 20:35:07,127][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0461, Metrics: {'mse': 0.047807976603507996, 'rmse': 0.2186503523974018, 'r2': 0.03833240270614624}
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0881Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0679Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0702Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0691Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0707Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0724Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0701Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0720Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0725Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0704Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0676Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0696Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0684Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0658Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0668Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0672Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0662Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0674Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0665Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0655Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0643Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0655Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0657Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0640Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0674Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0673Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0689Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0680Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0678Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0683Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0691Epoch 11/15: [================              ] 32/60 batches, loss: 0.0683Epoch 11/15: [================              ] 33/60 batches, loss: 0.0674Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0683Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0674Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0682Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0682Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0685Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0685Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0692Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0688Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0689Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0687Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0686Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0698Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0699Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0703Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0706Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0700Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0702Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0702Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0699Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0703Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0706Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0711Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0724Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0723Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0718Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0716Epoch 11/15: [==============================] 60/60 batches, loss: 0.0717
[2025-05-07 20:35:09,349][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0717
[2025-05-07 20:35:09,629][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0494, Metrics: {'mse': 0.05065898969769478, 'rmse': 0.22507551998761388, 'r2': -0.019016265869140625}
[2025-05-07 20:35:09,630][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.1004Epoch 12/15: [=                             ] 2/60 batches, loss: 0.1089Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0923Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0846Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0828Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0801Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0761Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0748Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0722Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0690Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0696Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0715Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0712Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0734Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0737Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0734Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0734Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0725Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0735Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0738Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0726Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0728Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0725Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0718Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0705Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0695Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0694Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0691Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0689Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0685Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0683Epoch 12/15: [================              ] 32/60 batches, loss: 0.0684Epoch 12/15: [================              ] 33/60 batches, loss: 0.0690Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0693Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0691Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0692Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0684Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0681Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0689Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0680Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0675Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0668Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0669Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0667Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0681Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0682Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0680Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0682Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0676Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0669Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0668Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0665Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0663Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0663Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0663Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0668Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0662Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0661Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0660Epoch 12/15: [==============================] 60/60 batches, loss: 0.0665
[2025-05-07 20:35:11,509][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0665
[2025-05-07 20:35:11,769][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0493, Metrics: {'mse': 0.050751540809869766, 'rmse': 0.22528102629797692, 'r2': -0.020877957344055176}
[2025-05-07 20:35:11,770][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0896Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0777Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0694Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0667Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0702Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0657Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0643Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0643Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0666Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0680Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0708Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0674Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0657Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0631Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0635Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0625Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0633Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0611Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0592Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0601Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0604Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0591Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0591Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0586Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0575Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0569Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0563Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0567Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0581Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0587Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0586Epoch 13/15: [================              ] 32/60 batches, loss: 0.0577Epoch 13/15: [================              ] 33/60 batches, loss: 0.0575Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0581Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0583Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0581Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0587Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0593Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0587Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0583Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0584Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0587Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0585Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0586Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0587Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0580Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0578Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0576Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0579Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0589Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0595Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0590Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0590Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0586Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0589Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0592Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0590Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0592Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0599Epoch 13/15: [==============================] 60/60 batches, loss: 0.0601
[2025-05-07 20:35:13,656][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0601
[2025-05-07 20:35:13,918][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0513, Metrics: {'mse': 0.052589673548936844, 'rmse': 0.22932438498541066, 'r2': -0.05785238742828369}
[2025-05-07 20:35:13,918][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0379Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0433Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0516Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0535Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0624Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0599Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0666Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0620Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0594Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0696Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0667Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0660Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0654Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0709Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0699Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0685Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0720Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0725Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0715Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0699Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0701Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0688Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0682Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0674Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0676Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0664Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0661Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0666Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0670Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0668Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0667Epoch 14/15: [================              ] 32/60 batches, loss: 0.0660Epoch 14/15: [================              ] 33/60 batches, loss: 0.0664Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0661Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0663Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0661Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0669Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0666Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0665Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0660Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0648Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0655Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0653Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0658Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0659Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0658Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0670Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0667Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0674Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0675Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0686Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0687Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0687Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0682Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0683Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0676Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0674Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0676Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0670Epoch 14/15: [==============================] 60/60 batches, loss: 0.0665
[2025-05-07 20:35:15,811][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0665
[2025-05-07 20:35:16,160][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0520, Metrics: {'mse': 0.053226832300424576, 'rmse': 0.23070941094897837, 'r2': -0.07066893577575684}
[2025-05-07 20:35:16,161][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 20:35:16,161][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 14
[2025-05-07 20:35:16,161][src.training.lm_trainer][INFO] - Training completed in 32.69 seconds
[2025-05-07 20:35:16,161][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:35:18,731][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.04645346850156784, 'rmse': 0.2155306671951067, 'r2': -0.040280938148498535}
[2025-05-07 20:35:18,732][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.047807976603507996, 'rmse': 0.2186503523974018, 'r2': 0.03833240270614624}
[2025-05-07 20:35:18,732][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.042687177658081055, 'rmse': 0.2066087550373436, 'r2': -0.04320335388183594}
[2025-05-07 20:35:20,381][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control3/layer2/id/id/model.pt
[2025-05-07 20:35:20,388][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▃▁
wandb:     best_val_mse █▆▅▃▁
wandb:      best_val_r2 ▁▃▄▆█
wandb:    best_val_rmse █▆▅▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▃▂▁▂▃▂▄▄▄▄▄▄
wandb:       train_loss █▅▃▂▂▂▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▅▃▇█▅▃▆▂▃▁▂▂▃▃
wandb:          val_mse ▅▃▇█▅▃▆▂▃▁▂▂▃▃
wandb:           val_r2 ▄▆▂▁▄▆▃▇▆█▇▇▆▆
wandb:         val_rmse ▅▄▇█▅▃▆▂▃▁▂▂▃▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04606
wandb:     best_val_mse 0.04781
wandb:      best_val_r2 0.03833
wandb:    best_val_rmse 0.21865
wandb: early_stop_epoch 14
wandb:            epoch 14
wandb:   final_test_mse 0.04269
wandb:    final_test_r2 -0.0432
wandb:  final_test_rmse 0.20661
wandb:  final_train_mse 0.04645
wandb:   final_train_r2 -0.04028
wandb: final_train_rmse 0.21553
wandb:    final_val_mse 0.04781
wandb:     final_val_r2 0.03833
wandb:   final_val_rmse 0.21865
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06646
wandb:       train_time 32.69339
wandb:         val_loss 0.052
wandb:          val_mse 0.05323
wandb:           val_r2 -0.07067
wandb:         val_rmse 0.23071
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_203428-1eoeeh77
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_203428-1eoeeh77/logs
Experiment probe_layer2_lexical_density_control3_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control3/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_n_tokens_control1_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_n_tokens_control1_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control1/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=n_tokens"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:35:44,313][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control1/layer2/id
experiment_name: probe_layer2_n_tokens_control1_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: n_tokens
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:35:44,313][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:35:44,313][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 20:35:44,313][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:35:44,313][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:35:44,317][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:35:44,317][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 20:35:44,318][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:35:47,290][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'n_tokens'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:35:49,650][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:35:49,651][src.data.datasets][INFO] - Loading 'control_n_tokens_seed1' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:35:49,807][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_n_tokens_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:50:23 2025).
[2025-05-07 20:35:49,865][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_n_tokens_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:50:23 2025).
[2025-05-07 20:35:50,012][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:35:50,019][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:35:50,020][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:35:50,021][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:35:50,064][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:35:50,130][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:35:50,166][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:35:50,167][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:35:50,167][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:35:50,168][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:35:50,245][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:35:50,302][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:35:50,316][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:35:50,318][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:35:50,318][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:35:50,319][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:35:50,320][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:35:50,320][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 20:35:50,320][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 20:35:50,320][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 20:35:50,320][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:35:50,321][src.data.datasets][INFO] -   Mean: 0.2923, Std: 0.1789
[2025-05-07 20:35:50,321][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:35:50,321][src.data.datasets][INFO] - Sample label: 0.20000000298023224
[2025-05-07 20:35:50,321][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:35:50,321][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 20:35:50,321][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 20:35:50,321][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 20:35:50,321][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:35:50,321][src.data.datasets][INFO] -   Mean: 0.2550, Std: 0.1823
[2025-05-07 20:35:50,322][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:35:50,322][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:35:50,322][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:35:50,322][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 20:35:50,322][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 20:35:50,322][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 20:35:50,322][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:35:50,322][src.data.datasets][INFO] -   Mean: 0.1905, Std: 0.1719
[2025-05-07 20:35:50,322][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:35:50,322][src.data.datasets][INFO] - Sample label: 0.28600001335144043
[2025-05-07 20:35:50,322][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:35:50,323][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:35:50,323][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:35:50,323][__main__][INFO] - Using model type: lm_probe for submetric n_tokens
[2025-05-07 20:35:50,323][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:35:56,530][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:35:56,531][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:35:56,531][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:35:56,531][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:35:56,535][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:35:56,535][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:35:56,535][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:35:56,536][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:35:56,536][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:35:56,537][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:35:56,537][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.4216Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4658Epoch 1/15: [=                             ] 3/60 batches, loss: 0.4898Epoch 1/15: [==                            ] 4/60 batches, loss: 0.4595Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4300Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4110Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4558Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4620Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4686Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4774Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4602Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4581Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4458Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4368Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4278Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4298Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4209Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4323Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4175Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4102Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4066Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4078Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.3998Epoch 1/15: [============                  ] 24/60 batches, loss: 0.3937Epoch 1/15: [============                  ] 25/60 batches, loss: 0.3908Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.3874Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3812Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3758Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3782Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3828Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3769Epoch 1/15: [================              ] 32/60 batches, loss: 0.3722Epoch 1/15: [================              ] 33/60 batches, loss: 0.3668Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3653Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3607Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3592Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3533Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3484Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3445Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3399Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3385Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3381Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3338Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3329Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3318Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3281Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3275Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3244Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3220Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3185Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3168Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3148Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3128Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3140Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3126Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3138Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3159Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3148Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3126Epoch 1/15: [==============================] 60/60 batches, loss: 0.3092
[2025-05-07 20:36:02,237][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3092
[2025-05-07 20:36:02,495][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0638, Metrics: {'mse': 0.06044737994670868, 'rmse': 0.2458604887872565, 'r2': -0.8186492919921875}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.4082Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2790Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2451Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2285Epoch 2/15: [==                            ] 5/60 batches, loss: 0.2008Epoch 2/15: [===                           ] 6/60 batches, loss: 0.2238Epoch 2/15: [===                           ] 7/60 batches, loss: 0.2174Epoch 2/15: [====                          ] 8/60 batches, loss: 0.2197Epoch 2/15: [====                          ] 9/60 batches, loss: 0.2196Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.2173Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.2126Epoch 2/15: [======                        ] 12/60 batches, loss: 0.2114Epoch 2/15: [======                        ] 13/60 batches, loss: 0.2119Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.2104Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.2028Epoch 2/15: [========                      ] 16/60 batches, loss: 0.2004Epoch 2/15: [========                      ] 17/60 batches, loss: 0.1970Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.1923Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.1926Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1925Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1910Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1921Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1932Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1897Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1937Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1917Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1918Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1918Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1886Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1861Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1844Epoch 2/15: [================              ] 32/60 batches, loss: 0.1814Epoch 2/15: [================              ] 33/60 batches, loss: 0.1792Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1799Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1797Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1789Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1776Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1759Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1749Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1782Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1788Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1771Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1760Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1775Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1762Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1741Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1730Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1721Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1712Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1705Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1682Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1667Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1670Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1659Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1657Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1644Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1639Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1629Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1616Epoch 2/15: [==============================] 60/60 batches, loss: 0.1601
[2025-05-07 20:36:04,786][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1601
[2025-05-07 20:36:05,040][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0538, Metrics: {'mse': 0.05083191394805908, 'rmse': 0.22545933989981226, 'r2': -0.5293537378311157}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.0880Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1020Epoch 3/15: [=                             ] 3/60 batches, loss: 0.0941Epoch 3/15: [==                            ] 4/60 batches, loss: 0.0980Epoch 3/15: [==                            ] 5/60 batches, loss: 0.0939Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1003Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1067Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1107Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1096Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1082Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1085Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1075Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1183Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1167Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1128Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1171Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1185Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1154Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1134Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1124Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1114Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1099Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1083Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1075Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1092Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1143Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1144Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1139Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1145Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1132Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1132Epoch 3/15: [================              ] 32/60 batches, loss: 0.1136Epoch 3/15: [================              ] 33/60 batches, loss: 0.1167Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1154Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1151Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1157Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1163Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1155Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1153Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1150Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1160Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1156Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1161Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1164Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1149Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1136Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1140Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1135Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1145Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1148Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1150Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1151Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1138Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1148Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1148Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1137Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1131Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1121Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1121Epoch 3/15: [==============================] 60/60 batches, loss: 0.1138
[2025-05-07 20:36:07,312][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1138
[2025-05-07 20:36:07,592][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0642, Metrics: {'mse': 0.06051964312791824, 'rmse': 0.24600740462010132, 'r2': -0.8208234310150146}
[2025-05-07 20:36:07,593][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0352Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0783Epoch 4/15: [=                             ] 3/60 batches, loss: 0.0854Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1105Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1051Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1059Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1212Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1211Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1155Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.1093Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1095Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1124Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1109Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.1101Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.1085Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1104Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1137Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1119Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1097Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1114Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1181Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1208Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1202Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1223Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1197Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1202Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1181Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1183Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1179Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1154Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1143Epoch 4/15: [================              ] 32/60 batches, loss: 0.1122Epoch 4/15: [================              ] 33/60 batches, loss: 0.1116Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1104Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1099Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1092Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1081Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1082Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1073Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1070Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1073Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1081Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1070Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1066Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1064Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1050Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.1042Epoch 4/15: [========================      ] 48/60 batches, loss: 0.1047Epoch 4/15: [========================      ] 49/60 batches, loss: 0.1049Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.1045Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.1041Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1040Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.1033Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1032Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1029Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1022Epoch 4/15: [============================  ] 57/60 batches, loss: 0.1020Epoch 4/15: [============================= ] 58/60 batches, loss: 0.1014Epoch 4/15: [============================= ] 59/60 batches, loss: 0.1011Epoch 4/15: [==============================] 60/60 batches, loss: 0.1001
[2025-05-07 20:36:09,464][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1001
[2025-05-07 20:36:09,747][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0529, Metrics: {'mse': 0.05035754665732384, 'rmse': 0.2244048721782213, 'r2': -0.5150816440582275}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.0551Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0701Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0773Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0741Epoch 5/15: [==                            ] 5/60 batches, loss: 0.0919Epoch 5/15: [===                           ] 6/60 batches, loss: 0.0976Epoch 5/15: [===                           ] 7/60 batches, loss: 0.0990Epoch 5/15: [====                          ] 8/60 batches, loss: 0.1021Epoch 5/15: [====                          ] 9/60 batches, loss: 0.0980Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.0998Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1021Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1029Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1019Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1044Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1009Epoch 5/15: [========                      ] 16/60 batches, loss: 0.0996Epoch 5/15: [========                      ] 17/60 batches, loss: 0.0976Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.0972Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.0957Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.0972Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.0961Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.0943Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.0948Epoch 5/15: [============                  ] 24/60 batches, loss: 0.0943Epoch 5/15: [============                  ] 25/60 batches, loss: 0.0933Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.0944Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.0925Epoch 5/15: [==============                ] 28/60 batches, loss: 0.0917Epoch 5/15: [==============                ] 29/60 batches, loss: 0.0937Epoch 5/15: [===============               ] 30/60 batches, loss: 0.0956Epoch 5/15: [===============               ] 31/60 batches, loss: 0.0963Epoch 5/15: [================              ] 32/60 batches, loss: 0.0957Epoch 5/15: [================              ] 33/60 batches, loss: 0.0948Epoch 5/15: [=================             ] 34/60 batches, loss: 0.0958Epoch 5/15: [=================             ] 35/60 batches, loss: 0.0965Epoch 5/15: [==================            ] 36/60 batches, loss: 0.0961Epoch 5/15: [==================            ] 37/60 batches, loss: 0.0974Epoch 5/15: [===================           ] 38/60 batches, loss: 0.0972Epoch 5/15: [===================           ] 39/60 batches, loss: 0.0975Epoch 5/15: [====================          ] 40/60 batches, loss: 0.0966Epoch 5/15: [====================          ] 41/60 batches, loss: 0.0964Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.0965Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.0984Epoch 5/15: [======================        ] 44/60 batches, loss: 0.0970Epoch 5/15: [======================        ] 45/60 batches, loss: 0.0968Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.0976Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0967Epoch 5/15: [========================      ] 48/60 batches, loss: 0.0973Epoch 5/15: [========================      ] 49/60 batches, loss: 0.0969Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.0968Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0963Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0970Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0968Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0962Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0955Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0946Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0939Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0939Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0935Epoch 5/15: [==============================] 60/60 batches, loss: 0.0925
[2025-05-07 20:36:12,004][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0925
[2025-05-07 20:36:12,278][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0483, Metrics: {'mse': 0.045887600630521774, 'rmse': 0.2142139132515014, 'r2': -0.3805966377258301}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1429Epoch 6/15: [=                             ] 2/60 batches, loss: 0.1276Epoch 6/15: [=                             ] 3/60 batches, loss: 0.1153Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0984Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0898Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0877Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0922Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0996Epoch 6/15: [====                          ] 9/60 batches, loss: 0.1001Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0951Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0933Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0919Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0900Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0911Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0883Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0887Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0871Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0838Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0829Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0817Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0805Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0795Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0798Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0800Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0821Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0826Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0818Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0807Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0798Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0793Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0781Epoch 6/15: [================              ] 32/60 batches, loss: 0.0779Epoch 6/15: [================              ] 33/60 batches, loss: 0.0793Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0792Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0790Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0807Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0799Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0800Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0792Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0786Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0777Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0769Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0758Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0757Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0757Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0750Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0754Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0759Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0754Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0757Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0780Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0774Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0778Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0774Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0775Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0777Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0784Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0777Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0773Epoch 6/15: [==============================] 60/60 batches, loss: 0.0771
[2025-05-07 20:36:14,639][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0771
[2025-05-07 20:36:14,917][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0411, Metrics: {'mse': 0.03942827880382538, 'rmse': 0.19856555291345318, 'r2': -0.1862584352493286}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0368Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0692Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0873Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0802Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0773Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0716Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0728Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0700Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0726Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0662Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0655Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0695Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0727Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0736Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0735Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0736Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0721Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0730Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0721Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0721Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0703Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0734Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0737Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0739Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0728Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0718Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0712Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0712Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0718Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0720Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0715Epoch 7/15: [================              ] 32/60 batches, loss: 0.0707Epoch 7/15: [================              ] 33/60 batches, loss: 0.0703Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0693Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0696Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0701Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0700Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0696Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0695Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0690Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0688Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0700Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0697Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0697Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0688Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0685Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0691Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0689Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0698Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0695Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0694Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0704Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0702Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0704Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0708Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0706Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0703Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0701Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0701Epoch 7/15: [==============================] 60/60 batches, loss: 0.0702
[2025-05-07 20:36:17,166][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0702
[2025-05-07 20:36:17,430][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0454, Metrics: {'mse': 0.04354742541909218, 'rmse': 0.20868019891473216, 'r2': -0.31018900871276855}
[2025-05-07 20:36:17,430][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0771Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0715Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0683Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0672Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0646Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0684Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0618Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0607Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0590Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0598Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0580Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0597Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0596Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0578Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0574Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0568Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0596Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0586Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0591Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0598Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0597Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0600Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0598Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0599Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0596Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0599Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0606Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0621Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0619Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0616Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0613Epoch 8/15: [================              ] 32/60 batches, loss: 0.0607Epoch 8/15: [================              ] 33/60 batches, loss: 0.0601Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0592Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0590Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0591Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0596Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0613Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0624Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0626Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0633Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0631Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0635Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0640Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0637Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0636Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0631Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0635Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0637Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0636Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0636Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0633Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0636Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0631Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0631Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0631Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0630Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0634Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0629Epoch 8/15: [==============================] 60/60 batches, loss: 0.0629
[2025-05-07 20:36:19,290][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0629
[2025-05-07 20:36:19,545][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0452, Metrics: {'mse': 0.04338373616337776, 'rmse': 0.208287628445325, 'r2': -0.3052642345428467}
[2025-05-07 20:36:19,545][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.0674Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0799Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0695Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0586Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0620Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0587Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0553Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0595Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0577Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0561Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0586Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0592Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0592Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0612Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0602Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0590Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0586Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0590Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0591Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0603Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0601Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0625Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0645Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0649Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0656Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0659Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0669Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0681Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0672Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0670Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0670Epoch 9/15: [================              ] 32/60 batches, loss: 0.0665Epoch 9/15: [================              ] 33/60 batches, loss: 0.0659Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0652Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0651Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0639Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0639Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0637Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0635Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0638Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0638Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0638Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0634Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0630Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0630Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0631Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0626Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0624Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0626Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0631Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0634Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0635Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0628Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0620Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0617Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0619Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0615Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0619Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0613Epoch 9/15: [==============================] 60/60 batches, loss: 0.0606
[2025-05-07 20:36:21,424][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0606
[2025-05-07 20:36:21,679][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0394, Metrics: {'mse': 0.03812973201274872, 'rmse': 0.19526835896465336, 'r2': -0.14718961715698242}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0614Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0485Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0505Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0597Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0636Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0586Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0600Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0589Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0582Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0558Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0529Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0501Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0520Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0521Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0529Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0524Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0530Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0533Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0523Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0523Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0510Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0513Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0517Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0518Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0504Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0517Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0516Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0534Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0529Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0536Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0534Epoch 10/15: [================              ] 32/60 batches, loss: 0.0536Epoch 10/15: [================              ] 33/60 batches, loss: 0.0546Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0541Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0541Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0541Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0541Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0531Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0541Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0550Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0546Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0548Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0544Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0543Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0546Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0545Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0552Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0551Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0550Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0549Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0546Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0543Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0540Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0543Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0545Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0545Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0550Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0546Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0543Epoch 10/15: [==============================] 60/60 batches, loss: 0.0541
[2025-05-07 20:36:23,975][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0541
[2025-05-07 20:36:24,278][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0396, Metrics: {'mse': 0.03837612643837929, 'rmse': 0.19589825532244867, 'r2': -0.1546027660369873}
[2025-05-07 20:36:24,279][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0543Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0567Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0583Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0518Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0529Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0543Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0549Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0526Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0518Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0513Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0535Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0529Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0510Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0524Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0537Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0511Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0512Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0513Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0512Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0505Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0509Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0512Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0513Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0507Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0507Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0503Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0510Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0500Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0504Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0513Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0511Epoch 11/15: [================              ] 32/60 batches, loss: 0.0503Epoch 11/15: [================              ] 33/60 batches, loss: 0.0505Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0505Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0501Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0506Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0506Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0502Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0506Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0505Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0514Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0512Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0511Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0509Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0505Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0508Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0509Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0507Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0505Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0501Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0504Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0505Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0503Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0511Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0511Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0517Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0519Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0514Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0516Epoch 11/15: [==============================] 60/60 batches, loss: 0.0515
[2025-05-07 20:36:26,152][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0515
[2025-05-07 20:36:26,424][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0378, Metrics: {'mse': 0.036757633090019226, 'rmse': 0.19172280273879586, 'r2': -0.10590803623199463}
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0370Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0650Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0639Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0766Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0725Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0653Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0611Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0582Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0572Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0554Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0557Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0552Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0539Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0536Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0530Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0553Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0547Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0545Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0541Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0546Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0545Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0541Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0533Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0524Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0540Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0546Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0535Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0549Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0549Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0553Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0549Epoch 12/15: [================              ] 32/60 batches, loss: 0.0548Epoch 12/15: [================              ] 33/60 batches, loss: 0.0543Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0550Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0549Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0539Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0534Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0536Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0544Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0547Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0550Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0545Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0547Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0546Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0548Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0552Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0551Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0548Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0544Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0543Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0542Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0543Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0547Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0561Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0557Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0553Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0551Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0546Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0547Epoch 12/15: [==============================] 60/60 batches, loss: 0.0550
[2025-05-07 20:36:28,701][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0550
[2025-05-07 20:36:28,973][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0364, Metrics: {'mse': 0.03550663962960243, 'rmse': 0.18843205573787714, 'r2': -0.06826996803283691}
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0547Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0606Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0507Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0477Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0508Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0481Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0469Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0449Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0457Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0476Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0469Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0462Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0474Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0475Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0469Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0469Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0477Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0475Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0469Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0467Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0494Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0495Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0491Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0500Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0502Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0497Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0495Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0495Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0498Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0502Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0508Epoch 13/15: [================              ] 32/60 batches, loss: 0.0503Epoch 13/15: [================              ] 33/60 batches, loss: 0.0496Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0501Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0507Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0507Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0503Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0501Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0493Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0488Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0496Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0501Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0499Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0498Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0501Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0500Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0495Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0497Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0497Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0501Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0504Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0502Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0498Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0495Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0495Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0493Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0491Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0490Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0489Epoch 13/15: [==============================] 60/60 batches, loss: 0.0485
[2025-05-07 20:36:31,197][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0485
[2025-05-07 20:36:31,474][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0385, Metrics: {'mse': 0.03752109035849571, 'rmse': 0.19370361472748956, 'r2': -0.12887775897979736}
[2025-05-07 20:36:31,474][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0427Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0702Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0621Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0561Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0518Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0532Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0514Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0494Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0498Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0492Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0509Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0500Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0498Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0516Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0508Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0495Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0497Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0499Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0492Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0495Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0499Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0502Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0496Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0493Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0489Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0481Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0490Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0489Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0489Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0489Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0488Epoch 14/15: [================              ] 32/60 batches, loss: 0.0488Epoch 14/15: [================              ] 33/60 batches, loss: 0.0486Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0482Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0480Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0477Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0489Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0492Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0507Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0503Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0499Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0493Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0493Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0492Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0497Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0500Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0496Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0495Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0500Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0514Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0510Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0509Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0502Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0501Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0497Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0495Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0492Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0489Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0487Epoch 14/15: [==============================] 60/60 batches, loss: 0.0485
[2025-05-07 20:36:33,354][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0485
[2025-05-07 20:36:33,638][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0340, Metrics: {'mse': 0.03352262079715729, 'rmse': 0.1830918370576834, 'r2': -0.008577823638916016}
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0461Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0464Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0429Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0409Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0437Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0461Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0502Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0537Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0528Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0544Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0528Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0515Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0505Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0502Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0498Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0491Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0480Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0489Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0485Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0466Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0479Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0474Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0469Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0493Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0494Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0511Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0517Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0508Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0508Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0513Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0513Epoch 15/15: [================              ] 32/60 batches, loss: 0.0505Epoch 15/15: [================              ] 33/60 batches, loss: 0.0503Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0495Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0492Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0496Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0495Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0490Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0491Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0498Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0499Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0497Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0492Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0485Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0482Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0490Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0491Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0491Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0490Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0485Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0483Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0481Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0480Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0476Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0471Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0471Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0474Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0472Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0469Epoch 15/15: [==============================] 60/60 batches, loss: 0.0465
[2025-05-07 20:36:35,903][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0465
[2025-05-07 20:36:36,171][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0379, Metrics: {'mse': 0.03703320771455765, 'rmse': 0.19244014060106496, 'r2': -0.11419904232025146}
[2025-05-07 20:36:36,172][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-07 20:36:36,172][src.training.lm_trainer][INFO] - Training completed in 36.66 seconds
[2025-05-07 20:36:36,172][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:36:38,689][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03218350559473038, 'rmse': 0.17939761869860585, 'r2': -0.005118012428283691}
[2025-05-07 20:36:38,689][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03352262079715729, 'rmse': 0.1830918370576834, 'r2': -0.008577823638916016}
[2025-05-07 20:36:38,689][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03717322647571564, 'rmse': 0.19280359559851482, 'r2': -0.2583831548690796}
[2025-05-07 20:36:40,368][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control1/layer2/id/id/model.pt
[2025-05-07 20:36:40,369][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▄▃▂▂▂▁
wandb:     best_val_mse █▆▅▄▃▂▂▂▁
wandb:      best_val_r2 ▁▃▄▅▆▇▇▇█
wandb:    best_val_rmse █▆▆▄▃▂▂▂▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▁▃▄▅▅▅▅▅▆▆▅▆
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆█▅▄▃▄▄▂▂▂▂▂▁▂
wandb:          val_mse █▅█▅▄▃▄▄▂▂▂▂▂▁▂
wandb:           val_r2 ▁▄▁▄▅▆▅▅▇▇▇▇▇█▇
wandb:         val_rmse █▆█▆▄▃▄▄▂▂▂▂▂▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03405
wandb:     best_val_mse 0.03352
wandb:      best_val_r2 -0.00858
wandb:    best_val_rmse 0.18309
wandb:            epoch 15
wandb:   final_test_mse 0.03717
wandb:    final_test_r2 -0.25838
wandb:  final_test_rmse 0.1928
wandb:  final_train_mse 0.03218
wandb:   final_train_r2 -0.00512
wandb: final_train_rmse 0.1794
wandb:    final_val_mse 0.03352
wandb:     final_val_r2 -0.00858
wandb:   final_val_rmse 0.18309
wandb:    learning_rate 0.0001
wandb:       train_loss 0.0465
wandb:       train_time 36.6563
wandb:         val_loss 0.03792
wandb:          val_mse 0.03703
wandb:           val_r2 -0.1142
wandb:         val_rmse 0.19244
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_203544-6zdhov9u
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_203544-6zdhov9u/logs
Experiment probe_layer2_n_tokens_control1_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control1/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_n_tokens_control2_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_n_tokens_control2_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control2/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=n_tokens"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:37:03,081][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control2/layer2/id
experiment_name: probe_layer2_n_tokens_control2_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: n_tokens
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:37:03,081][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:37:03,081][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 20:37:03,081][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:37:03,081][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:37:03,085][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:37:03,085][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 20:37:03,086][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:37:05,750][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'n_tokens'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:37:08,086][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:37:08,087][src.data.datasets][INFO] - Loading 'control_n_tokens_seed2' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:37:08,253][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_n_tokens_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:51:35 2025).
[2025-05-07 20:37:08,305][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_n_tokens_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:51:35 2025).
[2025-05-07 20:37:08,541][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:37:08,548][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:37:08,549][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:37:08,550][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:37:08,696][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:37:08,781][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:37:08,818][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:37:08,821][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:37:08,821][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:37:08,823][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:37:08,889][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:37:08,936][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:37:08,983][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:37:08,984][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:37:08,985][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:37:08,986][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:37:08,986][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:37:08,987][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 20:37:08,987][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 20:37:08,987][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 20:37:08,987][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:37:08,987][src.data.datasets][INFO] -   Mean: 0.2923, Std: 0.1789
[2025-05-07 20:37:08,987][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:37:08,987][src.data.datasets][INFO] - Sample label: 0.11800000071525574
[2025-05-07 20:37:08,987][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:37:08,988][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 20:37:08,988][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 20:37:08,988][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 20:37:08,988][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:37:08,988][src.data.datasets][INFO] -   Mean: 0.2550, Std: 0.1823
[2025-05-07 20:37:08,988][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:37:08,988][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:37:08,988][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:37:08,988][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 20:37:08,988][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 20:37:08,989][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 20:37:08,989][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:37:08,989][src.data.datasets][INFO] -   Mean: 0.1905, Std: 0.1719
[2025-05-07 20:37:08,989][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:37:08,989][src.data.datasets][INFO] - Sample label: 0.28600001335144043
[2025-05-07 20:37:08,989][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:37:08,989][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:37:08,989][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:37:08,990][__main__][INFO] - Using model type: lm_probe for submetric n_tokens
[2025-05-07 20:37:08,990][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:37:14,894][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:37:14,895][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:37:14,895][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:37:14,895][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:37:14,898][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:37:14,898][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:37:14,898][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:37:14,899][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:37:14,899][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:37:14,900][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:37:14,900][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.2986Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4268Epoch 1/15: [=                             ] 3/60 batches, loss: 0.4673Epoch 1/15: [==                            ] 4/60 batches, loss: 0.4673Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4501Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4205Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4574Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4747Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4877Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4790Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4652Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4627Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4497Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4449Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4413Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4433Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4354Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4431Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4262Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4169Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4128Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4148Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4060Epoch 1/15: [============                  ] 24/60 batches, loss: 0.4007Epoch 1/15: [============                  ] 25/60 batches, loss: 0.3998Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.3970Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3902Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3831Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3847Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3875Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3817Epoch 1/15: [================              ] 32/60 batches, loss: 0.3755Epoch 1/15: [================              ] 33/60 batches, loss: 0.3710Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3692Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3661Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3631Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3564Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3504Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3457Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3426Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3416Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3378Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3344Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3344Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3308Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3273Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3255Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3211Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3202Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3167Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3153Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3147Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3112Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3121Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3108Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3118Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3142Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3126Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3109Epoch 1/15: [==============================] 60/60 batches, loss: 0.3076
[2025-05-07 20:37:20,559][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3076
[2025-05-07 20:37:20,800][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0738, Metrics: {'mse': 0.06968358159065247, 'rmse': 0.26397647923754963, 'r2': -1.096534013748169}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.2226Epoch 2/15: [=                             ] 2/60 batches, loss: 0.1808Epoch 2/15: [=                             ] 3/60 batches, loss: 0.1650Epoch 2/15: [==                            ] 4/60 batches, loss: 0.1716Epoch 2/15: [==                            ] 5/60 batches, loss: 0.1527Epoch 2/15: [===                           ] 6/60 batches, loss: 0.1699Epoch 2/15: [===                           ] 7/60 batches, loss: 0.1717Epoch 2/15: [====                          ] 8/60 batches, loss: 0.1898Epoch 2/15: [====                          ] 9/60 batches, loss: 0.1904Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.1842Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.1803Epoch 2/15: [======                        ] 12/60 batches, loss: 0.1782Epoch 2/15: [======                        ] 13/60 batches, loss: 0.1838Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.1841Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.1784Epoch 2/15: [========                      ] 16/60 batches, loss: 0.1807Epoch 2/15: [========                      ] 17/60 batches, loss: 0.1806Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.1757Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.1759Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1787Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1790Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1836Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1820Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1794Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1849Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1818Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1779Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1777Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1756Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1738Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1735Epoch 2/15: [================              ] 32/60 batches, loss: 0.1731Epoch 2/15: [================              ] 33/60 batches, loss: 0.1717Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1713Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1748Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1728Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1725Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1716Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1716Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1745Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1768Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1761Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1741Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1756Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1739Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1734Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1733Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1726Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1742Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1733Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1722Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1712Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1702Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1690Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1682Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1667Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1659Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1663Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1652Epoch 2/15: [==============================] 60/60 batches, loss: 0.1637
[2025-05-07 20:37:23,053][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1637
[2025-05-07 20:37:23,295][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0690, Metrics: {'mse': 0.06514070928096771, 'rmse': 0.25522678010147704, 'r2': -0.9598550796508789}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.1333Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1308Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1215Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1323Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1239Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1207Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1211Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1253Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1244Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1184Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1197Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1229Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1256Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1235Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1209Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1194Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1174Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1150Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1149Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1141Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1123Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1126Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1095Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1088Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1101Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1110Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1105Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1083Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1079Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1065Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1055Epoch 3/15: [================              ] 32/60 batches, loss: 0.1053Epoch 3/15: [================              ] 33/60 batches, loss: 0.1096Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1075Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1062Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1071Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1072Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1066Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1061Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1047Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1044Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1050Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1071Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1070Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1061Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1053Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1054Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1045Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1051Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1051Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1042Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1061Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1057Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1068Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1060Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1052Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1050Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1042Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1042Epoch 3/15: [==============================] 60/60 batches, loss: 0.1056
[2025-05-07 20:37:25,601][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1056
[2025-05-07 20:37:25,856][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0665, Metrics: {'mse': 0.06302383542060852, 'rmse': 0.2510454847644317, 'r2': -0.8961656093597412}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0340Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0689Epoch 4/15: [=                             ] 3/60 batches, loss: 0.0614Epoch 4/15: [==                            ] 4/60 batches, loss: 0.0880Epoch 4/15: [==                            ] 5/60 batches, loss: 0.0882Epoch 4/15: [===                           ] 6/60 batches, loss: 0.0808Epoch 4/15: [===                           ] 7/60 batches, loss: 0.0839Epoch 4/15: [====                          ] 8/60 batches, loss: 0.0843Epoch 4/15: [====                          ] 9/60 batches, loss: 0.0815Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.0768Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.0787Epoch 4/15: [======                        ] 12/60 batches, loss: 0.0826Epoch 4/15: [======                        ] 13/60 batches, loss: 0.0830Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.0827Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.0809Epoch 4/15: [========                      ] 16/60 batches, loss: 0.0842Epoch 4/15: [========                      ] 17/60 batches, loss: 0.0858Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.0850Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.0830Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.0862Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.0900Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.0929Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.0928Epoch 4/15: [============                  ] 24/60 batches, loss: 0.0967Epoch 4/15: [============                  ] 25/60 batches, loss: 0.0958Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.0960Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.0941Epoch 4/15: [==============                ] 28/60 batches, loss: 0.0965Epoch 4/15: [==============                ] 29/60 batches, loss: 0.0974Epoch 4/15: [===============               ] 30/60 batches, loss: 0.0978Epoch 4/15: [===============               ] 31/60 batches, loss: 0.0975Epoch 4/15: [================              ] 32/60 batches, loss: 0.0959Epoch 4/15: [================              ] 33/60 batches, loss: 0.0953Epoch 4/15: [=================             ] 34/60 batches, loss: 0.0973Epoch 4/15: [=================             ] 35/60 batches, loss: 0.0961Epoch 4/15: [==================            ] 36/60 batches, loss: 0.0945Epoch 4/15: [==================            ] 37/60 batches, loss: 0.0947Epoch 4/15: [===================           ] 38/60 batches, loss: 0.0955Epoch 4/15: [===================           ] 39/60 batches, loss: 0.0964Epoch 4/15: [====================          ] 40/60 batches, loss: 0.0959Epoch 4/15: [====================          ] 41/60 batches, loss: 0.0964Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.0982Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.0975Epoch 4/15: [======================        ] 44/60 batches, loss: 0.0963Epoch 4/15: [======================        ] 45/60 batches, loss: 0.0956Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.0946Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.0929Epoch 4/15: [========================      ] 48/60 batches, loss: 0.0928Epoch 4/15: [========================      ] 49/60 batches, loss: 0.0925Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.0922Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.0936Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.0943Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.0944Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.0937Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.0939Epoch 4/15: [============================  ] 56/60 batches, loss: 0.0940Epoch 4/15: [============================  ] 57/60 batches, loss: 0.0937Epoch 4/15: [============================= ] 58/60 batches, loss: 0.0934Epoch 4/15: [============================= ] 59/60 batches, loss: 0.0929Epoch 4/15: [==============================] 60/60 batches, loss: 0.0923
[2025-05-07 20:37:28,115][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0923
[2025-05-07 20:37:28,390][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0508, Metrics: {'mse': 0.04825370013713837, 'rmse': 0.21966724866747517, 'r2': -0.45178425312042236}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.0674Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0644Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0788Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0945Epoch 5/15: [==                            ] 5/60 batches, loss: 0.1064Epoch 5/15: [===                           ] 6/60 batches, loss: 0.1075Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1010Epoch 5/15: [====                          ] 8/60 batches, loss: 0.0991Epoch 5/15: [====                          ] 9/60 batches, loss: 0.0993Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.1008Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1076Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1087Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1079Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1086Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1074Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1111Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1095Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1075Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1082Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1118Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1103Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1090Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1099Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1085Epoch 5/15: [============                  ] 25/60 batches, loss: 0.1071Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1071Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.1051Epoch 5/15: [==============                ] 28/60 batches, loss: 0.1036Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1045Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1048Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1034Epoch 5/15: [================              ] 32/60 batches, loss: 0.1022Epoch 5/15: [================              ] 33/60 batches, loss: 0.1011Epoch 5/15: [=================             ] 34/60 batches, loss: 0.0995Epoch 5/15: [=================             ] 35/60 batches, loss: 0.1000Epoch 5/15: [==================            ] 36/60 batches, loss: 0.0996Epoch 5/15: [==================            ] 37/60 batches, loss: 0.0997Epoch 5/15: [===================           ] 38/60 batches, loss: 0.0986Epoch 5/15: [===================           ] 39/60 batches, loss: 0.0991Epoch 5/15: [====================          ] 40/60 batches, loss: 0.0984Epoch 5/15: [====================          ] 41/60 batches, loss: 0.0982Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.0983Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.0994Epoch 5/15: [======================        ] 44/60 batches, loss: 0.0993Epoch 5/15: [======================        ] 45/60 batches, loss: 0.0987Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.0980Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0970Epoch 5/15: [========================      ] 48/60 batches, loss: 0.0972Epoch 5/15: [========================      ] 49/60 batches, loss: 0.0964Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.0964Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0956Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0957Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0954Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0948Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0948Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0940Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0937Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0941Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0939Epoch 5/15: [==============================] 60/60 batches, loss: 0.0929
[2025-05-07 20:37:30,608][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0929
[2025-05-07 20:37:30,874][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0524, Metrics: {'mse': 0.0499897226691246, 'rmse': 0.223583815758486, 'r2': -0.5040150880813599}
[2025-05-07 20:37:30,874][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1081Epoch 6/15: [=                             ] 2/60 batches, loss: 0.0765Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0864Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0776Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0716Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0705Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0745Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0850Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0846Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0840Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0826Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0797Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0781Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0762Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0762Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0805Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0789Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0775Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0766Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0756Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0763Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0764Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0750Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0751Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0773Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0763Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0763Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0758Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0747Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0749Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0747Epoch 6/15: [================              ] 32/60 batches, loss: 0.0752Epoch 6/15: [================              ] 33/60 batches, loss: 0.0762Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0757Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0754Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0751Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0756Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0760Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0754Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0756Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0749Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0759Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0747Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0752Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0762Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0752Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0750Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0748Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0739Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0743Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0756Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0752Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0754Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0750Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0748Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0747Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0746Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0742Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0751Epoch 6/15: [==============================] 60/60 batches, loss: 0.0748
[2025-05-07 20:37:32,727][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0748
[2025-05-07 20:37:33,000][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0415, Metrics: {'mse': 0.04000197350978851, 'rmse': 0.20000493371361747, 'r2': -0.20351886749267578}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0582Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0828Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0812Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0784Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0862Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0899Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0913Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0880Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0877Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0824Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0783Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0777Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0795Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0770Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0823Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0806Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0779Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0774Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0758Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0742Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0724Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0721Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0738Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0757Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0750Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0734Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0730Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0720Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0727Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0724Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0736Epoch 7/15: [================              ] 32/60 batches, loss: 0.0742Epoch 7/15: [================              ] 33/60 batches, loss: 0.0740Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0729Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0730Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0723Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0717Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0706Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0701Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0705Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0697Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0711Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0704Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0701Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0696Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0701Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0697Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0699Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0701Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0702Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0700Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0695Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0694Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0697Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0701Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0697Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0696Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0696Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0700Epoch 7/15: [==============================] 60/60 batches, loss: 0.0706
[2025-05-07 20:37:35,219][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0706
[2025-05-07 20:37:35,482][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0469, Metrics: {'mse': 0.044978246092796326, 'rmse': 0.21208075370668675, 'r2': -0.3532373905181885}
[2025-05-07 20:37:35,482][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0743Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0561Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0575Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0553Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0560Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0570Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0598Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0557Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0517Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0533Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0520Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0527Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0527Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0550Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0545Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0547Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0537Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0531Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0520Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0533Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0529Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0532Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0558Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0569Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0567Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0568Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0574Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0584Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0580Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0575Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0579Epoch 8/15: [================              ] 32/60 batches, loss: 0.0577Epoch 8/15: [================              ] 33/60 batches, loss: 0.0573Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0569Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0563Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0560Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0566Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0570Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0570Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0574Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0578Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0586Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0595Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0602Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0599Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0596Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0596Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0601Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0599Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0600Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0606Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0608Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0607Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0605Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0606Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0604Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0599Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0604Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0605Epoch 8/15: [==============================] 60/60 batches, loss: 0.0604
[2025-05-07 20:37:37,341][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0604
[2025-05-07 20:37:37,603][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0477, Metrics: {'mse': 0.0455908328294754, 'rmse': 0.21352009935712235, 'r2': -0.3716679811477661}
[2025-05-07 20:37:37,604][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.1011Epoch 9/15: [=                             ] 2/60 batches, loss: 0.1168Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0947Epoch 9/15: [==                            ] 4/60 batches, loss: 0.1014Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0936Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0842Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0834Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0852Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0784Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0766Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0789Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0777Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0756Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0735Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0733Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0709Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0703Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0691Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0678Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0665Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0665Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0657Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0669Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0649Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0650Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0659Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0662Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0672Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0669Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0673Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0679Epoch 9/15: [================              ] 32/60 batches, loss: 0.0669Epoch 9/15: [================              ] 33/60 batches, loss: 0.0661Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0664Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0673Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0669Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0668Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0663Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0656Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0663Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0663Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0670Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0665Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0659Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0654Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0651Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0649Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0640Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0643Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0644Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0647Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0656Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0653Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0645Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0643Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0641Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0637Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0640Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0639Epoch 9/15: [==============================] 60/60 batches, loss: 0.0637
[2025-05-07 20:37:39,535][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0637
[2025-05-07 20:37:39,803][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0443, Metrics: {'mse': 0.04257799685001373, 'rmse': 0.20634436471591303, 'r2': -0.281022310256958}
[2025-05-07 20:37:39,804][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0496Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0565Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0671Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0579Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0554Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0500Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0511Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0524Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0532Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0538Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0544Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0530Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0536Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0527Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0526Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0523Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0535Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0535Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0538Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0566Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0563Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0578Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0577Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0584Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0579Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0590Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0587Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0590Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0588Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0587Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0584Epoch 10/15: [================              ] 32/60 batches, loss: 0.0581Epoch 10/15: [================              ] 33/60 batches, loss: 0.0582Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0578Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0597Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0600Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0593Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0599Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0607Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0612Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0609Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0605Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0599Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0600Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0598Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0599Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0598Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0595Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0596Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0594Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0591Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0589Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0584Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0578Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0576Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0573Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0573Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0574Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0572Epoch 10/15: [==============================] 60/60 batches, loss: 0.0570
[2025-05-07 20:37:41,674][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0570
[2025-05-07 20:37:41,939][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0429, Metrics: {'mse': 0.04143496975302696, 'rmse': 0.20355581483472035, 'r2': -0.24663269519805908}
[2025-05-07 20:37:41,940][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 20:37:41,940][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 20:37:41,940][src.training.lm_trainer][INFO] - Training completed in 24.15 seconds
[2025-05-07 20:37:41,940][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:37:44,453][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03325842693448067, 'rmse': 0.18236893083658923, 'r2': -0.0386887788772583}
[2025-05-07 20:37:44,453][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.04000197350978851, 'rmse': 0.20000493371361747, 'r2': -0.20351886749267578}
[2025-05-07 20:37:44,453][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04273426905274391, 'rmse': 0.20672268635237864, 'r2': -0.4466346502304077}
[2025-05-07 20:37:46,132][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control2/layer2/id/id/model.pt
[2025-05-07 20:37:46,133][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▆▃▁
wandb:     best_val_mse █▇▆▃▁
wandb:      best_val_r2 ▁▂▃▆█
wandb:    best_val_rmse █▇▇▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▂▅▄▆▅▅▅
wandb:       train_loss █▄▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▆▃▃▁▂▂▂▁
wandb:          val_mse █▇▆▃▃▁▂▂▂▁
wandb:           val_r2 ▁▂▃▆▆█▇▇▇█
wandb:         val_rmse █▇▇▃▄▁▂▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04151
wandb:     best_val_mse 0.04
wandb:      best_val_r2 -0.20352
wandb:    best_val_rmse 0.2
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.04273
wandb:    final_test_r2 -0.44663
wandb:  final_test_rmse 0.20672
wandb:  final_train_mse 0.03326
wandb:   final_train_r2 -0.03869
wandb: final_train_rmse 0.18237
wandb:    final_val_mse 0.04
wandb:     final_val_r2 -0.20352
wandb:   final_val_rmse 0.2
wandb:    learning_rate 0.0001
wandb:       train_loss 0.05703
wandb:       train_time 24.14537
wandb:         val_loss 0.04292
wandb:          val_mse 0.04143
wandb:           val_r2 -0.24663
wandb:         val_rmse 0.20356
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_203703-4ks0rez2
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_203703-4ks0rez2/logs
Experiment probe_layer2_n_tokens_control2_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control2/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_n_tokens_control3_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_n_tokens_control3_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control3/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=n_tokens"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:38:13,553][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control3/layer2/id
experiment_name: probe_layer2_n_tokens_control3_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: n_tokens
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:38:13,553][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:38:13,553][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 20:38:13,553][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:38:13,553][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:38:13,557][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:38:13,557][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 20:38:13,557][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:38:16,563][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'n_tokens'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:38:18,865][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:38:18,865][src.data.datasets][INFO] - Loading 'control_n_tokens_seed3' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:38:19,049][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_n_tokens_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:52:51 2025).
[2025-05-07 20:38:19,147][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_n_tokens_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:52:51 2025).
[2025-05-07 20:38:19,426][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:38:19,433][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:38:19,434][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:38:19,435][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:38:19,560][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:38:19,702][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:38:19,731][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:38:19,732][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:38:19,732][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:38:19,734][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:38:19,781][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:38:19,895][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:38:19,927][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:38:19,929][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:38:19,929][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:38:19,931][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:38:19,931][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:38:19,931][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 20:38:19,931][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 20:38:19,932][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 20:38:19,932][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:38:19,932][src.data.datasets][INFO] -   Mean: 0.2923, Std: 0.1789
[2025-05-07 20:38:19,932][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:38:19,932][src.data.datasets][INFO] - Sample label: 0.11800000071525574
[2025-05-07 20:38:19,932][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:38:19,932][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 20:38:19,932][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 20:38:19,933][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 20:38:19,933][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:38:19,933][src.data.datasets][INFO] -   Mean: 0.2550, Std: 0.1823
[2025-05-07 20:38:19,933][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:38:19,933][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:38:19,933][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:38:19,933][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 20:38:19,933][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 20:38:19,933][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 20:38:19,933][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:38:19,934][src.data.datasets][INFO] -   Mean: 0.1905, Std: 0.1719
[2025-05-07 20:38:19,934][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:38:19,934][src.data.datasets][INFO] - Sample label: 0.28600001335144043
[2025-05-07 20:38:19,934][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:38:19,934][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:38:19,934][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:38:19,934][__main__][INFO] - Using model type: lm_probe for submetric n_tokens
[2025-05-07 20:38:19,935][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:38:27,186][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:38:27,187][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:38:27,187][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:38:27,187][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:38:27,190][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:38:27,191][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:38:27,191][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:38:27,191][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:38:27,191][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:38:27,192][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:38:27,192][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.3997Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4786Epoch 1/15: [=                             ] 3/60 batches, loss: 0.4863Epoch 1/15: [==                            ] 4/60 batches, loss: 0.5098Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4783Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4448Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4797Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4804Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4982Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4901Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4754Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4715Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4536Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4421Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4341Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4368Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4379Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4471Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4321Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4217Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4137Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4121Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4044Epoch 1/15: [============                  ] 24/60 batches, loss: 0.3978Epoch 1/15: [============                  ] 25/60 batches, loss: 0.3967Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.3958Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3908Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3874Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3892Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3919Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3858Epoch 1/15: [================              ] 32/60 batches, loss: 0.3801Epoch 1/15: [================              ] 33/60 batches, loss: 0.3753Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3733Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3690Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3695Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3645Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3598Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3588Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3551Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3553Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3551Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3516Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3512Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3474Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3428Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3411Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3363Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3351Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3313Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3308Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3288Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3270Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3267Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3243Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3240Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3251Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3232Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3208Epoch 1/15: [==============================] 60/60 batches, loss: 0.3190
[2025-05-07 20:38:32,790][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3190
[2025-05-07 20:38:33,024][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0801, Metrics: {'mse': 0.0759357213973999, 'rmse': 0.2755643688821178, 'r2': -1.2846391201019287}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.3094Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2317Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2019Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2108Epoch 2/15: [==                            ] 5/60 batches, loss: 0.1879Epoch 2/15: [===                           ] 6/60 batches, loss: 0.1881Epoch 2/15: [===                           ] 7/60 batches, loss: 0.1989Epoch 2/15: [====                          ] 8/60 batches, loss: 0.2035Epoch 2/15: [====                          ] 9/60 batches, loss: 0.2032Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.1942Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.1968Epoch 2/15: [======                        ] 12/60 batches, loss: 0.1917Epoch 2/15: [======                        ] 13/60 batches, loss: 0.1975Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.2034Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.1959Epoch 2/15: [========                      ] 16/60 batches, loss: 0.1962Epoch 2/15: [========                      ] 17/60 batches, loss: 0.1945Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.1940Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.1963Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1948Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1904Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1907Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1884Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1840Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1855Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1861Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1853Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1818Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1802Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1802Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1787Epoch 2/15: [================              ] 32/60 batches, loss: 0.1764Epoch 2/15: [================              ] 33/60 batches, loss: 0.1760Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1773Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1782Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1765Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1755Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1743Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1746Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1766Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1768Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1752Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1745Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1757Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1748Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1735Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1716Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1714Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1723Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1721Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1718Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1715Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1705Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1691Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1682Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1665Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1662Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1653Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1640Epoch 2/15: [==============================] 60/60 batches, loss: 0.1627
[2025-05-07 20:38:35,349][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1627
[2025-05-07 20:38:35,585][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0695, Metrics: {'mse': 0.06582147628068924, 'rmse': 0.2565569649818325, 'r2': -0.9803369045257568}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.0937Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1005Epoch 3/15: [=                             ] 3/60 batches, loss: 0.0844Epoch 3/15: [==                            ] 4/60 batches, loss: 0.0897Epoch 3/15: [==                            ] 5/60 batches, loss: 0.0961Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1052Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1011Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1091Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1133Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1119Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1143Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1149Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1177Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1184Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1216Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1226Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1238Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1200Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1189Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1180Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1159Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1150Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1145Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1139Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1144Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1144Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1127Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1123Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1128Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1117Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1122Epoch 3/15: [================              ] 32/60 batches, loss: 0.1115Epoch 3/15: [================              ] 33/60 batches, loss: 0.1153Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1137Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1125Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1122Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1104Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1100Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1118Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1113Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1113Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1120Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1129Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1125Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1113Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1110Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1116Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1115Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1130Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1130Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1126Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1129Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1124Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1148Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1142Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1134Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1145Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1135Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1130Epoch 3/15: [==============================] 60/60 batches, loss: 0.1149
[2025-05-07 20:38:37,847][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1149
[2025-05-07 20:38:38,114][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0560, Metrics: {'mse': 0.05324076861143112, 'rmse': 0.23073961214197947, 'r2': -0.6018277406692505}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0824Epoch 4/15: [=                             ] 2/60 batches, loss: 0.1076Epoch 4/15: [=                             ] 3/60 batches, loss: 0.0951Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1271Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1187Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1098Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1087Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1019Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1003Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.0933Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.0976Epoch 4/15: [======                        ] 12/60 batches, loss: 0.0991Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1016Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.1012Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.1009Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1039Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1044Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1068Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1057Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1082Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1096Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1089Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1103Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1113Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1108Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1120Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1096Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1106Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1102Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1087Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1086Epoch 4/15: [================              ] 32/60 batches, loss: 0.1070Epoch 4/15: [================              ] 33/60 batches, loss: 0.1079Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1072Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1066Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1062Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1050Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1051Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1035Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1022Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1023Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1016Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1014Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1004Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1010Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.0996Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.0980Epoch 4/15: [========================      ] 48/60 batches, loss: 0.0977Epoch 4/15: [========================      ] 49/60 batches, loss: 0.0978Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.0982Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.0980Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1001Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.0994Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1000Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.0996Epoch 4/15: [============================  ] 56/60 batches, loss: 0.0996Epoch 4/15: [============================  ] 57/60 batches, loss: 0.0992Epoch 4/15: [============================= ] 58/60 batches, loss: 0.0989Epoch 4/15: [============================= ] 59/60 batches, loss: 0.0980Epoch 4/15: [==============================] 60/60 batches, loss: 0.0977
[2025-05-07 20:38:40,306][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0977
[2025-05-07 20:38:40,565][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0538, Metrics: {'mse': 0.05142779275774956, 'rmse': 0.22677696699124794, 'r2': -0.5472815036773682}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.0550Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0767Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0920Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0951Epoch 5/15: [==                            ] 5/60 batches, loss: 0.1028Epoch 5/15: [===                           ] 6/60 batches, loss: 0.0978Epoch 5/15: [===                           ] 7/60 batches, loss: 0.0965Epoch 5/15: [====                          ] 8/60 batches, loss: 0.0985Epoch 5/15: [====                          ] 9/60 batches, loss: 0.0945Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.0973Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1012Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1080Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1061Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1137Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1112Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1133Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1148Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1140Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1123Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1128Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1130Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1108Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1110Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1085Epoch 5/15: [============                  ] 25/60 batches, loss: 0.1071Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1064Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.1043Epoch 5/15: [==============                ] 28/60 batches, loss: 0.1037Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1055Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1045Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1037Epoch 5/15: [================              ] 32/60 batches, loss: 0.1020Epoch 5/15: [================              ] 33/60 batches, loss: 0.1004Epoch 5/15: [=================             ] 34/60 batches, loss: 0.0991Epoch 5/15: [=================             ] 35/60 batches, loss: 0.0983Epoch 5/15: [==================            ] 36/60 batches, loss: 0.0971Epoch 5/15: [==================            ] 37/60 batches, loss: 0.0963Epoch 5/15: [===================           ] 38/60 batches, loss: 0.0958Epoch 5/15: [===================           ] 39/60 batches, loss: 0.0967Epoch 5/15: [====================          ] 40/60 batches, loss: 0.0956Epoch 5/15: [====================          ] 41/60 batches, loss: 0.0961Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.0968Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.0977Epoch 5/15: [======================        ] 44/60 batches, loss: 0.0972Epoch 5/15: [======================        ] 45/60 batches, loss: 0.0967Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.0960Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0946Epoch 5/15: [========================      ] 48/60 batches, loss: 0.0943Epoch 5/15: [========================      ] 49/60 batches, loss: 0.0945Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.0943Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0932Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0924Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0925Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0919Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0916Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0912Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0911Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0920Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0912Epoch 5/15: [==============================] 60/60 batches, loss: 0.0916
[2025-05-07 20:38:42,779][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0916
[2025-05-07 20:38:43,040][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0495, Metrics: {'mse': 0.047414012253284454, 'rmse': 0.21774758839832062, 'r2': -0.42652106285095215}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1358Epoch 6/15: [=                             ] 2/60 batches, loss: 0.1093Epoch 6/15: [=                             ] 3/60 batches, loss: 0.1028Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0880Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0887Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0904Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0870Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0932Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0921Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0884Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0856Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0818Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0813Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0824Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0832Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0845Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0816Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0795Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0780Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0764Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0751Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0752Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0759Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0762Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0769Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0773Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0774Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0759Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0761Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0758Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0755Epoch 6/15: [================              ] 32/60 batches, loss: 0.0752Epoch 6/15: [================              ] 33/60 batches, loss: 0.0783Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0779Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0777Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0775Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0778Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0783Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0783Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0784Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0773Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0770Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0758Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0750Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0754Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0741Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0748Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0747Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0747Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0741Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0747Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0744Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0751Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0746Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0749Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0747Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0745Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0746Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0744Epoch 6/15: [==============================] 60/60 batches, loss: 0.0737
[2025-05-07 20:38:45,261][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0737
[2025-05-07 20:38:45,530][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0433, Metrics: {'mse': 0.04159313440322876, 'rmse': 0.20394394917042466, 'r2': -0.25139129161834717}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0450Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0958Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0922Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0912Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0945Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0843Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0818Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0821Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0816Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0789Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0750Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0718Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0724Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0715Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0721Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0733Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0725Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0729Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0734Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0717Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0703Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0695Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0706Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0709Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0702Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0699Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0698Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0683Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0690Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0679Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0686Epoch 7/15: [================              ] 32/60 batches, loss: 0.0687Epoch 7/15: [================              ] 33/60 batches, loss: 0.0695Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0689Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0695Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0701Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0690Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0687Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0679Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0685Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0689Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0689Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0680Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0681Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0683Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0688Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0687Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0690Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0686Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0695Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0693Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0691Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0685Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0690Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0684Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0684Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0690Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0683Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0683Epoch 7/15: [==============================] 60/60 batches, loss: 0.0692
[2025-05-07 20:38:47,791][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0692
[2025-05-07 20:38:48,124][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0410, Metrics: {'mse': 0.03959893435239792, 'rmse': 0.19899480986296583, 'r2': -0.19139277935028076}
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0636Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0603Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0728Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0695Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0668Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0630Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0599Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0567Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0556Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0564Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0563Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0557Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0542Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0551Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0558Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0545Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0543Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0537Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0543Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0539Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0525Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0532Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0537Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0541Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0536Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0529Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0538Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0540Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0544Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0538Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0544Epoch 8/15: [================              ] 32/60 batches, loss: 0.0545Epoch 8/15: [================              ] 33/60 batches, loss: 0.0548Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0547Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0544Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0548Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0565Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0574Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0577Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0574Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0584Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0581Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0583Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0591Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0588Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0582Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0583Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0581Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0579Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0575Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0583Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0583Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0581Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0584Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0584Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0578Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0578Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0583Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0585Epoch 8/15: [==============================] 60/60 batches, loss: 0.0589
[2025-05-07 20:38:50,424][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0589
[2025-05-07 20:38:50,734][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0395, Metrics: {'mse': 0.038200926035642624, 'rmse': 0.1954505718478271, 'r2': -0.14933156967163086}
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.1127Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0960Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0798Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0735Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0701Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0647Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0656Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0699Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0682Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0665Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0658Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0649Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0627Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0636Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0653Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0638Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0637Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0662Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0655Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0644Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0641Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0646Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0642Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0650Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0651Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0651Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0652Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0655Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0657Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0663Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0656Epoch 9/15: [================              ] 32/60 batches, loss: 0.0661Epoch 9/15: [================              ] 33/60 batches, loss: 0.0653Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0654Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0651Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0655Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0657Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0658Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0668Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0689Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0682Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0680Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0672Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0675Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0681Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0674Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0674Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0670Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0672Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0667Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0664Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0666Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0659Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0651Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0646Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0652Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0648Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0646Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0647Epoch 9/15: [==============================] 60/60 batches, loss: 0.0645
[2025-05-07 20:38:52,985][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0645
[2025-05-07 20:38:53,258][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0401, Metrics: {'mse': 0.03883431851863861, 'rmse': 0.19706424972236494, 'r2': -0.1683882474899292}
[2025-05-07 20:38:53,258][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0561Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0469Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0454Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0560Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0581Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0537Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0526Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0550Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0563Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0563Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0551Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0548Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0545Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0544Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0560Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0554Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0546Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0531Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0537Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0546Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0550Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0571Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0570Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0576Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0570Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0566Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0573Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0570Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0580Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0572Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0572Epoch 10/15: [================              ] 32/60 batches, loss: 0.0568Epoch 10/15: [================              ] 33/60 batches, loss: 0.0565Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0555Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0558Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0552Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0549Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0546Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0547Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0545Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0554Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0550Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0550Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0561Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0554Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0555Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0552Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0550Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0549Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0553Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0558Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0563Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0559Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0552Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0555Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0558Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0556Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0549Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0544Epoch 10/15: [==============================] 60/60 batches, loss: 0.0542
[2025-05-07 20:38:55,159][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0542
[2025-05-07 20:38:55,428][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0396, Metrics: {'mse': 0.0384662039577961, 'rmse': 0.19612802950571878, 'r2': -0.15731298923492432}
[2025-05-07 20:38:55,428][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0329Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0445Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0416Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0447Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0540Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0543Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0550Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0569Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0574Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0556Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0559Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0577Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0555Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0550Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0539Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0531Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0519Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0536Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0539Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0527Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0547Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0549Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0536Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0529Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0542Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0534Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0531Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0526Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0520Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0517Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0512Epoch 11/15: [================              ] 32/60 batches, loss: 0.0523Epoch 11/15: [================              ] 33/60 batches, loss: 0.0520Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0520Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0516Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0514Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0520Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0515Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0516Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0528Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0523Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0515Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0522Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0520Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0515Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0512Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0509Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0507Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0506Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0505Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0505Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0515Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0512Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0519Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0516Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0518Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0513Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0509Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0507Epoch 11/15: [==============================] 60/60 batches, loss: 0.0509
[2025-05-07 20:38:57,275][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0509
[2025-05-07 20:38:57,549][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0412, Metrics: {'mse': 0.03989221528172493, 'rmse': 0.1997303564351822, 'r2': -0.2002166509628296}
[2025-05-07 20:38:57,550][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0486Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0499Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0475Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0518Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0574Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0551Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0516Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0494Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0469Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0478Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0479Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0508Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0502Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0506Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0504Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0520Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0503Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0514Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0523Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0525Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0519Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0519Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0520Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0520Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0528Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0525Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0526Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0522Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0534Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0530Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0537Epoch 12/15: [================              ] 32/60 batches, loss: 0.0541Epoch 12/15: [================              ] 33/60 batches, loss: 0.0552Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0554Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0556Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0550Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0544Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0556Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0554Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0552Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0558Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0555Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0554Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0550Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0550Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0552Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0550Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0546Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0542Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0536Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0538Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0531Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0531Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0538Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0537Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0532Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0534Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0534Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0534Epoch 12/15: [==============================] 60/60 batches, loss: 0.0531
[2025-05-07 20:38:59,496][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0531
[2025-05-07 20:38:59,851][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0391, Metrics: {'mse': 0.03803026303648949, 'rmse': 0.19501349449843078, 'r2': -0.1441969871520996}
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0754Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0651Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0603Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0603Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0586Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0533Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0526Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0553Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0544Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0544Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0502Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0508Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0528Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0539Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0518Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0529Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0528Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0528Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0554Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0555Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0549Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0545Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0539Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0543Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0555Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0540Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0541Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0538Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0532Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0543Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0541Epoch 13/15: [================              ] 32/60 batches, loss: 0.0535Epoch 13/15: [================              ] 33/60 batches, loss: 0.0544Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0541Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0542Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0541Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0536Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0530Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0537Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0540Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0540Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0538Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0541Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0538Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0534Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0534Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0533Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0533Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0529Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0524Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0528Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0527Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0526Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0520Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0522Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0520Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0523Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0521Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0519Epoch 13/15: [==============================] 60/60 batches, loss: 0.0517
[2025-05-07 20:39:02,132][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0517
[2025-05-07 20:39:02,418][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0362, Metrics: {'mse': 0.035406846553087234, 'rmse': 0.18816707085217443, 'r2': -0.06526768207550049}
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0513Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0497Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0647Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0576Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0550Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0608Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0600Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0619Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0617Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0600Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0576Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0545Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0555Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0551Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0537Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0522Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0515Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0503Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0510Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0512Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0512Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0513Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0509Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0503Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0507Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0504Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0502Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0492Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0493Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0496Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0489Epoch 14/15: [================              ] 32/60 batches, loss: 0.0489Epoch 14/15: [================              ] 33/60 batches, loss: 0.0494Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0488Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0482Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0480Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0486Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0483Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0482Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0478Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0484Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0485Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0481Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0481Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0477Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0473Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0470Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0468Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0469Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0471Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0472Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0474Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0470Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0474Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0470Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0468Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0466Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0467Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0469Epoch 14/15: [==============================] 60/60 batches, loss: 0.0467
[2025-05-07 20:39:04,731][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0467
[2025-05-07 20:39:04,997][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0385, Metrics: {'mse': 0.03761148452758789, 'rmse': 0.19393680550011103, 'r2': -0.1315973997116089}
[2025-05-07 20:39:04,998][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0435Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0513Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0552Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0596Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0549Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0549Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0552Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0524Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0576Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0602Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0610Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0604Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0581Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0582Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0563Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0559Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0562Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0575Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0568Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0550Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0543Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0543Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0537Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0533Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0534Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0524Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0524Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0525Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0518Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0519Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0512Epoch 15/15: [================              ] 32/60 batches, loss: 0.0510Epoch 15/15: [================              ] 33/60 batches, loss: 0.0506Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0500Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0493Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0490Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0484Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0480Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0475Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0474Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0474Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0472Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0468Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0463Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0461Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0459Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0457Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0459Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0460Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0455Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0463Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0460Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0463Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0461Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0465Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0466Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0470Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0471Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0471Epoch 15/15: [==============================] 60/60 batches, loss: 0.0470
[2025-05-07 20:39:06,873][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0470
[2025-05-07 20:39:07,135][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0372, Metrics: {'mse': 0.03632282465696335, 'rmse': 0.19058547860989658, 'r2': -0.092826247215271}
[2025-05-07 20:39:07,136][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
[2025-05-07 20:39:07,136][src.training.lm_trainer][INFO] - Training completed in 37.03 seconds
[2025-05-07 20:39:07,136][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:39:09,622][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03398488089442253, 'rmse': 0.1843498871559799, 'r2': -0.06137645244598389}
[2025-05-07 20:39:09,622][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.035406846553087234, 'rmse': 0.18816707085217443, 'r2': -0.06526768207550049}
[2025-05-07 20:39:09,622][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03460120037198067, 'rmse': 0.18601397896927174, 'r2': -0.17131519317626953}
[2025-05-07 20:39:11,286][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control3/layer2/id/id/model.pt
[2025-05-07 20:39:11,288][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▄▄▃▂▂▂▁▁
wandb:     best_val_mse █▆▄▄▃▂▂▁▁▁
wandb:      best_val_r2 ▁▃▅▅▆▇▇███
wandb:    best_val_rmse █▆▄▄▃▂▂▂▂▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▅▅▅▆▆▆▆▆▆▆▇▆
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▄▄▃▂▂▂▂▂▂▁▁▁▁
wandb:          val_mse █▆▄▄▃▂▂▁▂▂▂▁▁▁▁
wandb:           val_r2 ▁▃▅▅▆▇▇█▇▇▇████
wandb:         val_rmse █▆▄▄▃▂▂▂▂▂▂▂▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03618
wandb:     best_val_mse 0.03541
wandb:      best_val_r2 -0.06527
wandb:    best_val_rmse 0.18817
wandb:            epoch 15
wandb:   final_test_mse 0.0346
wandb:    final_test_r2 -0.17132
wandb:  final_test_rmse 0.18601
wandb:  final_train_mse 0.03398
wandb:   final_train_r2 -0.06138
wandb: final_train_rmse 0.18435
wandb:    final_val_mse 0.03541
wandb:     final_val_r2 -0.06527
wandb:   final_val_rmse 0.18817
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04695
wandb:       train_time 37.03386
wandb:         val_loss 0.03717
wandb:          val_mse 0.03632
wandb:           val_r2 -0.09283
wandb:         val_rmse 0.19059
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_203813-3wobznpk
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_203813-3wobznpk/logs
Experiment probe_layer2_n_tokens_control3_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control3/layer2/id/id/results.json for layer 2
Some experiments failed. See /scratch/leuven/371/vsc37132/makeup_probes_output/failed_experiments.log for details.
Failed experiments (2):
probe_layer2_n_tokens,_fi
probe_layer2_avg_verb_edges,_fi
==============================================
probing experiments completed!
 planned experiments: 28
 completed: 100
