SLURM_JOB_ID: 64435715
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: finetune_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Thu May  1 12:12:03 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main finetuning experiments (non-control)...
Running experiment: finetune_question_type_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 12:12:21,635][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type
experiment_name: finetune_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-01 12:12:21,635][__main__][INFO] - Normalized task: question_type
[2025-05-01 12:12:21,635][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-01 12:12:21,636][__main__][INFO] - Determined Task Type: classification
[2025-05-01 12:12:21,640][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-01 12:12:21,640][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 12:12:23,560][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 12:12:25,775][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 12:12:25,775][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:12:25,876][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:12:25,927][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:12:26,037][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-01 12:12:26,044][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:12:26,044][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-01 12:12:26,046][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:12:26,068][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:12:26,103][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:12:26,117][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-01 12:12:26,118][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:12:26,118][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-01 12:12:26,120][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:12:26,142][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:12:26,180][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:12:26,196][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-01 12:12:26,197][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:12:26,197][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-01 12:12:26,198][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-01 12:12:26,198][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:12:26,198][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:12:26,199][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:12:26,199][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:12:26,199][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-01 12:12:26,199][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-01 12:12:26,199][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-01 12:12:26,199][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 12:12:26,199][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:12:26,199][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:12:26,199][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:12:26,199][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:12:26,200][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-01 12:12:26,200][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-01 12:12:26,200][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-01 12:12:26,200][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 12:12:26,200][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:12:26,200][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:12:26,200][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:12:26,200][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:12:26,200][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-01 12:12:26,200][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-01 12:12:26,201][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-01 12:12:26,201][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 12:12:26,201][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-01 12:12:26,201][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 12:12:26,201][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 12:12:26,201][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-01 12:12:26,201][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 12:12:31,160][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 12:12:31,161][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 12:12:31,161][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 12:12:31,161][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 12:12:31,165][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 12:12:31,166][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 12:12:31,166][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 12:12:31,166][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-01 12:12:31,167][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 12:12:31,167][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.8145Epoch 1/10: [                              ] 2/63 batches, loss: 0.8446Epoch 1/10: [=                             ] 3/63 batches, loss: 0.7718Epoch 1/10: [=                             ] 4/63 batches, loss: 0.7524Epoch 1/10: [==                            ] 5/63 batches, loss: 0.7532Epoch 1/10: [==                            ] 6/63 batches, loss: 0.7585Epoch 1/10: [===                           ] 7/63 batches, loss: 0.7449Epoch 1/10: [===                           ] 8/63 batches, loss: 0.7424Epoch 1/10: [====                          ] 9/63 batches, loss: 0.7300Epoch 1/10: [====                          ] 10/63 batches, loss: 0.7407Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.7360Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.7349Epoch 1/10: [======                        ] 13/63 batches, loss: 0.7268Epoch 1/10: [======                        ] 14/63 batches, loss: 0.7327Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.7320Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.7330Epoch 1/10: [========                      ] 17/63 batches, loss: 0.7372Epoch 1/10: [========                      ] 18/63 batches, loss: 0.7427Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.7398Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.7346Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.7308Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.7302Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.7258Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.7253Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.7248Epoch 1/10: [============                  ] 26/63 batches, loss: 0.7245Epoch 1/10: [============                  ] 27/63 batches, loss: 0.7265Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.7232Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.7250Epoch 1/10: [==============                ] 30/63 batches, loss: 0.7245Epoch 1/10: [==============                ] 31/63 batches, loss: 0.7214Epoch 1/10: [===============               ] 32/63 batches, loss: 0.7210Epoch 1/10: [===============               ] 33/63 batches, loss: 0.7208Epoch 1/10: [================              ] 34/63 batches, loss: 0.7221Epoch 1/10: [================              ] 35/63 batches, loss: 0.7219Epoch 1/10: [=================             ] 36/63 batches, loss: 0.7234Epoch 1/10: [=================             ] 37/63 batches, loss: 0.7241Epoch 1/10: [==================            ] 38/63 batches, loss: 0.7213Epoch 1/10: [==================            ] 39/63 batches, loss: 0.7219Epoch 1/10: [===================           ] 40/63 batches, loss: 0.7217Epoch 1/10: [===================           ] 41/63 batches, loss: 0.7207Epoch 1/10: [====================          ] 42/63 batches, loss: 0.7211Epoch 1/10: [====================          ] 43/63 batches, loss: 0.7201Epoch 1/10: [====================          ] 44/63 batches, loss: 0.7200Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.7206Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.7215Epoch 1/10: [======================        ] 47/63 batches, loss: 0.7212Epoch 1/10: [======================        ] 48/63 batches, loss: 0.7190Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.7185Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.7166Epoch 1/10: [========================      ] 51/63 batches, loss: 0.7153Epoch 1/10: [========================      ] 52/63 batches, loss: 0.7172Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.7159Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.7157Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.7160Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.7148Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.7164Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.7147Epoch 1/10: [============================  ] 59/63 batches, loss: 0.7147Epoch 1/10: [============================  ] 60/63 batches, loss: 0.7137Epoch 1/10: [============================= ] 61/63 batches, loss: 0.7126Epoch 1/10: [============================= ] 62/63 batches, loss: 0.7117Epoch 1/10: [==============================] 63/63 batches, loss: 0.7132
[2025-05-01 12:12:40,466][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.7132
[2025-05-01 12:12:40,685][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.7450, Metrics: {'accuracy': 0.75, 'f1': 0.7843137254901961, 'precision': 0.6451612903225806, 'recall': 1.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.7625Epoch 2/10: [                              ] 2/63 batches, loss: 0.7733Epoch 2/10: [=                             ] 3/63 batches, loss: 0.7537Epoch 2/10: [=                             ] 4/63 batches, loss: 0.7262Epoch 2/10: [==                            ] 5/63 batches, loss: 0.7238Epoch 2/10: [==                            ] 6/63 batches, loss: 0.7200Epoch 2/10: [===                           ] 7/63 batches, loss: 0.7277Epoch 2/10: [===                           ] 8/63 batches, loss: 0.7272Epoch 2/10: [====                          ] 9/63 batches, loss: 0.7212Epoch 2/10: [====                          ] 10/63 batches, loss: 0.7156Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.7228Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.7157Epoch 2/10: [======                        ] 13/63 batches, loss: 0.7044Epoch 2/10: [======                        ] 14/63 batches, loss: 0.6984Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.6949Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.6984Epoch 2/10: [========                      ] 17/63 batches, loss: 0.6938Epoch 2/10: [========                      ] 18/63 batches, loss: 0.6917Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.6919Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.6906Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.6866Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.6902Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.6886Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.6851Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.6877Epoch 2/10: [============                  ] 26/63 batches, loss: 0.6925Epoch 2/10: [============                  ] 27/63 batches, loss: 0.6933Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.6917Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.6880Epoch 2/10: [==============                ] 30/63 batches, loss: 0.6829Epoch 2/10: [==============                ] 31/63 batches, loss: 0.6802Epoch 2/10: [===============               ] 32/63 batches, loss: 0.6788Epoch 2/10: [===============               ] 33/63 batches, loss: 0.6767Epoch 2/10: [================              ] 34/63 batches, loss: 0.6758Epoch 2/10: [================              ] 35/63 batches, loss: 0.6757Epoch 2/10: [=================             ] 36/63 batches, loss: 0.6719Epoch 2/10: [=================             ] 37/63 batches, loss: 0.6684Epoch 2/10: [==================            ] 38/63 batches, loss: 0.6657Epoch 2/10: [==================            ] 39/63 batches, loss: 0.6629Epoch 2/10: [===================           ] 40/63 batches, loss: 0.6609Epoch 2/10: [===================           ] 41/63 batches, loss: 0.6594Epoch 2/10: [====================          ] 42/63 batches, loss: 0.6601Epoch 2/10: [====================          ] 43/63 batches, loss: 0.6576Epoch 2/10: [====================          ] 44/63 batches, loss: 0.6539Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.6510Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.6495Epoch 2/10: [======================        ] 47/63 batches, loss: 0.6459Epoch 2/10: [======================        ] 48/63 batches, loss: 0.6424Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.6424Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.6408Epoch 2/10: [========================      ] 51/63 batches, loss: 0.6394Epoch 2/10: [========================      ] 52/63 batches, loss: 0.6372Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.6370Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.6357Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.6345Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.6338Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.6323Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.6305Epoch 2/10: [============================  ] 59/63 batches, loss: 0.6284Epoch 2/10: [============================  ] 60/63 batches, loss: 0.6270Epoch 2/10: [============================= ] 61/63 batches, loss: 0.6262Epoch 2/10: [============================= ] 62/63 batches, loss: 0.6235Epoch 2/10: [==============================] 63/63 batches, loss: 0.6238
[2025-05-01 12:12:47,416][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6238
[2025-05-01 12:12:47,595][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5717, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.6418Epoch 3/10: [                              ] 2/63 batches, loss: 0.6040Epoch 3/10: [=                             ] 3/63 batches, loss: 0.5731Epoch 3/10: [=                             ] 4/63 batches, loss: 0.5429Epoch 3/10: [==                            ] 5/63 batches, loss: 0.5245Epoch 3/10: [==                            ] 6/63 batches, loss: 0.5065Epoch 3/10: [===                           ] 7/63 batches, loss: 0.5174Epoch 3/10: [===                           ] 8/63 batches, loss: 0.5236Epoch 3/10: [====                          ] 9/63 batches, loss: 0.5277Epoch 3/10: [====                          ] 10/63 batches, loss: 0.5290Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.5317Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.5225Epoch 3/10: [======                        ] 13/63 batches, loss: 0.5197Epoch 3/10: [======                        ] 14/63 batches, loss: 0.5229Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.5190Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.5170Epoch 3/10: [========                      ] 17/63 batches, loss: 0.5166Epoch 3/10: [========                      ] 18/63 batches, loss: 0.5137Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.5125Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.5101Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.5125Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.5125Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.5134Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.5132Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.5148Epoch 3/10: [============                  ] 26/63 batches, loss: 0.5146Epoch 3/10: [============                  ] 27/63 batches, loss: 0.5160Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.5191Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.5195Epoch 3/10: [==============                ] 30/63 batches, loss: 0.5167Epoch 3/10: [==============                ] 31/63 batches, loss: 0.5141Epoch 3/10: [===============               ] 32/63 batches, loss: 0.5139Epoch 3/10: [===============               ] 33/63 batches, loss: 0.5144Epoch 3/10: [================              ] 34/63 batches, loss: 0.5156Epoch 3/10: [================              ] 35/63 batches, loss: 0.5174Epoch 3/10: [=================             ] 36/63 batches, loss: 0.5164Epoch 3/10: [=================             ] 37/63 batches, loss: 0.5162Epoch 3/10: [==================            ] 38/63 batches, loss: 0.5159Epoch 3/10: [==================            ] 39/63 batches, loss: 0.5138Epoch 3/10: [===================           ] 40/63 batches, loss: 0.5137Epoch 3/10: [===================           ] 41/63 batches, loss: 0.5140Epoch 3/10: [====================          ] 42/63 batches, loss: 0.5139Epoch 3/10: [====================          ] 43/63 batches, loss: 0.5125Epoch 3/10: [====================          ] 44/63 batches, loss: 0.5146Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.5133Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.5137Epoch 3/10: [======================        ] 47/63 batches, loss: 0.5130Epoch 3/10: [======================        ] 48/63 batches, loss: 0.5138Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.5137Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.5130Epoch 3/10: [========================      ] 51/63 batches, loss: 0.5115Epoch 3/10: [========================      ] 52/63 batches, loss: 0.5114Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.5122Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.5120Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.5136Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.5131Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.5125Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.5123Epoch 3/10: [============================  ] 59/63 batches, loss: 0.5126Epoch 3/10: [============================  ] 60/63 batches, loss: 0.5121Epoch 3/10: [============================= ] 61/63 batches, loss: 0.5128Epoch 3/10: [============================= ] 62/63 batches, loss: 0.5115Epoch 3/10: [==============================] 63/63 batches, loss: 0.5144
[2025-05-01 12:12:54,340][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5144
[2025-05-01 12:12:54,539][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.5252, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.4093Epoch 4/10: [                              ] 2/63 batches, loss: 0.4450Epoch 4/10: [=                             ] 3/63 batches, loss: 0.4568Epoch 4/10: [=                             ] 4/63 batches, loss: 0.4627Epoch 4/10: [==                            ] 5/63 batches, loss: 0.4666Epoch 4/10: [==                            ] 6/63 batches, loss: 0.4730Epoch 4/10: [===                           ] 7/63 batches, loss: 0.4810Epoch 4/10: [===                           ] 8/63 batches, loss: 0.4779Epoch 4/10: [====                          ] 9/63 batches, loss: 0.4808Epoch 4/10: [====                          ] 10/63 batches, loss: 0.4856Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.4808Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.4828Epoch 4/10: [======                        ] 13/63 batches, loss: 0.4863Epoch 4/10: [======                        ] 14/63 batches, loss: 0.4876Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.4887Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.4881Epoch 4/10: [========                      ] 17/63 batches, loss: 0.4878Epoch 4/10: [========                      ] 18/63 batches, loss: 0.4860Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.4882Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.4878Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.4897Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.4936Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.4974Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.4967Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.4989Epoch 4/10: [============                  ] 26/63 batches, loss: 0.4973Epoch 4/10: [============                  ] 27/63 batches, loss: 0.4967Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.5004Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.4981Epoch 4/10: [==============                ] 30/63 batches, loss: 0.4991Epoch 4/10: [==============                ] 31/63 batches, loss: 0.5000Epoch 4/10: [===============               ] 32/63 batches, loss: 0.4994Epoch 4/10: [===============               ] 33/63 batches, loss: 0.4989Epoch 4/10: [================              ] 34/63 batches, loss: 0.5006Epoch 4/10: [================              ] 35/63 batches, loss: 0.5007Epoch 4/10: [=================             ] 36/63 batches, loss: 0.5008Epoch 4/10: [=================             ] 37/63 batches, loss: 0.5028Epoch 4/10: [==================            ] 38/63 batches, loss: 0.5029Epoch 4/10: [==================            ] 39/63 batches, loss: 0.5041Epoch 4/10: [===================           ] 40/63 batches, loss: 0.5036Epoch 4/10: [===================           ] 41/63 batches, loss: 0.5053Epoch 4/10: [====================          ] 42/63 batches, loss: 0.5047Epoch 4/10: [====================          ] 43/63 batches, loss: 0.5059Epoch 4/10: [====================          ] 44/63 batches, loss: 0.5064Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.5074Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.5089Epoch 4/10: [======================        ] 47/63 batches, loss: 0.5073Epoch 4/10: [======================        ] 48/63 batches, loss: 0.5052Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.5047Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.5047Epoch 4/10: [========================      ] 51/63 batches, loss: 0.5056Epoch 4/10: [========================      ] 52/63 batches, loss: 0.5042Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.5047Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.5051Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.5047Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.5051Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.5063Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.5063Epoch 4/10: [============================  ] 59/63 batches, loss: 0.5067Epoch 4/10: [============================  ] 60/63 batches, loss: 0.5066Epoch 4/10: [============================= ] 61/63 batches, loss: 0.5058Epoch 4/10: [============================= ] 62/63 batches, loss: 0.5058Epoch 4/10: [==============================] 63/63 batches, loss: 0.5067
[2025-05-01 12:13:01,235][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5067
[2025-05-01 12:13:01,443][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5243, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.6225Epoch 5/10: [                              ] 2/63 batches, loss: 0.6108Epoch 5/10: [=                             ] 3/63 batches, loss: 0.5751Epoch 5/10: [=                             ] 4/63 batches, loss: 0.5577Epoch 5/10: [==                            ] 5/63 batches, loss: 0.5426Epoch 5/10: [==                            ] 6/63 batches, loss: 0.5322Epoch 5/10: [===                           ] 7/63 batches, loss: 0.5180Epoch 5/10: [===                           ] 8/63 batches, loss: 0.5192Epoch 5/10: [====                          ] 9/63 batches, loss: 0.5176Epoch 5/10: [====                          ] 10/63 batches, loss: 0.5186Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.5152Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.5143Epoch 5/10: [======                        ] 13/63 batches, loss: 0.5135Epoch 5/10: [======                        ] 14/63 batches, loss: 0.5128Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.5075Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.5072Epoch 5/10: [========                      ] 17/63 batches, loss: 0.5112Epoch 5/10: [========                      ] 18/63 batches, loss: 0.5095Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.5079Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.5018Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.5019Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.4998Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.5021Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.5022Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.5013Epoch 5/10: [============                  ] 26/63 batches, loss: 0.5023Epoch 5/10: [============                  ] 27/63 batches, loss: 0.5024Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.5016Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.5041Epoch 5/10: [==============                ] 30/63 batches, loss: 0.5065Epoch 5/10: [==============                ] 31/63 batches, loss: 0.5087Epoch 5/10: [===============               ] 32/63 batches, loss: 0.5093Epoch 5/10: [===============               ] 33/63 batches, loss: 0.5084Epoch 5/10: [================              ] 34/63 batches, loss: 0.5082Epoch 5/10: [================              ] 35/63 batches, loss: 0.5095Epoch 5/10: [=================             ] 36/63 batches, loss: 0.5060Epoch 5/10: [=================             ] 37/63 batches, loss: 0.5053Epoch 5/10: [==================            ] 38/63 batches, loss: 0.5046Epoch 5/10: [==================            ] 39/63 batches, loss: 0.5058Epoch 5/10: [===================           ] 40/63 batches, loss: 0.5058Epoch 5/10: [===================           ] 41/63 batches, loss: 0.5057Epoch 5/10: [====================          ] 42/63 batches, loss: 0.5046Epoch 5/10: [====================          ] 43/63 batches, loss: 0.5023Epoch 5/10: [====================          ] 44/63 batches, loss: 0.5040Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.5035Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.5040Epoch 5/10: [======================        ] 47/63 batches, loss: 0.5045Epoch 5/10: [======================        ] 48/63 batches, loss: 0.5055Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.5050Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.5040Epoch 5/10: [========================      ] 51/63 batches, loss: 0.5049Epoch 5/10: [========================      ] 52/63 batches, loss: 0.5063Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.5073Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.5068Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.5063Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.5062Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.5074Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.5070Epoch 5/10: [============================  ] 59/63 batches, loss: 0.5065Epoch 5/10: [============================  ] 60/63 batches, loss: 0.5061Epoch 5/10: [============================= ] 61/63 batches, loss: 0.5053Epoch 5/10: [============================= ] 62/63 batches, loss: 0.5053Epoch 5/10: [==============================] 63/63 batches, loss: 0.5042
[2025-05-01 12:13:08,145][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5042
[2025-05-01 12:13:08,379][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5520, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523, 'precision': 0.9090909090909091, 'recall': 1.0}
[2025-05-01 12:13:08,380][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.5647Epoch 6/10: [                              ] 2/63 batches, loss: 0.5578Epoch 6/10: [=                             ] 3/63 batches, loss: 0.5329Epoch 6/10: [=                             ] 4/63 batches, loss: 0.5315Epoch 6/10: [==                            ] 5/63 batches, loss: 0.5164Epoch 6/10: [==                            ] 6/63 batches, loss: 0.5104Epoch 6/10: [===                           ] 7/63 batches, loss: 0.5196Epoch 6/10: [===                           ] 8/63 batches, loss: 0.5116Epoch 6/10: [====                          ] 9/63 batches, loss: 0.5071Epoch 6/10: [====                          ] 10/63 batches, loss: 0.5068Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.5003Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.5026Epoch 6/10: [======                        ] 13/63 batches, loss: 0.5045Epoch 6/10: [======                        ] 14/63 batches, loss: 0.5044Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.5075Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.5087Epoch 6/10: [========                      ] 17/63 batches, loss: 0.5070Epoch 6/10: [========                      ] 18/63 batches, loss: 0.5082Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.5117Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.5137Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.5155Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.5139Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.5134Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.5120Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.5079Epoch 6/10: [============                  ] 26/63 batches, loss: 0.5059Epoch 6/10: [============                  ] 27/63 batches, loss: 0.5076Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.5074Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.5089Epoch 6/10: [==============                ] 30/63 batches, loss: 0.5056Epoch 6/10: [==============                ] 31/63 batches, loss: 0.5040Epoch 6/10: [===============               ] 32/63 batches, loss: 0.5062Epoch 6/10: [===============               ] 33/63 batches, loss: 0.5061Epoch 6/10: [================              ] 34/63 batches, loss: 0.5089Epoch 6/10: [================              ] 35/63 batches, loss: 0.5094Epoch 6/10: [=================             ] 36/63 batches, loss: 0.5099Epoch 6/10: [=================             ] 37/63 batches, loss: 0.5097Epoch 6/10: [==================            ] 38/63 batches, loss: 0.5102Epoch 6/10: [==================            ] 39/63 batches, loss: 0.5088Epoch 6/10: [===================           ] 40/63 batches, loss: 0.5093Epoch 6/10: [===================           ] 41/63 batches, loss: 0.5097Epoch 6/10: [====================          ] 42/63 batches, loss: 0.5096Epoch 6/10: [====================          ] 43/63 batches, loss: 0.5095Epoch 6/10: [====================          ] 44/63 batches, loss: 0.5099Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.5102Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.5101Epoch 6/10: [======================        ] 47/63 batches, loss: 0.5115Epoch 6/10: [======================        ] 48/63 batches, loss: 0.5118Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.5107Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.5105Epoch 6/10: [========================      ] 51/63 batches, loss: 0.5099Epoch 6/10: [========================      ] 52/63 batches, loss: 0.5084Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.5074Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.5069Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.5064Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.5077Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.5080Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.5059Epoch 6/10: [============================  ] 59/63 batches, loss: 0.5055Epoch 6/10: [============================  ] 60/63 batches, loss: 0.5050Epoch 6/10: [============================= ] 61/63 batches, loss: 0.5050Epoch 6/10: [============================= ] 62/63 batches, loss: 0.5054Epoch 6/10: [==============================] 63/63 batches, loss: 0.5064
[2025-05-01 12:13:14,711][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.5064
[2025-05-01 12:13:14,947][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.5391, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561, 'precision': 0.9523809523809523, 'recall': 1.0}
[2025-05-01 12:13:14,948][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/63 batches, loss: 0.4559Epoch 7/10: [                              ] 2/63 batches, loss: 0.5157Epoch 7/10: [=                             ] 3/63 batches, loss: 0.5037Epoch 7/10: [=                             ] 4/63 batches, loss: 0.5037Epoch 7/10: [==                            ] 5/63 batches, loss: 0.4941Epoch 7/10: [==                            ] 6/63 batches, loss: 0.4918Epoch 7/10: [===                           ] 7/63 batches, loss: 0.4907Epoch 7/10: [===                           ] 8/63 batches, loss: 0.4923Epoch 7/10: [====                          ] 9/63 batches, loss: 0.4883Epoch 7/10: [====                          ] 10/63 batches, loss: 0.4947Epoch 7/10: [=====                         ] 11/63 batches, loss: 0.4976Epoch 7/10: [=====                         ] 12/63 batches, loss: 0.5061Epoch 7/10: [======                        ] 13/63 batches, loss: 0.5095Epoch 7/10: [======                        ] 14/63 batches, loss: 0.5041Epoch 7/10: [=======                       ] 15/63 batches, loss: 0.5025Epoch 7/10: [=======                       ] 16/63 batches, loss: 0.4996Epoch 7/10: [========                      ] 17/63 batches, loss: 0.5013Epoch 7/10: [========                      ] 18/63 batches, loss: 0.4987Epoch 7/10: [=========                     ] 19/63 batches, loss: 0.4965Epoch 7/10: [=========                     ] 20/63 batches, loss: 0.4968Epoch 7/10: [==========                    ] 21/63 batches, loss: 0.4960Epoch 7/10: [==========                    ] 22/63 batches, loss: 0.4974Epoch 7/10: [==========                    ] 23/63 batches, loss: 0.4967Epoch 7/10: [===========                   ] 24/63 batches, loss: 0.4990Epoch 7/10: [===========                   ] 25/63 batches, loss: 0.5011Epoch 7/10: [============                  ] 26/63 batches, loss: 0.5011Epoch 7/10: [============                  ] 27/63 batches, loss: 0.5012Epoch 7/10: [=============                 ] 28/63 batches, loss: 0.5013Epoch 7/10: [=============                 ] 29/63 batches, loss: 0.5030Epoch 7/10: [==============                ] 30/63 batches, loss: 0.5046Epoch 7/10: [==============                ] 31/63 batches, loss: 0.5061Epoch 7/10: [===============               ] 32/63 batches, loss: 0.5069Epoch 7/10: [===============               ] 33/63 batches, loss: 0.5090Epoch 7/10: [================              ] 34/63 batches, loss: 0.5074Epoch 7/10: [================              ] 35/63 batches, loss: 0.5060Epoch 7/10: [=================             ] 36/63 batches, loss: 0.5073Epoch 7/10: [=================             ] 37/63 batches, loss: 0.5065Epoch 7/10: [==================            ] 38/63 batches, loss: 0.5046Epoch 7/10: [==================            ] 39/63 batches, loss: 0.5045Epoch 7/10: [===================           ] 40/63 batches, loss: 0.5057Epoch 7/10: [===================           ] 41/63 batches, loss: 0.5051Epoch 7/10: [====================          ] 42/63 batches, loss: 0.5067Epoch 7/10: [====================          ] 43/63 batches, loss: 0.5072Epoch 7/10: [====================          ] 44/63 batches, loss: 0.5077Epoch 7/10: [=====================         ] 45/63 batches, loss: 0.5092Epoch 7/10: [=====================         ] 46/63 batches, loss: 0.5080Epoch 7/10: [======================        ] 47/63 batches, loss: 0.5089Epoch 7/10: [======================        ] 48/63 batches, loss: 0.5083Epoch 7/10: [=======================       ] 49/63 batches, loss: 0.5077Epoch 7/10: [=======================       ] 50/63 batches, loss: 0.5072Epoch 7/10: [========================      ] 51/63 batches, loss: 0.5066Epoch 7/10: [========================      ] 52/63 batches, loss: 0.5066Epoch 7/10: [=========================     ] 53/63 batches, loss: 0.5061Epoch 7/10: [=========================     ] 54/63 batches, loss: 0.5047Epoch 7/10: [==========================    ] 55/63 batches, loss: 0.5051Epoch 7/10: [==========================    ] 56/63 batches, loss: 0.5042Epoch 7/10: [===========================   ] 57/63 batches, loss: 0.5046Epoch 7/10: [===========================   ] 58/63 batches, loss: 0.5050Epoch 7/10: [============================  ] 59/63 batches, loss: 0.5046Epoch 7/10: [============================  ] 60/63 batches, loss: 0.5042Epoch 7/10: [============================= ] 61/63 batches, loss: 0.5046Epoch 7/10: [============================= ] 62/63 batches, loss: 0.5038Epoch 7/10: [==============================] 63/63 batches, loss: 0.5048
[2025-05-01 12:13:21,285][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5048
[2025-05-01 12:13:21,472][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.5218, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 8/10: [Epoch 8/10: [                              ] 1/63 batches, loss: 0.5040Epoch 8/10: [                              ] 2/63 batches, loss: 0.5392Epoch 8/10: [=                             ] 3/63 batches, loss: 0.5116Epoch 8/10: [=                             ] 4/63 batches, loss: 0.5333Epoch 8/10: [==                            ] 5/63 batches, loss: 0.5131Epoch 8/10: [==                            ] 6/63 batches, loss: 0.5114Epoch 8/10: [===                           ] 7/63 batches, loss: 0.5137Epoch 8/10: [===                           ] 8/63 batches, loss: 0.5065Epoch 8/10: [====                          ] 9/63 batches, loss: 0.5009Epoch 8/10: [====                          ] 10/63 batches, loss: 0.4988Epoch 8/10: [=====                         ] 11/63 batches, loss: 0.4970Epoch 8/10: [=====                         ] 12/63 batches, loss: 0.4996Epoch 8/10: [======                        ] 13/63 batches, loss: 0.4926Epoch 8/10: [======                        ] 14/63 batches, loss: 0.4917Epoch 8/10: [=======                       ] 15/63 batches, loss: 0.4941Epoch 8/10: [=======                       ] 16/63 batches, loss: 0.4888Epoch 8/10: [========                      ] 17/63 batches, loss: 0.4883Epoch 8/10: [========                      ] 18/63 batches, loss: 0.4931Epoch 8/10: [=========                     ] 19/63 batches, loss: 0.4936Epoch 8/10: [=========                     ] 20/63 batches, loss: 0.4977Epoch 8/10: [==========                    ] 21/63 batches, loss: 0.4979Epoch 8/10: [==========                    ] 22/63 batches, loss: 0.4960Epoch 8/10: [==========                    ] 23/63 batches, loss: 0.4974Epoch 8/10: [===========                   ] 24/63 batches, loss: 0.4947Epoch 8/10: [===========                   ] 25/63 batches, loss: 0.4969Epoch 8/10: [============                  ] 26/63 batches, loss: 0.4954Epoch 8/10: [============                  ] 27/63 batches, loss: 0.4948Epoch 8/10: [=============                 ] 28/63 batches, loss: 0.4952Epoch 8/10: [=============                 ] 29/63 batches, loss: 0.4963Epoch 8/10: [==============                ] 30/63 batches, loss: 0.4963Epoch 8/10: [==============                ] 31/63 batches, loss: 0.4973Epoch 8/10: [===============               ] 32/63 batches, loss: 0.4960Epoch 8/10: [===============               ] 33/63 batches, loss: 0.4941Epoch 8/10: [================              ] 34/63 batches, loss: 0.4951Epoch 8/10: [================              ] 35/63 batches, loss: 0.4953Epoch 8/10: [=================             ] 36/63 batches, loss: 0.4949Epoch 8/10: [=================             ] 37/63 batches, loss: 0.4964Epoch 8/10: [==================            ] 38/63 batches, loss: 0.4972Epoch 8/10: [==================            ] 39/63 batches, loss: 0.4992Epoch 8/10: [===================           ] 40/63 batches, loss: 0.5011Epoch 8/10: [===================           ] 41/63 batches, loss: 0.5012Epoch 8/10: [====================          ] 42/63 batches, loss: 0.5013Epoch 8/10: [====================          ] 43/63 batches, loss: 0.5019Epoch 8/10: [====================          ] 44/63 batches, loss: 0.5030Epoch 8/10: [=====================         ] 45/63 batches, loss: 0.5036Epoch 8/10: [=====================         ] 46/63 batches, loss: 0.5025Epoch 8/10: [======================        ] 47/63 batches, loss: 0.5036Epoch 8/10: [======================        ] 48/63 batches, loss: 0.5036Epoch 8/10: [=======================       ] 49/63 batches, loss: 0.5022Epoch 8/10: [=======================       ] 50/63 batches, loss: 0.5013Epoch 8/10: [========================      ] 51/63 batches, loss: 0.5024Epoch 8/10: [========================      ] 52/63 batches, loss: 0.5019Epoch 8/10: [=========================     ] 53/63 batches, loss: 0.5015Epoch 8/10: [=========================     ] 54/63 batches, loss: 0.5011Epoch 8/10: [==========================    ] 55/63 batches, loss: 0.5020Epoch 8/10: [==========================    ] 56/63 batches, loss: 0.5016Epoch 8/10: [===========================   ] 57/63 batches, loss: 0.5008Epoch 8/10: [===========================   ] 58/63 batches, loss: 0.5009Epoch 8/10: [============================  ] 59/63 batches, loss: 0.5013Epoch 8/10: [============================  ] 60/63 batches, loss: 0.5017Epoch 8/10: [============================= ] 61/63 batches, loss: 0.5037Epoch 8/10: [============================= ] 62/63 batches, loss: 0.5041Epoch 8/10: [==============================] 63/63 batches, loss: 0.5071
[2025-05-01 12:13:28,223][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.5071
[2025-05-01 12:13:28,438][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.5239, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-01 12:13:28,439][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 9/10: [Epoch 9/10: [                              ] 1/63 batches, loss: 0.4561Epoch 9/10: [                              ] 2/63 batches, loss: 0.5154Epoch 9/10: [=                             ] 3/63 batches, loss: 0.5036Epoch 9/10: [=                             ] 4/63 batches, loss: 0.5214Epoch 9/10: [==                            ] 5/63 batches, loss: 0.5225Epoch 9/10: [==                            ] 6/63 batches, loss: 0.5272Epoch 9/10: [===                           ] 7/63 batches, loss: 0.5102Epoch 9/10: [===                           ] 8/63 batches, loss: 0.5005Epoch 9/10: [====                          ] 9/63 batches, loss: 0.5061Epoch 9/10: [====                          ] 10/63 batches, loss: 0.5034Epoch 9/10: [=====                         ] 11/63 batches, loss: 0.5014Epoch 9/10: [=====                         ] 12/63 batches, loss: 0.5075Epoch 9/10: [======                        ] 13/63 batches, loss: 0.5090Epoch 9/10: [======                        ] 14/63 batches, loss: 0.5069Epoch 9/10: [=======                       ] 15/63 batches, loss: 0.5082Epoch 9/10: [=======                       ] 16/63 batches, loss: 0.5079Epoch 9/10: [========                      ] 17/63 batches, loss: 0.5077Epoch 9/10: [========                      ] 18/63 batches, loss: 0.5062Epoch 9/10: [=========                     ] 19/63 batches, loss: 0.5073Epoch 9/10: [=========                     ] 20/63 batches, loss: 0.5083Epoch 9/10: [==========                    ] 21/63 batches, loss: 0.5092Epoch 9/10: [==========                    ] 22/63 batches, loss: 0.5100Epoch 9/10: [==========                    ] 23/63 batches, loss: 0.5107Epoch 9/10: [===========                   ] 24/63 batches, loss: 0.5075Epoch 9/10: [===========                   ] 25/63 batches, loss: 0.5054Epoch 9/10: [============                  ] 26/63 batches, loss: 0.5044Epoch 9/10: [============                  ] 27/63 batches, loss: 0.5026Epoch 9/10: [=============                 ] 28/63 batches, loss: 0.5035Epoch 9/10: [=============                 ] 29/63 batches, loss: 0.5018Epoch 9/10: [==============                ] 30/63 batches, loss: 0.5019Epoch 9/10: [==============                ] 31/63 batches, loss: 0.5019Epoch 9/10: [===============               ] 32/63 batches, loss: 0.5042Epoch 9/10: [===============               ] 33/63 batches, loss: 0.5049Epoch 9/10: [================              ] 34/63 batches, loss: 0.5070Epoch 9/10: [================              ] 35/63 batches, loss: 0.5055Epoch 9/10: [=================             ] 36/63 batches, loss: 0.5081Epoch 9/10: [=================             ] 37/63 batches, loss: 0.5079Epoch 9/10: [==================            ] 38/63 batches, loss: 0.5091Epoch 9/10: [==================            ] 39/63 batches, loss: 0.5077Epoch 9/10: [===================           ] 40/63 batches, loss: 0.5094Epoch 9/10: [===================           ] 41/63 batches, loss: 0.5104Epoch 9/10: [====================          ] 42/63 batches, loss: 0.5096Epoch 9/10: [====================          ] 43/63 batches, loss: 0.5084Epoch 9/10: [====================          ] 44/63 batches, loss: 0.5094Epoch 9/10: [=====================         ] 45/63 batches, loss: 0.5098Epoch 9/10: [=====================         ] 46/63 batches, loss: 0.5096Epoch 9/10: [======================        ] 47/63 batches, loss: 0.5085Epoch 9/10: [======================        ] 48/63 batches, loss: 0.5079Epoch 9/10: [=======================       ] 49/63 batches, loss: 0.5078Epoch 9/10: [=======================       ] 50/63 batches, loss: 0.5072Epoch 9/10: [========================      ] 51/63 batches, loss: 0.5072Epoch 9/10: [========================      ] 52/63 batches, loss: 0.5067Epoch 9/10: [=========================     ] 53/63 batches, loss: 0.5062Epoch 9/10: [=========================     ] 54/63 batches, loss: 0.5061Epoch 9/10: [==========================    ] 55/63 batches, loss: 0.5052Epoch 9/10: [==========================    ] 56/63 batches, loss: 0.5052Epoch 9/10: [===========================   ] 57/63 batches, loss: 0.5060Epoch 9/10: [===========================   ] 58/63 batches, loss: 0.5052Epoch 9/10: [============================  ] 59/63 batches, loss: 0.5048Epoch 9/10: [============================  ] 60/63 batches, loss: 0.5047Epoch 9/10: [============================= ] 61/63 batches, loss: 0.5043Epoch 9/10: [============================= ] 62/63 batches, loss: 0.5036Epoch 9/10: [==============================] 63/63 batches, loss: 0.5046
[2025-05-01 12:13:34,782][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.5046
[2025-05-01 12:13:34,982][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.5224, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-01 12:13:34,983][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 10/10: [Epoch 10/10: [                              ] 1/63 batches, loss: 0.5508Epoch 10/10: [                              ] 2/63 batches, loss: 0.5033Epoch 10/10: [=                             ] 3/63 batches, loss: 0.5191Epoch 10/10: [=                             ] 4/63 batches, loss: 0.5211Epoch 10/10: [==                            ] 5/63 batches, loss: 0.5081Epoch 10/10: [==                            ] 6/63 batches, loss: 0.5073Epoch 10/10: [===                           ] 7/63 batches, loss: 0.5000Epoch 10/10: [===                           ] 8/63 batches, loss: 0.5123Epoch 10/10: [====                          ] 9/63 batches, loss: 0.5140Epoch 10/10: [====                          ] 10/63 batches, loss: 0.5153Epoch 10/10: [=====                         ] 11/63 batches, loss: 0.5056Epoch 10/10: [=====                         ] 12/63 batches, loss: 0.4995Epoch 10/10: [======                        ] 13/63 batches, loss: 0.5017Epoch 10/10: [======                        ] 14/63 batches, loss: 0.4968Epoch 10/10: [=======                       ] 15/63 batches, loss: 0.5035Epoch 10/10: [=======                       ] 16/63 batches, loss: 0.5006Epoch 10/10: [========                      ] 17/63 batches, loss: 0.5035Epoch 10/10: [========                      ] 18/63 batches, loss: 0.5035Epoch 10/10: [=========                     ] 19/63 batches, loss: 0.5035Epoch 10/10: [=========                     ] 20/63 batches, loss: 0.5035Epoch 10/10: [==========                    ] 21/63 batches, loss: 0.5058Epoch 10/10: [==========                    ] 22/63 batches, loss: 0.5057Epoch 10/10: [==========                    ] 23/63 batches, loss: 0.5077Epoch 10/10: [===========                   ] 24/63 batches, loss: 0.5085Epoch 10/10: [===========                   ] 25/63 batches, loss: 0.5092Epoch 10/10: [============                  ] 26/63 batches, loss: 0.5090Epoch 10/10: [============                  ] 27/63 batches, loss: 0.5097Epoch 10/10: [=============                 ] 28/63 batches, loss: 0.5078Epoch 10/10: [=============                 ] 29/63 batches, loss: 0.5061Epoch 10/10: [==============                ] 30/63 batches, loss: 0.5044Epoch 10/10: [==============                ] 31/63 batches, loss: 0.5051Epoch 10/10: [===============               ] 32/63 batches, loss: 0.5036Epoch 10/10: [===============               ] 33/63 batches, loss: 0.5043Epoch 10/10: [================              ] 34/63 batches, loss: 0.5029Epoch 10/10: [================              ] 35/63 batches, loss: 0.5022Epoch 10/10: [=================             ] 36/63 batches, loss: 0.5023Epoch 10/10: [=================             ] 37/63 batches, loss: 0.5017Epoch 10/10: [==================            ] 38/63 batches, loss: 0.5023Epoch 10/10: [==================            ] 39/63 batches, loss: 0.4993Epoch 10/10: [===================           ] 40/63 batches, loss: 0.4988Epoch 10/10: [===================           ] 41/63 batches, loss: 0.4984Epoch 10/10: [====================          ] 42/63 batches, loss: 0.4996Epoch 10/10: [====================          ] 43/63 batches, loss: 0.4997Epoch 10/10: [====================          ] 44/63 batches, loss: 0.4998Epoch 10/10: [=====================         ] 45/63 batches, loss: 0.5004Epoch 10/10: [=====================         ] 46/63 batches, loss: 0.5005Epoch 10/10: [======================        ] 47/63 batches, loss: 0.5005Epoch 10/10: [======================        ] 48/63 batches, loss: 0.5016Epoch 10/10: [=======================       ] 49/63 batches, loss: 0.5018Epoch 10/10: [=======================       ] 50/63 batches, loss: 0.5027Epoch 10/10: [========================      ] 51/63 batches, loss: 0.5023Epoch 10/10: [========================      ] 52/63 batches, loss: 0.5005Epoch 10/10: [=========================     ] 53/63 batches, loss: 0.5001Epoch 10/10: [=========================     ] 54/63 batches, loss: 0.5006Epoch 10/10: [==========================    ] 55/63 batches, loss: 0.5020Epoch 10/10: [==========================    ] 56/63 batches, loss: 0.5020Epoch 10/10: [===========================   ] 57/63 batches, loss: 0.5037Epoch 10/10: [===========================   ] 58/63 batches, loss: 0.5037Epoch 10/10: [============================  ] 59/63 batches, loss: 0.5037Epoch 10/10: [============================  ] 60/63 batches, loss: 0.5037Epoch 10/10: [============================= ] 61/63 batches, loss: 0.5033Epoch 10/10: [============================= ] 62/63 batches, loss: 0.5041Epoch 10/10: [==============================] 63/63 batches, loss: 0.5030
[2025-05-01 12:13:41,321][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.5030
[2025-05-01 12:13:41,509][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.5531, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523, 'precision': 0.9090909090909091, 'recall': 1.0}
[2025-05-01 12:13:41,510][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 12:13:41,510][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-01 12:13:41,510][src.training.lm_trainer][INFO] - Training completed in 68.11 seconds
[2025-05-01 12:13:41,510][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 12:13:43,967][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-01 12:13:43,968][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-01 12:13:43,969][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.922077922077922, 'f1': 0.875, 'precision': 0.8076923076923077, 'recall': 0.9545454545454546}
[2025-05-01 12:13:45,615][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/model.pt
[2025-05-01 12:13:45,621][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▆███
wandb:           best_val_f1 ▁▆███
wandb:         best_val_loss █▃▁▁▁
wandb:    best_val_precision ▁▅███
wandb:       best_val_recall ▁▁▁▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▄▅▅▄▄▅▅▅
wandb:            train_loss █▅▁▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▆██▇▇███▇
wandb:                val_f1 ▁▆██▆▇███▆
wandb:              val_loss █▃▁▁▂▂▁▁▁▂
wandb:         val_precision ▁▅██▆▇███▆
wandb:            val_recall ▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 1
wandb:           best_val_f1 1
wandb:         best_val_loss 0.5218
wandb:    best_val_precision 1
wandb:       best_val_recall 1
wandb:      early_stop_epoch 10
wandb:                 epoch 10
wandb:   final_test_accuracy 0.92208
wandb:         final_test_f1 0.875
wandb:  final_test_precision 0.80769
wandb:     final_test_recall 0.95455
wandb:  final_train_accuracy 1
wandb:        final_train_f1 1
wandb: final_train_precision 1
wandb:    final_train_recall 1
wandb:    final_val_accuracy 1
wandb:          final_val_f1 1
wandb:   final_val_precision 1
wandb:      final_val_recall 1
wandb:         learning_rate 2e-05
wandb:            train_loss 0.50304
wandb:            train_time 68.10928
wandb:          val_accuracy 0.95455
wandb:                val_f1 0.95238
wandb:              val_loss 0.55315
wandb:         val_precision 0.90909
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_121221-93u4ovzx
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_121221-93u4ovzx/logs
Experiment finetune_question_type_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/results.json
Running experiment: finetune_complexity_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 12:14:00,487][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity
experiment_name: finetune_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-01 12:14:00,487][__main__][INFO] - Normalized task: complexity
[2025-05-01 12:14:00,487][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-01 12:14:00,487][__main__][INFO] - Determined Task Type: regression
[2025-05-01 12:14:00,491][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-01 12:14:00,492][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 12:14:02,054][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 12:14:04,313][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 12:14:04,313][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:14:04,395][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:14:04,437][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:14:04,556][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-01 12:14:04,563][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:14:04,563][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-01 12:14:04,564][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:14:04,590][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:14:04,632][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:14:04,648][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-01 12:14:04,649][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:14:04,649][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-01 12:14:04,650][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:14:04,677][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:14:04,718][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:14:04,732][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-01 12:14:04,733][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:14:04,733][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-01 12:14:04,735][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-01 12:14:04,736][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 12:14:04,737][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 12:14:04,737][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 12:14:04,737][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 12:14:04,737][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 12:14:04,737][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-05-01 12:14:04,737][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-01 12:14:04,737][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-05-01 12:14:04,737][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 12:14:04,738][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 12:14:04,738][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 12:14:04,738][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 12:14:04,738][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 12:14:04,738][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-05-01 12:14:04,738][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-01 12:14:04,738][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-05-01 12:14:04,738][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 12:14:04,738][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 12:14:04,738][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 12:14:04,739][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 12:14:04,739][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 12:14:04,739][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-05-01 12:14:04,739][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-01 12:14:04,739][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-05-01 12:14:04,739][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-01 12:14:04,739][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 12:14:04,739][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 12:14:04,740][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-01 12:14:04,740][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 12:14:09,325][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 12:14:09,326][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 12:14:09,326][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 12:14:09,326][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 12:14:09,330][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 12:14:09,331][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 12:14:09,331][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 12:14:09,331][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-01 12:14:09,332][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 12:14:09,332][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.2303Epoch 1/10: [                              ] 2/63 batches, loss: 0.2078Epoch 1/10: [=                             ] 3/63 batches, loss: 0.2220Epoch 1/10: [=                             ] 4/63 batches, loss: 0.2301Epoch 1/10: [==                            ] 5/63 batches, loss: 0.2037Epoch 1/10: [==                            ] 6/63 batches, loss: 0.1926Epoch 1/10: [===                           ] 7/63 batches, loss: 0.1881Epoch 1/10: [===                           ] 8/63 batches, loss: 0.1857Epoch 1/10: [====                          ] 9/63 batches, loss: 0.1875Epoch 1/10: [====                          ] 10/63 batches, loss: 0.1871Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.1827Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.1822Epoch 1/10: [======                        ] 13/63 batches, loss: 0.1838Epoch 1/10: [======                        ] 14/63 batches, loss: 0.1789Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.1721Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.1689Epoch 1/10: [========                      ] 17/63 batches, loss: 0.1630Epoch 1/10: [========                      ] 18/63 batches, loss: 0.1590Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.1582Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.1561Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.1553Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.1519Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.1510Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.1471Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.1437Epoch 1/10: [============                  ] 26/63 batches, loss: 0.1411Epoch 1/10: [============                  ] 27/63 batches, loss: 0.1407Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.1398Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.1378Epoch 1/10: [==============                ] 30/63 batches, loss: 0.1350Epoch 1/10: [==============                ] 31/63 batches, loss: 0.1325Epoch 1/10: [===============               ] 32/63 batches, loss: 0.1296Epoch 1/10: [===============               ] 33/63 batches, loss: 0.1272Epoch 1/10: [================              ] 34/63 batches, loss: 0.1249Epoch 1/10: [================              ] 35/63 batches, loss: 0.1232Epoch 1/10: [=================             ] 36/63 batches, loss: 0.1213Epoch 1/10: [=================             ] 37/63 batches, loss: 0.1195Epoch 1/10: [==================            ] 38/63 batches, loss: 0.1187Epoch 1/10: [==================            ] 39/63 batches, loss: 0.1166Epoch 1/10: [===================           ] 40/63 batches, loss: 0.1147Epoch 1/10: [===================           ] 41/63 batches, loss: 0.1134Epoch 1/10: [====================          ] 42/63 batches, loss: 0.1116Epoch 1/10: [====================          ] 43/63 batches, loss: 0.1112Epoch 1/10: [====================          ] 44/63 batches, loss: 0.1099Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.1088Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.1069Epoch 1/10: [======================        ] 47/63 batches, loss: 0.1060Epoch 1/10: [======================        ] 48/63 batches, loss: 0.1051Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.1037Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.1024Epoch 1/10: [========================      ] 51/63 batches, loss: 0.1008Epoch 1/10: [========================      ] 52/63 batches, loss: 0.0993Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.0983Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.0969Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.0962Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.0953Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.0942Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.0937Epoch 1/10: [============================  ] 59/63 batches, loss: 0.0928Epoch 1/10: [============================  ] 60/63 batches, loss: 0.0916Epoch 1/10: [============================= ] 61/63 batches, loss: 0.0906Epoch 1/10: [============================= ] 62/63 batches, loss: 0.0896Epoch 1/10: [==============================] 63/63 batches, loss: 0.0889
[2025-05-01 12:14:18,230][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0889
[2025-05-01 12:14:18,393][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0799, Metrics: {'mse': 0.08098221570253372, 'rmse': 0.2845737438741208, 'r2': -0.248205304145813}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.0437Epoch 2/10: [                              ] 2/63 batches, loss: 0.0360Epoch 2/10: [=                             ] 3/63 batches, loss: 0.0361Epoch 2/10: [=                             ] 4/63 batches, loss: 0.0326Epoch 2/10: [==                            ] 5/63 batches, loss: 0.0318Epoch 2/10: [==                            ] 6/63 batches, loss: 0.0303Epoch 2/10: [===                           ] 7/63 batches, loss: 0.0295Epoch 2/10: [===                           ] 8/63 batches, loss: 0.0285Epoch 2/10: [====                          ] 9/63 batches, loss: 0.0291Epoch 2/10: [====                          ] 10/63 batches, loss: 0.0275Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.0278Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.0271Epoch 2/10: [======                        ] 13/63 batches, loss: 0.0285Epoch 2/10: [======                        ] 14/63 batches, loss: 0.0289Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.0293Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.0286Epoch 2/10: [========                      ] 17/63 batches, loss: 0.0287Epoch 2/10: [========                      ] 18/63 batches, loss: 0.0277Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.0273Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.0269Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.0263Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.0260Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.0256Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.0252Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.0252Epoch 2/10: [============                  ] 26/63 batches, loss: 0.0251Epoch 2/10: [============                  ] 27/63 batches, loss: 0.0251Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.0250Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.0250Epoch 2/10: [==============                ] 30/63 batches, loss: 0.0247Epoch 2/10: [==============                ] 31/63 batches, loss: 0.0247Epoch 2/10: [===============               ] 32/63 batches, loss: 0.0250Epoch 2/10: [===============               ] 33/63 batches, loss: 0.0250Epoch 2/10: [================              ] 34/63 batches, loss: 0.0253Epoch 2/10: [================              ] 35/63 batches, loss: 0.0251Epoch 2/10: [=================             ] 36/63 batches, loss: 0.0250Epoch 2/10: [=================             ] 37/63 batches, loss: 0.0246Epoch 2/10: [==================            ] 38/63 batches, loss: 0.0241Epoch 2/10: [==================            ] 39/63 batches, loss: 0.0241Epoch 2/10: [===================           ] 40/63 batches, loss: 0.0238Epoch 2/10: [===================           ] 41/63 batches, loss: 0.0238Epoch 2/10: [====================          ] 42/63 batches, loss: 0.0242Epoch 2/10: [====================          ] 43/63 batches, loss: 0.0239Epoch 2/10: [====================          ] 44/63 batches, loss: 0.0237Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.0237Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.0239Epoch 2/10: [======================        ] 47/63 batches, loss: 0.0237Epoch 2/10: [======================        ] 48/63 batches, loss: 0.0236Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.0233Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.0232Epoch 2/10: [========================      ] 51/63 batches, loss: 0.0231Epoch 2/10: [========================      ] 52/63 batches, loss: 0.0230Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.0228Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.0227Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.0227Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.0224Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.0224Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.0222Epoch 2/10: [============================  ] 59/63 batches, loss: 0.0221Epoch 2/10: [============================  ] 60/63 batches, loss: 0.0219Epoch 2/10: [============================= ] 61/63 batches, loss: 0.0219Epoch 2/10: [============================= ] 62/63 batches, loss: 0.0216Epoch 2/10: [==============================] 63/63 batches, loss: 0.0215
[2025-05-01 12:14:25,146][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0215
[2025-05-01 12:14:25,327][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0622, Metrics: {'mse': 0.06337393075227737, 'rmse': 0.2517417938131795, 'r2': 0.0231969952583313}
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.0210Epoch 3/10: [                              ] 2/63 batches, loss: 0.0156Epoch 3/10: [=                             ] 3/63 batches, loss: 0.0158Epoch 3/10: [=                             ] 4/63 batches, loss: 0.0144Epoch 3/10: [==                            ] 5/63 batches, loss: 0.0153Epoch 3/10: [==                            ] 6/63 batches, loss: 0.0155Epoch 3/10: [===                           ] 7/63 batches, loss: 0.0146Epoch 3/10: [===                           ] 8/63 batches, loss: 0.0145Epoch 3/10: [====                          ] 9/63 batches, loss: 0.0144Epoch 3/10: [====                          ] 10/63 batches, loss: 0.0142Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.0141Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.0138Epoch 3/10: [======                        ] 13/63 batches, loss: 0.0157Epoch 3/10: [======                        ] 14/63 batches, loss: 0.0157Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.0154Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.0154Epoch 3/10: [========                      ] 17/63 batches, loss: 0.0158Epoch 3/10: [========                      ] 18/63 batches, loss: 0.0156Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.0158Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.0159Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.0162Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.0157Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.0156Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.0157Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.0157Epoch 3/10: [============                  ] 26/63 batches, loss: 0.0161Epoch 3/10: [============                  ] 27/63 batches, loss: 0.0161Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.0160Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.0160Epoch 3/10: [==============                ] 30/63 batches, loss: 0.0159Epoch 3/10: [==============                ] 31/63 batches, loss: 0.0160Epoch 3/10: [===============               ] 32/63 batches, loss: 0.0169Epoch 3/10: [===============               ] 33/63 batches, loss: 0.0170Epoch 3/10: [================              ] 34/63 batches, loss: 0.0167Epoch 3/10: [================              ] 35/63 batches, loss: 0.0166Epoch 3/10: [=================             ] 36/63 batches, loss: 0.0166Epoch 3/10: [=================             ] 37/63 batches, loss: 0.0164Epoch 3/10: [==================            ] 38/63 batches, loss: 0.0163Epoch 3/10: [==================            ] 39/63 batches, loss: 0.0163Epoch 3/10: [===================           ] 40/63 batches, loss: 0.0161Epoch 3/10: [===================           ] 41/63 batches, loss: 0.0160Epoch 3/10: [====================          ] 42/63 batches, loss: 0.0161Epoch 3/10: [====================          ] 43/63 batches, loss: 0.0162Epoch 3/10: [====================          ] 44/63 batches, loss: 0.0162Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.0162Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.0163Epoch 3/10: [======================        ] 47/63 batches, loss: 0.0163Epoch 3/10: [======================        ] 48/63 batches, loss: 0.0165Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.0166Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.0167Epoch 3/10: [========================      ] 51/63 batches, loss: 0.0166Epoch 3/10: [========================      ] 52/63 batches, loss: 0.0165Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.0165Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.0164Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.0163Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.0161Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.0162Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.0163Epoch 3/10: [============================  ] 59/63 batches, loss: 0.0163Epoch 3/10: [============================  ] 60/63 batches, loss: 0.0164Epoch 3/10: [============================= ] 61/63 batches, loss: 0.0163Epoch 3/10: [============================= ] 62/63 batches, loss: 0.0162Epoch 3/10: [==============================] 63/63 batches, loss: 0.0161
[2025-05-01 12:14:32,084][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0161
[2025-05-01 12:14:32,279][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0502, Metrics: {'mse': 0.05127166584134102, 'rmse': 0.22643247523564508, 'r2': 0.20973312854766846}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.0150Epoch 4/10: [                              ] 2/63 batches, loss: 0.0187Epoch 4/10: [=                             ] 3/63 batches, loss: 0.0150Epoch 4/10: [=                             ] 4/63 batches, loss: 0.0129Epoch 4/10: [==                            ] 5/63 batches, loss: 0.0135Epoch 4/10: [==                            ] 6/63 batches, loss: 0.0136Epoch 4/10: [===                           ] 7/63 batches, loss: 0.0134Epoch 4/10: [===                           ] 8/63 batches, loss: 0.0132Epoch 4/10: [====                          ] 9/63 batches, loss: 0.0162Epoch 4/10: [====                          ] 10/63 batches, loss: 0.0168Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.0178Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.0179Epoch 4/10: [======                        ] 13/63 batches, loss: 0.0173Epoch 4/10: [======                        ] 14/63 batches, loss: 0.0177Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.0182Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.0182Epoch 4/10: [========                      ] 17/63 batches, loss: 0.0178Epoch 4/10: [========                      ] 18/63 batches, loss: 0.0172Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.0169Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.0165Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.0164Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.0162Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.0167Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.0169Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.0167Epoch 4/10: [============                  ] 26/63 batches, loss: 0.0167Epoch 4/10: [============                  ] 27/63 batches, loss: 0.0165Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.0163Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.0165Epoch 4/10: [==============                ] 30/63 batches, loss: 0.0164Epoch 4/10: [==============                ] 31/63 batches, loss: 0.0162Epoch 4/10: [===============               ] 32/63 batches, loss: 0.0160Epoch 4/10: [===============               ] 33/63 batches, loss: 0.0158Epoch 4/10: [================              ] 34/63 batches, loss: 0.0158Epoch 4/10: [================              ] 35/63 batches, loss: 0.0158Epoch 4/10: [=================             ] 36/63 batches, loss: 0.0157Epoch 4/10: [=================             ] 37/63 batches, loss: 0.0156Epoch 4/10: [==================            ] 38/63 batches, loss: 0.0154Epoch 4/10: [==================            ] 39/63 batches, loss: 0.0151Epoch 4/10: [===================           ] 40/63 batches, loss: 0.0150Epoch 4/10: [===================           ] 41/63 batches, loss: 0.0151Epoch 4/10: [====================          ] 42/63 batches, loss: 0.0150Epoch 4/10: [====================          ] 43/63 batches, loss: 0.0150Epoch 4/10: [====================          ] 44/63 batches, loss: 0.0151Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.0150Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.0151Epoch 4/10: [======================        ] 47/63 batches, loss: 0.0150Epoch 4/10: [======================        ] 48/63 batches, loss: 0.0150Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.0150Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.0152Epoch 4/10: [========================      ] 51/63 batches, loss: 0.0152Epoch 4/10: [========================      ] 52/63 batches, loss: 0.0151Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.0152Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.0152Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.0154Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.0154Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.0152Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.0152Epoch 4/10: [============================  ] 59/63 batches, loss: 0.0152Epoch 4/10: [============================  ] 60/63 batches, loss: 0.0150Epoch 4/10: [============================= ] 61/63 batches, loss: 0.0151Epoch 4/10: [============================= ] 62/63 batches, loss: 0.0153Epoch 4/10: [==============================] 63/63 batches, loss: 0.0153
[2025-05-01 12:14:38,995][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0153
[2025-05-01 12:14:39,184][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0382, Metrics: {'mse': 0.03820837289094925, 'rmse': 0.19546962140176474, 'r2': 0.4110819101333618}
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.0113Epoch 5/10: [                              ] 2/63 batches, loss: 0.0082Epoch 5/10: [=                             ] 3/63 batches, loss: 0.0114Epoch 5/10: [=                             ] 4/63 batches, loss: 0.0153Epoch 5/10: [==                            ] 5/63 batches, loss: 0.0170Epoch 5/10: [==                            ] 6/63 batches, loss: 0.0162Epoch 5/10: [===                           ] 7/63 batches, loss: 0.0168Epoch 5/10: [===                           ] 8/63 batches, loss: 0.0171Epoch 5/10: [====                          ] 9/63 batches, loss: 0.0171Epoch 5/10: [====                          ] 10/63 batches, loss: 0.0166Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.0175Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.0167Epoch 5/10: [======                        ] 13/63 batches, loss: 0.0165Epoch 5/10: [======                        ] 14/63 batches, loss: 0.0162Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.0154Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.0150Epoch 5/10: [========                      ] 17/63 batches, loss: 0.0146Epoch 5/10: [========                      ] 18/63 batches, loss: 0.0145Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.0141Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.0139Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.0138Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.0136Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.0138Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.0135Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.0134Epoch 5/10: [============                  ] 26/63 batches, loss: 0.0136Epoch 5/10: [============                  ] 27/63 batches, loss: 0.0134Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.0132Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.0136Epoch 5/10: [==============                ] 30/63 batches, loss: 0.0135Epoch 5/10: [==============                ] 31/63 batches, loss: 0.0134Epoch 5/10: [===============               ] 32/63 batches, loss: 0.0134Epoch 5/10: [===============               ] 33/63 batches, loss: 0.0135Epoch 5/10: [================              ] 34/63 batches, loss: 0.0135Epoch 5/10: [================              ] 35/63 batches, loss: 0.0134Epoch 5/10: [=================             ] 36/63 batches, loss: 0.0133Epoch 5/10: [=================             ] 37/63 batches, loss: 0.0132Epoch 5/10: [==================            ] 38/63 batches, loss: 0.0131Epoch 5/10: [==================            ] 39/63 batches, loss: 0.0130Epoch 5/10: [===================           ] 40/63 batches, loss: 0.0132Epoch 5/10: [===================           ] 41/63 batches, loss: 0.0132Epoch 5/10: [====================          ] 42/63 batches, loss: 0.0131Epoch 5/10: [====================          ] 43/63 batches, loss: 0.0134Epoch 5/10: [====================          ] 44/63 batches, loss: 0.0133Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.0131Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.0131Epoch 5/10: [======================        ] 47/63 batches, loss: 0.0130Epoch 5/10: [======================        ] 48/63 batches, loss: 0.0131Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.0130Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.0130Epoch 5/10: [========================      ] 51/63 batches, loss: 0.0131Epoch 5/10: [========================      ] 52/63 batches, loss: 0.0132Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.0133Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.0132Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.0134Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.0133Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.0133Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.0134Epoch 5/10: [============================  ] 59/63 batches, loss: 0.0133Epoch 5/10: [============================  ] 60/63 batches, loss: 0.0135Epoch 5/10: [============================= ] 61/63 batches, loss: 0.0136Epoch 5/10: [============================= ] 62/63 batches, loss: 0.0135Epoch 5/10: [==============================] 63/63 batches, loss: 0.0134
[2025-05-01 12:14:45,898][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0134
[2025-05-01 12:14:46,094][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0359, Metrics: {'mse': 0.03661568462848663, 'rmse': 0.19135225273951345, 'r2': 0.43563055992126465}
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.0175Epoch 6/10: [                              ] 2/63 batches, loss: 0.0172Epoch 6/10: [=                             ] 3/63 batches, loss: 0.0150Epoch 6/10: [=                             ] 4/63 batches, loss: 0.0135Epoch 6/10: [==                            ] 5/63 batches, loss: 0.0130Epoch 6/10: [==                            ] 6/63 batches, loss: 0.0130Epoch 6/10: [===                           ] 7/63 batches, loss: 0.0124Epoch 6/10: [===                           ] 8/63 batches, loss: 0.0123Epoch 6/10: [====                          ] 9/63 batches, loss: 0.0127Epoch 6/10: [====                          ] 10/63 batches, loss: 0.0122Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.0117Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.0131Epoch 6/10: [======                        ] 13/63 batches, loss: 0.0130Epoch 6/10: [======                        ] 14/63 batches, loss: 0.0134Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.0139Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.0137Epoch 6/10: [========                      ] 17/63 batches, loss: 0.0132Epoch 6/10: [========                      ] 18/63 batches, loss: 0.0130Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.0125Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.0127Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.0127Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.0124Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.0124Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.0121Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.0122Epoch 6/10: [============                  ] 26/63 batches, loss: 0.0120Epoch 6/10: [============                  ] 27/63 batches, loss: 0.0119Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.0117Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.0116Epoch 6/10: [==============                ] 30/63 batches, loss: 0.0116Epoch 6/10: [==============                ] 31/63 batches, loss: 0.0115Epoch 6/10: [===============               ] 32/63 batches, loss: 0.0112Epoch 6/10: [===============               ] 33/63 batches, loss: 0.0111Epoch 6/10: [================              ] 34/63 batches, loss: 0.0111Epoch 6/10: [================              ] 35/63 batches, loss: 0.0110Epoch 6/10: [=================             ] 36/63 batches, loss: 0.0110Epoch 6/10: [=================             ] 37/63 batches, loss: 0.0108Epoch 6/10: [==================            ] 38/63 batches, loss: 0.0107Epoch 6/10: [==================            ] 39/63 batches, loss: 0.0108Epoch 6/10: [===================           ] 40/63 batches, loss: 0.0109Epoch 6/10: [===================           ] 41/63 batches, loss: 0.0109Epoch 6/10: [====================          ] 42/63 batches, loss: 0.0108Epoch 6/10: [====================          ] 43/63 batches, loss: 0.0107Epoch 6/10: [====================          ] 44/63 batches, loss: 0.0106Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.0105Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.0105Epoch 6/10: [======================        ] 47/63 batches, loss: 0.0105Epoch 6/10: [======================        ] 48/63 batches, loss: 0.0105Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.0106Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.0107Epoch 6/10: [========================      ] 51/63 batches, loss: 0.0107Epoch 6/10: [========================      ] 52/63 batches, loss: 0.0106Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.0105Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.0105Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.0106Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.0106Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.0106Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.0105Epoch 6/10: [============================  ] 59/63 batches, loss: 0.0105Epoch 6/10: [============================  ] 60/63 batches, loss: 0.0105Epoch 6/10: [============================= ] 61/63 batches, loss: 0.0105Epoch 6/10: [============================= ] 62/63 batches, loss: 0.0106Epoch 6/10: [==============================] 63/63 batches, loss: 0.0104
[2025-05-01 12:14:52,838][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0104
[2025-05-01 12:14:53,027][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0371, Metrics: {'mse': 0.03692213073372841, 'rmse': 0.19215132248758635, 'r2': 0.4309071898460388}
[2025-05-01 12:14:53,028][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/63 batches, loss: 0.0070Epoch 7/10: [                              ] 2/63 batches, loss: 0.0090Epoch 7/10: [=                             ] 3/63 batches, loss: 0.0095Epoch 7/10: [=                             ] 4/63 batches, loss: 0.0104Epoch 7/10: [==                            ] 5/63 batches, loss: 0.0096Epoch 7/10: [==                            ] 6/63 batches, loss: 0.0110Epoch 7/10: [===                           ] 7/63 batches, loss: 0.0111Epoch 7/10: [===                           ] 8/63 batches, loss: 0.0107Epoch 7/10: [====                          ] 9/63 batches, loss: 0.0100Epoch 7/10: [====                          ] 10/63 batches, loss: 0.0102Epoch 7/10: [=====                         ] 11/63 batches, loss: 0.0096Epoch 7/10: [=====                         ] 12/63 batches, loss: 0.0095Epoch 7/10: [======                        ] 13/63 batches, loss: 0.0091Epoch 7/10: [======                        ] 14/63 batches, loss: 0.0093Epoch 7/10: [=======                       ] 15/63 batches, loss: 0.0093Epoch 7/10: [=======                       ] 16/63 batches, loss: 0.0096Epoch 7/10: [========                      ] 17/63 batches, loss: 0.0095Epoch 7/10: [========                      ] 18/63 batches, loss: 0.0091Epoch 7/10: [=========                     ] 19/63 batches, loss: 0.0094Epoch 7/10: [=========                     ] 20/63 batches, loss: 0.0093Epoch 7/10: [==========                    ] 21/63 batches, loss: 0.0092Epoch 7/10: [==========                    ] 22/63 batches, loss: 0.0091Epoch 7/10: [==========                    ] 23/63 batches, loss: 0.0089Epoch 7/10: [===========                   ] 24/63 batches, loss: 0.0088Epoch 7/10: [===========                   ] 25/63 batches, loss: 0.0087Epoch 7/10: [============                  ] 26/63 batches, loss: 0.0087Epoch 7/10: [============                  ] 27/63 batches, loss: 0.0085Epoch 7/10: [=============                 ] 28/63 batches, loss: 0.0089Epoch 7/10: [=============                 ] 29/63 batches, loss: 0.0087Epoch 7/10: [==============                ] 30/63 batches, loss: 0.0088Epoch 7/10: [==============                ] 31/63 batches, loss: 0.0088Epoch 7/10: [===============               ] 32/63 batches, loss: 0.0087Epoch 7/10: [===============               ] 33/63 batches, loss: 0.0087Epoch 7/10: [================              ] 34/63 batches, loss: 0.0087Epoch 7/10: [================              ] 35/63 batches, loss: 0.0087Epoch 7/10: [=================             ] 36/63 batches, loss: 0.0086Epoch 7/10: [=================             ] 37/63 batches, loss: 0.0087Epoch 7/10: [==================            ] 38/63 batches, loss: 0.0086Epoch 7/10: [==================            ] 39/63 batches, loss: 0.0087Epoch 7/10: [===================           ] 40/63 batches, loss: 0.0087Epoch 7/10: [===================           ] 41/63 batches, loss: 0.0087Epoch 7/10: [====================          ] 42/63 batches, loss: 0.0087Epoch 7/10: [====================          ] 43/63 batches, loss: 0.0086Epoch 7/10: [====================          ] 44/63 batches, loss: 0.0085Epoch 7/10: [=====================         ] 45/63 batches, loss: 0.0084Epoch 7/10: [=====================         ] 46/63 batches, loss: 0.0083Epoch 7/10: [======================        ] 47/63 batches, loss: 0.0082Epoch 7/10: [======================        ] 48/63 batches, loss: 0.0084Epoch 7/10: [=======================       ] 49/63 batches, loss: 0.0084Epoch 7/10: [=======================       ] 50/63 batches, loss: 0.0083Epoch 7/10: [========================      ] 51/63 batches, loss: 0.0084Epoch 7/10: [========================      ] 52/63 batches, loss: 0.0084Epoch 7/10: [=========================     ] 53/63 batches, loss: 0.0084Epoch 7/10: [=========================     ] 54/63 batches, loss: 0.0083Epoch 7/10: [==========================    ] 55/63 batches, loss: 0.0083Epoch 7/10: [==========================    ] 56/63 batches, loss: 0.0084Epoch 7/10: [===========================   ] 57/63 batches, loss: 0.0084Epoch 7/10: [===========================   ] 58/63 batches, loss: 0.0085Epoch 7/10: [============================  ] 59/63 batches, loss: 0.0085Epoch 7/10: [============================  ] 60/63 batches, loss: 0.0085Epoch 7/10: [============================= ] 61/63 batches, loss: 0.0085Epoch 7/10: [============================= ] 62/63 batches, loss: 0.0085Epoch 7/10: [==============================] 63/63 batches, loss: 0.0084
[2025-05-01 12:14:59,376][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0084
[2025-05-01 12:14:59,568][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0329, Metrics: {'mse': 0.03260713443160057, 'rmse': 0.18057445675288786, 'r2': 0.4974156618118286}
Epoch 8/10: [Epoch 8/10: [                              ] 1/63 batches, loss: 0.0032Epoch 8/10: [                              ] 2/63 batches, loss: 0.0052Epoch 8/10: [=                             ] 3/63 batches, loss: 0.0054Epoch 8/10: [=                             ] 4/63 batches, loss: 0.0054Epoch 8/10: [==                            ] 5/63 batches, loss: 0.0060Epoch 8/10: [==                            ] 6/63 batches, loss: 0.0059Epoch 8/10: [===                           ] 7/63 batches, loss: 0.0061Epoch 8/10: [===                           ] 8/63 batches, loss: 0.0068Epoch 8/10: [====                          ] 9/63 batches, loss: 0.0073Epoch 8/10: [====                          ] 10/63 batches, loss: 0.0075Epoch 8/10: [=====                         ] 11/63 batches, loss: 0.0081Epoch 8/10: [=====                         ] 12/63 batches, loss: 0.0087Epoch 8/10: [======                        ] 13/63 batches, loss: 0.0090Epoch 8/10: [======                        ] 14/63 batches, loss: 0.0090Epoch 8/10: [=======                       ] 15/63 batches, loss: 0.0087Epoch 8/10: [=======                       ] 16/63 batches, loss: 0.0083Epoch 8/10: [========                      ] 17/63 batches, loss: 0.0082Epoch 8/10: [========                      ] 18/63 batches, loss: 0.0083Epoch 8/10: [=========                     ] 19/63 batches, loss: 0.0081Epoch 8/10: [=========                     ] 20/63 batches, loss: 0.0083Epoch 8/10: [==========                    ] 21/63 batches, loss: 0.0087Epoch 8/10: [==========                    ] 22/63 batches, loss: 0.0090Epoch 8/10: [==========                    ] 23/63 batches, loss: 0.0091Epoch 8/10: [===========                   ] 24/63 batches, loss: 0.0094Epoch 8/10: [===========                   ] 25/63 batches, loss: 0.0100Epoch 8/10: [============                  ] 26/63 batches, loss: 0.0098Epoch 8/10: [============                  ] 27/63 batches, loss: 0.0097Epoch 8/10: [=============                 ] 28/63 batches, loss: 0.0096Epoch 8/10: [=============                 ] 29/63 batches, loss: 0.0096Epoch 8/10: [==============                ] 30/63 batches, loss: 0.0097Epoch 8/10: [==============                ] 31/63 batches, loss: 0.0097Epoch 8/10: [===============               ] 32/63 batches, loss: 0.0096Epoch 8/10: [===============               ] 33/63 batches, loss: 0.0098Epoch 8/10: [================              ] 34/63 batches, loss: 0.0096Epoch 8/10: [================              ] 35/63 batches, loss: 0.0095Epoch 8/10: [=================             ] 36/63 batches, loss: 0.0096Epoch 8/10: [=================             ] 37/63 batches, loss: 0.0096Epoch 8/10: [==================            ] 38/63 batches, loss: 0.0097Epoch 8/10: [==================            ] 39/63 batches, loss: 0.0098Epoch 8/10: [===================           ] 40/63 batches, loss: 0.0097Epoch 8/10: [===================           ] 41/63 batches, loss: 0.0097Epoch 8/10: [====================          ] 42/63 batches, loss: 0.0097Epoch 8/10: [====================          ] 43/63 batches, loss: 0.0099Epoch 8/10: [====================          ] 44/63 batches, loss: 0.0101Epoch 8/10: [=====================         ] 45/63 batches, loss: 0.0102Epoch 8/10: [=====================         ] 46/63 batches, loss: 0.0102Epoch 8/10: [======================        ] 47/63 batches, loss: 0.0101Epoch 8/10: [======================        ] 48/63 batches, loss: 0.0101Epoch 8/10: [=======================       ] 49/63 batches, loss: 0.0103Epoch 8/10: [=======================       ] 50/63 batches, loss: 0.0104Epoch 8/10: [========================      ] 51/63 batches, loss: 0.0104Epoch 8/10: [========================      ] 52/63 batches, loss: 0.0105Epoch 8/10: [=========================     ] 53/63 batches, loss: 0.0105Epoch 8/10: [=========================     ] 54/63 batches, loss: 0.0104Epoch 8/10: [==========================    ] 55/63 batches, loss: 0.0105Epoch 8/10: [==========================    ] 56/63 batches, loss: 0.0105Epoch 8/10: [===========================   ] 57/63 batches, loss: 0.0106Epoch 8/10: [===========================   ] 58/63 batches, loss: 0.0105Epoch 8/10: [============================  ] 59/63 batches, loss: 0.0104Epoch 8/10: [============================  ] 60/63 batches, loss: 0.0104Epoch 8/10: [============================= ] 61/63 batches, loss: 0.0103Epoch 8/10: [============================= ] 62/63 batches, loss: 0.0103Epoch 8/10: [==============================] 63/63 batches, loss: 0.0106
[2025-05-01 12:15:06,361][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0106
[2025-05-01 12:15:06,561][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0266, Metrics: {'mse': 0.026841821148991585, 'rmse': 0.16383473730864156, 'r2': 0.5862782597541809}
Epoch 9/10: [Epoch 9/10: [                              ] 1/63 batches, loss: 0.0077Epoch 9/10: [                              ] 2/63 batches, loss: 0.0091Epoch 9/10: [=                             ] 3/63 batches, loss: 0.0088Epoch 9/10: [=                             ] 4/63 batches, loss: 0.0091Epoch 9/10: [==                            ] 5/63 batches, loss: 0.0097Epoch 9/10: [==                            ] 6/63 batches, loss: 0.0097Epoch 9/10: [===                           ] 7/63 batches, loss: 0.0090Epoch 9/10: [===                           ] 8/63 batches, loss: 0.0091Epoch 9/10: [====                          ] 9/63 batches, loss: 0.0086Epoch 9/10: [====                          ] 10/63 batches, loss: 0.0085Epoch 9/10: [=====                         ] 11/63 batches, loss: 0.0082Epoch 9/10: [=====                         ] 12/63 batches, loss: 0.0082Epoch 9/10: [======                        ] 13/63 batches, loss: 0.0080Epoch 9/10: [======                        ] 14/63 batches, loss: 0.0078Epoch 9/10: [=======                       ] 15/63 batches, loss: 0.0080Epoch 9/10: [=======                       ] 16/63 batches, loss: 0.0077Epoch 9/10: [========                      ] 17/63 batches, loss: 0.0076Epoch 9/10: [========                      ] 18/63 batches, loss: 0.0077Epoch 9/10: [=========                     ] 19/63 batches, loss: 0.0075Epoch 9/10: [=========                     ] 20/63 batches, loss: 0.0077Epoch 9/10: [==========                    ] 21/63 batches, loss: 0.0077Epoch 9/10: [==========                    ] 22/63 batches, loss: 0.0076Epoch 9/10: [==========                    ] 23/63 batches, loss: 0.0077Epoch 9/10: [===========                   ] 24/63 batches, loss: 0.0078Epoch 9/10: [===========                   ] 25/63 batches, loss: 0.0080Epoch 9/10: [============                  ] 26/63 batches, loss: 0.0080Epoch 9/10: [============                  ] 27/63 batches, loss: 0.0079Epoch 9/10: [=============                 ] 28/63 batches, loss: 0.0079Epoch 9/10: [=============                 ] 29/63 batches, loss: 0.0081Epoch 9/10: [==============                ] 30/63 batches, loss: 0.0080Epoch 9/10: [==============                ] 31/63 batches, loss: 0.0079Epoch 9/10: [===============               ] 32/63 batches, loss: 0.0079Epoch 9/10: [===============               ] 33/63 batches, loss: 0.0079Epoch 9/10: [================              ] 34/63 batches, loss: 0.0079Epoch 9/10: [================              ] 35/63 batches, loss: 0.0080Epoch 9/10: [=================             ] 36/63 batches, loss: 0.0080Epoch 9/10: [=================             ] 37/63 batches, loss: 0.0081Epoch 9/10: [==================            ] 38/63 batches, loss: 0.0082Epoch 9/10: [==================            ] 39/63 batches, loss: 0.0082Epoch 9/10: [===================           ] 40/63 batches, loss: 0.0081Epoch 9/10: [===================           ] 41/63 batches, loss: 0.0081Epoch 9/10: [====================          ] 42/63 batches, loss: 0.0081Epoch 9/10: [====================          ] 43/63 batches, loss: 0.0081Epoch 9/10: [====================          ] 44/63 batches, loss: 0.0081Epoch 9/10: [=====================         ] 45/63 batches, loss: 0.0081Epoch 9/10: [=====================         ] 46/63 batches, loss: 0.0080Epoch 9/10: [======================        ] 47/63 batches, loss: 0.0080Epoch 9/10: [======================        ] 48/63 batches, loss: 0.0080Epoch 9/10: [=======================       ] 49/63 batches, loss: 0.0081Epoch 9/10: [=======================       ] 50/63 batches, loss: 0.0081Epoch 9/10: [========================      ] 51/63 batches, loss: 0.0081Epoch 9/10: [========================      ] 52/63 batches, loss: 0.0081Epoch 9/10: [=========================     ] 53/63 batches, loss: 0.0082Epoch 9/10: [=========================     ] 54/63 batches, loss: 0.0081Epoch 9/10: [==========================    ] 55/63 batches, loss: 0.0081Epoch 9/10: [==========================    ] 56/63 batches, loss: 0.0081Epoch 9/10: [===========================   ] 57/63 batches, loss: 0.0081Epoch 9/10: [===========================   ] 58/63 batches, loss: 0.0081Epoch 9/10: [============================  ] 59/63 batches, loss: 0.0081Epoch 9/10: [============================  ] 60/63 batches, loss: 0.0080Epoch 9/10: [============================= ] 61/63 batches, loss: 0.0080Epoch 9/10: [============================= ] 62/63 batches, loss: 0.0080Epoch 9/10: [==============================] 63/63 batches, loss: 0.0079
[2025-05-01 12:15:13,321][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0079
[2025-05-01 12:15:13,536][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0328, Metrics: {'mse': 0.03263333812355995, 'rmse': 0.18064699865638498, 'r2': 0.4970117211341858}
[2025-05-01 12:15:13,537][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 10/10: [Epoch 10/10: [                              ] 1/63 batches, loss: 0.0058Epoch 10/10: [                              ] 2/63 batches, loss: 0.0090Epoch 10/10: [=                             ] 3/63 batches, loss: 0.0076Epoch 10/10: [=                             ] 4/63 batches, loss: 0.0071Epoch 10/10: [==                            ] 5/63 batches, loss: 0.0074Epoch 10/10: [==                            ] 6/63 batches, loss: 0.0073Epoch 10/10: [===                           ] 7/63 batches, loss: 0.0073Epoch 10/10: [===                           ] 8/63 batches, loss: 0.0069Epoch 10/10: [====                          ] 9/63 batches, loss: 0.0071Epoch 10/10: [====                          ] 10/63 batches, loss: 0.0071Epoch 10/10: [=====                         ] 11/63 batches, loss: 0.0072Epoch 10/10: [=====                         ] 12/63 batches, loss: 0.0071Epoch 10/10: [======                        ] 13/63 batches, loss: 0.0072Epoch 10/10: [======                        ] 14/63 batches, loss: 0.0072Epoch 10/10: [=======                       ] 15/63 batches, loss: 0.0070Epoch 10/10: [=======                       ] 16/63 batches, loss: 0.0069Epoch 10/10: [========                      ] 17/63 batches, loss: 0.0069Epoch 10/10: [========                      ] 18/63 batches, loss: 0.0070Epoch 10/10: [=========                     ] 19/63 batches, loss: 0.0072Epoch 10/10: [=========                     ] 20/63 batches, loss: 0.0071Epoch 10/10: [==========                    ] 21/63 batches, loss: 0.0073Epoch 10/10: [==========                    ] 22/63 batches, loss: 0.0075Epoch 10/10: [==========                    ] 23/63 batches, loss: 0.0075Epoch 10/10: [===========                   ] 24/63 batches, loss: 0.0076Epoch 10/10: [===========                   ] 25/63 batches, loss: 0.0077Epoch 10/10: [============                  ] 26/63 batches, loss: 0.0079Epoch 10/10: [============                  ] 27/63 batches, loss: 0.0078Epoch 10/10: [=============                 ] 28/63 batches, loss: 0.0078Epoch 10/10: [=============                 ] 29/63 batches, loss: 0.0077Epoch 10/10: [==============                ] 30/63 batches, loss: 0.0077Epoch 10/10: [==============                ] 31/63 batches, loss: 0.0076Epoch 10/10: [===============               ] 32/63 batches, loss: 0.0078Epoch 10/10: [===============               ] 33/63 batches, loss: 0.0080Epoch 10/10: [================              ] 34/63 batches, loss: 0.0079Epoch 10/10: [================              ] 35/63 batches, loss: 0.0078Epoch 10/10: [=================             ] 36/63 batches, loss: 0.0077Epoch 10/10: [=================             ] 37/63 batches, loss: 0.0078Epoch 10/10: [==================            ] 38/63 batches, loss: 0.0077Epoch 10/10: [==================            ] 39/63 batches, loss: 0.0078Epoch 10/10: [===================           ] 40/63 batches, loss: 0.0079Epoch 10/10: [===================           ] 41/63 batches, loss: 0.0078Epoch 10/10: [====================          ] 42/63 batches, loss: 0.0077Epoch 10/10: [====================          ] 43/63 batches, loss: 0.0077Epoch 10/10: [====================          ] 44/63 batches, loss: 0.0076Epoch 10/10: [=====================         ] 45/63 batches, loss: 0.0076Epoch 10/10: [=====================         ] 46/63 batches, loss: 0.0076Epoch 10/10: [======================        ] 47/63 batches, loss: 0.0076Epoch 10/10: [======================        ] 48/63 batches, loss: 0.0076Epoch 10/10: [=======================       ] 49/63 batches, loss: 0.0076Epoch 10/10: [=======================       ] 50/63 batches, loss: 0.0075Epoch 10/10: [========================      ] 51/63 batches, loss: 0.0074Epoch 10/10: [========================      ] 52/63 batches, loss: 0.0073Epoch 10/10: [=========================     ] 53/63 batches, loss: 0.0073Epoch 10/10: [=========================     ] 54/63 batches, loss: 0.0073Epoch 10/10: [==========================    ] 55/63 batches, loss: 0.0073Epoch 10/10: [==========================    ] 56/63 batches, loss: 0.0074Epoch 10/10: [===========================   ] 57/63 batches, loss: 0.0073Epoch 10/10: [===========================   ] 58/63 batches, loss: 0.0073Epoch 10/10: [============================  ] 59/63 batches, loss: 0.0073Epoch 10/10: [============================  ] 60/63 batches, loss: 0.0072Epoch 10/10: [============================= ] 61/63 batches, loss: 0.0072Epoch 10/10: [============================= ] 62/63 batches, loss: 0.0073Epoch 10/10: [==============================] 63/63 batches, loss: 0.0072
[2025-05-01 12:15:19,912][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0072
[2025-05-01 12:15:20,110][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0373, Metrics: {'mse': 0.0370272658765316, 'rmse': 0.1924247018356313, 'r2': 0.4292866587638855}
[2025-05-01 12:15:20,111][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
[2025-05-01 12:15:20,111][src.training.lm_trainer][INFO] - Training completed in 68.93 seconds
[2025-05-01 12:15:20,111][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 12:15:22,564][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.007992316037416458, 'rmse': 0.089399754123915, 'r2': 0.7396416068077087}
[2025-05-01 12:15:22,564][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.026841821148991585, 'rmse': 0.16383473730864156, 'r2': 0.5862782597541809}
[2025-05-01 12:15:22,564][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.07233811914920807, 'rmse': 0.26895746717503133, 'r2': -0.24707698822021484}
[2025-05-01 12:15:24,205][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/model.pt
[2025-05-01 12:15:24,210][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▄▃▂▂▁
wandb:     best_val_mse █▆▄▂▂▂▁
wandb:      best_val_r2 ▁▃▅▇▇▇█
wandb:    best_val_rmse █▆▅▃▃▂▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▄▅▆▇▆▇▇▇
wandb:       train_loss █▂▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▄▃▂▂▂▁▂▂
wandb:          val_mse █▆▄▂▂▂▂▁▂▂
wandb:           val_r2 ▁▃▅▇▇▇▇█▇▇
wandb:         val_rmse █▆▅▃▃▃▂▁▂▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02665
wandb:     best_val_mse 0.02684
wandb:      best_val_r2 0.58628
wandb:    best_val_rmse 0.16383
wandb:            epoch 10
wandb:   final_test_mse 0.07234
wandb:    final_test_r2 -0.24708
wandb:  final_test_rmse 0.26896
wandb:  final_train_mse 0.00799
wandb:   final_train_r2 0.73964
wandb: final_train_rmse 0.0894
wandb:    final_val_mse 0.02684
wandb:     final_val_r2 0.58628
wandb:   final_val_rmse 0.16383
wandb:    learning_rate 2e-05
wandb:       train_loss 0.00717
wandb:       train_time 68.92555
wandb:         val_loss 0.03734
wandb:          val_mse 0.03703
wandb:           val_r2 0.42929
wandb:         val_rmse 0.19242
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_121400-dfer04xk
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_121400-dfer04xk/logs
Experiment finetune_complexity_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/results.json
Running experiment: finetune_question_type_en
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[en]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"         +training.debug_mode=true         "experiment_name=finetune_question_type_en"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 12:15:36,556][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type
experiment_name: finetune_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
  debug_mode: true
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-01 12:15:36,557][__main__][INFO] - Normalized task: question_type
[2025-05-01 12:15:36,557][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-01 12:15:36,557][__main__][INFO] - Determined Task Type: classification
[2025-05-01 12:15:36,561][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-05-01 12:15:36,561][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 12:15:37,977][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 12:15:40,196][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 12:15:40,196][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:15:40,242][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:15:40,264][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:15:40,341][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-05-01 12:15:40,351][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:15:40,352][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-05-01 12:15:40,353][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:15:40,377][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:15:40,406][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:15:40,419][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-05-01 12:15:40,420][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:15:40,421][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-05-01 12:15:40,421][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:15:40,437][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:15:40,459][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:15:40,468][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-05-01 12:15:40,470][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:15:40,470][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-05-01 12:15:40,471][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-05-01 12:15:40,471][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:15:40,472][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:15:40,472][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:15:40,472][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:15:40,472][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-05-01 12:15:40,472][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-05-01 12:15:40,472][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-05-01 12:15:40,472][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 12:15:40,472][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:15:40,472][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:15:40,473][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:15:40,473][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:15:40,473][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-01 12:15:40,473][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-01 12:15:40,473][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-05-01 12:15:40,473][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 12:15:40,473][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:15:40,473][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:15:40,473][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:15:40,473][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:15:40,474][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-01 12:15:40,474][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-01 12:15:40,474][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-05-01 12:15:40,474][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 12:15:40,474][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-05-01 12:15:40,474][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 12:15:40,474][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 12:15:40,474][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-01 12:15:40,475][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 12:15:44,079][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 12:15:44,080][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 12:15:44,080][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 12:15:44,080][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 12:15:44,085][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 12:15:44,085][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 12:15:44,085][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 12:15:44,085][__main__][INFO] - Successfully created lm_probe model for en
[2025-05-01 12:15:44,086][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 12:15:44,086][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.7244Epoch 1/10: [                              ] 2/75 batches, loss: 0.7709Epoch 1/10: [=                             ] 3/75 batches, loss: 0.7345Epoch 1/10: [=                             ] 4/75 batches, loss: 0.7469Epoch 1/10: [==                            ] 5/75 batches, loss: 0.7655Epoch 1/10: [==                            ] 6/75 batches, loss: 0.7285Epoch 1/10: [==                            ] 7/75 batches, loss: 0.7406Epoch 1/10: [===                           ] 8/75 batches, loss: 0.7416Epoch 1/10: [===                           ] 9/75 batches, loss: 0.7494Epoch 1/10: [====                          ] 10/75 batches, loss: 0.7379Epoch 1/10: [====                          ] 11/75 batches, loss: 0.7309Epoch 1/10: [====                          ] 12/75 batches, loss: 0.7394Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.7360Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.7374Epoch 1/10: [======                        ] 15/75 batches, loss: 0.7416Epoch 1/10: [======                        ] 16/75 batches, loss: 0.7414Epoch 1/10: [======                        ] 17/75 batches, loss: 0.7419Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.7397Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.7407Epoch 1/10: [========                      ] 20/75 batches, loss: 0.7387Epoch 1/10: [========                      ] 21/75 batches, loss: 0.7353Epoch 1/10: [========                      ] 22/75 batches, loss: 0.7348Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.7330Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.7302Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.7263Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.7272Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.7254Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.7283Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.7299Epoch 1/10: [============                  ] 30/75 batches, loss: 0.7300Epoch 1/10: [============                  ] 31/75 batches, loss: 0.7288Epoch 1/10: [============                  ] 32/75 batches, loss: 0.7278Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.7278Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.7264Epoch 1/10: [==============                ] 35/75 batches, loss: 0.7268Epoch 1/10: [==============                ] 36/75 batches, loss: 0.7256Epoch 1/10: [==============                ] 37/75 batches, loss: 0.7222Epoch 1/10: [===============               ] 38/75 batches, loss: 0.7218Epoch 1/10: [===============               ] 39/75 batches, loss: 0.7207Epoch 1/10: [================              ] 40/75 batches, loss: 0.7197Epoch 1/10: [================              ] 41/75 batches, loss: 0.7194Epoch 1/10: [================              ] 42/75 batches, loss: 0.7208Epoch 1/10: [=================             ] 43/75 batches, loss: 0.7211Epoch 1/10: [=================             ] 44/75 batches, loss: 0.7192Epoch 1/10: [==================            ] 45/75 batches, loss: 0.7178Epoch 1/10: [==================            ] 46/75 batches, loss: 0.7169Epoch 1/10: [==================            ] 47/75 batches, loss: 0.7182Epoch 1/10: [===================           ] 48/75 batches, loss: 0.7161Epoch 1/10: [===================           ] 49/75 batches, loss: 0.7159Epoch 1/10: [====================          ] 50/75 batches, loss: 0.7154Epoch 1/10: [====================          ] 51/75 batches, loss: 0.7154Epoch 1/10: [====================          ] 52/75 batches, loss: 0.7168Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.7172Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.7176Epoch 1/10: [======================        ] 55/75 batches, loss: 0.7172Epoch 1/10: [======================        ] 56/75 batches, loss: 0.7161Epoch 1/10: [======================        ] 57/75 batches, loss: 0.7151Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.7146Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.7146Epoch 1/10: [========================      ] 60/75 batches, loss: 0.7156Epoch 1/10: [========================      ] 61/75 batches, loss: 0.7145Epoch 1/10: [========================      ] 62/75 batches, loss: 0.7131Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.7118Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.7114Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.7120Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.7115Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.7095Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.7084Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.7064Epoch 1/10: [============================  ] 70/75 batches, loss: 0.7060Epoch 1/10: [============================  ] 71/75 batches, loss: 0.7046Epoch 1/10: [============================  ] 72/75 batches, loss: 0.7022Epoch 1/10: [============================= ] 73/75 batches, loss: 0.7006Epoch 1/10: [============================= ] 74/75 batches, loss: 0.6999Epoch 1/10: [==============================] 75/75 batches, loss: 0.6998
[2025-05-01 12:15:54,049][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6998
[2025-05-01 12:15:54,274][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6192, Metrics: {'accuracy': 0.9583333333333334, 'f1': 0.96, 'precision': 0.9230769230769231, 'recall': 1.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.5898Epoch 2/10: [                              ] 2/75 batches, loss: 0.6357Epoch 2/10: [=                             ] 3/75 batches, loss: 0.6189Epoch 2/10: [=                             ] 4/75 batches, loss: 0.6576Epoch 2/10: [==                            ] 5/75 batches, loss: 0.6427Epoch 2/10: [==                            ] 6/75 batches, loss: 0.6484Epoch 2/10: [==                            ] 7/75 batches, loss: 0.6562Epoch 2/10: [===                           ] 8/75 batches, loss: 0.6519Epoch 2/10: [===                           ] 9/75 batches, loss: 0.6442Epoch 2/10: [====                          ] 10/75 batches, loss: 0.6339Epoch 2/10: [====                          ] 11/75 batches, loss: 0.6304Epoch 2/10: [====                          ] 12/75 batches, loss: 0.6263Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.6254Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.6207Epoch 2/10: [======                        ] 15/75 batches, loss: 0.6138Epoch 2/10: [======                        ] 16/75 batches, loss: 0.6117Epoch 2/10: [======                        ] 17/75 batches, loss: 0.6112Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.6107Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.6047Epoch 2/10: [========                      ] 20/75 batches, loss: 0.6032Epoch 2/10: [========                      ] 21/75 batches, loss: 0.5990Epoch 2/10: [========                      ] 22/75 batches, loss: 0.5964Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.5949Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.5953Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.5908Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.5898Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.5912Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.5897Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.5896Epoch 2/10: [============                  ] 30/75 batches, loss: 0.5882Epoch 2/10: [============                  ] 31/75 batches, loss: 0.5892Epoch 2/10: [============                  ] 32/75 batches, loss: 0.5884Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.5876Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.5861Epoch 2/10: [==============                ] 35/75 batches, loss: 0.5834Epoch 2/10: [==============                ] 36/75 batches, loss: 0.5833Epoch 2/10: [==============                ] 37/75 batches, loss: 0.5826Epoch 2/10: [===============               ] 38/75 batches, loss: 0.5795Epoch 2/10: [===============               ] 39/75 batches, loss: 0.5760Epoch 2/10: [================              ] 40/75 batches, loss: 0.5735Epoch 2/10: [================              ] 41/75 batches, loss: 0.5713Epoch 2/10: [================              ] 42/75 batches, loss: 0.5692Epoch 2/10: [=================             ] 43/75 batches, loss: 0.5681Epoch 2/10: [=================             ] 44/75 batches, loss: 0.5665Epoch 2/10: [==================            ] 45/75 batches, loss: 0.5643Epoch 2/10: [==================            ] 46/75 batches, loss: 0.5630Epoch 2/10: [==================            ] 47/75 batches, loss: 0.5609Epoch 2/10: [===================           ] 48/75 batches, loss: 0.5606Epoch 2/10: [===================           ] 49/75 batches, loss: 0.5595Epoch 2/10: [====================          ] 50/75 batches, loss: 0.5605Epoch 2/10: [====================          ] 51/75 batches, loss: 0.5604Epoch 2/10: [====================          ] 52/75 batches, loss: 0.5594Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.5589Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.5581Epoch 2/10: [======================        ] 55/75 batches, loss: 0.5563Epoch 2/10: [======================        ] 56/75 batches, loss: 0.5555Epoch 2/10: [======================        ] 57/75 batches, loss: 0.5559Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.5552Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.5545Epoch 2/10: [========================      ] 60/75 batches, loss: 0.5542Epoch 2/10: [========================      ] 61/75 batches, loss: 0.5535Epoch 2/10: [========================      ] 62/75 batches, loss: 0.5514Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.5510Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.5503Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.5514Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.5500Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.5486Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.5481Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.5491Epoch 2/10: [============================  ] 70/75 batches, loss: 0.5492Epoch 2/10: [============================  ] 71/75 batches, loss: 0.5482Epoch 2/10: [============================  ] 72/75 batches, loss: 0.5477Epoch 2/10: [============================= ] 73/75 batches, loss: 0.5471Epoch 2/10: [============================= ] 74/75 batches, loss: 0.5462Epoch 2/10: [==============================] 75/75 batches, loss: 0.5457
[2025-05-01 12:16:02,248][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5457
[2025-05-01 12:16:02,493][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5355, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315, 'precision': 0.9, 'recall': 1.0}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.5078Epoch 3/10: [                              ] 2/75 batches, loss: 0.4961Epoch 3/10: [=                             ] 3/75 batches, loss: 0.4835Epoch 3/10: [=                             ] 4/75 batches, loss: 0.4891Epoch 3/10: [==                            ] 5/75 batches, loss: 0.4831Epoch 3/10: [==                            ] 6/75 batches, loss: 0.4988Epoch 3/10: [==                            ] 7/75 batches, loss: 0.5000Epoch 3/10: [===                           ] 8/75 batches, loss: 0.5039Epoch 3/10: [===                           ] 9/75 batches, loss: 0.5068Epoch 3/10: [====                          ] 10/75 batches, loss: 0.5067Epoch 3/10: [====                          ] 11/75 batches, loss: 0.5066Epoch 3/10: [====                          ] 12/75 batches, loss: 0.5084Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.5119Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.5063Epoch 3/10: [======                        ] 15/75 batches, loss: 0.5063Epoch 3/10: [======                        ] 16/75 batches, loss: 0.5078Epoch 3/10: [======                        ] 17/75 batches, loss: 0.5066Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.5066Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.5015Epoch 3/10: [========                      ] 20/75 batches, loss: 0.5042Epoch 3/10: [========                      ] 21/75 batches, loss: 0.5031Epoch 3/10: [========                      ] 22/75 batches, loss: 0.5021Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.4991Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.4994Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.4987Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.4990Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.5036Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.5053Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.5045Epoch 3/10: [============                  ] 30/75 batches, loss: 0.5066Epoch 3/10: [============                  ] 31/75 batches, loss: 0.5094Epoch 3/10: [============                  ] 32/75 batches, loss: 0.5072Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.5071Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.5050Epoch 3/10: [==============                ] 35/75 batches, loss: 0.5056Epoch 3/10: [==============                ] 36/75 batches, loss: 0.5057Epoch 3/10: [==============                ] 37/75 batches, loss: 0.5069Epoch 3/10: [===============               ] 38/75 batches, loss: 0.5069Epoch 3/10: [===============               ] 39/75 batches, loss: 0.5074Epoch 3/10: [================              ] 40/75 batches, loss: 0.5068Epoch 3/10: [================              ] 41/75 batches, loss: 0.5067Epoch 3/10: [================              ] 42/75 batches, loss: 0.5067Epoch 3/10: [=================             ] 43/75 batches, loss: 0.5061Epoch 3/10: [=================             ] 44/75 batches, loss: 0.5072Epoch 3/10: [==================            ] 45/75 batches, loss: 0.5066Epoch 3/10: [==================            ] 46/75 batches, loss: 0.5056Epoch 3/10: [==================            ] 47/75 batches, loss: 0.5061Epoch 3/10: [===================           ] 48/75 batches, loss: 0.5055Epoch 3/10: [===================           ] 49/75 batches, loss: 0.5065Epoch 3/10: [====================          ] 50/75 batches, loss: 0.5074Epoch 3/10: [====================          ] 51/75 batches, loss: 0.5078Epoch 3/10: [====================          ] 52/75 batches, loss: 0.5055Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.5064Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.5055Epoch 3/10: [======================        ] 55/75 batches, loss: 0.5054Epoch 3/10: [======================        ] 56/75 batches, loss: 0.5054Epoch 3/10: [======================        ] 57/75 batches, loss: 0.5057Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.5065Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.5065Epoch 3/10: [========================      ] 60/75 batches, loss: 0.5065Epoch 3/10: [========================      ] 61/75 batches, loss: 0.5061Epoch 3/10: [========================      ] 62/75 batches, loss: 0.5065Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.5061Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.5061Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.5064Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.5060Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.5067Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.5063Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.5060Epoch 3/10: [============================  ] 70/75 batches, loss: 0.5063Epoch 3/10: [============================  ] 71/75 batches, loss: 0.5061Epoch 3/10: [============================  ] 72/75 batches, loss: 0.5064Epoch 3/10: [============================= ] 73/75 batches, loss: 0.5071Epoch 3/10: [============================= ] 74/75 batches, loss: 0.5077Epoch 3/10: [==============================] 75/75 batches, loss: 0.5083
[2025-05-01 12:16:10,488][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5083
[2025-05-01 12:16:10,734][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.5405, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.935064935064935, 'precision': 0.8780487804878049, 'recall': 1.0}
[2025-05-01 12:16:10,734][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.5045Epoch 4/10: [                              ] 2/75 batches, loss: 0.5169Epoch 4/10: [=                             ] 3/75 batches, loss: 0.5049Epoch 4/10: [=                             ] 4/75 batches, loss: 0.5106Epoch 4/10: [==                            ] 5/75 batches, loss: 0.5143Epoch 4/10: [==                            ] 6/75 batches, loss: 0.4930Epoch 4/10: [==                            ] 7/75 batches, loss: 0.4981Epoch 4/10: [===                           ] 8/75 batches, loss: 0.5019Epoch 4/10: [===                           ] 9/75 batches, loss: 0.5022Epoch 4/10: [====                          ] 10/75 batches, loss: 0.5048Epoch 4/10: [====                          ] 11/75 batches, loss: 0.5039Epoch 4/10: [====                          ] 12/75 batches, loss: 0.4982Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.4951Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.4940Epoch 4/10: [======                        ] 15/75 batches, loss: 0.4900Epoch 4/10: [======                        ] 16/75 batches, loss: 0.4864Epoch 4/10: [======                        ] 17/75 batches, loss: 0.4884Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.4906Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.4926Epoch 4/10: [========                      ] 20/75 batches, loss: 0.4957Epoch 4/10: [========                      ] 21/75 batches, loss: 0.4972Epoch 4/10: [========                      ] 22/75 batches, loss: 0.4965Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.4989Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.5027Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.4980Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.4992Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.5011Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.5012Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.5013Epoch 4/10: [============                  ] 30/75 batches, loss: 0.5022Epoch 4/10: [============                  ] 31/75 batches, loss: 0.5045Epoch 4/10: [============                  ] 32/75 batches, loss: 0.5052Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.5066Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.5072Epoch 4/10: [==============                ] 35/75 batches, loss: 0.5079Epoch 4/10: [==============                ] 36/75 batches, loss: 0.5078Epoch 4/10: [==============                ] 37/75 batches, loss: 0.5096Epoch 4/10: [===============               ] 38/75 batches, loss: 0.5095Epoch 4/10: [===============               ] 39/75 batches, loss: 0.5087Epoch 4/10: [================              ] 40/75 batches, loss: 0.5056Epoch 4/10: [================              ] 41/75 batches, loss: 0.5056Epoch 4/10: [================              ] 42/75 batches, loss: 0.5055Epoch 4/10: [=================             ] 43/75 batches, loss: 0.5060Epoch 4/10: [=================             ] 44/75 batches, loss: 0.5060Epoch 4/10: [==================            ] 45/75 batches, loss: 0.5049Epoch 4/10: [==================            ] 46/75 batches, loss: 0.5049Epoch 4/10: [==================            ] 47/75 batches, loss: 0.5044Epoch 4/10: [===================           ] 48/75 batches, loss: 0.5044Epoch 4/10: [===================           ] 49/75 batches, loss: 0.5034Epoch 4/10: [====================          ] 50/75 batches, loss: 0.5030Epoch 4/10: [====================          ] 51/75 batches, loss: 0.5025Epoch 4/10: [====================          ] 52/75 batches, loss: 0.5021Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.5021Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.5030Epoch 4/10: [======================        ] 55/75 batches, loss: 0.5030Epoch 4/10: [======================        ] 56/75 batches, loss: 0.5026Epoch 4/10: [======================        ] 57/75 batches, loss: 0.5018Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.5002Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.5015Epoch 4/10: [========================      ] 60/75 batches, loss: 0.5012Epoch 4/10: [========================      ] 61/75 batches, loss: 0.5012Epoch 4/10: [========================      ] 62/75 batches, loss: 0.5012Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.5020Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.5024Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.5030Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.5027Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.5045Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.5038Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.5048Epoch 4/10: [============================  ] 70/75 batches, loss: 0.5051Epoch 4/10: [============================  ] 71/75 batches, loss: 0.5058Epoch 4/10: [============================  ] 72/75 batches, loss: 0.5054Epoch 4/10: [============================= ] 73/75 batches, loss: 0.5061Epoch 4/10: [============================= ] 74/75 batches, loss: 0.5060Epoch 4/10: [==============================] 75/75 batches, loss: 0.5073
[2025-05-01 12:16:18,331][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5073
[2025-05-01 12:16:18,590][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5446, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.935064935064935, 'precision': 0.8780487804878049, 'recall': 1.0}
[2025-05-01 12:16:18,591][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.4084Epoch 5/10: [                              ] 2/75 batches, loss: 0.4206Epoch 5/10: [=                             ] 3/75 batches, loss: 0.4087Epoch 5/10: [=                             ] 4/75 batches, loss: 0.4324Epoch 5/10: [==                            ] 5/75 batches, loss: 0.4277Epoch 5/10: [==                            ] 6/75 batches, loss: 0.4523Epoch 5/10: [==                            ] 7/75 batches, loss: 0.4698Epoch 5/10: [===                           ] 8/75 batches, loss: 0.4800Epoch 5/10: [===                           ] 9/75 batches, loss: 0.4826Epoch 5/10: [====                          ] 10/75 batches, loss: 0.4847Epoch 5/10: [====                          ] 11/75 batches, loss: 0.4864Epoch 5/10: [====                          ] 12/75 batches, loss: 0.4898Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.4836Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.4816Epoch 5/10: [======                        ] 15/75 batches, loss: 0.4847Epoch 5/10: [======                        ] 16/75 batches, loss: 0.4873Epoch 5/10: [======                        ] 17/75 batches, loss: 0.4925Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.4958Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.4974Epoch 5/10: [========                      ] 20/75 batches, loss: 0.4930Epoch 5/10: [========                      ] 21/75 batches, loss: 0.4935Epoch 5/10: [========                      ] 22/75 batches, loss: 0.4907Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.4934Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.4958Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.4923Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.4960Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.4945Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.4932Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.4927Epoch 5/10: [============                  ] 30/75 batches, loss: 0.4915Epoch 5/10: [============                  ] 31/75 batches, loss: 0.4949Epoch 5/10: [============                  ] 32/75 batches, loss: 0.4937Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.4947Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.4950Epoch 5/10: [==============                ] 35/75 batches, loss: 0.4946Epoch 5/10: [==============                ] 36/75 batches, loss: 0.4948Epoch 5/10: [==============                ] 37/75 batches, loss: 0.4963Epoch 5/10: [===============               ] 38/75 batches, loss: 0.4972Epoch 5/10: [===============               ] 39/75 batches, loss: 0.4985Epoch 5/10: [================              ] 40/75 batches, loss: 0.5004Epoch 5/10: [================              ] 41/75 batches, loss: 0.5017Epoch 5/10: [================              ] 42/75 batches, loss: 0.5017Epoch 5/10: [=================             ] 43/75 batches, loss: 0.5018Epoch 5/10: [=================             ] 44/75 batches, loss: 0.5029Epoch 5/10: [==================            ] 45/75 batches, loss: 0.5019Epoch 5/10: [==================            ] 46/75 batches, loss: 0.5033Epoch 5/10: [==================            ] 47/75 batches, loss: 0.5038Epoch 5/10: [===================           ] 48/75 batches, loss: 0.5033Epoch 5/10: [===================           ] 49/75 batches, loss: 0.5023Epoch 5/10: [====================          ] 50/75 batches, loss: 0.5009Epoch 5/10: [====================          ] 51/75 batches, loss: 0.5005Epoch 5/10: [====================          ] 52/75 batches, loss: 0.5015Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.5011Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.5027Epoch 5/10: [======================        ] 55/75 batches, loss: 0.5032Epoch 5/10: [======================        ] 56/75 batches, loss: 0.5032Epoch 5/10: [======================        ] 57/75 batches, loss: 0.5036Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.5032Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.5036Epoch 5/10: [========================      ] 60/75 batches, loss: 0.5048Epoch 5/10: [========================      ] 61/75 batches, loss: 0.5040Epoch 5/10: [========================      ] 62/75 batches, loss: 0.5048Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.5059Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.5066Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.5073Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.5065Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.5065Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.5061Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.5054Epoch 5/10: [============================  ] 70/75 batches, loss: 0.5060Epoch 5/10: [============================  ] 71/75 batches, loss: 0.5063Epoch 5/10: [============================  ] 72/75 batches, loss: 0.5049Epoch 5/10: [============================= ] 73/75 batches, loss: 0.5058Epoch 5/10: [============================= ] 74/75 batches, loss: 0.5064Epoch 5/10: [==============================] 75/75 batches, loss: 0.5057
[2025-05-01 12:16:26,158][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5057
[2025-05-01 12:16:26,387][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5448, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.935064935064935, 'precision': 0.8780487804878049, 'recall': 1.0}
[2025-05-01 12:16:26,388][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 12:16:26,388][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-01 12:16:26,388][src.training.lm_trainer][INFO] - Training completed in 40.66 seconds
[2025-05-01 12:16:26,388][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 12:16:29,280][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9966442953020134, 'f1': 0.9966555183946488, 'precision': 0.9933333333333333, 'recall': 1.0}
[2025-05-01 12:16:29,280][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315, 'precision': 0.9, 'recall': 1.0}
[2025-05-01 12:16:29,281][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9363636363636364, 'f1': 0.9369369369369369, 'precision': 0.9285714285714286, 'recall': 0.9454545454545454}
[2025-05-01 12:16:30,934][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/en/model.pt
[2025-05-01 12:16:30,939][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy █▁
wandb:           best_val_f1 █▁
wandb:         best_val_loss █▁
wandb:    best_val_precision █▁
wandb:       best_val_recall ▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▃▃▃
wandb:            train_loss █▂▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy █▄▁▁▁
wandb:                val_f1 █▄▁▁▁
wandb:              val_loss █▁▁▂▂
wandb:         val_precision █▄▁▁▁
wandb:            val_recall ▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.94444
wandb:           best_val_f1 0.94737
wandb:         best_val_loss 0.53551
wandb:    best_val_precision 0.9
wandb:       best_val_recall 1
wandb:      early_stop_epoch 5
wandb:                 epoch 5
wandb:   final_test_accuracy 0.93636
wandb:         final_test_f1 0.93694
wandb:  final_test_precision 0.92857
wandb:     final_test_recall 0.94545
wandb:  final_train_accuracy 0.99664
wandb:        final_train_f1 0.99666
wandb: final_train_precision 0.99333
wandb:    final_train_recall 1
wandb:    final_val_accuracy 0.94444
wandb:          final_val_f1 0.94737
wandb:   final_val_precision 0.9
wandb:      final_val_recall 1
wandb:         learning_rate 2e-05
wandb:            train_loss 0.50574
wandb:            train_time 40.65601
wandb:          val_accuracy 0.93056
wandb:                val_f1 0.93506
wandb:              val_loss 0.54483
wandb:         val_precision 0.87805
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_121536-uyq6lfqk
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_121536-uyq6lfqk/logs
Experiment finetune_question_type_en completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/results.json
Running experiment: finetune_complexity_en
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[en]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"         +training.debug_mode=true         "experiment_name=finetune_complexity_en"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 12:16:41,805][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity
experiment_name: finetune_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
  debug_mode: true
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-01 12:16:41,806][__main__][INFO] - Normalized task: complexity
[2025-05-01 12:16:41,806][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-01 12:16:41,806][__main__][INFO] - Determined Task Type: regression
[2025-05-01 12:16:41,810][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-05-01 12:16:41,810][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 12:16:43,155][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 12:16:45,377][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 12:16:45,377][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:16:45,420][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:16:45,449][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:16:45,537][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-05-01 12:16:45,546][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:16:45,546][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-05-01 12:16:45,547][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:16:45,569][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:16:45,598][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:16:45,610][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-05-01 12:16:45,612][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:16:45,612][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-05-01 12:16:45,613][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:16:45,631][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:16:45,660][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:16:45,672][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-05-01 12:16:45,674][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:16:45,674][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-05-01 12:16:45,675][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-05-01 12:16:45,676][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 12:16:45,676][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 12:16:45,676][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 12:16:45,676][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 12:16:45,676][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 12:16:45,677][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-05-01 12:16:45,677][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-05-01 12:16:45,677][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-05-01 12:16:45,677][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 12:16:45,677][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 12:16:45,677][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 12:16:45,677][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 12:16:45,677][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 12:16:45,678][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-05-01 12:16:45,678][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-05-01 12:16:45,678][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-05-01 12:16:45,678][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 12:16:45,678][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 12:16:45,678][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 12:16:45,678][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 12:16:45,678][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 12:16:45,678][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-05-01 12:16:45,678][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-05-01 12:16:45,679][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-05-01 12:16:45,679][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-05-01 12:16:45,679][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 12:16:45,679][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 12:16:45,679][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-01 12:16:45,679][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 12:16:49,580][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 12:16:49,582][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 12:16:49,582][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 12:16:49,582][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 12:16:49,586][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 12:16:49,587][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 12:16:49,587][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 12:16:49,587][__main__][INFO] - Successfully created lm_probe model for en
[2025-05-01 12:16:49,588][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 12:16:49,588][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.2066Epoch 1/10: [                              ] 2/75 batches, loss: 0.1540Epoch 1/10: [=                             ] 3/75 batches, loss: 0.1667Epoch 1/10: [=                             ] 4/75 batches, loss: 0.1615Epoch 1/10: [==                            ] 5/75 batches, loss: 0.1548Epoch 1/10: [==                            ] 6/75 batches, loss: 0.1549Epoch 1/10: [==                            ] 7/75 batches, loss: 0.1526Epoch 1/10: [===                           ] 8/75 batches, loss: 0.1525Epoch 1/10: [===                           ] 9/75 batches, loss: 0.1538Epoch 1/10: [====                          ] 10/75 batches, loss: 0.1519Epoch 1/10: [====                          ] 11/75 batches, loss: 0.1514Epoch 1/10: [====                          ] 12/75 batches, loss: 0.1493Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.1487Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.1461Epoch 1/10: [======                        ] 15/75 batches, loss: 0.1438Epoch 1/10: [======                        ] 16/75 batches, loss: 0.1426Epoch 1/10: [======                        ] 17/75 batches, loss: 0.1370Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.1344Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.1307Epoch 1/10: [========                      ] 20/75 batches, loss: 0.1295Epoch 1/10: [========                      ] 21/75 batches, loss: 0.1290Epoch 1/10: [========                      ] 22/75 batches, loss: 0.1307Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.1324Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.1332Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.1346Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.1334Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.1322Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.1296Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.1286Epoch 1/10: [============                  ] 30/75 batches, loss: 0.1269Epoch 1/10: [============                  ] 31/75 batches, loss: 0.1253Epoch 1/10: [============                  ] 32/75 batches, loss: 0.1240Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.1229Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.1238Epoch 1/10: [==============                ] 35/75 batches, loss: 0.1226Epoch 1/10: [==============                ] 36/75 batches, loss: 0.1207Epoch 1/10: [==============                ] 37/75 batches, loss: 0.1200Epoch 1/10: [===============               ] 38/75 batches, loss: 0.1187Epoch 1/10: [===============               ] 39/75 batches, loss: 0.1176Epoch 1/10: [================              ] 40/75 batches, loss: 0.1170Epoch 1/10: [================              ] 41/75 batches, loss: 0.1176Epoch 1/10: [================              ] 42/75 batches, loss: 0.1187Epoch 1/10: [=================             ] 43/75 batches, loss: 0.1177Epoch 1/10: [=================             ] 44/75 batches, loss: 0.1176Epoch 1/10: [==================            ] 45/75 batches, loss: 0.1167Epoch 1/10: [==================            ] 46/75 batches, loss: 0.1151Epoch 1/10: [==================            ] 47/75 batches, loss: 0.1138Epoch 1/10: [===================           ] 48/75 batches, loss: 0.1125Epoch 1/10: [===================           ] 49/75 batches, loss: 0.1113Epoch 1/10: [====================          ] 50/75 batches, loss: 0.1096Epoch 1/10: [====================          ] 51/75 batches, loss: 0.1083Epoch 1/10: [====================          ] 52/75 batches, loss: 0.1075Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.1064Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.1054Epoch 1/10: [======================        ] 55/75 batches, loss: 0.1045Epoch 1/10: [======================        ] 56/75 batches, loss: 0.1032Epoch 1/10: [======================        ] 57/75 batches, loss: 0.1028Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.1014Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.1008Epoch 1/10: [========================      ] 60/75 batches, loss: 0.0998Epoch 1/10: [========================      ] 61/75 batches, loss: 0.0991Epoch 1/10: [========================      ] 62/75 batches, loss: 0.0980Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.0970Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.0963Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.0956Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.0948Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.0939Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.0930Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.0918Epoch 1/10: [============================  ] 70/75 batches, loss: 0.0910Epoch 1/10: [============================  ] 71/75 batches, loss: 0.0904Epoch 1/10: [============================  ] 72/75 batches, loss: 0.0895Epoch 1/10: [============================= ] 73/75 batches, loss: 0.0890Epoch 1/10: [============================= ] 74/75 batches, loss: 0.0881Epoch 1/10: [==============================] 75/75 batches, loss: 0.0873
[2025-05-01 12:16:59,549][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0873
[2025-05-01 12:16:59,777][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0408, Metrics: {'mse': 0.04292728006839752, 'rmse': 0.2071889960118479, 'r2': -0.025711774826049805}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.0591Epoch 2/10: [                              ] 2/75 batches, loss: 0.0610Epoch 2/10: [=                             ] 3/75 batches, loss: 0.0468Epoch 2/10: [=                             ] 4/75 batches, loss: 0.0415Epoch 2/10: [==                            ] 5/75 batches, loss: 0.0435Epoch 2/10: [==                            ] 6/75 batches, loss: 0.0436Epoch 2/10: [==                            ] 7/75 batches, loss: 0.0406Epoch 2/10: [===                           ] 8/75 batches, loss: 0.0380Epoch 2/10: [===                           ] 9/75 batches, loss: 0.0350Epoch 2/10: [====                          ] 10/75 batches, loss: 0.0334Epoch 2/10: [====                          ] 11/75 batches, loss: 0.0317Epoch 2/10: [====                          ] 12/75 batches, loss: 0.0312Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.0310Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.0307Epoch 2/10: [======                        ] 15/75 batches, loss: 0.0304Epoch 2/10: [======                        ] 16/75 batches, loss: 0.0310Epoch 2/10: [======                        ] 17/75 batches, loss: 0.0303Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.0304Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.0308Epoch 2/10: [========                      ] 20/75 batches, loss: 0.0299Epoch 2/10: [========                      ] 21/75 batches, loss: 0.0294Epoch 2/10: [========                      ] 22/75 batches, loss: 0.0303Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.0305Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.0305Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.0309Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.0308Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.0304Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.0320Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.0323Epoch 2/10: [============                  ] 30/75 batches, loss: 0.0330Epoch 2/10: [============                  ] 31/75 batches, loss: 0.0327Epoch 2/10: [============                  ] 32/75 batches, loss: 0.0321Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.0316Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.0313Epoch 2/10: [==============                ] 35/75 batches, loss: 0.0310Epoch 2/10: [==============                ] 36/75 batches, loss: 0.0310Epoch 2/10: [==============                ] 37/75 batches, loss: 0.0309Epoch 2/10: [===============               ] 38/75 batches, loss: 0.0313Epoch 2/10: [===============               ] 39/75 batches, loss: 0.0309Epoch 2/10: [================              ] 40/75 batches, loss: 0.0305Epoch 2/10: [================              ] 41/75 batches, loss: 0.0307Epoch 2/10: [================              ] 42/75 batches, loss: 0.0308Epoch 2/10: [=================             ] 43/75 batches, loss: 0.0304Epoch 2/10: [=================             ] 44/75 batches, loss: 0.0300Epoch 2/10: [==================            ] 45/75 batches, loss: 0.0298Epoch 2/10: [==================            ] 46/75 batches, loss: 0.0296Epoch 2/10: [==================            ] 47/75 batches, loss: 0.0301Epoch 2/10: [===================           ] 48/75 batches, loss: 0.0301Epoch 2/10: [===================           ] 49/75 batches, loss: 0.0300Epoch 2/10: [====================          ] 50/75 batches, loss: 0.0302Epoch 2/10: [====================          ] 51/75 batches, loss: 0.0306Epoch 2/10: [====================          ] 52/75 batches, loss: 0.0305Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.0308Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.0311Epoch 2/10: [======================        ] 55/75 batches, loss: 0.0307Epoch 2/10: [======================        ] 56/75 batches, loss: 0.0305Epoch 2/10: [======================        ] 57/75 batches, loss: 0.0303Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.0302Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.0300Epoch 2/10: [========================      ] 60/75 batches, loss: 0.0297Epoch 2/10: [========================      ] 61/75 batches, loss: 0.0298Epoch 2/10: [========================      ] 62/75 batches, loss: 0.0296Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.0294Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.0293Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.0293Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.0291Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.0291Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.0289Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.0288Epoch 2/10: [============================  ] 70/75 batches, loss: 0.0290Epoch 2/10: [============================  ] 71/75 batches, loss: 0.0289Epoch 2/10: [============================  ] 72/75 batches, loss: 0.0287Epoch 2/10: [============================= ] 73/75 batches, loss: 0.0287Epoch 2/10: [============================= ] 74/75 batches, loss: 0.0286Epoch 2/10: [==============================] 75/75 batches, loss: 0.0283
[2025-05-01 12:17:07,765][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0283
[2025-05-01 12:17:07,998][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0278, Metrics: {'mse': 0.027115143835544586, 'rmse': 0.16466676603232538, 'r2': 0.35210609436035156}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.0207Epoch 3/10: [                              ] 2/75 batches, loss: 0.0363Epoch 3/10: [=                             ] 3/75 batches, loss: 0.0314Epoch 3/10: [=                             ] 4/75 batches, loss: 0.0312Epoch 3/10: [==                            ] 5/75 batches, loss: 0.0305Epoch 3/10: [==                            ] 6/75 batches, loss: 0.0275Epoch 3/10: [==                            ] 7/75 batches, loss: 0.0283Epoch 3/10: [===                           ] 8/75 batches, loss: 0.0270Epoch 3/10: [===                           ] 9/75 batches, loss: 0.0298Epoch 3/10: [====                          ] 10/75 batches, loss: 0.0279Epoch 3/10: [====                          ] 11/75 batches, loss: 0.0285Epoch 3/10: [====                          ] 12/75 batches, loss: 0.0297Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.0301Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.0299Epoch 3/10: [======                        ] 15/75 batches, loss: 0.0297Epoch 3/10: [======                        ] 16/75 batches, loss: 0.0313Epoch 3/10: [======                        ] 17/75 batches, loss: 0.0308Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.0298Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.0297Epoch 3/10: [========                      ] 20/75 batches, loss: 0.0287Epoch 3/10: [========                      ] 21/75 batches, loss: 0.0283Epoch 3/10: [========                      ] 22/75 batches, loss: 0.0285Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.0280Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.0275Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.0276Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.0275Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.0275Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.0273Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.0266Epoch 3/10: [============                  ] 30/75 batches, loss: 0.0262Epoch 3/10: [============                  ] 31/75 batches, loss: 0.0258Epoch 3/10: [============                  ] 32/75 batches, loss: 0.0257Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.0256Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.0255Epoch 3/10: [==============                ] 35/75 batches, loss: 0.0257Epoch 3/10: [==============                ] 36/75 batches, loss: 0.0255Epoch 3/10: [==============                ] 37/75 batches, loss: 0.0255Epoch 3/10: [===============               ] 38/75 batches, loss: 0.0253Epoch 3/10: [===============               ] 39/75 batches, loss: 0.0253Epoch 3/10: [================              ] 40/75 batches, loss: 0.0248Epoch 3/10: [================              ] 41/75 batches, loss: 0.0248Epoch 3/10: [================              ] 42/75 batches, loss: 0.0249Epoch 3/10: [=================             ] 43/75 batches, loss: 0.0248Epoch 3/10: [=================             ] 44/75 batches, loss: 0.0246Epoch 3/10: [==================            ] 45/75 batches, loss: 0.0246Epoch 3/10: [==================            ] 46/75 batches, loss: 0.0250Epoch 3/10: [==================            ] 47/75 batches, loss: 0.0249Epoch 3/10: [===================           ] 48/75 batches, loss: 0.0250Epoch 3/10: [===================           ] 49/75 batches, loss: 0.0251Epoch 3/10: [====================          ] 50/75 batches, loss: 0.0248Epoch 3/10: [====================          ] 51/75 batches, loss: 0.0249Epoch 3/10: [====================          ] 52/75 batches, loss: 0.0250Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.0249Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.0247Epoch 3/10: [======================        ] 55/75 batches, loss: 0.0249Epoch 3/10: [======================        ] 56/75 batches, loss: 0.0251Epoch 3/10: [======================        ] 57/75 batches, loss: 0.0250Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.0250Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.0249Epoch 3/10: [========================      ] 60/75 batches, loss: 0.0250Epoch 3/10: [========================      ] 61/75 batches, loss: 0.0249Epoch 3/10: [========================      ] 62/75 batches, loss: 0.0249Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.0249Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.0247Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.0247Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.0250Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.0250Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.0250Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.0252Epoch 3/10: [============================  ] 70/75 batches, loss: 0.0251Epoch 3/10: [============================  ] 71/75 batches, loss: 0.0249Epoch 3/10: [============================  ] 72/75 batches, loss: 0.0249Epoch 3/10: [============================= ] 73/75 batches, loss: 0.0248Epoch 3/10: [============================= ] 74/75 batches, loss: 0.0246Epoch 3/10: [==============================] 75/75 batches, loss: 0.0247
[2025-05-01 12:17:15,976][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0247
[2025-05-01 12:17:16,223][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0504, Metrics: {'mse': 0.04933957755565643, 'rmse': 0.2221251394049229, 'r2': -0.17892837524414062}
[2025-05-01 12:17:16,224][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.0155Epoch 4/10: [                              ] 2/75 batches, loss: 0.0169Epoch 4/10: [=                             ] 3/75 batches, loss: 0.0168Epoch 4/10: [=                             ] 4/75 batches, loss: 0.0182Epoch 4/10: [==                            ] 5/75 batches, loss: 0.0182Epoch 4/10: [==                            ] 6/75 batches, loss: 0.0193Epoch 4/10: [==                            ] 7/75 batches, loss: 0.0183Epoch 4/10: [===                           ] 8/75 batches, loss: 0.0175Epoch 4/10: [===                           ] 9/75 batches, loss: 0.0171Epoch 4/10: [====                          ] 10/75 batches, loss: 0.0169Epoch 4/10: [====                          ] 11/75 batches, loss: 0.0175Epoch 4/10: [====                          ] 12/75 batches, loss: 0.0178Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.0177Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.0173Epoch 4/10: [======                        ] 15/75 batches, loss: 0.0175Epoch 4/10: [======                        ] 16/75 batches, loss: 0.0179Epoch 4/10: [======                        ] 17/75 batches, loss: 0.0176Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.0175Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.0175Epoch 4/10: [========                      ] 20/75 batches, loss: 0.0181Epoch 4/10: [========                      ] 21/75 batches, loss: 0.0178Epoch 4/10: [========                      ] 22/75 batches, loss: 0.0178Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.0176Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.0174Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.0172Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.0178Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.0176Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.0175Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.0173Epoch 4/10: [============                  ] 30/75 batches, loss: 0.0174Epoch 4/10: [============                  ] 31/75 batches, loss: 0.0172Epoch 4/10: [============                  ] 32/75 batches, loss: 0.0172Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.0171Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.0170Epoch 4/10: [==============                ] 35/75 batches, loss: 0.0172Epoch 4/10: [==============                ] 36/75 batches, loss: 0.0171Epoch 4/10: [==============                ] 37/75 batches, loss: 0.0174Epoch 4/10: [===============               ] 38/75 batches, loss: 0.0175Epoch 4/10: [===============               ] 39/75 batches, loss: 0.0175Epoch 4/10: [================              ] 40/75 batches, loss: 0.0175Epoch 4/10: [================              ] 41/75 batches, loss: 0.0176Epoch 4/10: [================              ] 42/75 batches, loss: 0.0175Epoch 4/10: [=================             ] 43/75 batches, loss: 0.0174Epoch 4/10: [=================             ] 44/75 batches, loss: 0.0174Epoch 4/10: [==================            ] 45/75 batches, loss: 0.0173Epoch 4/10: [==================            ] 46/75 batches, loss: 0.0171Epoch 4/10: [==================            ] 47/75 batches, loss: 0.0173Epoch 4/10: [===================           ] 48/75 batches, loss: 0.0174Epoch 4/10: [===================           ] 49/75 batches, loss: 0.0176Epoch 4/10: [====================          ] 50/75 batches, loss: 0.0174Epoch 4/10: [====================          ] 51/75 batches, loss: 0.0178Epoch 4/10: [====================          ] 52/75 batches, loss: 0.0177Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.0178Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.0179Epoch 4/10: [======================        ] 55/75 batches, loss: 0.0179Epoch 4/10: [======================        ] 56/75 batches, loss: 0.0178Epoch 4/10: [======================        ] 57/75 batches, loss: 0.0179Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.0179Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.0179Epoch 4/10: [========================      ] 60/75 batches, loss: 0.0178Epoch 4/10: [========================      ] 61/75 batches, loss: 0.0178Epoch 4/10: [========================      ] 62/75 batches, loss: 0.0181Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.0181Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.0181Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.0182Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.0182Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.0182Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.0181Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.0180Epoch 4/10: [============================  ] 70/75 batches, loss: 0.0179Epoch 4/10: [============================  ] 71/75 batches, loss: 0.0180Epoch 4/10: [============================  ] 72/75 batches, loss: 0.0180Epoch 4/10: [============================= ] 73/75 batches, loss: 0.0180Epoch 4/10: [============================= ] 74/75 batches, loss: 0.0179Epoch 4/10: [==============================] 75/75 batches, loss: 0.0178
[2025-05-01 12:17:23,801][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0178
[2025-05-01 12:17:24,059][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0353, Metrics: {'mse': 0.03462854400277138, 'rmse': 0.18608746331435488, 'r2': 0.17257964611053467}
[2025-05-01 12:17:24,059][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.0090Epoch 5/10: [                              ] 2/75 batches, loss: 0.0128Epoch 5/10: [=                             ] 3/75 batches, loss: 0.0166Epoch 5/10: [=                             ] 4/75 batches, loss: 0.0164Epoch 5/10: [==                            ] 5/75 batches, loss: 0.0165Epoch 5/10: [==                            ] 6/75 batches, loss: 0.0166Epoch 5/10: [==                            ] 7/75 batches, loss: 0.0165Epoch 5/10: [===                           ] 8/75 batches, loss: 0.0183Epoch 5/10: [===                           ] 9/75 batches, loss: 0.0186Epoch 5/10: [====                          ] 10/75 batches, loss: 0.0185Epoch 5/10: [====                          ] 11/75 batches, loss: 0.0180Epoch 5/10: [====                          ] 12/75 batches, loss: 0.0175Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.0172Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.0167Epoch 5/10: [======                        ] 15/75 batches, loss: 0.0163Epoch 5/10: [======                        ] 16/75 batches, loss: 0.0163Epoch 5/10: [======                        ] 17/75 batches, loss: 0.0159Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.0167Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.0162Epoch 5/10: [========                      ] 20/75 batches, loss: 0.0160Epoch 5/10: [========                      ] 21/75 batches, loss: 0.0164Epoch 5/10: [========                      ] 22/75 batches, loss: 0.0165Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.0162Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.0160Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.0166Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.0163Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.0167Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.0167Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.0166Epoch 5/10: [============                  ] 30/75 batches, loss: 0.0163Epoch 5/10: [============                  ] 31/75 batches, loss: 0.0165Epoch 5/10: [============                  ] 32/75 batches, loss: 0.0162Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.0169Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.0171Epoch 5/10: [==============                ] 35/75 batches, loss: 0.0171Epoch 5/10: [==============                ] 36/75 batches, loss: 0.0175Epoch 5/10: [==============                ] 37/75 batches, loss: 0.0174Epoch 5/10: [===============               ] 38/75 batches, loss: 0.0173Epoch 5/10: [===============               ] 39/75 batches, loss: 0.0173Epoch 5/10: [================              ] 40/75 batches, loss: 0.0173Epoch 5/10: [================              ] 41/75 batches, loss: 0.0178Epoch 5/10: [================              ] 42/75 batches, loss: 0.0179Epoch 5/10: [=================             ] 43/75 batches, loss: 0.0181Epoch 5/10: [=================             ] 44/75 batches, loss: 0.0182Epoch 5/10: [==================            ] 45/75 batches, loss: 0.0184Epoch 5/10: [==================            ] 46/75 batches, loss: 0.0183Epoch 5/10: [==================            ] 47/75 batches, loss: 0.0185Epoch 5/10: [===================           ] 48/75 batches, loss: 0.0184Epoch 5/10: [===================           ] 49/75 batches, loss: 0.0184Epoch 5/10: [====================          ] 50/75 batches, loss: 0.0190Epoch 5/10: [====================          ] 51/75 batches, loss: 0.0191Epoch 5/10: [====================          ] 52/75 batches, loss: 0.0192Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.0191Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.0191Epoch 5/10: [======================        ] 55/75 batches, loss: 0.0190Epoch 5/10: [======================        ] 56/75 batches, loss: 0.0188Epoch 5/10: [======================        ] 57/75 batches, loss: 0.0187Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.0187Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.0187Epoch 5/10: [========================      ] 60/75 batches, loss: 0.0186Epoch 5/10: [========================      ] 61/75 batches, loss: 0.0184Epoch 5/10: [========================      ] 62/75 batches, loss: 0.0184Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.0183Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.0183Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.0183Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.0184Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.0183Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.0182Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.0181Epoch 5/10: [============================  ] 70/75 batches, loss: 0.0181Epoch 5/10: [============================  ] 71/75 batches, loss: 0.0181Epoch 5/10: [============================  ] 72/75 batches, loss: 0.0182Epoch 5/10: [============================= ] 73/75 batches, loss: 0.0182Epoch 5/10: [============================= ] 74/75 batches, loss: 0.0183Epoch 5/10: [==============================] 75/75 batches, loss: 0.0183
[2025-05-01 12:17:31,661][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0183
[2025-05-01 12:17:31,907][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0332, Metrics: {'mse': 0.03250707685947418, 'rmse': 0.1802971903815314, 'r2': 0.22327035665512085}
[2025-05-01 12:17:31,908][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 12:17:31,908][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-01 12:17:31,908][src.training.lm_trainer][INFO] - Training completed in 40.78 seconds
[2025-05-01 12:17:31,908][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 12:17:34,811][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.02293914183974266, 'rmse': 0.15145673256657383, 'r2': 0.14497649669647217}
[2025-05-01 12:17:34,811][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.027115143835544586, 'rmse': 0.16466676603232538, 'r2': 0.35210609436035156}
[2025-05-01 12:17:34,811][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.022624490782618523, 'rmse': 0.15041439685953775, 'r2': 0.4129396677017212}
[2025-05-01 12:17:36,489][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/en/model.pt
[2025-05-01 12:17:36,495][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▆▁▅
wandb:       train_loss █▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▅▁█▃▃
wandb:          val_mse ▆▁█▃▃
wandb:           val_r2 ▃█▁▆▆
wandb:         val_rmse ▆▁█▄▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02783
wandb:     best_val_mse 0.02712
wandb:      best_val_r2 0.35211
wandb:    best_val_rmse 0.16467
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.02262
wandb:    final_test_r2 0.41294
wandb:  final_test_rmse 0.15041
wandb:  final_train_mse 0.02294
wandb:   final_train_r2 0.14498
wandb: final_train_rmse 0.15146
wandb:    final_val_mse 0.02712
wandb:     final_val_r2 0.35211
wandb:   final_val_rmse 0.16467
wandb:    learning_rate 2e-05
wandb:       train_loss 0.01835
wandb:       train_time 40.77979
wandb:         val_loss 0.03321
wandb:          val_mse 0.03251
wandb:           val_r2 0.22327
wandb:         val_rmse 0.1803
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_121641-vyocrysp
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_121641-vyocrysp/logs
Experiment finetune_complexity_en completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/results.json
Running experiment: finetune_question_type_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_fi"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 12:17:46,608][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type
experiment_name: finetune_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-01 12:17:46,608][__main__][INFO] - Normalized task: question_type
[2025-05-01 12:17:46,608][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-01 12:17:46,608][__main__][INFO] - Determined Task Type: classification
[2025-05-01 12:17:46,612][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-05-01 12:17:46,612][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 12:17:47,991][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 12:17:50,203][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 12:17:50,203][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:17:50,253][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:17:50,286][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:17:50,372][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-01 12:17:50,381][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:17:50,381][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-01 12:17:50,382][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:17:50,401][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:17:50,432][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:17:50,444][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-01 12:17:50,445][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:17:50,446][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-01 12:17:50,446][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:17:50,461][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:17:50,489][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:17:50,500][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-01 12:17:50,501][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:17:50,501][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-01 12:17:50,502][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-01 12:17:50,503][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:17:50,503][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:17:50,503][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:17:50,503][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:17:50,503][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-05-01 12:17:50,503][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-05-01 12:17:50,503][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-01 12:17:50,503][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 12:17:50,503][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:17:50,504][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:17:50,504][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:17:50,504][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:17:50,504][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-05-01 12:17:50,504][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-05-01 12:17:50,504][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-01 12:17:50,504][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 12:17:50,504][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:17:50,504][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:17:50,504][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:17:50,504][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:17:50,505][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-01 12:17:50,505][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-01 12:17:50,505][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-01 12:17:50,505][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 12:17:50,505][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-01 12:17:50,505][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 12:17:50,505][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 12:17:50,505][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-01 12:17:50,506][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 12:17:54,328][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 12:17:54,329][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 12:17:54,329][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 12:17:54,329][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 12:17:54,334][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 12:17:54,334][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 12:17:54,335][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 12:17:54,335][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-01 12:17:54,335][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 12:17:54,336][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.7251Epoch 1/10: [                              ] 2/75 batches, loss: 0.7237Epoch 1/10: [=                             ] 3/75 batches, loss: 0.7232Epoch 1/10: [=                             ] 4/75 batches, loss: 0.7154Epoch 1/10: [==                            ] 5/75 batches, loss: 0.6918Epoch 1/10: [==                            ] 6/75 batches, loss: 0.6868Epoch 1/10: [==                            ] 7/75 batches, loss: 0.6918Epoch 1/10: [===                           ] 8/75 batches, loss: 0.6915Epoch 1/10: [===                           ] 9/75 batches, loss: 0.6982Epoch 1/10: [====                          ] 10/75 batches, loss: 0.7075Epoch 1/10: [====                          ] 11/75 batches, loss: 0.7120Epoch 1/10: [====                          ] 12/75 batches, loss: 0.6995Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.7033Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.7046Epoch 1/10: [======                        ] 15/75 batches, loss: 0.7039Epoch 1/10: [======                        ] 16/75 batches, loss: 0.7053Epoch 1/10: [======                        ] 17/75 batches, loss: 0.7037Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.7076Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.7071Epoch 1/10: [========                      ] 20/75 batches, loss: 0.7057Epoch 1/10: [========                      ] 21/75 batches, loss: 0.7142Epoch 1/10: [========                      ] 22/75 batches, loss: 0.7131Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.7165Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.7166Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.7206Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.7235Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.7237Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.7236Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.7235Epoch 1/10: [============                  ] 30/75 batches, loss: 0.7244Epoch 1/10: [============                  ] 31/75 batches, loss: 0.7224Epoch 1/10: [============                  ] 32/75 batches, loss: 0.7214Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.7231Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.7249Epoch 1/10: [==============                ] 35/75 batches, loss: 0.7237Epoch 1/10: [==============                ] 36/75 batches, loss: 0.7219Epoch 1/10: [==============                ] 37/75 batches, loss: 0.7227Epoch 1/10: [===============               ] 38/75 batches, loss: 0.7220Epoch 1/10: [===============               ] 39/75 batches, loss: 0.7210Epoch 1/10: [================              ] 40/75 batches, loss: 0.7223Epoch 1/10: [================              ] 41/75 batches, loss: 0.7245Epoch 1/10: [================              ] 42/75 batches, loss: 0.7255Epoch 1/10: [=================             ] 43/75 batches, loss: 0.7263Epoch 1/10: [=================             ] 44/75 batches, loss: 0.7245Epoch 1/10: [==================            ] 45/75 batches, loss: 0.7277Epoch 1/10: [==================            ] 46/75 batches, loss: 0.7277Epoch 1/10: [==================            ] 47/75 batches, loss: 0.7288Epoch 1/10: [===================           ] 48/75 batches, loss: 0.7291Epoch 1/10: [===================           ] 49/75 batches, loss: 0.7306Epoch 1/10: [====================          ] 50/75 batches, loss: 0.7283Epoch 1/10: [====================          ] 51/75 batches, loss: 0.7270Epoch 1/10: [====================          ] 52/75 batches, loss: 0.7248Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.7230Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.7232Epoch 1/10: [======================        ] 55/75 batches, loss: 0.7218Epoch 1/10: [======================        ] 56/75 batches, loss: 0.7223Epoch 1/10: [======================        ] 57/75 batches, loss: 0.7216Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.7220Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.7204Epoch 1/10: [========================      ] 60/75 batches, loss: 0.7205Epoch 1/10: [========================      ] 61/75 batches, loss: 0.7194Epoch 1/10: [========================      ] 62/75 batches, loss: 0.7199Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.7211Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.7209Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.7196Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.7196Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.7210Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.7202Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.7205Epoch 1/10: [============================  ] 70/75 batches, loss: 0.7209Epoch 1/10: [============================  ] 71/75 batches, loss: 0.7209Epoch 1/10: [============================  ] 72/75 batches, loss: 0.7220Epoch 1/10: [============================= ] 73/75 batches, loss: 0.7220Epoch 1/10: [============================= ] 74/75 batches, loss: 0.7203Epoch 1/10: [==============================] 75/75 batches, loss: 0.7186
[2025-05-01 12:18:04,320][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.7186
[2025-05-01 12:18:04,528][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.7320, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.7242Epoch 2/10: [                              ] 2/75 batches, loss: 0.7205Epoch 2/10: [=                             ] 3/75 batches, loss: 0.7064Epoch 2/10: [=                             ] 4/75 batches, loss: 0.7046Epoch 2/10: [==                            ] 5/75 batches, loss: 0.6933Epoch 2/10: [==                            ] 6/75 batches, loss: 0.6944Epoch 2/10: [==                            ] 7/75 batches, loss: 0.7067Epoch 2/10: [===                           ] 8/75 batches, loss: 0.7179Epoch 2/10: [===                           ] 9/75 batches, loss: 0.7155Epoch 2/10: [====                          ] 10/75 batches, loss: 0.7152Epoch 2/10: [====                          ] 11/75 batches, loss: 0.7094Epoch 2/10: [====                          ] 12/75 batches, loss: 0.7034Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.7042Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.6998Epoch 2/10: [======                        ] 15/75 batches, loss: 0.7004Epoch 2/10: [======                        ] 16/75 batches, loss: 0.6975Epoch 2/10: [======                        ] 17/75 batches, loss: 0.7006Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.7019Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.7028Epoch 2/10: [========                      ] 20/75 batches, loss: 0.7052Epoch 2/10: [========                      ] 21/75 batches, loss: 0.7100Epoch 2/10: [========                      ] 22/75 batches, loss: 0.7144Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.7172Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.7195Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.7193Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.7207Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.7200Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.7176Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.7156Epoch 2/10: [============                  ] 30/75 batches, loss: 0.7145Epoch 2/10: [============                  ] 31/75 batches, loss: 0.7127Epoch 2/10: [============                  ] 32/75 batches, loss: 0.7154Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.7116Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.7072Epoch 2/10: [==============                ] 35/75 batches, loss: 0.7090Epoch 2/10: [==============                ] 36/75 batches, loss: 0.7053Epoch 2/10: [==============                ] 37/75 batches, loss: 0.7053Epoch 2/10: [===============               ] 38/75 batches, loss: 0.7035Epoch 2/10: [===============               ] 39/75 batches, loss: 0.7028Epoch 2/10: [================              ] 40/75 batches, loss: 0.7020Epoch 2/10: [================              ] 41/75 batches, loss: 0.7016Epoch 2/10: [================              ] 42/75 batches, loss: 0.6999Epoch 2/10: [=================             ] 43/75 batches, loss: 0.6986Epoch 2/10: [=================             ] 44/75 batches, loss: 0.6967Epoch 2/10: [==================            ] 45/75 batches, loss: 0.6976Epoch 2/10: [==================            ] 46/75 batches, loss: 0.6984Epoch 2/10: [==================            ] 47/75 batches, loss: 0.6991Epoch 2/10: [===================           ] 48/75 batches, loss: 0.7012Epoch 2/10: [===================           ] 49/75 batches, loss: 0.6995Epoch 2/10: [====================          ] 50/75 batches, loss: 0.6988Epoch 2/10: [====================          ] 51/75 batches, loss: 0.6982Epoch 2/10: [====================          ] 52/75 batches, loss: 0.6985Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.6979Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.6983Epoch 2/10: [======================        ] 55/75 batches, loss: 0.6974Epoch 2/10: [======================        ] 56/75 batches, loss: 0.6972Epoch 2/10: [======================        ] 57/75 batches, loss: 0.6975Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.6968Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.6970Epoch 2/10: [========================      ] 60/75 batches, loss: 0.6967Epoch 2/10: [========================      ] 61/75 batches, loss: 0.6946Epoch 2/10: [========================      ] 62/75 batches, loss: 0.6951Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.6951Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.6940Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.6932Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.6936Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.6914Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.6917Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.6911Epoch 2/10: [============================  ] 70/75 batches, loss: 0.6911Epoch 2/10: [============================  ] 71/75 batches, loss: 0.6892Epoch 2/10: [============================  ] 72/75 batches, loss: 0.6876Epoch 2/10: [============================= ] 73/75 batches, loss: 0.6872Epoch 2/10: [============================= ] 74/75 batches, loss: 0.6871Epoch 2/10: [==============================] 75/75 batches, loss: 0.6854
[2025-05-01 12:18:12,513][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6854
[2025-05-01 12:18:12,734][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6544, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9473684210526315, 'precision': 1.0, 'recall': 0.9}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.6111Epoch 3/10: [                              ] 2/75 batches, loss: 0.6070Epoch 3/10: [=                             ] 3/75 batches, loss: 0.5885Epoch 3/10: [=                             ] 4/75 batches, loss: 0.6017Epoch 3/10: [==                            ] 5/75 batches, loss: 0.6115Epoch 3/10: [==                            ] 6/75 batches, loss: 0.6209Epoch 3/10: [==                            ] 7/75 batches, loss: 0.6121Epoch 3/10: [===                           ] 8/75 batches, loss: 0.6182Epoch 3/10: [===                           ] 9/75 batches, loss: 0.6020Epoch 3/10: [====                          ] 10/75 batches, loss: 0.5966Epoch 3/10: [====                          ] 11/75 batches, loss: 0.5901Epoch 3/10: [====                          ] 12/75 batches, loss: 0.5919Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.5863Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.5812Epoch 3/10: [======                        ] 15/75 batches, loss: 0.5789Epoch 3/10: [======                        ] 16/75 batches, loss: 0.5707Epoch 3/10: [======                        ] 17/75 batches, loss: 0.5708Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.5712Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.5739Epoch 3/10: [========                      ] 20/75 batches, loss: 0.5699Epoch 3/10: [========                      ] 21/75 batches, loss: 0.5725Epoch 3/10: [========                      ] 22/75 batches, loss: 0.5714Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.5681Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.5681Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.5671Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.5655Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.5620Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.5629Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.5591Epoch 3/10: [============                  ] 30/75 batches, loss: 0.5578Epoch 3/10: [============                  ] 31/75 batches, loss: 0.5541Epoch 3/10: [============                  ] 32/75 batches, loss: 0.5548Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.5513Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.5509Epoch 3/10: [==============                ] 35/75 batches, loss: 0.5497Epoch 3/10: [==============                ] 36/75 batches, loss: 0.5499Epoch 3/10: [==============                ] 37/75 batches, loss: 0.5494Epoch 3/10: [===============               ] 38/75 batches, loss: 0.5479Epoch 3/10: [===============               ] 39/75 batches, loss: 0.5482Epoch 3/10: [================              ] 40/75 batches, loss: 0.5496Epoch 3/10: [================              ] 41/75 batches, loss: 0.5474Epoch 3/10: [================              ] 42/75 batches, loss: 0.5487Epoch 3/10: [=================             ] 43/75 batches, loss: 0.5488Epoch 3/10: [=================             ] 44/75 batches, loss: 0.5489Epoch 3/10: [==================            ] 45/75 batches, loss: 0.5486Epoch 3/10: [==================            ] 46/75 batches, loss: 0.5483Epoch 3/10: [==================            ] 47/75 batches, loss: 0.5464Epoch 3/10: [===================           ] 48/75 batches, loss: 0.5466Epoch 3/10: [===================           ] 49/75 batches, loss: 0.5454Epoch 3/10: [====================          ] 50/75 batches, loss: 0.5441Epoch 3/10: [====================          ] 51/75 batches, loss: 0.5439Epoch 3/10: [====================          ] 52/75 batches, loss: 0.5438Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.5440Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.5425Epoch 3/10: [======================        ] 55/75 batches, loss: 0.5415Epoch 3/10: [======================        ] 56/75 batches, loss: 0.5408Epoch 3/10: [======================        ] 57/75 batches, loss: 0.5403Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.5389Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.5380Epoch 3/10: [========================      ] 60/75 batches, loss: 0.5363Epoch 3/10: [========================      ] 61/75 batches, loss: 0.5358Epoch 3/10: [========================      ] 62/75 batches, loss: 0.5361Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.5359Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.5358Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.5344Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.5336Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.5337Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.5336Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.5334Epoch 3/10: [============================  ] 70/75 batches, loss: 0.5327Epoch 3/10: [============================  ] 71/75 batches, loss: 0.5323Epoch 3/10: [============================  ] 72/75 batches, loss: 0.5315Epoch 3/10: [============================= ] 73/75 batches, loss: 0.5312Epoch 3/10: [============================= ] 74/75 batches, loss: 0.5315Epoch 3/10: [==============================] 75/75 batches, loss: 0.5327
[2025-05-01 12:18:20,750][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5327
[2025-05-01 12:18:20,985][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.5255, Metrics: {'accuracy': 0.9682539682539683, 'f1': 0.9655172413793104, 'precision': 1.0, 'recall': 0.9333333333333333}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.5542Epoch 4/10: [                              ] 2/75 batches, loss: 0.5660Epoch 4/10: [=                             ] 3/75 batches, loss: 0.5297Epoch 4/10: [=                             ] 4/75 batches, loss: 0.5116Epoch 4/10: [==                            ] 5/75 batches, loss: 0.5151Epoch 4/10: [==                            ] 6/75 batches, loss: 0.5095Epoch 4/10: [==                            ] 7/75 batches, loss: 0.5043Epoch 4/10: [===                           ] 8/75 batches, loss: 0.5046Epoch 4/10: [===                           ] 9/75 batches, loss: 0.5074Epoch 4/10: [====                          ] 10/75 batches, loss: 0.5072Epoch 4/10: [====                          ] 11/75 batches, loss: 0.5004Epoch 4/10: [====                          ] 12/75 batches, loss: 0.5028Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.5028Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.5091Epoch 4/10: [======                        ] 15/75 batches, loss: 0.5170Epoch 4/10: [======                        ] 16/75 batches, loss: 0.5162Epoch 4/10: [======                        ] 17/75 batches, loss: 0.5127Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.5150Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.5106Epoch 4/10: [========                      ] 20/75 batches, loss: 0.5091Epoch 4/10: [========                      ] 21/75 batches, loss: 0.5108Epoch 4/10: [========                      ] 22/75 batches, loss: 0.5127Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.5150Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.5155Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.5180Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.5184Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.5135Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.5178Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.5182Epoch 4/10: [============                  ] 30/75 batches, loss: 0.5183Epoch 4/10: [============                  ] 31/75 batches, loss: 0.5185Epoch 4/10: [============                  ] 32/75 batches, loss: 0.5189Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.5204Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.5192Epoch 4/10: [==============                ] 35/75 batches, loss: 0.5175Epoch 4/10: [==============                ] 36/75 batches, loss: 0.5172Epoch 4/10: [==============                ] 37/75 batches, loss: 0.5182Epoch 4/10: [===============               ] 38/75 batches, loss: 0.5189Epoch 4/10: [===============               ] 39/75 batches, loss: 0.5204Epoch 4/10: [================              ] 40/75 batches, loss: 0.5206Epoch 4/10: [================              ] 41/75 batches, loss: 0.5199Epoch 4/10: [================              ] 42/75 batches, loss: 0.5179Epoch 4/10: [=================             ] 43/75 batches, loss: 0.5194Epoch 4/10: [=================             ] 44/75 batches, loss: 0.5206Epoch 4/10: [==================            ] 45/75 batches, loss: 0.5192Epoch 4/10: [==================            ] 46/75 batches, loss: 0.5206Epoch 4/10: [==================            ] 47/75 batches, loss: 0.5208Epoch 4/10: [===================           ] 48/75 batches, loss: 0.5200Epoch 4/10: [===================           ] 49/75 batches, loss: 0.5206Epoch 4/10: [====================          ] 50/75 batches, loss: 0.5194Epoch 4/10: [====================          ] 51/75 batches, loss: 0.5182Epoch 4/10: [====================          ] 52/75 batches, loss: 0.5188Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.5208Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.5196Epoch 4/10: [======================        ] 55/75 batches, loss: 0.5185Epoch 4/10: [======================        ] 56/75 batches, loss: 0.5191Epoch 4/10: [======================        ] 57/75 batches, loss: 0.5180Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.5174Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.5160Epoch 4/10: [========================      ] 60/75 batches, loss: 0.5154Epoch 4/10: [========================      ] 61/75 batches, loss: 0.5156Epoch 4/10: [========================      ] 62/75 batches, loss: 0.5147Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.5149Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.5144Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.5157Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.5151Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.5160Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.5159Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.5150Epoch 4/10: [============================  ] 70/75 batches, loss: 0.5145Epoch 4/10: [============================  ] 71/75 batches, loss: 0.5144Epoch 4/10: [============================  ] 72/75 batches, loss: 0.5142Epoch 4/10: [============================= ] 73/75 batches, loss: 0.5141Epoch 4/10: [============================= ] 74/75 batches, loss: 0.5146Epoch 4/10: [==============================] 75/75 batches, loss: 0.5140
[2025-05-01 12:18:28,956][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5140
[2025-05-01 12:18:29,193][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5248, Metrics: {'accuracy': 0.9682539682539683, 'f1': 0.9655172413793104, 'precision': 1.0, 'recall': 0.9333333333333333}
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.5751Epoch 5/10: [                              ] 2/75 batches, loss: 0.5396Epoch 5/10: [=                             ] 3/75 batches, loss: 0.5120Epoch 5/10: [=                             ] 4/75 batches, loss: 0.4922Epoch 5/10: [==                            ] 5/75 batches, loss: 0.4898Epoch 5/10: [==                            ] 6/75 batches, loss: 0.4762Epoch 5/10: [==                            ] 7/75 batches, loss: 0.4667Epoch 5/10: [===                           ] 8/75 batches, loss: 0.4654Epoch 5/10: [===                           ] 9/75 batches, loss: 0.4617Epoch 5/10: [====                          ] 10/75 batches, loss: 0.4659Epoch 5/10: [====                          ] 11/75 batches, loss: 0.4628Epoch 5/10: [====                          ] 12/75 batches, loss: 0.4643Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.4619Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.4648Epoch 5/10: [======                        ] 15/75 batches, loss: 0.4690Epoch 5/10: [======                        ] 16/75 batches, loss: 0.4771Epoch 5/10: [======                        ] 17/75 batches, loss: 0.4801Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.4828Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.4839Epoch 5/10: [========                      ] 20/75 batches, loss: 0.4822Epoch 5/10: [========                      ] 21/75 batches, loss: 0.4858Epoch 5/10: [========                      ] 22/75 batches, loss: 0.4856Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.4895Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.4941Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.4945Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.4922Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.4944Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.4964Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.4975Epoch 5/10: [============                  ] 30/75 batches, loss: 0.4985Epoch 5/10: [============                  ] 31/75 batches, loss: 0.4986Epoch 5/10: [============                  ] 32/75 batches, loss: 0.4973Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.4961Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.4970Epoch 5/10: [==============                ] 35/75 batches, loss: 0.4972Epoch 5/10: [==============                ] 36/75 batches, loss: 0.5007Epoch 5/10: [==============                ] 37/75 batches, loss: 0.5027Epoch 5/10: [===============               ] 38/75 batches, loss: 0.5034Epoch 5/10: [===============               ] 39/75 batches, loss: 0.5034Epoch 5/10: [================              ] 40/75 batches, loss: 0.5046Epoch 5/10: [================              ] 41/75 batches, loss: 0.5046Epoch 5/10: [================              ] 42/75 batches, loss: 0.5079Epoch 5/10: [=================             ] 43/75 batches, loss: 0.5067Epoch 5/10: [=================             ] 44/75 batches, loss: 0.5072Epoch 5/10: [==================            ] 45/75 batches, loss: 0.5092Epoch 5/10: [==================            ] 46/75 batches, loss: 0.5086Epoch 5/10: [==================            ] 47/75 batches, loss: 0.5090Epoch 5/10: [===================           ] 48/75 batches, loss: 0.5079Epoch 5/10: [===================           ] 49/75 batches, loss: 0.5098Epoch 5/10: [====================          ] 50/75 batches, loss: 0.5087Epoch 5/10: [====================          ] 51/75 batches, loss: 0.5086Epoch 5/10: [====================          ] 52/75 batches, loss: 0.5094Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.5102Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.5100Epoch 5/10: [======================        ] 55/75 batches, loss: 0.5091Epoch 5/10: [======================        ] 56/75 batches, loss: 0.5103Epoch 5/10: [======================        ] 57/75 batches, loss: 0.5097Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.5096Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.5111Epoch 5/10: [========================      ] 60/75 batches, loss: 0.5118Epoch 5/10: [========================      ] 61/75 batches, loss: 0.5128Epoch 5/10: [========================      ] 62/75 batches, loss: 0.5127Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.5122Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.5113Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.5105Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.5104Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.5096Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.5102Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.5108Epoch 5/10: [============================  ] 70/75 batches, loss: 0.5107Epoch 5/10: [============================  ] 71/75 batches, loss: 0.5116Epoch 5/10: [============================  ] 72/75 batches, loss: 0.5105Epoch 5/10: [============================= ] 73/75 batches, loss: 0.5104Epoch 5/10: [============================= ] 74/75 batches, loss: 0.5100Epoch 5/10: [==============================] 75/75 batches, loss: 0.5088
[2025-05-01 12:18:37,177][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5088
[2025-05-01 12:18:37,423][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5249, Metrics: {'accuracy': 0.9682539682539683, 'f1': 0.9655172413793104, 'precision': 1.0, 'recall': 0.9333333333333333}
[2025-05-01 12:18:37,423][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.5510Epoch 6/10: [                              ] 2/75 batches, loss: 0.4920Epoch 6/10: [=                             ] 3/75 batches, loss: 0.5119Epoch 6/10: [=                             ] 4/75 batches, loss: 0.4921Epoch 6/10: [==                            ] 5/75 batches, loss: 0.4991Epoch 6/10: [==                            ] 6/75 batches, loss: 0.4840Epoch 6/10: [==                            ] 7/75 batches, loss: 0.4969Epoch 6/10: [===                           ] 8/75 batches, loss: 0.4978Epoch 6/10: [===                           ] 9/75 batches, loss: 0.5090Epoch 6/10: [====                          ] 10/75 batches, loss: 0.5132Epoch 6/10: [====                          ] 11/75 batches, loss: 0.5102Epoch 6/10: [====                          ] 12/75 batches, loss: 0.5155Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.5146Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.5172Epoch 6/10: [======                        ] 15/75 batches, loss: 0.5115Epoch 6/10: [======                        ] 16/75 batches, loss: 0.5110Epoch 6/10: [======                        ] 17/75 batches, loss: 0.5106Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.5116Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.5124Epoch 6/10: [========                      ] 20/75 batches, loss: 0.5108Epoch 6/10: [========                      ] 21/75 batches, loss: 0.5104Epoch 6/10: [========                      ] 22/75 batches, loss: 0.5069Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.5058Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.5047Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.5075Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.5064Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.5037Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.5071Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.5078Epoch 6/10: [============                  ] 30/75 batches, loss: 0.5085Epoch 6/10: [============                  ] 31/75 batches, loss: 0.5068Epoch 6/10: [============                  ] 32/75 batches, loss: 0.5059Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.5051Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.5079Epoch 6/10: [==============                ] 35/75 batches, loss: 0.5091Epoch 6/10: [==============                ] 36/75 batches, loss: 0.5109Epoch 6/10: [==============                ] 37/75 batches, loss: 0.5101Epoch 6/10: [===============               ] 38/75 batches, loss: 0.5087Epoch 6/10: [===============               ] 39/75 batches, loss: 0.5086Epoch 6/10: [================              ] 40/75 batches, loss: 0.5091Epoch 6/10: [================              ] 41/75 batches, loss: 0.5090Epoch 6/10: [================              ] 42/75 batches, loss: 0.5077Epoch 6/10: [=================             ] 43/75 batches, loss: 0.5071Epoch 6/10: [=================             ] 44/75 batches, loss: 0.5059Epoch 6/10: [==================            ] 45/75 batches, loss: 0.5037Epoch 6/10: [==================            ] 46/75 batches, loss: 0.5037Epoch 6/10: [==================            ] 47/75 batches, loss: 0.5032Epoch 6/10: [===================           ] 48/75 batches, loss: 0.5037Epoch 6/10: [===================           ] 49/75 batches, loss: 0.5042Epoch 6/10: [====================          ] 50/75 batches, loss: 0.5042Epoch 6/10: [====================          ] 51/75 batches, loss: 0.5047Epoch 6/10: [====================          ] 52/75 batches, loss: 0.5042Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.5042Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.5033Epoch 6/10: [======================        ] 55/75 batches, loss: 0.5037Epoch 6/10: [======================        ] 56/75 batches, loss: 0.5033Epoch 6/10: [======================        ] 57/75 batches, loss: 0.5041Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.5049Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.5061Epoch 6/10: [========================      ] 60/75 batches, loss: 0.5073Epoch 6/10: [========================      ] 61/75 batches, loss: 0.5064Epoch 6/10: [========================      ] 62/75 batches, loss: 0.5064Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.5063Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.5052Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.5048Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.5048Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.5055Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.5061Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.5064Epoch 6/10: [============================  ] 70/75 batches, loss: 0.5074Epoch 6/10: [============================  ] 71/75 batches, loss: 0.5077Epoch 6/10: [============================  ] 72/75 batches, loss: 0.5089Epoch 6/10: [============================= ] 73/75 batches, loss: 0.5085Epoch 6/10: [============================= ] 74/75 batches, loss: 0.5078Epoch 6/10: [==============================] 75/75 batches, loss: 0.5071
[2025-05-01 12:18:45,040][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.5071
[2025-05-01 12:18:45,273][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.5256, Metrics: {'accuracy': 0.9682539682539683, 'f1': 0.9655172413793104, 'precision': 1.0, 'recall': 0.9333333333333333}
[2025-05-01 12:18:45,274][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.4801Epoch 7/10: [                              ] 2/75 batches, loss: 0.4917Epoch 7/10: [=                             ] 3/75 batches, loss: 0.5196Epoch 7/10: [=                             ] 4/75 batches, loss: 0.4919Epoch 7/10: [==                            ] 5/75 batches, loss: 0.4895Epoch 7/10: [==                            ] 6/75 batches, loss: 0.4959Epoch 7/10: [==                            ] 7/75 batches, loss: 0.4902Epoch 7/10: [===                           ] 8/75 batches, loss: 0.4859Epoch 7/10: [===                           ] 9/75 batches, loss: 0.4852Epoch 7/10: [====                          ] 10/75 batches, loss: 0.4847Epoch 7/10: [====                          ] 11/75 batches, loss: 0.4886Epoch 7/10: [====                          ] 12/75 batches, loss: 0.4858Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.4909Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.4969Epoch 7/10: [======                        ] 15/75 batches, loss: 0.4989Epoch 7/10: [======                        ] 16/75 batches, loss: 0.4992Epoch 7/10: [======                        ] 17/75 batches, loss: 0.5009Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.5089Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.5050Epoch 7/10: [========                      ] 20/75 batches, loss: 0.5026Epoch 7/10: [========                      ] 21/75 batches, loss: 0.5026Epoch 7/10: [========                      ] 22/75 batches, loss: 0.5026Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.5047Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.5037Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.5065Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.5082Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.5072Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.5096Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.5102Epoch 7/10: [============                  ] 30/75 batches, loss: 0.5084Epoch 7/10: [============                  ] 31/75 batches, loss: 0.5052Epoch 7/10: [============                  ] 32/75 batches, loss: 0.5044Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.5037Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.5058Epoch 7/10: [==============                ] 35/75 batches, loss: 0.5057Epoch 7/10: [==============                ] 36/75 batches, loss: 0.5069Epoch 7/10: [==============                ] 37/75 batches, loss: 0.5068Epoch 7/10: [===============               ] 38/75 batches, loss: 0.5074Epoch 7/10: [===============               ] 39/75 batches, loss: 0.5079Epoch 7/10: [================              ] 40/75 batches, loss: 0.5101Epoch 7/10: [================              ] 41/75 batches, loss: 0.5100Epoch 7/10: [================              ] 42/75 batches, loss: 0.5093Epoch 7/10: [=================             ] 43/75 batches, loss: 0.5086Epoch 7/10: [=================             ] 44/75 batches, loss: 0.5074Epoch 7/10: [==================            ] 45/75 batches, loss: 0.5089Epoch 7/10: [==================            ] 46/75 batches, loss: 0.5093Epoch 7/10: [==================            ] 47/75 batches, loss: 0.5096Epoch 7/10: [===================           ] 48/75 batches, loss: 0.5100Epoch 7/10: [===================           ] 49/75 batches, loss: 0.5099Epoch 7/10: [====================          ] 50/75 batches, loss: 0.5102Epoch 7/10: [====================          ] 51/75 batches, loss: 0.5092Epoch 7/10: [====================          ] 52/75 batches, loss: 0.5100Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.5101Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.5082Epoch 7/10: [======================        ] 55/75 batches, loss: 0.5090Epoch 7/10: [======================        ] 56/75 batches, loss: 0.5080Epoch 7/10: [======================        ] 57/75 batches, loss: 0.5080Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.5083Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.5066Epoch 7/10: [========================      ] 60/75 batches, loss: 0.5069Epoch 7/10: [========================      ] 61/75 batches, loss: 0.5073Epoch 7/10: [========================      ] 62/75 batches, loss: 0.5065Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.5068Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.5056Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.5056Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.5052Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.5052Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.5059Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.5062Epoch 7/10: [============================  ] 70/75 batches, loss: 0.5055Epoch 7/10: [============================  ] 71/75 batches, loss: 0.5061Epoch 7/10: [============================  ] 72/75 batches, loss: 0.5074Epoch 7/10: [============================= ] 73/75 batches, loss: 0.5067Epoch 7/10: [============================= ] 74/75 batches, loss: 0.5070Epoch 7/10: [==============================] 75/75 batches, loss: 0.5062
[2025-05-01 12:18:52,851][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5062
[2025-05-01 12:18:53,080][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.5262, Metrics: {'accuracy': 0.9682539682539683, 'f1': 0.9655172413793104, 'precision': 1.0, 'recall': 0.9333333333333333}
[2025-05-01 12:18:53,081][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 12:18:53,081][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 7
[2025-05-01 12:18:53,081][src.training.lm_trainer][INFO] - Training completed in 57.28 seconds
[2025-05-01 12:18:53,081][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 12:18:55,945][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9874476987447699, 'f1': 0.9872988992379339, 'precision': 1.0, 'recall': 0.9749163879598662}
[2025-05-01 12:18:55,945][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9682539682539683, 'f1': 0.9655172413793104, 'precision': 1.0, 'recall': 0.9333333333333333}
[2025-05-01 12:18:55,945][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9074074074074074, 'precision': 0.9245283018867925, 'recall': 0.8909090909090909}
[2025-05-01 12:18:57,584][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/fi/model.pt
[2025-05-01 12:18:57,590][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁███
wandb:           best_val_f1 ▁███
wandb:         best_val_loss █▅▁▁
wandb:    best_val_precision ▁███
wandb:       best_val_recall ▁███
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▄▄▄▄
wandb:            train_loss █▇▂▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁██████
wandb:                val_f1 ▁██████
wandb:              val_loss █▅▁▁▁▁▁
wandb:         val_precision ▁██████
wandb:            val_recall ▁██████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.96825
wandb:           best_val_f1 0.96552
wandb:         best_val_loss 0.52477
wandb:    best_val_precision 1
wandb:       best_val_recall 0.93333
wandb:      early_stop_epoch 7
wandb:                 epoch 7
wandb:   final_test_accuracy 0.90909
wandb:         final_test_f1 0.90741
wandb:  final_test_precision 0.92453
wandb:     final_test_recall 0.89091
wandb:  final_train_accuracy 0.98745
wandb:        final_train_f1 0.9873
wandb: final_train_precision 1
wandb:    final_train_recall 0.97492
wandb:    final_val_accuracy 0.96825
wandb:          final_val_f1 0.96552
wandb:   final_val_precision 1
wandb:      final_val_recall 0.93333
wandb:         learning_rate 2e-05
wandb:            train_loss 0.50621
wandb:            train_time 57.27986
wandb:          val_accuracy 0.96825
wandb:                val_f1 0.96552
wandb:              val_loss 0.52622
wandb:         val_precision 1
wandb:            val_recall 0.93333
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_121746-ixme09m7
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_121746-ixme09m7/logs
Experiment finetune_question_type_fi completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/results.json
Running experiment: finetune_complexity_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_fi"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 12:19:08,777][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity
experiment_name: finetune_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-01 12:19:08,777][__main__][INFO] - Normalized task: complexity
[2025-05-01 12:19:08,777][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-01 12:19:08,777][__main__][INFO] - Determined Task Type: regression
[2025-05-01 12:19:08,781][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-05-01 12:19:08,782][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 12:19:10,034][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 12:19:12,246][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 12:19:12,247][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:19:12,295][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:19:12,320][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:19:12,412][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-01 12:19:12,421][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:19:12,421][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-01 12:19:12,422][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:19:12,442][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:19:12,478][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:19:12,491][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-01 12:19:12,492][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:19:12,492][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-01 12:19:12,493][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:19:12,524][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:19:12,554][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:19:12,567][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-01 12:19:12,568][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:19:12,568][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-01 12:19:12,569][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-01 12:19:12,570][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 12:19:12,570][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 12:19:12,570][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 12:19:12,570][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 12:19:12,570][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 12:19:12,570][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-05-01 12:19:12,570][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-01 12:19:12,571][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-05-01 12:19:12,571][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 12:19:12,571][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 12:19:12,571][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 12:19:12,571][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 12:19:12,571][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 12:19:12,571][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-05-01 12:19:12,571][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-01 12:19:12,571][src.data.datasets][INFO] - Sample label: 1.0
[2025-05-01 12:19:12,572][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 12:19:12,572][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 12:19:12,572][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 12:19:12,572][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 12:19:12,572][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 12:19:12,572][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-05-01 12:19:12,572][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-01 12:19:12,572][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-05-01 12:19:12,572][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-01 12:19:12,572][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 12:19:12,573][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 12:19:12,573][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-01 12:19:12,573][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 12:19:16,374][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 12:19:16,375][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 12:19:16,375][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 12:19:16,375][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 12:19:16,380][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 12:19:16,380][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 12:19:16,380][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 12:19:16,380][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-01 12:19:16,381][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 12:19:16,381][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.1450Epoch 1/10: [                              ] 2/75 batches, loss: 0.1576Epoch 1/10: [=                             ] 3/75 batches, loss: 0.1505Epoch 1/10: [=                             ] 4/75 batches, loss: 0.1437Epoch 1/10: [==                            ] 5/75 batches, loss: 0.1353Epoch 1/10: [==                            ] 6/75 batches, loss: 0.1373Epoch 1/10: [==                            ] 7/75 batches, loss: 0.1349Epoch 1/10: [===                           ] 8/75 batches, loss: 0.1362Epoch 1/10: [===                           ] 9/75 batches, loss: 0.1400Epoch 1/10: [====                          ] 10/75 batches, loss: 0.1379Epoch 1/10: [====                          ] 11/75 batches, loss: 0.1364Epoch 1/10: [====                          ] 12/75 batches, loss: 0.1313Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.1318Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.1303Epoch 1/10: [======                        ] 15/75 batches, loss: 0.1276Epoch 1/10: [======                        ] 16/75 batches, loss: 0.1253Epoch 1/10: [======                        ] 17/75 batches, loss: 0.1244Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.1274Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.1266Epoch 1/10: [========                      ] 20/75 batches, loss: 0.1242Epoch 1/10: [========                      ] 21/75 batches, loss: 0.1213Epoch 1/10: [========                      ] 22/75 batches, loss: 0.1182Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.1176Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.1157Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.1135Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.1126Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.1126Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.1108Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.1082Epoch 1/10: [============                  ] 30/75 batches, loss: 0.1064Epoch 1/10: [============                  ] 31/75 batches, loss: 0.1054Epoch 1/10: [============                  ] 32/75 batches, loss: 0.1033Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.1019Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.1006Epoch 1/10: [==============                ] 35/75 batches, loss: 0.0997Epoch 1/10: [==============                ] 36/75 batches, loss: 0.0985Epoch 1/10: [==============                ] 37/75 batches, loss: 0.0969Epoch 1/10: [===============               ] 38/75 batches, loss: 0.0959Epoch 1/10: [===============               ] 39/75 batches, loss: 0.0951Epoch 1/10: [================              ] 40/75 batches, loss: 0.0934Epoch 1/10: [================              ] 41/75 batches, loss: 0.0925Epoch 1/10: [================              ] 42/75 batches, loss: 0.0910Epoch 1/10: [=================             ] 43/75 batches, loss: 0.0903Epoch 1/10: [=================             ] 44/75 batches, loss: 0.0899Epoch 1/10: [==================            ] 45/75 batches, loss: 0.0885Epoch 1/10: [==================            ] 46/75 batches, loss: 0.0881Epoch 1/10: [==================            ] 47/75 batches, loss: 0.0873Epoch 1/10: [===================           ] 48/75 batches, loss: 0.0862Epoch 1/10: [===================           ] 49/75 batches, loss: 0.0856Epoch 1/10: [====================          ] 50/75 batches, loss: 0.0846Epoch 1/10: [====================          ] 51/75 batches, loss: 0.0836Epoch 1/10: [====================          ] 52/75 batches, loss: 0.0828Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.0822Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.0819Epoch 1/10: [======================        ] 55/75 batches, loss: 0.0819Epoch 1/10: [======================        ] 56/75 batches, loss: 0.0810Epoch 1/10: [======================        ] 57/75 batches, loss: 0.0803Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.0793Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.0785Epoch 1/10: [========================      ] 60/75 batches, loss: 0.0778Epoch 1/10: [========================      ] 61/75 batches, loss: 0.0769Epoch 1/10: [========================      ] 62/75 batches, loss: 0.0762Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.0755Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.0745Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.0736Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.0728Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.0722Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.0713Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.0707Epoch 1/10: [============================  ] 70/75 batches, loss: 0.0700Epoch 1/10: [============================  ] 71/75 batches, loss: 0.0693Epoch 1/10: [============================  ] 72/75 batches, loss: 0.0684Epoch 1/10: [============================= ] 73/75 batches, loss: 0.0679Epoch 1/10: [============================= ] 74/75 batches, loss: 0.0673Epoch 1/10: [==============================] 75/75 batches, loss: 0.0666
[2025-05-01 12:19:26,262][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0666
[2025-05-01 12:19:26,466][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0897, Metrics: {'mse': 0.08961991965770721, 'rmse': 0.2993658625456604, 'r2': -0.3669794797897339}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.0185Epoch 2/10: [                              ] 2/75 batches, loss: 0.0216Epoch 2/10: [=                             ] 3/75 batches, loss: 0.0224Epoch 2/10: [=                             ] 4/75 batches, loss: 0.0247Epoch 2/10: [==                            ] 5/75 batches, loss: 0.0283Epoch 2/10: [==                            ] 6/75 batches, loss: 0.0270Epoch 2/10: [==                            ] 7/75 batches, loss: 0.0252Epoch 2/10: [===                           ] 8/75 batches, loss: 0.0243Epoch 2/10: [===                           ] 9/75 batches, loss: 0.0235Epoch 2/10: [====                          ] 10/75 batches, loss: 0.0239Epoch 2/10: [====                          ] 11/75 batches, loss: 0.0246Epoch 2/10: [====                          ] 12/75 batches, loss: 0.0251Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.0246Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.0246Epoch 2/10: [======                        ] 15/75 batches, loss: 0.0241Epoch 2/10: [======                        ] 16/75 batches, loss: 0.0252Epoch 2/10: [======                        ] 17/75 batches, loss: 0.0249Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.0241Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.0248Epoch 2/10: [========                      ] 20/75 batches, loss: 0.0242Epoch 2/10: [========                      ] 21/75 batches, loss: 0.0240Epoch 2/10: [========                      ] 22/75 batches, loss: 0.0237Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.0232Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.0233Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.0229Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.0226Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.0224Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.0225Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.0223Epoch 2/10: [============                  ] 30/75 batches, loss: 0.0223Epoch 2/10: [============                  ] 31/75 batches, loss: 0.0221Epoch 2/10: [============                  ] 32/75 batches, loss: 0.0218Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.0215Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.0216Epoch 2/10: [==============                ] 35/75 batches, loss: 0.0216Epoch 2/10: [==============                ] 36/75 batches, loss: 0.0219Epoch 2/10: [==============                ] 37/75 batches, loss: 0.0219Epoch 2/10: [===============               ] 38/75 batches, loss: 0.0217Epoch 2/10: [===============               ] 39/75 batches, loss: 0.0215Epoch 2/10: [================              ] 40/75 batches, loss: 0.0215Epoch 2/10: [================              ] 41/75 batches, loss: 0.0216Epoch 2/10: [================              ] 42/75 batches, loss: 0.0214Epoch 2/10: [=================             ] 43/75 batches, loss: 0.0213Epoch 2/10: [=================             ] 44/75 batches, loss: 0.0213Epoch 2/10: [==================            ] 45/75 batches, loss: 0.0212Epoch 2/10: [==================            ] 46/75 batches, loss: 0.0211Epoch 2/10: [==================            ] 47/75 batches, loss: 0.0212Epoch 2/10: [===================           ] 48/75 batches, loss: 0.0212Epoch 2/10: [===================           ] 49/75 batches, loss: 0.0213Epoch 2/10: [====================          ] 50/75 batches, loss: 0.0212Epoch 2/10: [====================          ] 51/75 batches, loss: 0.0211slurmstepd: error: *** JOB 64435715 ON k28i22 CANCELLED AT 2025-05-01T12:19:32 ***
