SLURM_JOB_ID: 64467324
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: qtype_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Wed May  7 22:36:54 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main probing experiments (non-control)...
=======================
PROBING LAYER 2
=======================
Experiment probe_layer2_complexity_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/ko/ko/results.json for layer 2
Running control probing experiments...
=======================
PROBING LAYER 2 (CONTROL EXPERIMENTS)
=======================
Experiment probe_layer2_complexity_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_complexity_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_complexity_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/ko/ko/results.json for layer 2
Running submetric probing experiments...
=======================
PROBING LAYER 2 (SUBMETRIC EXPERIMENTS)
=======================
Experiment probe_layer2_avg_links_len_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_avg_max_depth_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_avg_subordinate_chain_len_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_avg_verb_edges_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_lexical_density_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_n_tokens_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/layer2/ko/ko/results.json for layer 2
Running control submetric probing experiments...
=======================
PROBING LAYER 2 (SUBMETRIC CONTROL EXPERIMENTS)
=======================
Experiment probe_layer2_avg_links_len_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_avg_links_len_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_avg_links_len_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control3/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_avg_max_depth_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_avg_max_depth_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_avg_max_depth_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_avg_subordinate_chain_len_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_avg_subordinate_chain_len_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_avg_subordinate_chain_len_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control3/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_avg_verb_edges_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control1/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_avg_verb_edges_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control2/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_avg_verb_edges_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control3/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_lexical_density_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control1/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_lexical_density_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control2/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_lexical_density_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control3/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_n_tokens_control1_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_n_tokens_control1_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control1/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=n_tokens"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:37:45,095][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control1/layer2/ko
experiment_name: probe_layer2_n_tokens_control1_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: n_tokens
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:37:45,095][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:37:45,095][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 22:37:45,095][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:37:45,095][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:37:45,099][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:37:45,100][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 22:37:45,100][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:37:49,054][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'n_tokens'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:37:51,530][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:37:51,530][src.data.datasets][INFO] - Loading 'control_n_tokens_seed1' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:37:51,853][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_n_tokens_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:50:23 2025).
[2025-05-07 22:37:51,989][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_n_tokens_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:50:23 2025).
[2025-05-07 22:37:52,367][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:37:52,372][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:37:52,373][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:37:52,376][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:37:52,447][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:37:52,589][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:37:52,633][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:37:52,634][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:37:52,634][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:37:52,636][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:37:52,768][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:37:52,879][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:37:52,933][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:37:52,934][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:37:52,934][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:37:52,956][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:37:52,957][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:37:52,957][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 22:37:52,957][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 22:37:52,957][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 22:37:52,958][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:37:52,958][src.data.datasets][INFO] -   Mean: 0.0860, Std: 0.0995
[2025-05-07 22:37:52,958][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:37:52,958][src.data.datasets][INFO] - Sample label: 0.026000000536441803
[2025-05-07 22:37:52,958][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:37:52,958][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 22:37:52,958][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 22:37:52,958][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 22:37:52,959][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8890
[2025-05-07 22:37:52,959][src.data.datasets][INFO] -   Mean: 0.1868, Std: 0.2076
[2025-05-07 22:37:52,959][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:37:52,959][src.data.datasets][INFO] - Sample label: 0.22200000286102295
[2025-05-07 22:37:52,959][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:37:52,959][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 22:37:52,959][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 22:37:52,959][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 22:37:52,959][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9440
[2025-05-07 22:37:52,959][src.data.datasets][INFO] -   Mean: 0.2782, Std: 0.2042
[2025-05-07 22:37:52,960][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:37:52,960][src.data.datasets][INFO] - Sample label: 0.4440000057220459
[2025-05-07 22:37:52,960][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:37:52,960][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:37:52,960][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:37:52,960][__main__][INFO] - Using model type: lm_probe for submetric n_tokens
[2025-05-07 22:37:52,960][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:38:01,496][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:38:01,497][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:38:01,497][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:38:01,497][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:38:01,500][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:38:01,501][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:38:01,501][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:38:01,501][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:38:01,501][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:38:01,502][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:38:01,502][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.3930Epoch 1/15: [=                             ] 2/47 batches, loss: 0.4480Epoch 1/15: [=                             ] 3/47 batches, loss: 0.4257Epoch 1/15: [==                            ] 4/47 batches, loss: 0.4125Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4115Epoch 1/15: [===                           ] 6/47 batches, loss: 0.3785Epoch 1/15: [====                          ] 7/47 batches, loss: 0.3668Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.3831Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.3939Epoch 1/15: [======                        ] 10/47 batches, loss: 0.3923Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.3804Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.3824Epoch 1/15: [========                      ] 13/47 batches, loss: 0.3746Epoch 1/15: [========                      ] 14/47 batches, loss: 0.3740Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.3631Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.3707Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.3638Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.3804Epoch 1/15: [============                  ] 19/47 batches, loss: 0.3751Epoch 1/15: [============                  ] 20/47 batches, loss: 0.3658Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.3744Epoch 1/15: [==============                ] 22/47 batches, loss: 0.3753Epoch 1/15: [==============                ] 23/47 batches, loss: 0.3648Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3554Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3542Epoch 1/15: [================              ] 26/47 batches, loss: 0.3501Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3496Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3427Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3472Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3416Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3345Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3290Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3272Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3279Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3233Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3215Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3160Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3149Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3120Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3137Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3122Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3110Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3090Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3074Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3074Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3023Epoch 1/15: [==============================] 47/47 batches, loss: 0.3053
[2025-05-07 22:38:07,880][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3053
[2025-05-07 22:38:08,151][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1413, Metrics: {'mse': 0.15306207537651062, 'rmse': 0.3912314856661087, 'r2': -2.551743984222412}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.3117Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2556Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2811Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2409Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2447Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2550Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2426Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2337Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2286Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2207Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2220Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2178Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2180Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2141Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2132Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2073Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2023Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2055Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2023Epoch 2/15: [============                  ] 20/47 batches, loss: 0.1977Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.1954Epoch 2/15: [==============                ] 22/47 batches, loss: 0.1936Epoch 2/15: [==============                ] 23/47 batches, loss: 0.1907Epoch 2/15: [===============               ] 24/47 batches, loss: 0.1910Epoch 2/15: [===============               ] 25/47 batches, loss: 0.1902Epoch 2/15: [================              ] 26/47 batches, loss: 0.1868Epoch 2/15: [=================             ] 27/47 batches, loss: 0.1835Epoch 2/15: [=================             ] 28/47 batches, loss: 0.1819Epoch 2/15: [==================            ] 29/47 batches, loss: 0.1790Epoch 2/15: [===================           ] 30/47 batches, loss: 0.1767Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1770Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1764Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1777Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1769Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1734Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1713Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1721Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1696Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1689Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1681Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1675Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1657Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1630Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1613Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1601Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1587Epoch 2/15: [==============================] 47/47 batches, loss: 0.1581
[2025-05-07 22:38:10,103][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1581
[2025-05-07 22:38:10,342][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0747, Metrics: {'mse': 0.08128780126571655, 'rmse': 0.28511015637068515, 'r2': -0.8862507343292236}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1586Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1227Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1051Epoch 3/15: [==                            ] 4/47 batches, loss: 0.0977Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1132Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1316Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1277Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1207Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1251Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1229Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1305Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1346Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1363Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1297Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1293Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1286Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1316Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1318Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1284Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1256Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1279Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1266Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1265Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1282Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1264Epoch 3/15: [================              ] 26/47 batches, loss: 0.1262Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1262Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1242Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1215Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1214Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1214Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1207Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1202Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1194Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1208Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1206Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1203Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1192Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1174Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1169Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1159Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1159Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1156Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1165Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1178Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1174Epoch 3/15: [==============================] 47/47 batches, loss: 0.1178
[2025-05-07 22:38:12,250][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1178
[2025-05-07 22:38:12,540][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0952, Metrics: {'mse': 0.10378783941268921, 'rmse': 0.3221612009735021, 'r2': -1.4083552360534668}
[2025-05-07 22:38:12,541][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1586Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1070Epoch 4/15: [=                             ] 3/47 batches, loss: 0.0885Epoch 4/15: [==                            ] 4/47 batches, loss: 0.0933Epoch 4/15: [===                           ] 5/47 batches, loss: 0.0920Epoch 4/15: [===                           ] 6/47 batches, loss: 0.0903Epoch 4/15: [====                          ] 7/47 batches, loss: 0.0898Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.0999Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.0986Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1014Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1071Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1041Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1056Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1009Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.0992Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.0956Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.0967Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.0955Epoch 4/15: [============                  ] 19/47 batches, loss: 0.0975Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1006Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.0997Epoch 4/15: [==============                ] 22/47 batches, loss: 0.0990Epoch 4/15: [==============                ] 23/47 batches, loss: 0.0979Epoch 4/15: [===============               ] 24/47 batches, loss: 0.0983Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1001Epoch 4/15: [================              ] 26/47 batches, loss: 0.1001Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1002Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1012Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1025Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1012Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1026Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1037Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1017Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1028Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1031Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1025Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1028Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1014Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1001Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.0997Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.0998Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.0999Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.0985Epoch 4/15: [============================  ] 44/47 batches, loss: 0.0985Epoch 4/15: [============================  ] 45/47 batches, loss: 0.0971Epoch 4/15: [============================= ] 46/47 batches, loss: 0.0972Epoch 4/15: [==============================] 47/47 batches, loss: 0.0980
[2025-05-07 22:38:13,994][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0980
[2025-05-07 22:38:14,289][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0788, Metrics: {'mse': 0.08618488907814026, 'rmse': 0.2935726299881177, 'r2': -0.9998857975006104}
[2025-05-07 22:38:14,290][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.0689Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1032Epoch 5/15: [=                             ] 3/47 batches, loss: 0.0936Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0762Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0667Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0665Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0674Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0660Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0698Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0645Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0651Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0671Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0655Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0643Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0642Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0642Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0640Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0641Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0641Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0666Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0684Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0693Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0688Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0698Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0692Epoch 5/15: [================              ] 26/47 batches, loss: 0.0676Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0677Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0665Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0658Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0668Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0672Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0671Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0673Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0669Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0685Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0686Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0681Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0684Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0698Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0703Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0703Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0701Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0699Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0698Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0710Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0706Epoch 5/15: [==============================] 47/47 batches, loss: 0.0700
[2025-05-07 22:38:15,743][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0700
[2025-05-07 22:38:16,023][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0812, Metrics: {'mse': 0.08879971504211426, 'rmse': 0.29799281038661696, 'r2': -1.0605616569519043}
[2025-05-07 22:38:16,024][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.1651Epoch 6/15: [=                             ] 2/47 batches, loss: 0.1145Epoch 6/15: [=                             ] 3/47 batches, loss: 0.1084Epoch 6/15: [==                            ] 4/47 batches, loss: 0.0982Epoch 6/15: [===                           ] 5/47 batches, loss: 0.0993Epoch 6/15: [===                           ] 6/47 batches, loss: 0.0959Epoch 6/15: [====                          ] 7/47 batches, loss: 0.0871Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.0876Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.0830Epoch 6/15: [======                        ] 10/47 batches, loss: 0.0843Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.0849Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.0868Epoch 6/15: [========                      ] 13/47 batches, loss: 0.0834Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0792Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0784Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.0800Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.0793Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.0799Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0767Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0748Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0724Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0724Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0713Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0699Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0690Epoch 6/15: [================              ] 26/47 batches, loss: 0.0681Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0678Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0673Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0687Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0680Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0685Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0670Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0676Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0668Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0663Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0661Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0663Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0659Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0657Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0652Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0647Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0647Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0655Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0654Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0650Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0648Epoch 6/15: [==============================] 47/47 batches, loss: 0.0648
[2025-05-07 22:38:17,541][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0648
[2025-05-07 22:38:17,858][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0825, Metrics: {'mse': 0.09015253186225891, 'rmse': 0.3002541121487912, 'r2': -1.0919532775878906}
[2025-05-07 22:38:17,859][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:38:17,859][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-07 22:38:17,859][src.training.lm_trainer][INFO] - Training completed in 12.31 seconds
[2025-05-07 22:38:17,859][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:38:20,099][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.017117740586400032, 'rmse': 0.13083478354933, 'r2': -0.7274229526519775}
[2025-05-07 22:38:20,099][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.08128780126571655, 'rmse': 0.28511015637068515, 'r2': -0.8862507343292236}
[2025-05-07 22:38:20,100][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.122195765376091, 'rmse': 0.3495651089226312, 'r2': -1.9303138256072998}
[2025-05-07 22:38:21,798][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control1/layer2/ko/ko/model.pt
[2025-05-07 22:38:21,799][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▆▅▆▆
wandb:       train_loss █▄▃▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▃▁▂▂
wandb:          val_mse █▁▃▁▂▂
wandb:           val_r2 ▁█▆█▇▇
wandb:         val_rmse █▁▃▂▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07466
wandb:     best_val_mse 0.08129
wandb:      best_val_r2 -0.88625
wandb:    best_val_rmse 0.28511
wandb: early_stop_epoch 6
wandb:            epoch 6
wandb:   final_test_mse 0.1222
wandb:    final_test_r2 -1.93031
wandb:  final_test_rmse 0.34957
wandb:  final_train_mse 0.01712
wandb:   final_train_r2 -0.72742
wandb: final_train_rmse 0.13083
wandb:    final_val_mse 0.08129
wandb:     final_val_r2 -0.88625
wandb:   final_val_rmse 0.28511
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06483
wandb:       train_time 12.30885
wandb:         val_loss 0.08246
wandb:          val_mse 0.09015
wandb:           val_r2 -1.09195
wandb:         val_rmse 0.30025
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_223745-yo200jmm
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_223745-yo200jmm/logs
Experiment probe_layer2_n_tokens_control1_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control1/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_n_tokens_control2_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_n_tokens_control2_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control2/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=n_tokens"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:38:48,051][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control2/layer2/ko
experiment_name: probe_layer2_n_tokens_control2_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: n_tokens
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:38:48,051][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:38:48,051][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 22:38:48,051][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:38:48,051][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:38:48,056][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:38:48,056][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 22:38:48,056][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:38:51,227][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'n_tokens'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:38:53,530][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:38:53,531][src.data.datasets][INFO] - Loading 'control_n_tokens_seed2' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:38:53,734][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_n_tokens_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:51:35 2025).
[2025-05-07 22:38:53,818][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_n_tokens_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:51:35 2025).
[2025-05-07 22:38:54,086][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:38:54,092][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:38:54,092][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:38:54,094][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:38:54,139][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:38:54,221][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:38:54,240][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:38:54,241][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:38:54,241][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:38:54,242][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:38:54,291][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:38:54,402][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:38:54,440][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:38:54,441][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:38:54,441][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:38:54,443][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:38:54,444][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:38:54,444][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 22:38:54,444][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 22:38:54,444][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 22:38:54,444][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:38:54,444][src.data.datasets][INFO] -   Mean: 0.0860, Std: 0.0995
[2025-05-07 22:38:54,444][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:38:54,444][src.data.datasets][INFO] - Sample label: 0.10300000011920929
[2025-05-07 22:38:54,445][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:38:54,445][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 22:38:54,445][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 22:38:54,445][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 22:38:54,445][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8890
[2025-05-07 22:38:54,445][src.data.datasets][INFO] -   Mean: 0.1868, Std: 0.2076
[2025-05-07 22:38:54,445][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:38:54,445][src.data.datasets][INFO] - Sample label: 0.22200000286102295
[2025-05-07 22:38:54,445][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:38:54,446][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 22:38:54,446][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 22:38:54,446][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 22:38:54,446][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9440
[2025-05-07 22:38:54,446][src.data.datasets][INFO] -   Mean: 0.2782, Std: 0.2042
[2025-05-07 22:38:54,446][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:38:54,446][src.data.datasets][INFO] - Sample label: 0.4440000057220459
[2025-05-07 22:38:54,446][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:38:54,446][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:38:54,447][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:38:54,447][__main__][INFO] - Using model type: lm_probe for submetric n_tokens
[2025-05-07 22:38:54,447][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:39:01,937][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:39:01,937][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:39:01,937][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:39:01,938][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:39:01,940][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:39:01,941][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:39:01,941][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:39:01,941][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:39:01,941][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:39:01,942][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:39:01,942][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.3971Epoch 1/15: [=                             ] 2/47 batches, loss: 0.4848Epoch 1/15: [=                             ] 3/47 batches, loss: 0.4749Epoch 1/15: [==                            ] 4/47 batches, loss: 0.4766Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4671Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4243Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4082Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4383Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4428Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4344Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4184Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4142Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4045Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4028Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.3895Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4029Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.3936Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4152Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4069Epoch 1/15: [============                  ] 20/47 batches, loss: 0.3990Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4024Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4006Epoch 1/15: [==============                ] 23/47 batches, loss: 0.3886Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3779Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3747Epoch 1/15: [================              ] 26/47 batches, loss: 0.3700Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3691Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3611Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3651Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3584Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3504Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3431Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3401Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3396Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3339Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3312Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3267Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3242Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3214Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3224Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3212Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3197Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3175Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3147Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3144Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3096Epoch 1/15: [==============================] 47/47 batches, loss: 0.3125
[2025-05-07 22:39:06,849][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3125
[2025-05-07 22:39:07,128][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1417, Metrics: {'mse': 0.1535465568304062, 'rmse': 0.3918501714053551, 'r2': -2.562986135482788}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2699Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2189Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2259Epoch 2/15: [==                            ] 4/47 batches, loss: 0.1909Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2144Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2253Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2210Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2126Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2130Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2022Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2047Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2067Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2028Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2067Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2047Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2027Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.1977Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2006Epoch 2/15: [============                  ] 19/47 batches, loss: 0.1983Epoch 2/15: [============                  ] 20/47 batches, loss: 0.1952Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.1942Epoch 2/15: [==============                ] 22/47 batches, loss: 0.1939Epoch 2/15: [==============                ] 23/47 batches, loss: 0.1927Epoch 2/15: [===============               ] 24/47 batches, loss: 0.1924Epoch 2/15: [===============               ] 25/47 batches, loss: 0.1912Epoch 2/15: [================              ] 26/47 batches, loss: 0.1880Epoch 2/15: [=================             ] 27/47 batches, loss: 0.1843Epoch 2/15: [=================             ] 28/47 batches, loss: 0.1826Epoch 2/15: [==================            ] 29/47 batches, loss: 0.1793Epoch 2/15: [===================           ] 30/47 batches, loss: 0.1772Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1757Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1752Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1767Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1770Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1740Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1718Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1725Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1694Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1681Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1671Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1672Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1656Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1631Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1621Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1615Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1596Epoch 2/15: [==============================] 47/47 batches, loss: 0.1588
[2025-05-07 22:39:08,972][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1588
[2025-05-07 22:39:09,229][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0746, Metrics: {'mse': 0.08114652335643768, 'rmse': 0.2848622884069383, 'r2': -0.8829724788665771}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.2197Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1435Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1257Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1102Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1194Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1428Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1325Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1244Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1359Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1326Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1327Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1363Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1339Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1288Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1286Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1262Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1313Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1306Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1267Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1240Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1260Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1227Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1264Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1271Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1248Epoch 3/15: [================              ] 26/47 batches, loss: 0.1232Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1225Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1204Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1177Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1168Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1168Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1163Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1163Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1157Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1170Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1162Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1157Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1143Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1125Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1124Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1117Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1117Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1114Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1130Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1140Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1136Epoch 3/15: [==============================] 47/47 batches, loss: 0.1125
[2025-05-07 22:39:11,100][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1125
[2025-05-07 22:39:11,357][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0931, Metrics: {'mse': 0.10144856572151184, 'rmse': 0.3185099146361253, 'r2': -1.3540730476379395}
[2025-05-07 22:39:11,358][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1342Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1165Epoch 4/15: [=                             ] 3/47 batches, loss: 0.0930Epoch 4/15: [==                            ] 4/47 batches, loss: 0.0958Epoch 4/15: [===                           ] 5/47 batches, loss: 0.0973Epoch 4/15: [===                           ] 6/47 batches, loss: 0.0925Epoch 4/15: [====                          ] 7/47 batches, loss: 0.0950Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.0977Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.0955Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1002Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1061Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1062Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1052Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1021Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1002Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.0972Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.0993Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.0989Epoch 4/15: [============                  ] 19/47 batches, loss: 0.0995Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1039Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1036Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1053Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1039Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1027Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1029Epoch 4/15: [================              ] 26/47 batches, loss: 0.1014Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1030Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1044Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1060Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1048Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1067Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1064Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1056Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1050Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1047Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1036Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1036Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1021Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1007Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1000Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1010Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1012Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.0999Epoch 4/15: [============================  ] 44/47 batches, loss: 0.0995Epoch 4/15: [============================  ] 45/47 batches, loss: 0.0983Epoch 4/15: [============================= ] 46/47 batches, loss: 0.0981Epoch 4/15: [==============================] 47/47 batches, loss: 0.0983
[2025-05-07 22:39:12,827][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0983
[2025-05-07 22:39:13,100][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0833, Metrics: {'mse': 0.09101729094982147, 'rmse': 0.3016907206889557, 'r2': -1.1120197772979736}
[2025-05-07 22:39:13,101][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.0926Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1019Epoch 5/15: [=                             ] 3/47 batches, loss: 0.0924Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0753Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0698Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0699Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0677Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0662Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0686Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0649Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0652Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0675Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0648Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0654Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0648Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0643Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0639Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0645Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0646Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0662Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0669Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0668Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0661Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0684Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0682Epoch 5/15: [================              ] 26/47 batches, loss: 0.0674Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0661Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0657Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0676Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0688Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0689Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0685Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0681Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0685Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0688Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0692Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0686Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0691Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0696Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0697Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0694Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0698Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0698Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0698Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0709Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0706Epoch 5/15: [==============================] 47/47 batches, loss: 0.0700
[2025-05-07 22:39:14,583][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0700
[2025-05-07 22:39:14,872][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0986, Metrics: {'mse': 0.10738810896873474, 'rmse': 0.3277012495684671, 'r2': -1.4918980598449707}
[2025-05-07 22:39:14,872][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.0673Epoch 6/15: [=                             ] 2/47 batches, loss: 0.0675Epoch 6/15: [=                             ] 3/47 batches, loss: 0.0745Epoch 6/15: [==                            ] 4/47 batches, loss: 0.0725Epoch 6/15: [===                           ] 5/47 batches, loss: 0.0776Epoch 6/15: [===                           ] 6/47 batches, loss: 0.0736Epoch 6/15: [====                          ] 7/47 batches, loss: 0.0687Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.0703Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.0675Epoch 6/15: [======                        ] 10/47 batches, loss: 0.0690Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.0707Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.0761Epoch 6/15: [========                      ] 13/47 batches, loss: 0.0746Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0716Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0700Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.0707Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.0707Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.0715Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0686Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0662Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0642Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0654Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0642Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0630Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0625Epoch 6/15: [================              ] 26/47 batches, loss: 0.0627Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0638Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0641Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0652Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0650Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0662Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0651Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0660Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0654Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0649Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0642Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0649Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0644Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0644Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0637Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0639Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0649Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0650Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0649Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0656Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0653Epoch 6/15: [==============================] 47/47 batches, loss: 0.0644
[2025-05-07 22:39:16,347][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0644
[2025-05-07 22:39:16,663][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0800, Metrics: {'mse': 0.08750239759683609, 'rmse': 0.2958080418055535, 'r2': -1.0304579734802246}
[2025-05-07 22:39:16,664][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:39:16,664][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-07 22:39:16,664][src.training.lm_trainer][INFO] - Training completed in 12.16 seconds
[2025-05-07 22:39:16,664][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:39:18,839][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.016988765448331833, 'rmse': 0.13034095844488727, 'r2': -0.7144075632095337}
[2025-05-07 22:39:18,840][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.08114652335643768, 'rmse': 0.2848622884069383, 'r2': -0.8829724788665771}
[2025-05-07 22:39:18,840][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.12142856419086456, 'rmse': 0.3484660158334878, 'r2': -1.9119162559509277}
[2025-05-07 22:39:20,505][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control2/layer2/ko/ko/model.pt
[2025-05-07 22:39:20,506][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▆▅▆▅
wandb:       train_loss █▄▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▃▂▄▂
wandb:          val_mse █▁▃▂▄▂
wandb:           val_r2 ▁█▆▇▅▇
wandb:         val_rmse █▁▃▂▄▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07457
wandb:     best_val_mse 0.08115
wandb:      best_val_r2 -0.88297
wandb:    best_val_rmse 0.28486
wandb: early_stop_epoch 6
wandb:            epoch 6
wandb:   final_test_mse 0.12143
wandb:    final_test_r2 -1.91192
wandb:  final_test_rmse 0.34847
wandb:  final_train_mse 0.01699
wandb:   final_train_r2 -0.71441
wandb: final_train_rmse 0.13034
wandb:    final_val_mse 0.08115
wandb:     final_val_r2 -0.88297
wandb:   final_val_rmse 0.28486
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06442
wandb:       train_time 12.15619
wandb:         val_loss 0.08003
wandb:          val_mse 0.0875
wandb:           val_r2 -1.03046
wandb:         val_rmse 0.29581
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_223848-zrqyd8nm
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_223848-zrqyd8nm/logs
Experiment probe_layer2_n_tokens_control2_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control2/layer2/ko/ko/results.json for layer 2
Running experiment: probe_layer2_n_tokens_control3_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_n_tokens_control3_ko"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control3/layer2/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=n_tokens"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 22:39:43,586][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control3/layer2/ko
experiment_name: probe_layer2_n_tokens_control3_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: n_tokens
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 22:39:43,586][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 22:39:43,586][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 22:39:43,586][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 22:39:43,586][__main__][INFO] - Determined Task Type: regression
[2025-05-07 22:39:43,591][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ko']
[2025-05-07 22:39:43,591][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 22:39:43,591][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 22:39:45,867][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'single_submetric', submetric: 'n_tokens'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 22:39:48,096][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 22:39:48,097][src.data.datasets][INFO] - Loading 'control_n_tokens_seed3' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:39:48,199][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_n_tokens_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:52:51 2025).
[2025-05-07 22:39:48,279][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_n_tokens_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:52:51 2025).
[2025-05-07 22:39:48,466][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-07 22:39:48,472][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:39:48,472][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-07 22:39:48,475][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:39:48,546][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:39:48,650][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:39:48,687][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-07 22:39:48,688][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:39:48,688][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-07 22:39:48,692][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 22:39:48,806][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:39:48,891][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 22:39:48,909][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-07 22:39:48,910][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 22:39:48,910][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-07 22:39:48,911][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-07 22:39:48,912][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:39:48,912][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 22:39:48,912][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 22:39:48,912][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 22:39:48,912][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 22:39:48,913][src.data.datasets][INFO] -   Mean: 0.0860, Std: 0.0995
[2025-05-07 22:39:48,913][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-07 22:39:48,913][src.data.datasets][INFO] - Sample label: 0.026000000536441803
[2025-05-07 22:39:48,913][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:39:48,913][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 22:39:48,913][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 22:39:48,913][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 22:39:48,913][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8890
[2025-05-07 22:39:48,914][src.data.datasets][INFO] -   Mean: 0.1868, Std: 0.2076
[2025-05-07 22:39:48,914][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-07 22:39:48,914][src.data.datasets][INFO] - Sample label: 0.22200000286102295
[2025-05-07 22:39:48,914][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 22:39:48,914][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 22:39:48,914][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 22:39:48,914][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 22:39:48,914][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9440
[2025-05-07 22:39:48,914][src.data.datasets][INFO] -   Mean: 0.2782, Std: 0.2042
[2025-05-07 22:39:48,914][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-07 22:39:48,914][src.data.datasets][INFO] - Sample label: 0.4440000057220459
[2025-05-07 22:39:48,915][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-07 22:39:48,915][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 22:39:48,915][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 22:39:48,915][__main__][INFO] - Using model type: lm_probe for submetric n_tokens
[2025-05-07 22:39:48,915][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 22:39:54,913][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 22:39:54,914][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 22:39:54,914][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 22:39:54,914][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 22:39:54,918][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 22:39:54,918][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 22:39:54,918][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 22:39:54,918][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 22:39:54,919][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-07 22:39:54,919][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 22:39:54,920][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.4080Epoch 1/15: [=                             ] 2/47 batches, loss: 0.5517Epoch 1/15: [=                             ] 3/47 batches, loss: 0.5085Epoch 1/15: [==                            ] 4/47 batches, loss: 0.4852Epoch 1/15: [===                           ] 5/47 batches, loss: 0.4645Epoch 1/15: [===                           ] 6/47 batches, loss: 0.4192Epoch 1/15: [====                          ] 7/47 batches, loss: 0.4093Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.4185Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.4255Epoch 1/15: [======                        ] 10/47 batches, loss: 0.4271Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.4167Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4080Epoch 1/15: [========                      ] 13/47 batches, loss: 0.3998Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4022Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.3886Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.3954Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.3853Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4078Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4037Epoch 1/15: [============                  ] 20/47 batches, loss: 0.3926Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.3979Epoch 1/15: [==============                ] 22/47 batches, loss: 0.3998Epoch 1/15: [==============                ] 23/47 batches, loss: 0.3887Epoch 1/15: [===============               ] 24/47 batches, loss: 0.3771Epoch 1/15: [===============               ] 25/47 batches, loss: 0.3752Epoch 1/15: [================              ] 26/47 batches, loss: 0.3702Epoch 1/15: [=================             ] 27/47 batches, loss: 0.3689Epoch 1/15: [=================             ] 28/47 batches, loss: 0.3607Epoch 1/15: [==================            ] 29/47 batches, loss: 0.3663Epoch 1/15: [===================           ] 30/47 batches, loss: 0.3603Epoch 1/15: [===================           ] 31/47 batches, loss: 0.3515Epoch 1/15: [====================          ] 32/47 batches, loss: 0.3454Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3420Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3424Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3365Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3355Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.3302Epoch 1/15: [========================      ] 38/47 batches, loss: 0.3277Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3233Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3234Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3220Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3196Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3177Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3153Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3149Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3097Epoch 1/15: [==============================] 47/47 batches, loss: 0.3126
[2025-05-07 22:40:04,149][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3126
[2025-05-07 22:40:04,409][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1577, Metrics: {'mse': 0.1704886555671692, 'rmse': 0.412902719253784, 'r2': -2.956120729446411}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2640Epoch 2/15: [=                             ] 2/47 batches, loss: 0.2133Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2448Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2044Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2060Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2231Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2158Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2010Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2026Epoch 2/15: [======                        ] 10/47 batches, loss: 0.1948Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.1953Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.1950Epoch 2/15: [========                      ] 13/47 batches, loss: 0.1951Epoch 2/15: [========                      ] 14/47 batches, loss: 0.1965Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.1940Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.1926Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.1872Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.1961Epoch 2/15: [============                  ] 19/47 batches, loss: 0.1938Epoch 2/15: [============                  ] 20/47 batches, loss: 0.1899Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.1892Epoch 2/15: [==============                ] 22/47 batches, loss: 0.1883Epoch 2/15: [==============                ] 23/47 batches, loss: 0.1865Epoch 2/15: [===============               ] 24/47 batches, loss: 0.1863Epoch 2/15: [===============               ] 25/47 batches, loss: 0.1859Epoch 2/15: [================              ] 26/47 batches, loss: 0.1839Epoch 2/15: [=================             ] 27/47 batches, loss: 0.1808Epoch 2/15: [=================             ] 28/47 batches, loss: 0.1789Epoch 2/15: [==================            ] 29/47 batches, loss: 0.1757Epoch 2/15: [===================           ] 30/47 batches, loss: 0.1735Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1734Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1725Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1733Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1740Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1712Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1692Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1705Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1676Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1661Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1659Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1648Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1630Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1606Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1587Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1594Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1580Epoch 2/15: [==============================] 47/47 batches, loss: 0.1573
[2025-05-07 22:40:06,302][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1573
[2025-05-07 22:40:06,567][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0718, Metrics: {'mse': 0.07805787026882172, 'rmse': 0.27938838606646077, 'r2': -0.811301589012146}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1512Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1105Epoch 3/15: [=                             ] 3/47 batches, loss: 0.0976Epoch 3/15: [==                            ] 4/47 batches, loss: 0.0980Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1090Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1274Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1222Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1168Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1257Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1239Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1241Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1284Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1292Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1244Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1254Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1245Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1293Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1276Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1238Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1218Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1244Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1213Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1220Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1236Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1204Epoch 3/15: [================              ] 26/47 batches, loss: 0.1191Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1192Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1173Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1156Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1150Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1162Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1156Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1150Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1148Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1163Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1152Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1147Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1132Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1112Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1112Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1105Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1102Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1101Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1122Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1139Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1140Epoch 3/15: [==============================] 47/47 batches, loss: 0.1140
[2025-05-07 22:40:08,441][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1140
[2025-05-07 22:40:08,724][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0923, Metrics: {'mse': 0.10068574547767639, 'rmse': 0.3173101723514019, 'r2': -1.3363723754882812}
[2025-05-07 22:40:08,725][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1324Epoch 4/15: [=                             ] 2/47 batches, loss: 0.0852Epoch 4/15: [=                             ] 3/47 batches, loss: 0.0690Epoch 4/15: [==                            ] 4/47 batches, loss: 0.0825Epoch 4/15: [===                           ] 5/47 batches, loss: 0.0873Epoch 4/15: [===                           ] 6/47 batches, loss: 0.0876Epoch 4/15: [====                          ] 7/47 batches, loss: 0.0893Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.0925Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.0916Epoch 4/15: [======                        ] 10/47 batches, loss: 0.0981Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1066Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1053Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1033Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1003Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.0992Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.0973Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.0978Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.0964Epoch 4/15: [============                  ] 19/47 batches, loss: 0.0973Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1016Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1019Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1009Epoch 4/15: [==============                ] 23/47 batches, loss: 0.0994Epoch 4/15: [===============               ] 24/47 batches, loss: 0.0987Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1000Epoch 4/15: [================              ] 26/47 batches, loss: 0.0987Epoch 4/15: [=================             ] 27/47 batches, loss: 0.0992Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1005Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1006Epoch 4/15: [===================           ] 30/47 batches, loss: 0.0994Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1001Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1003Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.0990Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.0998Epoch 4/15: [======================        ] 35/47 batches, loss: 0.0997Epoch 4/15: [======================        ] 36/47 batches, loss: 0.0986Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1001Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1011Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1003Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.0999Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1004Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1004Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.0989Epoch 4/15: [============================  ] 44/47 batches, loss: 0.0985Epoch 4/15: [============================  ] 45/47 batches, loss: 0.0974Epoch 4/15: [============================= ] 46/47 batches, loss: 0.0972Epoch 4/15: [==============================] 47/47 batches, loss: 0.0975
[2025-05-07 22:40:10,214][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0975
[2025-05-07 22:40:10,494][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0727, Metrics: {'mse': 0.07951964437961578, 'rmse': 0.28199227716307373, 'r2': -0.8452214002609253}
[2025-05-07 22:40:10,495][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.1030Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1075Epoch 5/15: [=                             ] 3/47 batches, loss: 0.0999Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0834Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0829Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0811Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0781Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0769Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0778Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0730Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0729Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0730Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0712Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0709Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0724Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0716Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0697Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0705Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0707Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0709Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0714Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0720Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0713Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0714Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0714Epoch 5/15: [================              ] 26/47 batches, loss: 0.0708Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0698Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0684Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0678Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0689Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0688Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0692Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0692Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0686Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0689Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0691Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0682Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0693Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0694Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0690Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0684Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0681Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0684Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0686Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0692Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0691Epoch 5/15: [==============================] 47/47 batches, loss: 0.0686
[2025-05-07 22:40:11,959][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0686
[2025-05-07 22:40:12,302][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0872, Metrics: {'mse': 0.09525790810585022, 'rmse': 0.3086387987694519, 'r2': -1.2104213237762451}
[2025-05-07 22:40:12,302][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.0730Epoch 6/15: [=                             ] 2/47 batches, loss: 0.0769Epoch 6/15: [=                             ] 3/47 batches, loss: 0.0792Epoch 6/15: [==                            ] 4/47 batches, loss: 0.0744Epoch 6/15: [===                           ] 5/47 batches, loss: 0.0852Epoch 6/15: [===                           ] 6/47 batches, loss: 0.0825Epoch 6/15: [====                          ] 7/47 batches, loss: 0.0782Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.0828Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.0773Epoch 6/15: [======                        ] 10/47 batches, loss: 0.0784Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.0756Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.0809Epoch 6/15: [========                      ] 13/47 batches, loss: 0.0775Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0743Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0719Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.0731Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.0737Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.0743Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0712Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0695Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0679Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0683Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0680Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0666Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0658Epoch 6/15: [================              ] 26/47 batches, loss: 0.0651Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0656Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0651Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0666Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0659Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0668Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0652Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0661Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0670Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0667Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0657Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0654Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0650Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0649Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0641Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0667Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0672Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0672Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0671Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0671Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0667Epoch 6/15: [==============================] 47/47 batches, loss: 0.0669
[2025-05-07 22:40:13,826][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0669
[2025-05-07 22:40:14,093][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0770, Metrics: {'mse': 0.08427724242210388, 'rmse': 0.29030542954292793, 'r2': -0.9556196928024292}
[2025-05-07 22:40:14,094][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 22:40:14,094][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-07 22:40:14,094][src.training.lm_trainer][INFO] - Training completed in 12.18 seconds
[2025-05-07 22:40:14,094][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 22:40:16,301][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.01601569354534149, 'rmse': 0.12655312538748892, 'r2': -0.6162108182907104}
[2025-05-07 22:40:16,303][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07805787026882172, 'rmse': 0.27938838606646077, 'r2': -0.811301589012146}
[2025-05-07 22:40:16,303][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.11676178872585297, 'rmse': 0.3417042415976907, 'r2': -1.800004482269287}
[2025-05-07 22:40:18,138][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control3/layer2/ko/ko/model.pt
[2025-05-07 22:40:18,140][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▇▆▇▆
wandb:       train_loss █▄▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▃▁▂▁
wandb:          val_mse █▁▃▁▂▁
wandb:           val_r2 ▁█▆█▇█
wandb:         val_rmse █▁▃▁▃▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07175
wandb:     best_val_mse 0.07806
wandb:      best_val_r2 -0.8113
wandb:    best_val_rmse 0.27939
wandb: early_stop_epoch 6
wandb:            epoch 6
wandb:   final_test_mse 0.11676
wandb:    final_test_r2 -1.8
wandb:  final_test_rmse 0.3417
wandb:  final_train_mse 0.01602
wandb:   final_train_r2 -0.61621
wandb: final_train_rmse 0.12655
wandb:    final_val_mse 0.07806
wandb:     final_val_r2 -0.8113
wandb:   final_val_rmse 0.27939
wandb:    learning_rate 0.0001
wandb:       train_loss 0.0669
wandb:       train_time 12.1801
wandb:         val_loss 0.07697
wandb:          val_mse 0.08428
wandb:           val_r2 -0.95562
wandb:         val_rmse 0.29031
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_223943-ys47wvcr
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_223943-ys47wvcr/logs
Experiment probe_layer2_n_tokens_control3_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control3/layer2/ko/ko/results.json for layer 2
Some experiments failed. See /scratch/leuven/371/vsc37132/makeup_probes_output/failed_experiments.log for details.
Failed experiments (2):
probe_layer2_n_tokens,_fi
probe_layer2_avg_verb_edges,_fi
==============================================
probing experiments completed!
 planned experiments: 28
 completed: 112
