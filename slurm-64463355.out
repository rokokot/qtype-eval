SLURM_JOB_ID: 64463355
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: qtype_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Wed May  7 11:39:35 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main probing experiments (non-control)...
=======================
PROBING LAYER 2
=======================
Running experiment: probe_layer2_question_type_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_question_type_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer2/fi"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 11:40:18,810][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer2/fi
experiment_name: probe_layer2_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-07 11:40:18,810][__main__][INFO] - Normalized task: question_type
[2025-05-07 11:40:18,811][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-07 11:40:18,811][__main__][INFO] - Determined Task Type: classification
[2025-05-07 11:40:18,815][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-05-07 11:40:18,815][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 11:40:23,102][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 11:40:25,635][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 11:40:25,635][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:40:25,935][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:40:26,045][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:40:26,300][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 11:40:26,309][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:40:26,309][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 11:40:26,311][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:40:26,396][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:40:26,460][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:40:26,475][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 11:40:26,476][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:40:26,476][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 11:40:26,477][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:40:26,588][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:40:26,709][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:40:26,732][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 11:40:26,734][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:40:26,734][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 11:40:26,745][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 11:40:26,746][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:40:26,746][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:40:26,746][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:40:26,746][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:40:26,746][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-05-07 11:40:26,746][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-05-07 11:40:26,746][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 11:40:26,747][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 11:40:26,747][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:40:26,747][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:40:26,747][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:40:26,747][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:40:26,747][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-05-07 11:40:26,747][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-05-07 11:40:26,747][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 11:40:26,747][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 11:40:26,747][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:40:26,748][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:40:26,748][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:40:26,748][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:40:26,748][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-07 11:40:26,748][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-07 11:40:26,748][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 11:40:26,748][src.data.datasets][INFO] - Sample label: 0
[2025-05-07 11:40:26,748][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 11:40:26,748][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 11:40:26,748][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 11:40:26,749][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-07 11:40:26,749][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 11:40:35,703][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 11:40:35,703][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 11:40:35,704][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 11:40:35,704][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-07 11:40:35,710][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-07 11:40:35,710][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-07 11:40:35,710][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-07 11:40:35,710][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-07 11:40:35,710][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 11:40:35,711][__main__][INFO] - Total parameters: 394,568,839
[2025-05-07 11:40:35,711][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-07 11:40:35,713][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7011Epoch 1/15: [                              ] 2/75 batches, loss: 0.7060Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7215Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7220Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7129Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7055Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7029Epoch 1/15: [===                           ] 8/75 batches, loss: 0.7020Epoch 1/15: [===                           ] 9/75 batches, loss: 0.7001Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6990Epoch 1/15: [====                          ] 11/75 batches, loss: 0.7000Epoch 1/15: [====                          ] 12/75 batches, loss: 0.6987Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.6988Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.6983Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6985Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6983Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6985Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6980Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6978Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6976Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6969Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6964Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6963Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6962Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6959Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6959Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6958Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6958Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6957Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6956Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6952Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6951Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6951Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6949Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6946Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6943Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6943Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6943Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6942Epoch 1/15: [================              ] 40/75 batches, loss: 0.6940Epoch 1/15: [================              ] 41/75 batches, loss: 0.6942Epoch 1/15: [================              ] 42/75 batches, loss: 0.6940Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6942Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6940Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6940Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6937Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6937Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6934Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6933Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6935Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6932Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6930Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6933Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6932Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6929Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6928Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6922Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6926Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6920Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6917Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6919Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6915Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6902Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6905Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6895Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6895Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6891Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6873Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6868Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6873Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6865Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6864Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6859Epoch 1/15: [==============================] 75/75 batches, loss: 0.6846
[2025-05-07 11:40:42,635][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6846
[2025-05-07 11:40:42,916][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6639, Metrics: {'accuracy': 0.5873015873015873, 'f1': 0.2777777777777778, 'precision': 0.8333333333333334, 'recall': 0.16666666666666666}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6244Epoch 2/15: [                              ] 2/75 batches, loss: 0.6270Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6530Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6601Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6596Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6528Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6510Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6528Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6503Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6536Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6506Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6475Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6429Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6361Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6353Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6307Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6222Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6226Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6300Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6340Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6344Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6298Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6303Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6280Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6292Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6300Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6285Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6275Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6265Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6252Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6262Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6243Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6259Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6278Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6276Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6270Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6263Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6256Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6256Epoch 2/15: [================              ] 40/75 batches, loss: 0.6247Epoch 2/15: [================              ] 41/75 batches, loss: 0.6249Epoch 2/15: [================              ] 42/75 batches, loss: 0.6247Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6236Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6220Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6198Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6207Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6205Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6202Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6207Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6199Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6203Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6191Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6192Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6183Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6180Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6168Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6175Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6162Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6169Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6154Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6155Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6159Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6165Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6182Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6173Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6174Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6164Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6157Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6151Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6144Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6143Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6122Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6117Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6107Epoch 2/15: [==============================] 75/75 batches, loss: 0.6099
[2025-05-07 11:40:45,581][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6099
[2025-05-07 11:40:45,809][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.5983, Metrics: {'accuracy': 0.8412698412698413, 'f1': 0.8275862068965517, 'precision': 0.8571428571428571, 'recall': 0.8}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.5320Epoch 3/15: [                              ] 2/75 batches, loss: 0.5306Epoch 3/15: [=                             ] 3/75 batches, loss: 0.5485Epoch 3/15: [=                             ] 4/75 batches, loss: 0.5767Epoch 3/15: [==                            ] 5/75 batches, loss: 0.5661Epoch 3/15: [==                            ] 6/75 batches, loss: 0.5635Epoch 3/15: [==                            ] 7/75 batches, loss: 0.5590Epoch 3/15: [===                           ] 8/75 batches, loss: 0.5618Epoch 3/15: [===                           ] 9/75 batches, loss: 0.5539Epoch 3/15: [====                          ] 10/75 batches, loss: 0.5677Epoch 3/15: [====                          ] 11/75 batches, loss: 0.5673Epoch 3/15: [====                          ] 12/75 batches, loss: 0.5644Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.5581Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.5613Epoch 3/15: [======                        ] 15/75 batches, loss: 0.5567Epoch 3/15: [======                        ] 16/75 batches, loss: 0.5578Epoch 3/15: [======                        ] 17/75 batches, loss: 0.5548Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.5487Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.5544Epoch 3/15: [========                      ] 20/75 batches, loss: 0.5491Epoch 3/15: [========                      ] 21/75 batches, loss: 0.5511Epoch 3/15: [========                      ] 22/75 batches, loss: 0.5537Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.5511Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.5546Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.5601Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.5634Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.5622Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.5626Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.5641Epoch 3/15: [============                  ] 30/75 batches, loss: 0.5653Epoch 3/15: [============                  ] 31/75 batches, loss: 0.5642Epoch 3/15: [============                  ] 32/75 batches, loss: 0.5644Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.5645Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.5619Epoch 3/15: [==============                ] 35/75 batches, loss: 0.5618Epoch 3/15: [==============                ] 36/75 batches, loss: 0.5605Epoch 3/15: [==============                ] 37/75 batches, loss: 0.5605Epoch 3/15: [===============               ] 38/75 batches, loss: 0.5600Epoch 3/15: [===============               ] 39/75 batches, loss: 0.5618Epoch 3/15: [================              ] 40/75 batches, loss: 0.5613Epoch 3/15: [================              ] 41/75 batches, loss: 0.5607Epoch 3/15: [================              ] 42/75 batches, loss: 0.5575Epoch 3/15: [=================             ] 43/75 batches, loss: 0.5565Epoch 3/15: [=================             ] 44/75 batches, loss: 0.5580Epoch 3/15: [==================            ] 45/75 batches, loss: 0.5575Epoch 3/15: [==================            ] 46/75 batches, loss: 0.5564Epoch 3/15: [==================            ] 47/75 batches, loss: 0.5571Epoch 3/15: [===================           ] 48/75 batches, loss: 0.5578Epoch 3/15: [===================           ] 49/75 batches, loss: 0.5564Epoch 3/15: [====================          ] 50/75 batches, loss: 0.5576Epoch 3/15: [====================          ] 51/75 batches, loss: 0.5578Epoch 3/15: [====================          ] 52/75 batches, loss: 0.5575Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.5586Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.5579Epoch 3/15: [======================        ] 55/75 batches, loss: 0.5592Epoch 3/15: [======================        ] 56/75 batches, loss: 0.5589Epoch 3/15: [======================        ] 57/75 batches, loss: 0.5566Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.5563Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.5584Epoch 3/15: [========================      ] 60/75 batches, loss: 0.5594Epoch 3/15: [========================      ] 61/75 batches, loss: 0.5593Epoch 3/15: [========================      ] 62/75 batches, loss: 0.5601Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.5614Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.5609Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.5614Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.5606Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.5594Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.5598Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.5599Epoch 3/15: [============================  ] 70/75 batches, loss: 0.5591Epoch 3/15: [============================  ] 71/75 batches, loss: 0.5594Epoch 3/15: [============================  ] 72/75 batches, loss: 0.5600Epoch 3/15: [============================= ] 73/75 batches, loss: 0.5603Epoch 3/15: [============================= ] 74/75 batches, loss: 0.5610Epoch 3/15: [==============================] 75/75 batches, loss: 0.5609
[2025-05-07 11:40:48,601][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.5609
[2025-05-07 11:40:48,951][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6039, Metrics: {'accuracy': 0.7936507936507936, 'f1': 0.7450980392156863, 'precision': 0.9047619047619048, 'recall': 0.6333333333333333}
[2025-05-07 11:40:48,951][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.5772Epoch 4/15: [                              ] 2/75 batches, loss: 0.5308Epoch 4/15: [=                             ] 3/75 batches, loss: 0.5539Epoch 4/15: [=                             ] 4/75 batches, loss: 0.5606Epoch 4/15: [==                            ] 5/75 batches, loss: 0.5555Epoch 4/15: [==                            ] 6/75 batches, loss: 0.5677Epoch 4/15: [==                            ] 7/75 batches, loss: 0.5648Epoch 4/15: [===                           ] 8/75 batches, loss: 0.5710Epoch 4/15: [===                           ] 9/75 batches, loss: 0.5693Epoch 4/15: [====                          ] 10/75 batches, loss: 0.5666Epoch 4/15: [====                          ] 11/75 batches, loss: 0.5658Epoch 4/15: [====                          ] 12/75 batches, loss: 0.5647Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.5575Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.5610Epoch 4/15: [======                        ] 15/75 batches, loss: 0.5610Epoch 4/15: [======                        ] 16/75 batches, loss: 0.5588Epoch 4/15: [======                        ] 17/75 batches, loss: 0.5569Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.5546Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.5520Epoch 4/15: [========                      ] 20/75 batches, loss: 0.5506Epoch 4/15: [========                      ] 21/75 batches, loss: 0.5525Epoch 4/15: [========                      ] 22/75 batches, loss: 0.5569Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.5601Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.5615Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.5605Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.5594Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.5584Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.5565Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.5588Epoch 4/15: [============                  ] 30/75 batches, loss: 0.5545Epoch 4/15: [============                  ] 31/75 batches, loss: 0.5548Epoch 4/15: [============                  ] 32/75 batches, loss: 0.5549Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.5565Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.5570Epoch 4/15: [==============                ] 35/75 batches, loss: 0.5562Epoch 4/15: [==============                ] 36/75 batches, loss: 0.5523Epoch 4/15: [==============                ] 37/75 batches, loss: 0.5535Epoch 4/15: [===============               ] 38/75 batches, loss: 0.5528Epoch 4/15: [===============               ] 39/75 batches, loss: 0.5494Epoch 4/15: [================              ] 40/75 batches, loss: 0.5494Epoch 4/15: [================              ] 41/75 batches, loss: 0.5495Epoch 4/15: [================              ] 42/75 batches, loss: 0.5491Epoch 4/15: [=================             ] 43/75 batches, loss: 0.5494Epoch 4/15: [=================             ] 44/75 batches, loss: 0.5469Epoch 4/15: [==================            ] 45/75 batches, loss: 0.5469Epoch 4/15: [==================            ] 46/75 batches, loss: 0.5476Epoch 4/15: [==================            ] 47/75 batches, loss: 0.5469Epoch 4/15: [===================           ] 48/75 batches, loss: 0.5458Epoch 4/15: [===================           ] 49/75 batches, loss: 0.5469Epoch 4/15: [====================          ] 50/75 batches, loss: 0.5476Epoch 4/15: [====================          ] 51/75 batches, loss: 0.5481Epoch 4/15: [====================          ] 52/75 batches, loss: 0.5474Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.5469Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.5458Epoch 4/15: [======================        ] 55/75 batches, loss: 0.5471Epoch 4/15: [======================        ] 56/75 batches, loss: 0.5464Epoch 4/15: [======================        ] 57/75 batches, loss: 0.5454Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.5455Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.5448Epoch 4/15: [========================      ] 60/75 batches, loss: 0.5455Epoch 4/15: [========================      ] 61/75 batches, loss: 0.5462Epoch 4/15: [========================      ] 62/75 batches, loss: 0.5459Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.5458Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.5454Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.5457Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.5458Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.5451Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.5451Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.5471Epoch 4/15: [============================  ] 70/75 batches, loss: 0.5463Epoch 4/15: [============================  ] 71/75 batches, loss: 0.5455Epoch 4/15: [============================  ] 72/75 batches, loss: 0.5454Epoch 4/15: [============================= ] 73/75 batches, loss: 0.5457Epoch 4/15: [============================= ] 74/75 batches, loss: 0.5451Epoch 4/15: [==============================] 75/75 batches, loss: 0.5457
[2025-05-07 11:40:51,300][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5457
[2025-05-07 11:40:51,610][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5758, Metrics: {'accuracy': 0.873015873015873, 'f1': 0.8620689655172413, 'precision': 0.8928571428571429, 'recall': 0.8333333333333334}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.4742Epoch 5/15: [                              ] 2/75 batches, loss: 0.4890Epoch 5/15: [=                             ] 3/75 batches, loss: 0.5342Epoch 5/15: [=                             ] 4/75 batches, loss: 0.5354Epoch 5/15: [==                            ] 5/75 batches, loss: 0.5252Epoch 5/15: [==                            ] 6/75 batches, loss: 0.5279Epoch 5/15: [==                            ] 7/75 batches, loss: 0.5271Epoch 5/15: [===                           ] 8/75 batches, loss: 0.5307Epoch 5/15: [===                           ] 9/75 batches, loss: 0.5385Epoch 5/15: [====                          ] 10/75 batches, loss: 0.5337Epoch 5/15: [====                          ] 11/75 batches, loss: 0.5402Epoch 5/15: [====                          ] 12/75 batches, loss: 0.5412Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.5383Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.5314Epoch 5/15: [======                        ] 15/75 batches, loss: 0.5258Epoch 5/15: [======                        ] 16/75 batches, loss: 0.5255Epoch 5/15: [======                        ] 17/75 batches, loss: 0.5238Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.5274Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.5241Epoch 5/15: [========                      ] 20/75 batches, loss: 0.5236Epoch 5/15: [========                      ] 21/75 batches, loss: 0.5230Epoch 5/15: [========                      ] 22/75 batches, loss: 0.5237Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.5280Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.5241Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.5208Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.5223Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.5283Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.5299Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.5343Epoch 5/15: [============                  ] 30/75 batches, loss: 0.5360Epoch 5/15: [============                  ] 31/75 batches, loss: 0.5368Epoch 5/15: [============                  ] 32/75 batches, loss: 0.5382Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.5382Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.5374Epoch 5/15: [==============                ] 35/75 batches, loss: 0.5391Epoch 5/15: [==============                ] 36/75 batches, loss: 0.5415Epoch 5/15: [==============                ] 37/75 batches, loss: 0.5407Epoch 5/15: [===============               ] 38/75 batches, loss: 0.5416Epoch 5/15: [===============               ] 39/75 batches, loss: 0.5396Epoch 5/15: [================              ] 40/75 batches, loss: 0.5400Epoch 5/15: [================              ] 41/75 batches, loss: 0.5415Epoch 5/15: [================              ] 42/75 batches, loss: 0.5412Epoch 5/15: [=================             ] 43/75 batches, loss: 0.5388Epoch 5/15: [=================             ] 44/75 batches, loss: 0.5378Epoch 5/15: [==================            ] 45/75 batches, loss: 0.5388Epoch 5/15: [==================            ] 46/75 batches, loss: 0.5371Epoch 5/15: [==================            ] 47/75 batches, loss: 0.5354Epoch 5/15: [===================           ] 48/75 batches, loss: 0.5319Epoch 5/15: [===================           ] 49/75 batches, loss: 0.5336Epoch 5/15: [====================          ] 50/75 batches, loss: 0.5346Epoch 5/15: [====================          ] 51/75 batches, loss: 0.5343Epoch 5/15: [====================          ] 52/75 batches, loss: 0.5343Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.5370Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.5361Epoch 5/15: [======================        ] 55/75 batches, loss: 0.5351Epoch 5/15: [======================        ] 56/75 batches, loss: 0.5354Epoch 5/15: [======================        ] 57/75 batches, loss: 0.5358Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.5352Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.5356Epoch 5/15: [========================      ] 60/75 batches, loss: 0.5351Epoch 5/15: [========================      ] 61/75 batches, loss: 0.5340Epoch 5/15: [========================      ] 62/75 batches, loss: 0.5340Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.5358Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.5375Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.5373Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.5375Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.5379Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.5367Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.5376Epoch 5/15: [============================  ] 70/75 batches, loss: 0.5374Epoch 5/15: [============================  ] 71/75 batches, loss: 0.5370Epoch 5/15: [============================  ] 72/75 batches, loss: 0.5380Epoch 5/15: [============================= ] 73/75 batches, loss: 0.5376Epoch 5/15: [============================= ] 74/75 batches, loss: 0.5365Epoch 5/15: [==============================] 75/75 batches, loss: 0.5370
[2025-05-07 11:40:54,422][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5370
[2025-05-07 11:40:54,833][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5902, Metrics: {'accuracy': 0.8095238095238095, 'f1': 0.7692307692307693, 'precision': 0.9090909090909091, 'recall': 0.6666666666666666}
[2025-05-07 11:40:54,834][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.5889Epoch 6/15: [                              ] 2/75 batches, loss: 0.5856Epoch 6/15: [=                             ] 3/75 batches, loss: 0.5756Epoch 6/15: [=                             ] 4/75 batches, loss: 0.5801Epoch 6/15: [==                            ] 5/75 batches, loss: 0.5561Epoch 6/15: [==                            ] 6/75 batches, loss: 0.5596Epoch 6/15: [==                            ] 7/75 batches, loss: 0.5606Epoch 6/15: [===                           ] 8/75 batches, loss: 0.5482Epoch 6/15: [===                           ] 9/75 batches, loss: 0.5443Epoch 6/15: [====                          ] 10/75 batches, loss: 0.5309Epoch 6/15: [====                          ] 11/75 batches, loss: 0.5296Epoch 6/15: [====                          ] 12/75 batches, loss: 0.5317Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.5386Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.5395Epoch 6/15: [======                        ] 15/75 batches, loss: 0.5360Epoch 6/15: [======                        ] 16/75 batches, loss: 0.5327Epoch 6/15: [======                        ] 17/75 batches, loss: 0.5286Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.5292Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.5292Epoch 6/15: [========                      ] 20/75 batches, loss: 0.5307Epoch 6/15: [========                      ] 21/75 batches, loss: 0.5329Epoch 6/15: [========                      ] 22/75 batches, loss: 0.5303Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.5296Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.5277Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.5331Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.5352Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.5335Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.5351Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.5348Epoch 6/15: [============                  ] 30/75 batches, loss: 0.5359Epoch 6/15: [============                  ] 31/75 batches, loss: 0.5363Epoch 6/15: [============                  ] 32/75 batches, loss: 0.5351Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.5326Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.5329Epoch 6/15: [==============                ] 35/75 batches, loss: 0.5330Epoch 6/15: [==============                ] 36/75 batches, loss: 0.5326Epoch 6/15: [==============                ] 37/75 batches, loss: 0.5312Epoch 6/15: [===============               ] 38/75 batches, loss: 0.5340Epoch 6/15: [===============               ] 39/75 batches, loss: 0.5356Epoch 6/15: [================              ] 40/75 batches, loss: 0.5349Epoch 6/15: [================              ] 41/75 batches, loss: 0.5334Epoch 6/15: [================              ] 42/75 batches, loss: 0.5336Epoch 6/15: [=================             ] 43/75 batches, loss: 0.5338Epoch 6/15: [=================             ] 44/75 batches, loss: 0.5344Epoch 6/15: [==================            ] 45/75 batches, loss: 0.5333Epoch 6/15: [==================            ] 46/75 batches, loss: 0.5327Epoch 6/15: [==================            ] 47/75 batches, loss: 0.5322Epoch 6/15: [===================           ] 48/75 batches, loss: 0.5323Epoch 6/15: [===================           ] 49/75 batches, loss: 0.5340Epoch 6/15: [====================          ] 50/75 batches, loss: 0.5336Epoch 6/15: [====================          ] 51/75 batches, loss: 0.5343Epoch 6/15: [====================          ] 52/75 batches, loss: 0.5335Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.5332Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.5324Epoch 6/15: [======================        ] 55/75 batches, loss: 0.5323Epoch 6/15: [======================        ] 56/75 batches, loss: 0.5338Epoch 6/15: [======================        ] 57/75 batches, loss: 0.5337Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.5323Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.5322Epoch 6/15: [========================      ] 60/75 batches, loss: 0.5312Epoch 6/15: [========================      ] 61/75 batches, loss: 0.5311Epoch 6/15: [========================      ] 62/75 batches, loss: 0.5310Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.5311Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.5324Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.5333Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.5339Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.5332Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.5329Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.5319Epoch 6/15: [============================  ] 70/75 batches, loss: 0.5318Epoch 6/15: [============================  ] 71/75 batches, loss: 0.5313Epoch 6/15: [============================  ] 72/75 batches, loss: 0.5315Epoch 6/15: [============================= ] 73/75 batches, loss: 0.5303Epoch 6/15: [============================= ] 74/75 batches, loss: 0.5300Epoch 6/15: [==============================] 75/75 batches, loss: 0.5304
[2025-05-07 11:40:57,109][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5304
[2025-05-07 11:40:57,433][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5790, Metrics: {'accuracy': 0.873015873015873, 'f1': 0.8571428571428571, 'precision': 0.9230769230769231, 'recall': 0.8}
[2025-05-07 11:40:57,434][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.5569Epoch 7/15: [                              ] 2/75 batches, loss: 0.5335Epoch 7/15: [=                             ] 3/75 batches, loss: 0.5471Epoch 7/15: [=                             ] 4/75 batches, loss: 0.5303Epoch 7/15: [==                            ] 5/75 batches, loss: 0.5351Epoch 7/15: [==                            ] 6/75 batches, loss: 0.5353Epoch 7/15: [==                            ] 7/75 batches, loss: 0.5433Epoch 7/15: [===                           ] 8/75 batches, loss: 0.5431Epoch 7/15: [===                           ] 9/75 batches, loss: 0.5434Epoch 7/15: [====                          ] 10/75 batches, loss: 0.5452Epoch 7/15: [====                          ] 11/75 batches, loss: 0.5345Epoch 7/15: [====                          ] 12/75 batches, loss: 0.5326Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.5367Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.5284Epoch 7/15: [======                        ] 15/75 batches, loss: 0.5286Epoch 7/15: [======                        ] 16/75 batches, loss: 0.5286Epoch 7/15: [======                        ] 17/75 batches, loss: 0.5277Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.5281Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.5329Epoch 7/15: [========                      ] 20/75 batches, loss: 0.5331Epoch 7/15: [========                      ] 21/75 batches, loss: 0.5306Epoch 7/15: [========                      ] 22/75 batches, loss: 0.5322Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.5322Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.5335Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.5374Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.5372Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.5381Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.5366Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.5360Epoch 7/15: [============                  ] 30/75 batches, loss: 0.5346Epoch 7/15: [============                  ] 31/75 batches, loss: 0.5325Epoch 7/15: [============                  ] 32/75 batches, loss: 0.5316Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.5322Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.5328Epoch 7/15: [==============                ] 35/75 batches, loss: 0.5330Epoch 7/15: [==============                ] 36/75 batches, loss: 0.5316Epoch 7/15: [==============                ] 37/75 batches, loss: 0.5313Epoch 7/15: [===============               ] 38/75 batches, loss: 0.5291Epoch 7/15: [===============               ] 39/75 batches, loss: 0.5267Epoch 7/15: [================              ] 40/75 batches, loss: 0.5246Epoch 7/15: [================              ] 41/75 batches, loss: 0.5247Epoch 7/15: [================              ] 42/75 batches, loss: 0.5247Epoch 7/15: [=================             ] 43/75 batches, loss: 0.5237Epoch 7/15: [=================             ] 44/75 batches, loss: 0.5222Epoch 7/15: [==================            ] 45/75 batches, loss: 0.5224Epoch 7/15: [==================            ] 46/75 batches, loss: 0.5206Epoch 7/15: [==================            ] 47/75 batches, loss: 0.5228Epoch 7/15: [===================           ] 48/75 batches, loss: 0.5241Epoch 7/15: [===================           ] 49/75 batches, loss: 0.5251Epoch 7/15: [====================          ] 50/75 batches, loss: 0.5253Epoch 7/15: [====================          ] 51/75 batches, loss: 0.5251Epoch 7/15: [====================          ] 52/75 batches, loss: 0.5256Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.5258Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.5248Epoch 7/15: [======================        ] 55/75 batches, loss: 0.5267Epoch 7/15: [======================        ] 56/75 batches, loss: 0.5267Epoch 7/15: [======================        ] 57/75 batches, loss: 0.5272Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.5288Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.5282Epoch 7/15: [========================      ] 60/75 batches, loss: 0.5286Epoch 7/15: [========================      ] 61/75 batches, loss: 0.5283Epoch 7/15: [========================      ] 62/75 batches, loss: 0.5275Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.5271Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.5278Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.5283Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.5273Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.5271Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.5268Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.5267Epoch 7/15: [============================  ] 70/75 batches, loss: 0.5270Epoch 7/15: [============================  ] 71/75 batches, loss: 0.5284Epoch 7/15: [============================  ] 72/75 batches, loss: 0.5279Epoch 7/15: [============================= ] 73/75 batches, loss: 0.5270Epoch 7/15: [============================= ] 74/75 batches, loss: 0.5262Epoch 7/15: [==============================] 75/75 batches, loss: 0.5255
[2025-05-07 11:40:59,706][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5255
[2025-05-07 11:41:00,094][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5757, Metrics: {'accuracy': 0.873015873015873, 'f1': 0.8620689655172413, 'precision': 0.8928571428571429, 'recall': 0.8333333333333334}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.6256Epoch 8/15: [                              ] 2/75 batches, loss: 0.5594Epoch 8/15: [=                             ] 3/75 batches, loss: 0.5588Epoch 8/15: [=                             ] 4/75 batches, loss: 0.5497Epoch 8/15: [==                            ] 5/75 batches, loss: 0.5647Epoch 8/15: [==                            ] 6/75 batches, loss: 0.5556Epoch 8/15: [==                            ] 7/75 batches, loss: 0.5448Epoch 8/15: [===                           ] 8/75 batches, loss: 0.5405Epoch 8/15: [===                           ] 9/75 batches, loss: 0.5415Epoch 8/15: [====                          ] 10/75 batches, loss: 0.5378Epoch 8/15: [====                          ] 11/75 batches, loss: 0.5380Epoch 8/15: [====                          ] 12/75 batches, loss: 0.5334Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.5346Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.5328Epoch 8/15: [======                        ] 15/75 batches, loss: 0.5295Epoch 8/15: [======                        ] 16/75 batches, loss: 0.5289Epoch 8/15: [======                        ] 17/75 batches, loss: 0.5306Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.5283Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.5290Epoch 8/15: [========                      ] 20/75 batches, loss: 0.5273Epoch 8/15: [========                      ] 21/75 batches, loss: 0.5309Epoch 8/15: [========                      ] 22/75 batches, loss: 0.5281Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.5274Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.5267Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.5253Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.5228Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.5245Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.5242Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.5261Epoch 8/15: [============                  ] 30/75 batches, loss: 0.5288Epoch 8/15: [============                  ] 31/75 batches, loss: 0.5274Epoch 8/15: [============                  ] 32/75 batches, loss: 0.5280Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.5252Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.5220Epoch 8/15: [==============                ] 35/75 batches, loss: 0.5216Epoch 8/15: [==============                ] 36/75 batches, loss: 0.5209Epoch 8/15: [==============                ] 37/75 batches, loss: 0.5218Epoch 8/15: [===============               ] 38/75 batches, loss: 0.5218Epoch 8/15: [===============               ] 39/75 batches, loss: 0.5218Epoch 8/15: [================              ] 40/75 batches, loss: 0.5232Epoch 8/15: [================              ] 41/75 batches, loss: 0.5222Epoch 8/15: [================              ] 42/75 batches, loss: 0.5210Epoch 8/15: [=================             ] 43/75 batches, loss: 0.5198Epoch 8/15: [=================             ] 44/75 batches, loss: 0.5192Epoch 8/15: [==================            ] 45/75 batches, loss: 0.5182Epoch 8/15: [==================            ] 46/75 batches, loss: 0.5190Epoch 8/15: [==================            ] 47/75 batches, loss: 0.5187Epoch 8/15: [===================           ] 48/75 batches, loss: 0.5184Epoch 8/15: [===================           ] 49/75 batches, loss: 0.5173Epoch 8/15: [====================          ] 50/75 batches, loss: 0.5176Epoch 8/15: [====================          ] 51/75 batches, loss: 0.5179Epoch 8/15: [====================          ] 52/75 batches, loss: 0.5172Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.5170Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.5181Epoch 8/15: [======================        ] 55/75 batches, loss: 0.5180Epoch 8/15: [======================        ] 56/75 batches, loss: 0.5186Epoch 8/15: [======================        ] 57/75 batches, loss: 0.5171Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.5167Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.5169Epoch 8/15: [========================      ] 60/75 batches, loss: 0.5178Epoch 8/15: [========================      ] 61/75 batches, loss: 0.5184Epoch 8/15: [========================      ] 62/75 batches, loss: 0.5189Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.5191Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.5196Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.5194Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.5203Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.5205Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.5208Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.5204Epoch 8/15: [============================  ] 70/75 batches, loss: 0.5207Epoch 8/15: [============================  ] 71/75 batches, loss: 0.5211Epoch 8/15: [============================  ] 72/75 batches, loss: 0.5219Epoch 8/15: [============================= ] 73/75 batches, loss: 0.5220Epoch 8/15: [============================= ] 74/75 batches, loss: 0.5205Epoch 8/15: [==============================] 75/75 batches, loss: 0.5219
[2025-05-07 11:41:02,902][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5219
[2025-05-07 11:41:03,224][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5745, Metrics: {'accuracy': 0.873015873015873, 'f1': 0.8620689655172413, 'precision': 0.8928571428571429, 'recall': 0.8333333333333334}
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.5582Epoch 9/15: [                              ] 2/75 batches, loss: 0.5203Epoch 9/15: [=                             ] 3/75 batches, loss: 0.5555Epoch 9/15: [=                             ] 4/75 batches, loss: 0.5531Epoch 9/15: [==                            ] 5/75 batches, loss: 0.5531Epoch 9/15: [==                            ] 6/75 batches, loss: 0.5417Epoch 9/15: [==                            ] 7/75 batches, loss: 0.5420Epoch 9/15: [===                           ] 8/75 batches, loss: 0.5454Epoch 9/15: [===                           ] 9/75 batches, loss: 0.5395Epoch 9/15: [====                          ] 10/75 batches, loss: 0.5415Epoch 9/15: [====                          ] 11/75 batches, loss: 0.5403Epoch 9/15: [====                          ] 12/75 batches, loss: 0.5400Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.5408Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.5403Epoch 9/15: [======                        ] 15/75 batches, loss: 0.5402Epoch 9/15: [======                        ] 16/75 batches, loss: 0.5397Epoch 9/15: [======                        ] 17/75 batches, loss: 0.5404Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.5414Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.5373Epoch 9/15: [========                      ] 20/75 batches, loss: 0.5348Epoch 9/15: [========                      ] 21/75 batches, loss: 0.5359Epoch 9/15: [========                      ] 22/75 batches, loss: 0.5356Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.5322Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.5329Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.5326Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.5306Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.5307Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.5286Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.5288Epoch 9/15: [============                  ] 30/75 batches, loss: 0.5290Epoch 9/15: [============                  ] 31/75 batches, loss: 0.5294Epoch 9/15: [============                  ] 32/75 batches, loss: 0.5264Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.5269Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.5249Epoch 9/15: [==============                ] 35/75 batches, loss: 0.5280Epoch 9/15: [==============                ] 36/75 batches, loss: 0.5287Epoch 9/15: [==============                ] 37/75 batches, loss: 0.5286Epoch 9/15: [===============               ] 38/75 batches, loss: 0.5271Epoch 9/15: [===============               ] 39/75 batches, loss: 0.5273Epoch 9/15: [================              ] 40/75 batches, loss: 0.5262Epoch 9/15: [================              ] 41/75 batches, loss: 0.5246Epoch 9/15: [================              ] 42/75 batches, loss: 0.5261Epoch 9/15: [=================             ] 43/75 batches, loss: 0.5262Epoch 9/15: [=================             ] 44/75 batches, loss: 0.5275Epoch 9/15: [==================            ] 45/75 batches, loss: 0.5281Epoch 9/15: [==================            ] 46/75 batches, loss: 0.5286Epoch 9/15: [==================            ] 47/75 batches, loss: 0.5273Epoch 9/15: [===================           ] 48/75 batches, loss: 0.5274Epoch 9/15: [===================           ] 49/75 batches, loss: 0.5259Epoch 9/15: [====================          ] 50/75 batches, loss: 0.5280Epoch 9/15: [====================          ] 51/75 batches, loss: 0.5279Epoch 9/15: [====================          ] 52/75 batches, loss: 0.5267Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.5278Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.5288Epoch 9/15: [======================        ] 55/75 batches, loss: 0.5288Epoch 9/15: [======================        ] 56/75 batches, loss: 0.5290Epoch 9/15: [======================        ] 57/75 batches, loss: 0.5284Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.5284Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.5272Epoch 9/15: [========================      ] 60/75 batches, loss: 0.5264Epoch 9/15: [========================      ] 61/75 batches, loss: 0.5257Epoch 9/15: [========================      ] 62/75 batches, loss: 0.5245Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.5227Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.5232Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.5237Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.5232Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.5233Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.5236Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.5231Epoch 9/15: [============================  ] 70/75 batches, loss: 0.5223Epoch 9/15: [============================  ] 71/75 batches, loss: 0.5221Epoch 9/15: [============================  ] 72/75 batches, loss: 0.5225Epoch 9/15: [============================= ] 73/75 batches, loss: 0.5218Epoch 9/15: [============================= ] 74/75 batches, loss: 0.5220Epoch 9/15: [==============================] 75/75 batches, loss: 0.5221
[2025-05-07 11:41:06,011][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5221
[2025-05-07 11:41:06,449][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.5812, Metrics: {'accuracy': 0.8571428571428571, 'f1': 0.8363636363636363, 'precision': 0.92, 'recall': 0.7666666666666667}
[2025-05-07 11:41:06,450][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.5051Epoch 10/15: [                              ] 2/75 batches, loss: 0.5293Epoch 10/15: [=                             ] 3/75 batches, loss: 0.5212Epoch 10/15: [=                             ] 4/75 batches, loss: 0.5324Epoch 10/15: [==                            ] 5/75 batches, loss: 0.5212Epoch 10/15: [==                            ] 6/75 batches, loss: 0.5301Epoch 10/15: [==                            ] 7/75 batches, loss: 0.5352Epoch 10/15: [===                           ] 8/75 batches, loss: 0.5375Epoch 10/15: [===                           ] 9/75 batches, loss: 0.5306Epoch 10/15: [====                          ] 10/75 batches, loss: 0.5331Epoch 10/15: [====                          ] 11/75 batches, loss: 0.5296Epoch 10/15: [====                          ] 12/75 batches, loss: 0.5295Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.5370Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.5313Epoch 10/15: [======                        ] 15/75 batches, loss: 0.5295Epoch 10/15: [======                        ] 16/75 batches, loss: 0.5236Epoch 10/15: [======                        ] 17/75 batches, loss: 0.5185Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.5252Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.5243Epoch 10/15: [========                      ] 20/75 batches, loss: 0.5224Epoch 10/15: [========                      ] 21/75 batches, loss: 0.5185Epoch 10/15: [========                      ] 22/75 batches, loss: 0.5178Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.5152Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.5205Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.5214Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.5201Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.5204Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.5215Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.5227Epoch 10/15: [============                  ] 30/75 batches, loss: 0.5208Epoch 10/15: [============                  ] 31/75 batches, loss: 0.5206Epoch 10/15: [============                  ] 32/75 batches, loss: 0.5206Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.5212Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.5211Epoch 10/15: [==============                ] 35/75 batches, loss: 0.5218Epoch 10/15: [==============                ] 36/75 batches, loss: 0.5228Epoch 10/15: [==============                ] 37/75 batches, loss: 0.5230Epoch 10/15: [===============               ] 38/75 batches, loss: 0.5227Epoch 10/15: [===============               ] 39/75 batches, loss: 0.5223Epoch 10/15: [================              ] 40/75 batches, loss: 0.5223Epoch 10/15: [================              ] 41/75 batches, loss: 0.5209Epoch 10/15: [================              ] 42/75 batches, loss: 0.5205Epoch 10/15: [=================             ] 43/75 batches, loss: 0.5214Epoch 10/15: [=================             ] 44/75 batches, loss: 0.5209Epoch 10/15: [==================            ] 45/75 batches, loss: 0.5211Epoch 10/15: [==================            ] 46/75 batches, loss: 0.5230Epoch 10/15: [==================            ] 47/75 batches, loss: 0.5233Epoch 10/15: [===================           ] 48/75 batches, loss: 0.5247Epoch 10/15: [===================           ] 49/75 batches, loss: 0.5243Epoch 10/15: [====================          ] 50/75 batches, loss: 0.5244Epoch 10/15: [====================          ] 51/75 batches, loss: 0.5249Epoch 10/15: [====================          ] 52/75 batches, loss: 0.5241Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.5237Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.5225Epoch 10/15: [======================        ] 55/75 batches, loss: 0.5234Epoch 10/15: [======================        ] 56/75 batches, loss: 0.5235Epoch 10/15: [======================        ] 57/75 batches, loss: 0.5252Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.5243Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.5243Epoch 10/15: [========================      ] 60/75 batches, loss: 0.5252Epoch 10/15: [========================      ] 61/75 batches, loss: 0.5251Epoch 10/15: [========================      ] 62/75 batches, loss: 0.5251Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.5254Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.5244Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.5223Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.5221Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.5218Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.5229Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.5230Epoch 10/15: [============================  ] 70/75 batches, loss: 0.5215Epoch 10/15: [============================  ] 71/75 batches, loss: 0.5205Epoch 10/15: [============================  ] 72/75 batches, loss: 0.5195Epoch 10/15: [============================= ] 73/75 batches, loss: 0.5197Epoch 10/15: [============================= ] 74/75 batches, loss: 0.5205Epoch 10/15: [==============================] 75/75 batches, loss: 0.5214
[2025-05-07 11:41:08,721][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5214
[2025-05-07 11:41:09,046][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.5741, Metrics: {'accuracy': 0.8571428571428571, 'f1': 0.8524590163934426, 'precision': 0.8387096774193549, 'recall': 0.8666666666666667}
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.5528Epoch 11/15: [                              ] 2/75 batches, loss: 0.5758Epoch 11/15: [=                             ] 3/75 batches, loss: 0.5838Epoch 11/15: [=                             ] 4/75 batches, loss: 0.5655Epoch 11/15: [==                            ] 5/75 batches, loss: 0.5560Epoch 11/15: [==                            ] 6/75 batches, loss: 0.5433Epoch 11/15: [==                            ] 7/75 batches, loss: 0.5388Epoch 11/15: [===                           ] 8/75 batches, loss: 0.5468Epoch 11/15: [===                           ] 9/75 batches, loss: 0.5431Epoch 11/15: [====                          ] 10/75 batches, loss: 0.5475Epoch 11/15: [====                          ] 11/75 batches, loss: 0.5370Epoch 11/15: [====                          ] 12/75 batches, loss: 0.5354Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.5331Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.5335Epoch 11/15: [======                        ] 15/75 batches, loss: 0.5318Epoch 11/15: [======                        ] 16/75 batches, loss: 0.5351Epoch 11/15: [======                        ] 17/75 batches, loss: 0.5380Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.5362Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.5373Epoch 11/15: [========                      ] 20/75 batches, loss: 0.5373Epoch 11/15: [========                      ] 21/75 batches, loss: 0.5346Epoch 11/15: [========                      ] 22/75 batches, loss: 0.5329Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.5287Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.5278Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.5289Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.5302Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.5296Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.5308Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.5307Epoch 11/15: [============                  ] 30/75 batches, loss: 0.5318Epoch 11/15: [============                  ] 31/75 batches, loss: 0.5309Epoch 11/15: [============                  ] 32/75 batches, loss: 0.5310Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.5288Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.5269Epoch 11/15: [==============                ] 35/75 batches, loss: 0.5269Epoch 11/15: [==============                ] 36/75 batches, loss: 0.5274Epoch 11/15: [==============                ] 37/75 batches, loss: 0.5263Epoch 11/15: [===============               ] 38/75 batches, loss: 0.5254Epoch 11/15: [===============               ] 39/75 batches, loss: 0.5231Epoch 11/15: [================              ] 40/75 batches, loss: 0.5232Epoch 11/15: [================              ] 41/75 batches, loss: 0.5229Epoch 11/15: [================              ] 42/75 batches, loss: 0.5220Epoch 11/15: [=================             ] 43/75 batches, loss: 0.5221Epoch 11/15: [=================             ] 44/75 batches, loss: 0.5204Epoch 11/15: [==================            ] 45/75 batches, loss: 0.5212Epoch 11/15: [==================            ] 46/75 batches, loss: 0.5203Epoch 11/15: [==================            ] 47/75 batches, loss: 0.5196Epoch 11/15: [===================           ] 48/75 batches, loss: 0.5173Epoch 11/15: [===================           ] 49/75 batches, loss: 0.5171Epoch 11/15: [====================          ] 50/75 batches, loss: 0.5157Epoch 11/15: [====================          ] 51/75 batches, loss: 0.5161Epoch 11/15: [====================          ] 52/75 batches, loss: 0.5177Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.5173Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.5175Epoch 11/15: [======================        ] 55/75 batches, loss: 0.5181Epoch 11/15: [======================        ] 56/75 batches, loss: 0.5193Epoch 11/15: [======================        ] 57/75 batches, loss: 0.5192Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.5185Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.5180Epoch 11/15: [========================      ] 60/75 batches, loss: 0.5186Epoch 11/15: [========================      ] 61/75 batches, loss: 0.5169Epoch 11/15: [========================      ] 62/75 batches, loss: 0.5182Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.5187Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.5179Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.5174Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.5176Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.5185Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.5184Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.5182Epoch 11/15: [============================  ] 70/75 batches, loss: 0.5181Epoch 11/15: [============================  ] 71/75 batches, loss: 0.5178Epoch 11/15: [============================  ] 72/75 batches, loss: 0.5179Epoch 11/15: [============================= ] 73/75 batches, loss: 0.5186Epoch 11/15: [============================= ] 74/75 batches, loss: 0.5203Epoch 11/15: [==============================] 75/75 batches, loss: 0.5198
[2025-05-07 11:41:11,733][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5198
[2025-05-07 11:41:12,067][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.5774, Metrics: {'accuracy': 0.8571428571428571, 'f1': 0.8524590163934426, 'precision': 0.8387096774193549, 'recall': 0.8666666666666667}
[2025-05-07 11:41:12,068][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.4849Epoch 12/15: [                              ] 2/75 batches, loss: 0.5105Epoch 12/15: [=                             ] 3/75 batches, loss: 0.5013Epoch 12/15: [=                             ] 4/75 batches, loss: 0.5094Epoch 12/15: [==                            ] 5/75 batches, loss: 0.5183Epoch 12/15: [==                            ] 6/75 batches, loss: 0.5080Epoch 12/15: [==                            ] 7/75 batches, loss: 0.5006Epoch 12/15: [===                           ] 8/75 batches, loss: 0.5042Epoch 12/15: [===                           ] 9/75 batches, loss: 0.5134Epoch 12/15: [====                          ] 10/75 batches, loss: 0.5203Epoch 12/15: [====                          ] 11/75 batches, loss: 0.5187Epoch 12/15: [====                          ] 12/75 batches, loss: 0.5275Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.5267Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.5205Epoch 12/15: [======                        ] 15/75 batches, loss: 0.5169Epoch 12/15: [======                        ] 16/75 batches, loss: 0.5192Epoch 12/15: [======                        ] 17/75 batches, loss: 0.5199Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.5229Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.5226Epoch 12/15: [========                      ] 20/75 batches, loss: 0.5205Epoch 12/15: [========                      ] 21/75 batches, loss: 0.5197Epoch 12/15: [========                      ] 22/75 batches, loss: 0.5195Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.5199Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.5183Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.5181Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.5228Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.5251Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.5236Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.5264Epoch 12/15: [============                  ] 30/75 batches, loss: 0.5233Epoch 12/15: [============                  ] 31/75 batches, loss: 0.5234Epoch 12/15: [============                  ] 32/75 batches, loss: 0.5237Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.5214Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.5201Epoch 12/15: [==============                ] 35/75 batches, loss: 0.5184Epoch 12/15: [==============                ] 36/75 batches, loss: 0.5176Epoch 12/15: [==============                ] 37/75 batches, loss: 0.5204Epoch 12/15: [===============               ] 38/75 batches, loss: 0.5194Epoch 12/15: [===============               ] 39/75 batches, loss: 0.5189Epoch 12/15: [================              ] 40/75 batches, loss: 0.5180Epoch 12/15: [================              ] 41/75 batches, loss: 0.5198Epoch 12/15: [================              ] 42/75 batches, loss: 0.5211Epoch 12/15: [=================             ] 43/75 batches, loss: 0.5232Epoch 12/15: [=================             ] 44/75 batches, loss: 0.5237Epoch 12/15: [==================            ] 45/75 batches, loss: 0.5229Epoch 12/15: [==================            ] 46/75 batches, loss: 0.5234Epoch 12/15: [==================            ] 47/75 batches, loss: 0.5239Epoch 12/15: [===================           ] 48/75 batches, loss: 0.5236Epoch 12/15: [===================           ] 49/75 batches, loss: 0.5232Epoch 12/15: [====================          ] 50/75 batches, loss: 0.5240Epoch 12/15: [====================          ] 51/75 batches, loss: 0.5245Epoch 12/15: [====================          ] 52/75 batches, loss: 0.5255Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.5251Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.5254Epoch 12/15: [======================        ] 55/75 batches, loss: 0.5250Epoch 12/15: [======================        ] 56/75 batches, loss: 0.5254Epoch 12/15: [======================        ] 57/75 batches, loss: 0.5246Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.5242Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.5235Epoch 12/15: [========================      ] 60/75 batches, loss: 0.5226Epoch 12/15: [========================      ] 61/75 batches, loss: 0.5211Epoch 12/15: [========================      ] 62/75 batches, loss: 0.5214Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.5204Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.5209Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.5199Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.5183Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.5191Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.5200Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.5197Epoch 12/15: [============================  ] 70/75 batches, loss: 0.5196Epoch 12/15: [============================  ] 71/75 batches, loss: 0.5205Epoch 12/15: [============================  ] 72/75 batches, loss: 0.5195Epoch 12/15: [============================= ] 73/75 batches, loss: 0.5185Epoch 12/15: [============================= ] 74/75 batches, loss: 0.5185Epoch 12/15: [==============================] 75/75 batches, loss: 0.5178
[2025-05-07 11:41:14,468][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.5178
[2025-05-07 11:41:14,708][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.5776, Metrics: {'accuracy': 0.873015873015873, 'f1': 0.8666666666666667, 'precision': 0.8666666666666667, 'recall': 0.8666666666666667}
[2025-05-07 11:41:14,709][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.4810Epoch 13/15: [                              ] 2/75 batches, loss: 0.4949Epoch 13/15: [=                             ] 3/75 batches, loss: 0.4902Epoch 13/15: [=                             ] 4/75 batches, loss: 0.5062Epoch 13/15: [==                            ] 5/75 batches, loss: 0.5073Epoch 13/15: [==                            ] 6/75 batches, loss: 0.4950Epoch 13/15: [==                            ] 7/75 batches, loss: 0.5018Epoch 13/15: [===                           ] 8/75 batches, loss: 0.5135Epoch 13/15: [===                           ] 9/75 batches, loss: 0.5102Epoch 13/15: [====                          ] 10/75 batches, loss: 0.4953Epoch 13/15: [====                          ] 11/75 batches, loss: 0.4875Epoch 13/15: [====                          ] 12/75 batches, loss: 0.4908Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.4936Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.4980Epoch 13/15: [======                        ] 15/75 batches, loss: 0.4992Epoch 13/15: [======                        ] 16/75 batches, loss: 0.4990Epoch 13/15: [======                        ] 17/75 batches, loss: 0.5032Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.5013Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.4967Epoch 13/15: [========                      ] 20/75 batches, loss: 0.4983Epoch 13/15: [========                      ] 21/75 batches, loss: 0.4998Epoch 13/15: [========                      ] 22/75 batches, loss: 0.5011Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.5033Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.5044Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.5066Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.5056Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.5077Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.5077Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.5077Epoch 13/15: [============                  ] 30/75 batches, loss: 0.5089Epoch 13/15: [============                  ] 31/75 batches, loss: 0.5098Epoch 13/15: [============                  ] 32/75 batches, loss: 0.5113Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.5119Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.5123Epoch 13/15: [==============                ] 35/75 batches, loss: 0.5109Epoch 13/15: [==============                ] 36/75 batches, loss: 0.5108Epoch 13/15: [==============                ] 37/75 batches, loss: 0.5102Epoch 13/15: [===============               ] 38/75 batches, loss: 0.5109Epoch 13/15: [===============               ] 39/75 batches, loss: 0.5088Epoch 13/15: [================              ] 40/75 batches, loss: 0.5099Epoch 13/15: [================              ] 41/75 batches, loss: 0.5125Epoch 13/15: [================              ] 42/75 batches, loss: 0.5133Epoch 13/15: [=================             ] 43/75 batches, loss: 0.5154Epoch 13/15: [=================             ] 44/75 batches, loss: 0.5157Epoch 13/15: [==================            ] 45/75 batches, loss: 0.5167Epoch 13/15: [==================            ] 46/75 batches, loss: 0.5154Epoch 13/15: [==================            ] 47/75 batches, loss: 0.5157Epoch 13/15: [===================           ] 48/75 batches, loss: 0.5141Epoch 13/15: [===================           ] 49/75 batches, loss: 0.5134Epoch 13/15: [====================          ] 50/75 batches, loss: 0.5133Epoch 13/15: [====================          ] 51/75 batches, loss: 0.5109Epoch 13/15: [====================          ] 52/75 batches, loss: 0.5103Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.5104Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.5102Epoch 13/15: [======================        ] 55/75 batches, loss: 0.5098Epoch 13/15: [======================        ] 56/75 batches, loss: 0.5103Epoch 13/15: [======================        ] 57/75 batches, loss: 0.5092Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.5089Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.5093Epoch 13/15: [========================      ] 60/75 batches, loss: 0.5088Epoch 13/15: [========================      ] 61/75 batches, loss: 0.5087Epoch 13/15: [========================      ] 62/75 batches, loss: 0.5094Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.5098Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.5110Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.5124Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.5141Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.5139Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.5145Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.5149Epoch 13/15: [============================  ] 70/75 batches, loss: 0.5153Epoch 13/15: [============================  ] 71/75 batches, loss: 0.5159Epoch 13/15: [============================  ] 72/75 batches, loss: 0.5168Epoch 13/15: [============================= ] 73/75 batches, loss: 0.5173Epoch 13/15: [============================= ] 74/75 batches, loss: 0.5174Epoch 13/15: [==============================] 75/75 batches, loss: 0.5170
[2025-05-07 11:41:17,138][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.5170
[2025-05-07 11:41:17,500][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.5844, Metrics: {'accuracy': 0.8253968253968254, 'f1': 0.8, 'precision': 0.88, 'recall': 0.7333333333333333}
[2025-05-07 11:41:17,501][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-07 11:41:17,501][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 13
[2025-05-07 11:41:17,501][src.training.lm_trainer][INFO] - Training completed in 37.98 seconds
[2025-05-07 11:41:17,501][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 11:41:20,525][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9866108786610879, 'f1': 0.9865319865319865, 'precision': 0.9932203389830508, 'recall': 0.979933110367893}
[2025-05-07 11:41:20,526][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.8571428571428571, 'f1': 0.8524590163934426, 'precision': 0.8387096774193549, 'recall': 0.8666666666666667}
[2025-05-07 11:41:20,526][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9181818181818182, 'f1': 0.9217391304347826, 'precision': 0.8833333333333333, 'recall': 0.9636363636363636}
[2025-05-07 11:41:22,152][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer2/fi/fi/model.pt
[2025-05-07 11:41:22,154][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▇████
wandb:           best_val_f1 ▁█████
wandb:         best_val_loss █▃▁▁▁▁
wandb:    best_val_precision ▁▄███▂
wandb:       best_val_recall ▁▇████
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▂▃▂▃▃▃▃▃▃▃
wandb:            train_loss █▅▃▂▂▂▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▇▆█▆███████▇
wandb:                val_f1 ▁█▇█▇███████▇
wandb:              val_loss █▃▃▁▂▁▁▁▂▁▁▁▂
wandb:         val_precision ▁▃▇▆▇█▆▆█▁▁▄▅
wandb:            val_recall ▁▇▆█▆▇██▇███▇
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.85714
wandb:           best_val_f1 0.85246
wandb:         best_val_loss 0.57407
wandb:    best_val_precision 0.83871
wandb:       best_val_recall 0.86667
wandb:      early_stop_epoch 13
wandb:                 epoch 13
wandb:   final_test_accuracy 0.91818
wandb:         final_test_f1 0.92174
wandb:  final_test_precision 0.88333
wandb:     final_test_recall 0.96364
wandb:  final_train_accuracy 0.98661
wandb:        final_train_f1 0.98653
wandb: final_train_precision 0.99322
wandb:    final_train_recall 0.97993
wandb:    final_val_accuracy 0.85714
wandb:          final_val_f1 0.85246
wandb:   final_val_precision 0.83871
wandb:      final_val_recall 0.86667
wandb:         learning_rate 0.0001
wandb:            train_loss 0.51701
wandb:            train_time 37.97716
wandb:          val_accuracy 0.8254
wandb:                val_f1 0.8
wandb:              val_loss 0.58438
wandb:         val_precision 0.88
wandb:            val_recall 0.73333
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_114018-mw5a49a5
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_114018-mw5a49a5/logs
Experiment probe_layer2_question_type_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_complexity_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/fi"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 11:41:51,424][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/fi
experiment_name: probe_layer2_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 11:41:51,424][__main__][INFO] - Normalized task: complexity
[2025-05-07 11:41:51,424][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 11:41:51,424][__main__][INFO] - Determined Task Type: regression
[2025-05-07 11:41:51,429][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-05-07 11:41:51,429][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 11:41:54,745][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 11:41:57,239][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 11:41:57,239][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:41:57,456][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:41:57,509][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:41:57,752][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 11:41:57,761][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:41:57,761][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 11:41:57,762][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:41:57,879][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:41:57,987][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:41:58,003][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 11:41:58,004][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:41:58,004][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 11:41:58,016][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:41:58,058][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:41:58,166][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:41:58,180][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 11:41:58,181][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:41:58,181][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 11:41:58,182][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 11:41:58,183][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 11:41:58,183][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 11:41:58,183][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 11:41:58,183][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 11:41:58,183][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 11:41:58,183][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-05-07 11:41:58,184][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 11:41:58,184][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-05-07 11:41:58,184][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 11:41:58,184][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 11:41:58,184][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 11:41:58,184][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 11:41:58,184][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 11:41:58,184][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-05-07 11:41:58,184][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 11:41:58,184][src.data.datasets][INFO] - Sample label: 1.0
[2025-05-07 11:41:58,185][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 11:41:58,185][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 11:41:58,185][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 11:41:58,185][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 11:41:58,185][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 11:41:58,185][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-05-07 11:41:58,185][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 11:41:58,185][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-05-07 11:41:58,185][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 11:41:58,185][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 11:41:58,186][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 11:41:58,186][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 11:41:58,186][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 11:42:05,508][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 11:42:05,509][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 11:42:05,509][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 11:42:05,509][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 11:42:05,512][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 11:42:05,513][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 11:42:05,513][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 11:42:05,513][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 11:42:05,513][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 11:42:05,514][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 11:42:05,514][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4582Epoch 1/15: [                              ] 2/75 batches, loss: 0.5299Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4481Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4208Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4128Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3831Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3705Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4161Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4214Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4072Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3982Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3905Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3765Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3901Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3885Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4056Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3963Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3930Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3927Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3905Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4002Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4041Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3986Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3887Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3829Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3763Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3690Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3640Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3625Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3634Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3582Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3535Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3501Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3491Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3451Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3466Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3422Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3412Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3406Epoch 1/15: [================              ] 40/75 batches, loss: 0.3361Epoch 1/15: [================              ] 41/75 batches, loss: 0.3320Epoch 1/15: [================              ] 42/75 batches, loss: 0.3276Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3252Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3233Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3258Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3214Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3212Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3164Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3140Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3129Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3131Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3117Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3087Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3090Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3076Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3060Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3091Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3090Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3086Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3062Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3025Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3014Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2993Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2975Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2954Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2948Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2920Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2887Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2891Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2887Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2871Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2855Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2828Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2801Epoch 1/15: [==============================] 75/75 batches, loss: 0.2784
[2025-05-07 11:42:12,476][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2784
[2025-05-07 11:42:12,703][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1096, Metrics: {'mse': 0.10950435698032379, 'rmse': 0.33091442546423355, 'r2': -0.6702783107757568}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1607Epoch 2/15: [                              ] 2/75 batches, loss: 0.1344Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1117Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1367Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1353Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1202Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1320Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1298Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1306Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1318Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1292Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1259Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1226Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1194Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1223Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1200Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1190Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1172Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1158Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1156Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1166Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1172Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1172Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1162Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1165Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1222Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1232Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1234Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1261Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1289Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1288Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1277Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1271Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1288Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1293Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1283Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1278Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1264Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1266Epoch 2/15: [================              ] 40/75 batches, loss: 0.1268Epoch 2/15: [================              ] 41/75 batches, loss: 0.1267Epoch 2/15: [================              ] 42/75 batches, loss: 0.1253Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1243Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1229Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1229Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1252Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1247Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1245Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1269Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1258Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1257Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1256Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1259Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1255Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1249Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1268Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1258Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1252Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1242Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1230Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1224Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1222Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1213Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1206Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1198Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1197Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1191Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1196Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1194Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1186Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1181Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1186Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1186Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1183Epoch 2/15: [==============================] 75/75 batches, loss: 0.1182
[2025-05-07 11:42:15,357][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1182
[2025-05-07 11:42:15,589][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1243, Metrics: {'mse': 0.12434173375368118, 'rmse': 0.3526212327039896, 'r2': -0.8965939283370972}
[2025-05-07 11:42:15,590][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1388Epoch 3/15: [                              ] 2/75 batches, loss: 0.1118Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1017Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0901Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0843Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0870Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0866Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0834Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0794Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0799Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0770Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0750Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0803Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0820Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0807Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0807Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0809Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0812Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0815Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0827Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0830Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0825Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0805Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0862Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0870Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0864Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0853Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0853Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0859Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0846Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0849Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0859Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0854Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0860Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0848Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0834Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0827Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0829Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0821Epoch 3/15: [================              ] 40/75 batches, loss: 0.0836Epoch 3/15: [================              ] 41/75 batches, loss: 0.0838Epoch 3/15: [================              ] 42/75 batches, loss: 0.0828Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0824Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0825Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0830Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0826Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0835Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0852Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0863Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0854Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0853Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0853Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0846Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0855Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0847Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0841Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0831Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0829Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0823Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0823Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0817Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0822Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0834Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0837Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0832Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0827Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0828Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0823Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0819Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0819Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0817Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0812Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0807Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0806Epoch 3/15: [==============================] 75/75 batches, loss: 0.0813
[2025-05-07 11:42:17,846][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0813
[2025-05-07 11:42:18,064][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0960, Metrics: {'mse': 0.09604870527982712, 'rmse': 0.30991725553738875, 'r2': -0.4650381803512573}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0377Epoch 4/15: [                              ] 2/75 batches, loss: 0.0527Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0473Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0663Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0601Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0641Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0616Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0588Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0607Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0592Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0624Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0627Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0593Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0580Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0605Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0621Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0638Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0643Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0636Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0629Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0631Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0652Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0644Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0644Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0660Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0663Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0660Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0655Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0655Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0649Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0655Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0659Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0653Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0666Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0661Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0658Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0654Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0658Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0650Epoch 4/15: [================              ] 40/75 batches, loss: 0.0645Epoch 4/15: [================              ] 41/75 batches, loss: 0.0645Epoch 4/15: [================              ] 42/75 batches, loss: 0.0638Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0642Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0636Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0634Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0626Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0622Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0623Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0628Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0625Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0628Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0633Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0634Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0642Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0637Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0643Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0639Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0639Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0638Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0637Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0640Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0634Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0635Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0635Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0634Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0633Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0628Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0631Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0631Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0627Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0622Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0622Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0624Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0628Epoch 4/15: [==============================] 75/75 batches, loss: 0.0629
[2025-05-07 11:42:20,811][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0629
[2025-05-07 11:42:21,052][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0817, Metrics: {'mse': 0.08170219510793686, 'rmse': 0.2858359583886129, 'r2': -0.24620985984802246}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1168Epoch 5/15: [                              ] 2/75 batches, loss: 0.0799Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0827Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0812Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0716Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0732Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0763Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0731Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0693Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0647Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0643Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0632Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0669Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0654Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0669Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0658Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0640Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0640Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0637Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0618Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0643Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0633Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0635Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0619Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0646Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0646Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0638Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0631Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0630Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0619Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0611Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0617Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0616Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0618Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0625Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0621Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0618Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0619Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0612Epoch 5/15: [================              ] 40/75 batches, loss: 0.0618Epoch 5/15: [================              ] 41/75 batches, loss: 0.0617Epoch 5/15: [================              ] 42/75 batches, loss: 0.0611Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0612Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0608Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0606Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0606Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0602Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0601Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0610Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0610Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0603Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0598Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0595Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0596Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0592Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0590Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0588Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0588Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0584Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0583Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0581Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0580Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0579Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0582Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0581Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0581Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0578Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0575Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0575Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0573Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0573Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0573Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0571Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0571Epoch 5/15: [==============================] 75/75 batches, loss: 0.0572
[2025-05-07 11:42:23,703][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0572
[2025-05-07 11:42:23,938][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0845, Metrics: {'mse': 0.08457466959953308, 'rmse': 0.2908172443297218, 'r2': -0.29002392292022705}
[2025-05-07 11:42:23,939][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0254Epoch 6/15: [                              ] 2/75 batches, loss: 0.0447Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0406Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0361Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0400Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0399Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0410Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0445Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0429Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0461Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0469Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0463Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0473Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0477Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0489Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0491Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0477Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0510Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0493Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0494Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0488Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0476Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0484Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0492Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0494Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0498Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0506Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0496Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0491Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0488Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0489Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0483Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0476Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0473Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0473Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0467Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0463Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0453Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0478Epoch 6/15: [================              ] 40/75 batches, loss: 0.0478Epoch 6/15: [================              ] 41/75 batches, loss: 0.0480Epoch 6/15: [================              ] 42/75 batches, loss: 0.0481Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0478Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0489Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0496Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0495Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0497Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0495Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0493Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0490Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0493Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0496Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0497Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0494Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0495Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0493Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0493Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0500Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0495Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0496Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0498Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0496Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0500Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0503Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0504Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0499Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0497Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0502Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0502Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0500Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0503Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0499Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0495Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0495Epoch 6/15: [==============================] 75/75 batches, loss: 0.0491
[2025-05-07 11:42:26,233][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0491
[2025-05-07 11:42:26,497][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0870, Metrics: {'mse': 0.08704845607280731, 'rmse': 0.2950397533770785, 'r2': -0.32775676250457764}
[2025-05-07 11:42:26,497][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0529Epoch 7/15: [                              ] 2/75 batches, loss: 0.0427Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0423Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0494Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0464Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0452Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0447Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0455Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0458Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0470Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0469Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0468Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0454Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0468Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0456Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0453Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0462Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0450Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0433Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0427Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0430Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0425Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0428Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0428Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0424Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0432Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0429Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0430Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0426Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0415Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0418Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0417Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0413Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0418Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0411Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0404Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0407Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0408Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0404Epoch 7/15: [================              ] 40/75 batches, loss: 0.0402Epoch 7/15: [================              ] 41/75 batches, loss: 0.0407Epoch 7/15: [================              ] 42/75 batches, loss: 0.0410Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0417Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0417Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0414Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0412Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0414Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0412Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0413Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0409Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0409Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0412Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0411Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0412Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0416Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0413Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0414Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0416Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0414Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0415Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0419Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0417Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0418Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0419Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0419Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0416Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0413Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0411Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0411Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0412Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0410Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0411Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0409Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0406Epoch 7/15: [==============================] 75/75 batches, loss: 0.0407
[2025-05-07 11:42:28,764][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0407
[2025-05-07 11:42:28,993][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0862, Metrics: {'mse': 0.08626780658960342, 'rmse': 0.29371381749860426, 'r2': -0.3158494234085083}
[2025-05-07 11:42:28,993][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0387Epoch 8/15: [                              ] 2/75 batches, loss: 0.0347Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0328Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0337Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0339Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0372Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0346Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0346Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0361Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0382Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0384Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0382Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0389Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0391Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0388Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0384Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0375Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0373Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0379Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0377Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0368Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0378Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0371Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0367Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0368Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0366Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0365Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0368Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0368Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0371Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0373Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0377Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0378Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0381Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0379Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0377Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0371Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0369Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0369Epoch 8/15: [================              ] 40/75 batches, loss: 0.0378Epoch 8/15: [================              ] 41/75 batches, loss: 0.0377Epoch 8/15: [================              ] 42/75 batches, loss: 0.0378Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0382Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0379Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0378Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0376Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0379Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0377Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0376Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0373Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0374Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0380Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0378Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0376Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0377Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0373Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0373Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0369Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0371Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0373Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0371Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0372Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0373Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0375Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0374Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0371Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0372Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0370Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0368Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0366Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0368Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0367Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0368Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0368Epoch 8/15: [==============================] 75/75 batches, loss: 0.0378
[2025-05-07 11:42:31,309][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0378
[2025-05-07 11:42:31,544][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0848, Metrics: {'mse': 0.08484388142824173, 'rmse': 0.2912797305482167, 'r2': -0.2941300868988037}
[2025-05-07 11:42:31,544][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 11:42:31,544][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-07 11:42:31,545][src.training.lm_trainer][INFO] - Training completed in 22.37 seconds
[2025-05-07 11:42:31,545][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 11:42:34,605][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.01580660603940487, 'rmse': 0.12572432556750848, 'r2': 0.2178063988685608}
[2025-05-07 11:42:34,606][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.08170219510793686, 'rmse': 0.2858359583886129, 'r2': -0.24620985984802246}
[2025-05-07 11:42:34,606][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03404341638088226, 'rmse': 0.18450858077846208, 'r2': 0.13785773515701294}
[2025-05-07 11:42:36,328][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/fi/fi/model.pt
[2025-05-07 11:42:36,330][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▁
wandb:     best_val_mse █▅▁
wandb:      best_val_r2 ▁▄█
wandb:    best_val_rmse █▅▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▁▄▅▅▅▅
wandb:       train_loss █▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆█▃▁▁▂▂▂
wandb:          val_mse ▆█▃▁▁▂▂▂
wandb:           val_r2 ▃▁▆██▇▇▇
wandb:         val_rmse ▆█▄▁▂▂▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.08169
wandb:     best_val_mse 0.0817
wandb:      best_val_r2 -0.24621
wandb:    best_val_rmse 0.28584
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.03404
wandb:    final_test_r2 0.13786
wandb:  final_test_rmse 0.18451
wandb:  final_train_mse 0.01581
wandb:   final_train_r2 0.21781
wandb: final_train_rmse 0.12572
wandb:    final_val_mse 0.0817
wandb:     final_val_r2 -0.24621
wandb:   final_val_rmse 0.28584
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03776
wandb:       train_time 22.36984
wandb:         val_loss 0.08477
wandb:          val_mse 0.08484
wandb:           val_r2 -0.29413
wandb:         val_rmse 0.29128
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_114151-fveeniej
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_114151-fveeniej/logs
Experiment probe_layer2_complexity_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_question_type_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_question_type_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer2/ja"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 11:43:13,910][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer2/ja
experiment_name: probe_layer2_question_type_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-07 11:43:13,910][__main__][INFO] - Normalized task: question_type
[2025-05-07 11:43:13,910][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-07 11:43:13,910][__main__][INFO] - Determined Task Type: classification
[2025-05-07 11:43:13,914][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ja']
[2025-05-07 11:43:13,914][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 11:43:17,304][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 11:43:19,747][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 11:43:19,747][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:43:20,036][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:43:20,105][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:43:20,372][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 11:43:20,381][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:43:20,381][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 11:43:20,382][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:43:20,465][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:43:20,618][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:43:20,654][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 11:43:20,655][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:43:20,655][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 11:43:20,656][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:43:20,735][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:43:20,851][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:43:20,897][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 11:43:20,899][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:43:20,899][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 11:43:20,900][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 11:43:20,900][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:43:20,900][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:43:20,901][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:43:20,901][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:43:20,901][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-05-07 11:43:20,901][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-05-07 11:43:20,901][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 11:43:20,901][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 11:43:20,901][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:43:20,901][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:43:20,901][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:43:20,901][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:43:20,902][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-05-07 11:43:20,902][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-05-07 11:43:20,902][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 11:43:20,902][src.data.datasets][INFO] - Sample label: 0
[2025-05-07 11:43:20,902][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:43:20,902][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:43:20,902][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:43:20,902][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:43:20,902][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-05-07 11:43:20,902][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-05-07 11:43:20,903][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 11:43:20,903][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 11:43:20,903][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 11:43:20,903][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 11:43:20,903][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 11:43:20,903][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-07 11:43:20,903][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 11:43:28,830][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 11:43:28,831][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 11:43:28,831][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 11:43:28,831][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-07 11:43:28,837][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-07 11:43:28,837][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-07 11:43:28,837][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-07 11:43:28,837][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-07 11:43:28,837][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 11:43:28,838][__main__][INFO] - Total parameters: 394,568,839
[2025-05-07 11:43:28,838][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-07 11:43:28,839][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7109Epoch 1/15: [                              ] 2/75 batches, loss: 0.7290Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7260Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7104Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7026Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7039Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7032Epoch 1/15: [===                           ] 8/75 batches, loss: 0.7013Epoch 1/15: [===                           ] 9/75 batches, loss: 0.7009Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6991Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6981Epoch 1/15: [====                          ] 12/75 batches, loss: 0.6968Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.6963Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.6961Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6954Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6944Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6950Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6945Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6942Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6934Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6926Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6915Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6918Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6906Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6908Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6906Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6894Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6909Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6905Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6900Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6900Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6889Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6878Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6881Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6874Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6857Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6839Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6823Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6825Epoch 1/15: [================              ] 40/75 batches, loss: 0.6826Epoch 1/15: [================              ] 41/75 batches, loss: 0.6815Epoch 1/15: [================              ] 42/75 batches, loss: 0.6817Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6808Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6806Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6798Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6799Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6784Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6766Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6758Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6756Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6751Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6746Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6738Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6728Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6716Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6697Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6705Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6704Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6698Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6692Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6667Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6656Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6657Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6649Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6622Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6623Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6623Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6622Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6610Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6603Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6588Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6585Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6582Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6566Epoch 1/15: [==============================] 75/75 batches, loss: 0.6571
[2025-05-07 11:43:35,272][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6571
[2025-05-07 11:43:35,459][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.5954, Metrics: {'accuracy': 0.782608695652174, 'f1': 0.75, 'precision': 0.9375, 'recall': 0.625}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.5370Epoch 2/15: [                              ] 2/75 batches, loss: 0.5573Epoch 2/15: [=                             ] 3/75 batches, loss: 0.5694Epoch 2/15: [=                             ] 4/75 batches, loss: 0.5607Epoch 2/15: [==                            ] 5/75 batches, loss: 0.5604Epoch 2/15: [==                            ] 6/75 batches, loss: 0.5749Epoch 2/15: [==                            ] 7/75 batches, loss: 0.5774Epoch 2/15: [===                           ] 8/75 batches, loss: 0.5788Epoch 2/15: [===                           ] 9/75 batches, loss: 0.5646Epoch 2/15: [====                          ] 10/75 batches, loss: 0.5656Epoch 2/15: [====                          ] 11/75 batches, loss: 0.5660Epoch 2/15: [====                          ] 12/75 batches, loss: 0.5686Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.5664Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.5709Epoch 2/15: [======                        ] 15/75 batches, loss: 0.5718Epoch 2/15: [======                        ] 16/75 batches, loss: 0.5667Epoch 2/15: [======                        ] 17/75 batches, loss: 0.5663Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.5676Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.5640Epoch 2/15: [========                      ] 20/75 batches, loss: 0.5665Epoch 2/15: [========                      ] 21/75 batches, loss: 0.5669Epoch 2/15: [========                      ] 22/75 batches, loss: 0.5655Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.5657Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.5676Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.5660Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.5661Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.5644Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.5673Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.5638Epoch 2/15: [============                  ] 30/75 batches, loss: 0.5616Epoch 2/15: [============                  ] 31/75 batches, loss: 0.5609Epoch 2/15: [============                  ] 32/75 batches, loss: 0.5629Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.5631Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.5613Epoch 2/15: [==============                ] 35/75 batches, loss: 0.5622Epoch 2/15: [==============                ] 36/75 batches, loss: 0.5617Epoch 2/15: [==============                ] 37/75 batches, loss: 0.5627Epoch 2/15: [===============               ] 38/75 batches, loss: 0.5622Epoch 2/15: [===============               ] 39/75 batches, loss: 0.5614Epoch 2/15: [================              ] 40/75 batches, loss: 0.5617Epoch 2/15: [================              ] 41/75 batches, loss: 0.5621Epoch 2/15: [================              ] 42/75 batches, loss: 0.5643Epoch 2/15: [=================             ] 43/75 batches, loss: 0.5628Epoch 2/15: [=================             ] 44/75 batches, loss: 0.5627Epoch 2/15: [==================            ] 45/75 batches, loss: 0.5640Epoch 2/15: [==================            ] 46/75 batches, loss: 0.5617Epoch 2/15: [==================            ] 47/75 batches, loss: 0.5619Epoch 2/15: [===================           ] 48/75 batches, loss: 0.5622Epoch 2/15: [===================           ] 49/75 batches, loss: 0.5629Epoch 2/15: [====================          ] 50/75 batches, loss: 0.5627Epoch 2/15: [====================          ] 51/75 batches, loss: 0.5616Epoch 2/15: [====================          ] 52/75 batches, loss: 0.5615Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.5618Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.5628Epoch 2/15: [======================        ] 55/75 batches, loss: 0.5623Epoch 2/15: [======================        ] 56/75 batches, loss: 0.5620Epoch 2/15: [======================        ] 57/75 batches, loss: 0.5608Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.5612Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.5608Epoch 2/15: [========================      ] 60/75 batches, loss: 0.5612Epoch 2/15: [========================      ] 61/75 batches, loss: 0.5621Epoch 2/15: [========================      ] 62/75 batches, loss: 0.5629Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.5632Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.5627Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.5620Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.5615Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.5608Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.5611Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.5606Epoch 2/15: [============================  ] 70/75 batches, loss: 0.5607Epoch 2/15: [============================  ] 71/75 batches, loss: 0.5602Epoch 2/15: [============================  ] 72/75 batches, loss: 0.5599Epoch 2/15: [============================= ] 73/75 batches, loss: 0.5605Epoch 2/15: [============================= ] 74/75 batches, loss: 0.5592Epoch 2/15: [==============================] 75/75 batches, loss: 0.5587
[2025-05-07 11:43:38,186][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.5587
[2025-05-07 11:43:38,410][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.5540, Metrics: {'accuracy': 0.8913043478260869, 'f1': 0.9019607843137255, 'precision': 0.8518518518518519, 'recall': 0.9583333333333334}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.5489Epoch 3/15: [                              ] 2/75 batches, loss: 0.5384Epoch 3/15: [=                             ] 3/75 batches, loss: 0.5235Epoch 3/15: [=                             ] 4/75 batches, loss: 0.5320Epoch 3/15: [==                            ] 5/75 batches, loss: 0.5247Epoch 3/15: [==                            ] 6/75 batches, loss: 0.5218Epoch 3/15: [==                            ] 7/75 batches, loss: 0.5211Epoch 3/15: [===                           ] 8/75 batches, loss: 0.5160Epoch 3/15: [===                           ] 9/75 batches, loss: 0.5163Epoch 3/15: [====                          ] 10/75 batches, loss: 0.5094Epoch 3/15: [====                          ] 11/75 batches, loss: 0.5108Epoch 3/15: [====                          ] 12/75 batches, loss: 0.5122Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.5095Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.5079Epoch 3/15: [======                        ] 15/75 batches, loss: 0.5072Epoch 3/15: [======                        ] 16/75 batches, loss: 0.5058Epoch 3/15: [======                        ] 17/75 batches, loss: 0.5074Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.5062Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.5058Epoch 3/15: [========                      ] 20/75 batches, loss: 0.5131Epoch 3/15: [========                      ] 21/75 batches, loss: 0.5107Epoch 3/15: [========                      ] 22/75 batches, loss: 0.5097Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.5102Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.5091Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.5134Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.5145Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.5149Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.5155Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.5149Epoch 3/15: [============                  ] 30/75 batches, loss: 0.5179Epoch 3/15: [============                  ] 31/75 batches, loss: 0.5172Epoch 3/15: [============                  ] 32/75 batches, loss: 0.5170Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.5152Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.5173Epoch 3/15: [==============                ] 35/75 batches, loss: 0.5175Epoch 3/15: [==============                ] 36/75 batches, loss: 0.5195Epoch 3/15: [==============                ] 37/75 batches, loss: 0.5205Epoch 3/15: [===============               ] 38/75 batches, loss: 0.5208Epoch 3/15: [===============               ] 39/75 batches, loss: 0.5219Epoch 3/15: [================              ] 40/75 batches, loss: 0.5223Epoch 3/15: [================              ] 41/75 batches, loss: 0.5238Epoch 3/15: [================              ] 42/75 batches, loss: 0.5252Epoch 3/15: [=================             ] 43/75 batches, loss: 0.5263Epoch 3/15: [=================             ] 44/75 batches, loss: 0.5271Epoch 3/15: [==================            ] 45/75 batches, loss: 0.5283Epoch 3/15: [==================            ] 46/75 batches, loss: 0.5301Epoch 3/15: [==================            ] 47/75 batches, loss: 0.5289Epoch 3/15: [===================           ] 48/75 batches, loss: 0.5304Epoch 3/15: [===================           ] 49/75 batches, loss: 0.5322Epoch 3/15: [====================          ] 50/75 batches, loss: 0.5322Epoch 3/15: [====================          ] 51/75 batches, loss: 0.5318Epoch 3/15: [====================          ] 52/75 batches, loss: 0.5320Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.5307Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.5311Epoch 3/15: [======================        ] 55/75 batches, loss: 0.5317Epoch 3/15: [======================        ] 56/75 batches, loss: 0.5309Epoch 3/15: [======================        ] 57/75 batches, loss: 0.5298Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.5296Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.5307Epoch 3/15: [========================      ] 60/75 batches, loss: 0.5303Epoch 3/15: [========================      ] 61/75 batches, loss: 0.5309Epoch 3/15: [========================      ] 62/75 batches, loss: 0.5304Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.5306Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.5304Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.5311Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.5320Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.5321Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.5332Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.5322Epoch 3/15: [============================  ] 70/75 batches, loss: 0.5318Epoch 3/15: [============================  ] 71/75 batches, loss: 0.5315Epoch 3/15: [============================  ] 72/75 batches, loss: 0.5317Epoch 3/15: [============================= ] 73/75 batches, loss: 0.5314Epoch 3/15: [============================= ] 74/75 batches, loss: 0.5319Epoch 3/15: [==============================] 75/75 batches, loss: 0.5341
[2025-05-07 11:43:41,084][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.5341
[2025-05-07 11:43:41,333][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.5337, Metrics: {'accuracy': 0.9347826086956522, 'f1': 0.9361702127659575, 'precision': 0.9565217391304348, 'recall': 0.9166666666666666}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6137Epoch 4/15: [                              ] 2/75 batches, loss: 0.5871Epoch 4/15: [=                             ] 3/75 batches, loss: 0.5793Epoch 4/15: [=                             ] 4/75 batches, loss: 0.5566Epoch 4/15: [==                            ] 5/75 batches, loss: 0.5613Epoch 4/15: [==                            ] 6/75 batches, loss: 0.5610Epoch 4/15: [==                            ] 7/75 batches, loss: 0.5566Epoch 4/15: [===                           ] 8/75 batches, loss: 0.5530Epoch 4/15: [===                           ] 9/75 batches, loss: 0.5497Epoch 4/15: [====                          ] 10/75 batches, loss: 0.5459Epoch 4/15: [====                          ] 11/75 batches, loss: 0.5359Epoch 4/15: [====                          ] 12/75 batches, loss: 0.5308Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.5241Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.5178Epoch 4/15: [======                        ] 15/75 batches, loss: 0.5219Epoch 4/15: [======                        ] 16/75 batches, loss: 0.5247Epoch 4/15: [======                        ] 17/75 batches, loss: 0.5246Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.5257Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.5262Epoch 4/15: [========                      ] 20/75 batches, loss: 0.5275Epoch 4/15: [========                      ] 21/75 batches, loss: 0.5299Epoch 4/15: [========                      ] 22/75 batches, loss: 0.5325Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.5310Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.5298Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.5268Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.5268Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.5272Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.5273Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.5302Epoch 4/15: [============                  ] 30/75 batches, loss: 0.5312Epoch 4/15: [============                  ] 31/75 batches, loss: 0.5297Epoch 4/15: [============                  ] 32/75 batches, loss: 0.5290Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.5256Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.5252Epoch 4/15: [==============                ] 35/75 batches, loss: 0.5240Epoch 4/15: [==============                ] 36/75 batches, loss: 0.5256Epoch 4/15: [==============                ] 37/75 batches, loss: 0.5270Epoch 4/15: [===============               ] 38/75 batches, loss: 0.5297Epoch 4/15: [===============               ] 39/75 batches, loss: 0.5312Epoch 4/15: [================              ] 40/75 batches, loss: 0.5305Epoch 4/15: [================              ] 41/75 batches, loss: 0.5310Epoch 4/15: [================              ] 42/75 batches, loss: 0.5295Epoch 4/15: [=================             ] 43/75 batches, loss: 0.5289Epoch 4/15: [=================             ] 44/75 batches, loss: 0.5259Epoch 4/15: [==================            ] 45/75 batches, loss: 0.5235Epoch 4/15: [==================            ] 46/75 batches, loss: 0.5238Epoch 4/15: [==================            ] 47/75 batches, loss: 0.5241Epoch 4/15: [===================           ] 48/75 batches, loss: 0.5248Epoch 4/15: [===================           ] 49/75 batches, loss: 0.5254Epoch 4/15: [====================          ] 50/75 batches, loss: 0.5254Epoch 4/15: [====================          ] 51/75 batches, loss: 0.5261Epoch 4/15: [====================          ] 52/75 batches, loss: 0.5267Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.5274Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.5262Epoch 4/15: [======================        ] 55/75 batches, loss: 0.5258Epoch 4/15: [======================        ] 56/75 batches, loss: 0.5268Epoch 4/15: [======================        ] 57/75 batches, loss: 0.5274Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.5279Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.5278Epoch 4/15: [========================      ] 60/75 batches, loss: 0.5269Epoch 4/15: [========================      ] 61/75 batches, loss: 0.5277Epoch 4/15: [========================      ] 62/75 batches, loss: 0.5286Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.5284Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.5289Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.5281Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.5267Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.5257Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.5263Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.5285Epoch 4/15: [============================  ] 70/75 batches, loss: 0.5279Epoch 4/15: [============================  ] 71/75 batches, loss: 0.5286Epoch 4/15: [============================  ] 72/75 batches, loss: 0.5284Epoch 4/15: [============================= ] 73/75 batches, loss: 0.5289Epoch 4/15: [============================= ] 74/75 batches, loss: 0.5271Epoch 4/15: [==============================] 75/75 batches, loss: 0.5279
[2025-05-07 11:43:43,987][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5279
[2025-05-07 11:43:44,277][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5270, Metrics: {'accuracy': 0.9347826086956522, 'f1': 0.9387755102040817, 'precision': 0.92, 'recall': 0.9583333333333334}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.4838Epoch 5/15: [                              ] 2/75 batches, loss: 0.5063Epoch 5/15: [=                             ] 3/75 batches, loss: 0.4998Epoch 5/15: [=                             ] 4/75 batches, loss: 0.4929Epoch 5/15: [==                            ] 5/75 batches, loss: 0.5057Epoch 5/15: [==                            ] 6/75 batches, loss: 0.5018Epoch 5/15: [==                            ] 7/75 batches, loss: 0.5075Epoch 5/15: [===                           ] 8/75 batches, loss: 0.5178Epoch 5/15: [===                           ] 9/75 batches, loss: 0.5250Epoch 5/15: [====                          ] 10/75 batches, loss: 0.5189Epoch 5/15: [====                          ] 11/75 batches, loss: 0.5181Epoch 5/15: [====                          ] 12/75 batches, loss: 0.5171Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.5233Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.5211Epoch 5/15: [======                        ] 15/75 batches, loss: 0.5231Epoch 5/15: [======                        ] 16/75 batches, loss: 0.5244Epoch 5/15: [======                        ] 17/75 batches, loss: 0.5216Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.5233Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.5268Epoch 5/15: [========                      ] 20/75 batches, loss: 0.5249Epoch 5/15: [========                      ] 21/75 batches, loss: 0.5226Epoch 5/15: [========                      ] 22/75 batches, loss: 0.5240Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.5246Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.5231Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.5217Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.5221Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.5210Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.5231Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.5233Epoch 5/15: [============                  ] 30/75 batches, loss: 0.5276Epoch 5/15: [============                  ] 31/75 batches, loss: 0.5258Epoch 5/15: [============                  ] 32/75 batches, loss: 0.5243Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.5248Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.5259Epoch 5/15: [==============                ] 35/75 batches, loss: 0.5258Epoch 5/15: [==============                ] 36/75 batches, loss: 0.5229Epoch 5/15: [==============                ] 37/75 batches, loss: 0.5217Epoch 5/15: [===============               ] 38/75 batches, loss: 0.5222Epoch 5/15: [===============               ] 39/75 batches, loss: 0.5219Epoch 5/15: [================              ] 40/75 batches, loss: 0.5223Epoch 5/15: [================              ] 41/75 batches, loss: 0.5214Epoch 5/15: [================              ] 42/75 batches, loss: 0.5221Epoch 5/15: [=================             ] 43/75 batches, loss: 0.5218Epoch 5/15: [=================             ] 44/75 batches, loss: 0.5216Epoch 5/15: [==================            ] 45/75 batches, loss: 0.5214Epoch 5/15: [==================            ] 46/75 batches, loss: 0.5210Epoch 5/15: [==================            ] 47/75 batches, loss: 0.5211Epoch 5/15: [===================           ] 48/75 batches, loss: 0.5209Epoch 5/15: [===================           ] 49/75 batches, loss: 0.5206Epoch 5/15: [====================          ] 50/75 batches, loss: 0.5198Epoch 5/15: [====================          ] 51/75 batches, loss: 0.5186Epoch 5/15: [====================          ] 52/75 batches, loss: 0.5187Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.5171Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.5165Epoch 5/15: [======================        ] 55/75 batches, loss: 0.5164Epoch 5/15: [======================        ] 56/75 batches, loss: 0.5166Epoch 5/15: [======================        ] 57/75 batches, loss: 0.5167Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.5166Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.5152Epoch 5/15: [========================      ] 60/75 batches, loss: 0.5151Epoch 5/15: [========================      ] 61/75 batches, loss: 0.5138Epoch 5/15: [========================      ] 62/75 batches, loss: 0.5144Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.5144Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.5134Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.5151Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.5143Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.5154Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.5150Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.5149Epoch 5/15: [============================  ] 70/75 batches, loss: 0.5160Epoch 5/15: [============================  ] 71/75 batches, loss: 0.5175Epoch 5/15: [============================  ] 72/75 batches, loss: 0.5187Epoch 5/15: [============================= ] 73/75 batches, loss: 0.5187Epoch 5/15: [============================= ] 74/75 batches, loss: 0.5183Epoch 5/15: [==============================] 75/75 batches, loss: 0.5185
[2025-05-07 11:43:46,912][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5185
[2025-05-07 11:43:47,153][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5262, Metrics: {'accuracy': 0.9347826086956522, 'f1': 0.9361702127659575, 'precision': 0.9565217391304348, 'recall': 0.9166666666666666}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.6023Epoch 6/15: [                              ] 2/75 batches, loss: 0.5948Epoch 6/15: [=                             ] 3/75 batches, loss: 0.5744Epoch 6/15: [=                             ] 4/75 batches, loss: 0.5812Epoch 6/15: [==                            ] 5/75 batches, loss: 0.5730Epoch 6/15: [==                            ] 6/75 batches, loss: 0.5505Epoch 6/15: [==                            ] 7/75 batches, loss: 0.5397Epoch 6/15: [===                           ] 8/75 batches, loss: 0.5324Epoch 6/15: [===                           ] 9/75 batches, loss: 0.5320Epoch 6/15: [====                          ] 10/75 batches, loss: 0.5336Epoch 6/15: [====                          ] 11/75 batches, loss: 0.5320Epoch 6/15: [====                          ] 12/75 batches, loss: 0.5286Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.5286Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.5291Epoch 6/15: [======                        ] 15/75 batches, loss: 0.5274Epoch 6/15: [======                        ] 16/75 batches, loss: 0.5276Epoch 6/15: [======                        ] 17/75 batches, loss: 0.5250Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.5244Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.5207Epoch 6/15: [========                      ] 20/75 batches, loss: 0.5172Epoch 6/15: [========                      ] 21/75 batches, loss: 0.5195Epoch 6/15: [========                      ] 22/75 batches, loss: 0.5206Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.5210Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.5205Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.5189Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.5178Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.5203Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.5214Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.5197Epoch 6/15: [============                  ] 30/75 batches, loss: 0.5187Epoch 6/15: [============                  ] 31/75 batches, loss: 0.5207Epoch 6/15: [============                  ] 32/75 batches, loss: 0.5217Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.5210Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.5213Epoch 6/15: [==============                ] 35/75 batches, loss: 0.5215Epoch 6/15: [==============                ] 36/75 batches, loss: 0.5224Epoch 6/15: [==============                ] 37/75 batches, loss: 0.5227Epoch 6/15: [===============               ] 38/75 batches, loss: 0.5217Epoch 6/15: [===============               ] 39/75 batches, loss: 0.5232Epoch 6/15: [================              ] 40/75 batches, loss: 0.5242Epoch 6/15: [================              ] 41/75 batches, loss: 0.5225Epoch 6/15: [================              ] 42/75 batches, loss: 0.5242Epoch 6/15: [=================             ] 43/75 batches, loss: 0.5242Epoch 6/15: [=================             ] 44/75 batches, loss: 0.5222Epoch 6/15: [==================            ] 45/75 batches, loss: 0.5212Epoch 6/15: [==================            ] 46/75 batches, loss: 0.5220Epoch 6/15: [==================            ] 47/75 batches, loss: 0.5217Epoch 6/15: [===================           ] 48/75 batches, loss: 0.5213Epoch 6/15: [===================           ] 49/75 batches, loss: 0.5214Epoch 6/15: [====================          ] 50/75 batches, loss: 0.5213Epoch 6/15: [====================          ] 51/75 batches, loss: 0.5197Epoch 6/15: [====================          ] 52/75 batches, loss: 0.5185Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.5173Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.5169Epoch 6/15: [======================        ] 55/75 batches, loss: 0.5168Epoch 6/15: [======================        ] 56/75 batches, loss: 0.5162Epoch 6/15: [======================        ] 57/75 batches, loss: 0.5164Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.5170Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.5161Epoch 6/15: [========================      ] 60/75 batches, loss: 0.5155Epoch 6/15: [========================      ] 61/75 batches, loss: 0.5157Epoch 6/15: [========================      ] 62/75 batches, loss: 0.5155Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.5163Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.5157Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.5163Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.5165Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.5160Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.5178Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.5168Epoch 6/15: [============================  ] 70/75 batches, loss: 0.5156Epoch 6/15: [============================  ] 71/75 batches, loss: 0.5157Epoch 6/15: [============================  ] 72/75 batches, loss: 0.5175Epoch 6/15: [============================= ] 73/75 batches, loss: 0.5170Epoch 6/15: [============================= ] 74/75 batches, loss: 0.5171Epoch 6/15: [==============================] 75/75 batches, loss: 0.5151
[2025-05-07 11:43:49,814][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5151
[2025-05-07 11:43:50,038][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5227, Metrics: {'accuracy': 0.9347826086956522, 'f1': 0.9387755102040817, 'precision': 0.92, 'recall': 0.9583333333333334}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.5071Epoch 7/15: [                              ] 2/75 batches, loss: 0.4702Epoch 7/15: [=                             ] 3/75 batches, loss: 0.4959Epoch 7/15: [=                             ] 4/75 batches, loss: 0.4809Epoch 7/15: [==                            ] 5/75 batches, loss: 0.4665Epoch 7/15: [==                            ] 6/75 batches, loss: 0.4648Epoch 7/15: [==                            ] 7/75 batches, loss: 0.4640Epoch 7/15: [===                           ] 8/75 batches, loss: 0.4702Epoch 7/15: [===                           ] 9/75 batches, loss: 0.4796Epoch 7/15: [====                          ] 10/75 batches, loss: 0.4773Epoch 7/15: [====                          ] 11/75 batches, loss: 0.4755Epoch 7/15: [====                          ] 12/75 batches, loss: 0.4804Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.4823Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.4879Epoch 7/15: [======                        ] 15/75 batches, loss: 0.4875Epoch 7/15: [======                        ] 16/75 batches, loss: 0.4856Epoch 7/15: [======                        ] 17/75 batches, loss: 0.4873Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.4898Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.4931Epoch 7/15: [========                      ] 20/75 batches, loss: 0.4972Epoch 7/15: [========                      ] 21/75 batches, loss: 0.4974Epoch 7/15: [========                      ] 22/75 batches, loss: 0.4988Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.5021Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.5017Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.5027Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.5058Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.5092Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.5100Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.5090Epoch 7/15: [============                  ] 30/75 batches, loss: 0.5074Epoch 7/15: [============                  ] 31/75 batches, loss: 0.5100Epoch 7/15: [============                  ] 32/75 batches, loss: 0.5099Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.5097Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.5117Epoch 7/15: [==============                ] 35/75 batches, loss: 0.5123Epoch 7/15: [==============                ] 36/75 batches, loss: 0.5114Epoch 7/15: [==============                ] 37/75 batches, loss: 0.5102Epoch 7/15: [===============               ] 38/75 batches, loss: 0.5121Epoch 7/15: [===============               ] 39/75 batches, loss: 0.5125Epoch 7/15: [================              ] 40/75 batches, loss: 0.5124Epoch 7/15: [================              ] 41/75 batches, loss: 0.5113Epoch 7/15: [================              ] 42/75 batches, loss: 0.5107Epoch 7/15: [=================             ] 43/75 batches, loss: 0.5127Epoch 7/15: [=================             ] 44/75 batches, loss: 0.5142Epoch 7/15: [==================            ] 45/75 batches, loss: 0.5150Epoch 7/15: [==================            ] 46/75 batches, loss: 0.5137Epoch 7/15: [==================            ] 47/75 batches, loss: 0.5141Epoch 7/15: [===================           ] 48/75 batches, loss: 0.5142Epoch 7/15: [===================           ] 49/75 batches, loss: 0.5163Epoch 7/15: [====================          ] 50/75 batches, loss: 0.5168Epoch 7/15: [====================          ] 51/75 batches, loss: 0.5171Epoch 7/15: [====================          ] 52/75 batches, loss: 0.5165Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.5159Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.5161Epoch 7/15: [======================        ] 55/75 batches, loss: 0.5177Epoch 7/15: [======================        ] 56/75 batches, loss: 0.5179Epoch 7/15: [======================        ] 57/75 batches, loss: 0.5178Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.5188Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.5182Epoch 7/15: [========================      ] 60/75 batches, loss: 0.5199Epoch 7/15: [========================      ] 61/75 batches, loss: 0.5189Epoch 7/15: [========================      ] 62/75 batches, loss: 0.5180Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.5199Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.5210Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.5219Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.5205Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.5189Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.5183Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.5178Epoch 7/15: [============================  ] 70/75 batches, loss: 0.5170Epoch 7/15: [============================  ] 71/75 batches, loss: 0.5168Epoch 7/15: [============================  ] 72/75 batches, loss: 0.5163Epoch 7/15: [============================= ] 73/75 batches, loss: 0.5158Epoch 7/15: [============================= ] 74/75 batches, loss: 0.5161Epoch 7/15: [==============================] 75/75 batches, loss: 0.5148
[2025-05-07 11:43:52,726][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5148
[2025-05-07 11:43:52,941][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5185, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.9583333333333334, 'precision': 0.9583333333333334, 'recall': 0.9583333333333334}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.5585Epoch 8/15: [                              ] 2/75 batches, loss: 0.5311Epoch 8/15: [=                             ] 3/75 batches, loss: 0.5460Epoch 8/15: [=                             ] 4/75 batches, loss: 0.5360Epoch 8/15: [==                            ] 5/75 batches, loss: 0.5411Epoch 8/15: [==                            ] 6/75 batches, loss: 0.5394Epoch 8/15: [==                            ] 7/75 batches, loss: 0.5309Epoch 8/15: [===                           ] 8/75 batches, loss: 0.5279Epoch 8/15: [===                           ] 9/75 batches, loss: 0.5152Epoch 8/15: [====                          ] 10/75 batches, loss: 0.5164Epoch 8/15: [====                          ] 11/75 batches, loss: 0.5178Epoch 8/15: [====                          ] 12/75 batches, loss: 0.5127Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.5089Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.5053Epoch 8/15: [======                        ] 15/75 batches, loss: 0.5054Epoch 8/15: [======                        ] 16/75 batches, loss: 0.5085Epoch 8/15: [======                        ] 17/75 batches, loss: 0.5085Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.5124Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.5083Epoch 8/15: [========                      ] 20/75 batches, loss: 0.5070Epoch 8/15: [========                      ] 21/75 batches, loss: 0.5081Epoch 8/15: [========                      ] 22/75 batches, loss: 0.5058Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.5049Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.5048Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.5054Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.5047Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.5067Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.5035Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.5027Epoch 8/15: [============                  ] 30/75 batches, loss: 0.5038Epoch 8/15: [============                  ] 31/75 batches, loss: 0.5062Epoch 8/15: [============                  ] 32/75 batches, loss: 0.5049Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.5070Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.5088Epoch 8/15: [==============                ] 35/75 batches, loss: 0.5100Epoch 8/15: [==============                ] 36/75 batches, loss: 0.5099Epoch 8/15: [==============                ] 37/75 batches, loss: 0.5072Epoch 8/15: [===============               ] 38/75 batches, loss: 0.5078Epoch 8/15: [===============               ] 39/75 batches, loss: 0.5059Epoch 8/15: [================              ] 40/75 batches, loss: 0.5059Epoch 8/15: [================              ] 41/75 batches, loss: 0.5058Epoch 8/15: [================              ] 42/75 batches, loss: 0.5075Epoch 8/15: [=================             ] 43/75 batches, loss: 0.5088Epoch 8/15: [=================             ] 44/75 batches, loss: 0.5087Epoch 8/15: [==================            ] 45/75 batches, loss: 0.5078Epoch 8/15: [==================            ] 46/75 batches, loss: 0.5068Epoch 8/15: [==================            ] 47/75 batches, loss: 0.5080Epoch 8/15: [===================           ] 48/75 batches, loss: 0.5098Epoch 8/15: [===================           ] 49/75 batches, loss: 0.5107Epoch 8/15: [====================          ] 50/75 batches, loss: 0.5101Epoch 8/15: [====================          ] 51/75 batches, loss: 0.5086Epoch 8/15: [====================          ] 52/75 batches, loss: 0.5099Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.5106Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.5114Epoch 8/15: [======================        ] 55/75 batches, loss: 0.5124Epoch 8/15: [======================        ] 56/75 batches, loss: 0.5120Epoch 8/15: [======================        ] 57/75 batches, loss: 0.5137Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.5128Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.5133Epoch 8/15: [========================      ] 60/75 batches, loss: 0.5127Epoch 8/15: [========================      ] 61/75 batches, loss: 0.5122Epoch 8/15: [========================      ] 62/75 batches, loss: 0.5128Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.5128Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.5123Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.5131Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.5115Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.5114Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.5116Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.5122Epoch 8/15: [============================  ] 70/75 batches, loss: 0.5119Epoch 8/15: [============================  ] 71/75 batches, loss: 0.5125Epoch 8/15: [============================  ] 72/75 batches, loss: 0.5127Epoch 8/15: [============================= ] 73/75 batches, loss: 0.5123Epoch 8/15: [============================= ] 74/75 batches, loss: 0.5124Epoch 8/15: [==============================] 75/75 batches, loss: 0.5127
[2025-05-07 11:43:55,646][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5127
[2025-05-07 11:43:55,927][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5171, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.9583333333333334, 'precision': 0.9583333333333334, 'recall': 0.9583333333333334}
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.5033Epoch 9/15: [                              ] 2/75 batches, loss: 0.5323Epoch 9/15: [=                             ] 3/75 batches, loss: 0.5424Epoch 9/15: [=                             ] 4/75 batches, loss: 0.5447Epoch 9/15: [==                            ] 5/75 batches, loss: 0.5249Epoch 9/15: [==                            ] 6/75 batches, loss: 0.5292Epoch 9/15: [==                            ] 7/75 batches, loss: 0.5304Epoch 9/15: [===                           ] 8/75 batches, loss: 0.5197Epoch 9/15: [===                           ] 9/75 batches, loss: 0.5308Epoch 9/15: [====                          ] 10/75 batches, loss: 0.5320Epoch 9/15: [====                          ] 11/75 batches, loss: 0.5186Epoch 9/15: [====                          ] 12/75 batches, loss: 0.5254Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.5219Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.5189Epoch 9/15: [======                        ] 15/75 batches, loss: 0.5179Epoch 9/15: [======                        ] 16/75 batches, loss: 0.5127Epoch 9/15: [======                        ] 17/75 batches, loss: 0.5123Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.5141Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.5136Epoch 9/15: [========                      ] 20/75 batches, loss: 0.5159Epoch 9/15: [========                      ] 21/75 batches, loss: 0.5166Epoch 9/15: [========                      ] 22/75 batches, loss: 0.5193Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.5214Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.5187Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.5200Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.5194Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.5214Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.5191Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.5232Epoch 9/15: [============                  ] 30/75 batches, loss: 0.5242Epoch 9/15: [============                  ] 31/75 batches, loss: 0.5235Epoch 9/15: [============                  ] 32/75 batches, loss: 0.5244Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.5252Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.5246Epoch 9/15: [==============                ] 35/75 batches, loss: 0.5240Epoch 9/15: [==============                ] 36/75 batches, loss: 0.5232Epoch 9/15: [==============                ] 37/75 batches, loss: 0.5208Epoch 9/15: [===============               ] 38/75 batches, loss: 0.5210Epoch 9/15: [===============               ] 39/75 batches, loss: 0.5213Epoch 9/15: [================              ] 40/75 batches, loss: 0.5197Epoch 9/15: [================              ] 41/75 batches, loss: 0.5199Epoch 9/15: [================              ] 42/75 batches, loss: 0.5174Epoch 9/15: [=================             ] 43/75 batches, loss: 0.5177Epoch 9/15: [=================             ] 44/75 batches, loss: 0.5164Epoch 9/15: [==================            ] 45/75 batches, loss: 0.5157Epoch 9/15: [==================            ] 46/75 batches, loss: 0.5155Epoch 9/15: [==================            ] 47/75 batches, loss: 0.5163Epoch 9/15: [===================           ] 48/75 batches, loss: 0.5161Epoch 9/15: [===================           ] 49/75 batches, loss: 0.5172Epoch 9/15: [====================          ] 50/75 batches, loss: 0.5165Epoch 9/15: [====================          ] 51/75 batches, loss: 0.5167Epoch 9/15: [====================          ] 52/75 batches, loss: 0.5175Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.5172Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.5176Epoch 9/15: [======================        ] 55/75 batches, loss: 0.5178Epoch 9/15: [======================        ] 56/75 batches, loss: 0.5181Epoch 9/15: [======================        ] 57/75 batches, loss: 0.5171Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.5161Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.5163Epoch 9/15: [========================      ] 60/75 batches, loss: 0.5161Epoch 9/15: [========================      ] 61/75 batches, loss: 0.5172Epoch 9/15: [========================      ] 62/75 batches, loss: 0.5170Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.5164Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.5158Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.5160Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.5144Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.5139Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.5134Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.5141Epoch 9/15: [============================  ] 70/75 batches, loss: 0.5131Epoch 9/15: [============================  ] 71/75 batches, loss: 0.5123Epoch 9/15: [============================  ] 72/75 batches, loss: 0.5120Epoch 9/15: [============================= ] 73/75 batches, loss: 0.5115Epoch 9/15: [============================= ] 74/75 batches, loss: 0.5111Epoch 9/15: [==============================] 75/75 batches, loss: 0.5107
[2025-05-07 11:43:58,568][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5107
[2025-05-07 11:43:58,781][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.5361, Metrics: {'accuracy': 0.9347826086956522, 'f1': 0.9411764705882353, 'precision': 0.8888888888888888, 'recall': 1.0}
[2025-05-07 11:43:58,782][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.4624Epoch 10/15: [                              ] 2/75 batches, loss: 0.4474Epoch 10/15: [=                             ] 3/75 batches, loss: 0.4726Epoch 10/15: [=                             ] 4/75 batches, loss: 0.4803Epoch 10/15: [==                            ] 5/75 batches, loss: 0.4803Epoch 10/15: [==                            ] 6/75 batches, loss: 0.4975Epoch 10/15: [==                            ] 7/75 batches, loss: 0.5052Epoch 10/15: [===                           ] 8/75 batches, loss: 0.5109Epoch 10/15: [===                           ] 9/75 batches, loss: 0.5050Epoch 10/15: [====                          ] 10/75 batches, loss: 0.5049Epoch 10/15: [====                          ] 11/75 batches, loss: 0.5093Epoch 10/15: [====                          ] 12/75 batches, loss: 0.5154Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.5157Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.5142Epoch 10/15: [======                        ] 15/75 batches, loss: 0.5120Epoch 10/15: [======                        ] 16/75 batches, loss: 0.5115Epoch 10/15: [======                        ] 17/75 batches, loss: 0.5122Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.5092Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.5091Epoch 10/15: [========                      ] 20/75 batches, loss: 0.5112Epoch 10/15: [========                      ] 21/75 batches, loss: 0.5144Epoch 10/15: [========                      ] 22/75 batches, loss: 0.5142Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.5152Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.5163Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.5202Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.5205Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.5199Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.5160Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.5173Epoch 10/15: [============                  ] 30/75 batches, loss: 0.5212Epoch 10/15: [============                  ] 31/75 batches, loss: 0.5186Epoch 10/15: [============                  ] 32/75 batches, loss: 0.5181Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.5169Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.5187Epoch 10/15: [==============                ] 35/75 batches, loss: 0.5195Epoch 10/15: [==============                ] 36/75 batches, loss: 0.5199Epoch 10/15: [==============                ] 37/75 batches, loss: 0.5185Epoch 10/15: [===============               ] 38/75 batches, loss: 0.5182Epoch 10/15: [===============               ] 39/75 batches, loss: 0.5179Epoch 10/15: [================              ] 40/75 batches, loss: 0.5181Epoch 10/15: [================              ] 41/75 batches, loss: 0.5184Epoch 10/15: [================              ] 42/75 batches, loss: 0.5164Epoch 10/15: [=================             ] 43/75 batches, loss: 0.5152Epoch 10/15: [=================             ] 44/75 batches, loss: 0.5155Epoch 10/15: [==================            ] 45/75 batches, loss: 0.5142Epoch 10/15: [==================            ] 46/75 batches, loss: 0.5150Epoch 10/15: [==================            ] 47/75 batches, loss: 0.5148Epoch 10/15: [===================           ] 48/75 batches, loss: 0.5146Epoch 10/15: [===================           ] 49/75 batches, loss: 0.5145Epoch 10/15: [====================          ] 50/75 batches, loss: 0.5148Epoch 10/15: [====================          ] 51/75 batches, loss: 0.5147Epoch 10/15: [====================          ] 52/75 batches, loss: 0.5147Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.5154Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.5138Epoch 10/15: [======================        ] 55/75 batches, loss: 0.5136Epoch 10/15: [======================        ] 56/75 batches, loss: 0.5134Epoch 10/15: [======================        ] 57/75 batches, loss: 0.5141Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.5124Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.5118Epoch 10/15: [========================      ] 60/75 batches, loss: 0.5109Epoch 10/15: [========================      ] 61/75 batches, loss: 0.5110Epoch 10/15: [========================      ] 62/75 batches, loss: 0.5128Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.5126Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.5124Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.5123Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.5115Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.5119Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.5114Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.5103Epoch 10/15: [============================  ] 70/75 batches, loss: 0.5112Epoch 10/15: [============================  ] 71/75 batches, loss: 0.5110Epoch 10/15: [============================  ] 72/75 batches, loss: 0.5122Epoch 10/15: [============================= ] 73/75 batches, loss: 0.5111Epoch 10/15: [============================= ] 74/75 batches, loss: 0.5114Epoch 10/15: [==============================] 75/75 batches, loss: 0.5124
[2025-05-07 11:44:01,079][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5124
[2025-05-07 11:44:01,306][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.5199, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9795918367346939, 'precision': 0.96, 'recall': 1.0}
[2025-05-07 11:44:01,307][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.4594Epoch 11/15: [                              ] 2/75 batches, loss: 0.4893Epoch 11/15: [=                             ] 3/75 batches, loss: 0.5386Epoch 11/15: [=                             ] 4/75 batches, loss: 0.5358Epoch 11/15: [==                            ] 5/75 batches, loss: 0.5342Epoch 11/15: [==                            ] 6/75 batches, loss: 0.5214Epoch 11/15: [==                            ] 7/75 batches, loss: 0.5189Epoch 11/15: [===                           ] 8/75 batches, loss: 0.5115Epoch 11/15: [===                           ] 9/75 batches, loss: 0.5135Epoch 11/15: [====                          ] 10/75 batches, loss: 0.5104Epoch 11/15: [====                          ] 11/75 batches, loss: 0.5119Epoch 11/15: [====                          ] 12/75 batches, loss: 0.5132Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.5157Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.5213Epoch 11/15: [======                        ] 15/75 batches, loss: 0.5232Epoch 11/15: [======                        ] 16/75 batches, loss: 0.5176Epoch 11/15: [======                        ] 17/75 batches, loss: 0.5162Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.5168Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.5123Epoch 11/15: [========                      ] 20/75 batches, loss: 0.5120Epoch 11/15: [========                      ] 21/75 batches, loss: 0.5116Epoch 11/15: [========                      ] 22/75 batches, loss: 0.5126Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.5112Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.5052Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.5045Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.5087Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.5123Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.5155Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.5163Epoch 11/15: [============                  ] 30/75 batches, loss: 0.5166Epoch 11/15: [============                  ] 31/75 batches, loss: 0.5187Epoch 11/15: [============                  ] 32/75 batches, loss: 0.5183Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.5164Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.5174Epoch 11/15: [==============                ] 35/75 batches, loss: 0.5165Epoch 11/15: [==============                ] 36/75 batches, loss: 0.5159Epoch 11/15: [==============                ] 37/75 batches, loss: 0.5137Epoch 11/15: [===============               ] 38/75 batches, loss: 0.5136Epoch 11/15: [===============               ] 39/75 batches, loss: 0.5127Epoch 11/15: [================              ] 40/75 batches, loss: 0.5137Epoch 11/15: [================              ] 41/75 batches, loss: 0.5123Epoch 11/15: [================              ] 42/75 batches, loss: 0.5138Epoch 11/15: [=================             ] 43/75 batches, loss: 0.5136Epoch 11/15: [=================             ] 44/75 batches, loss: 0.5113Epoch 11/15: [==================            ] 45/75 batches, loss: 0.5111Epoch 11/15: [==================            ] 46/75 batches, loss: 0.5105Epoch 11/15: [==================            ] 47/75 batches, loss: 0.5103Epoch 11/15: [===================           ] 48/75 batches, loss: 0.5092Epoch 11/15: [===================           ] 49/75 batches, loss: 0.5091Epoch 11/15: [====================          ] 50/75 batches, loss: 0.5100Epoch 11/15: [====================          ] 51/75 batches, loss: 0.5089Epoch 11/15: [====================          ] 52/75 batches, loss: 0.5100Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.5113Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.5107Epoch 11/15: [======================        ] 55/75 batches, loss: 0.5097Epoch 11/15: [======================        ] 56/75 batches, loss: 0.5089Epoch 11/15: [======================        ] 57/75 batches, loss: 0.5101Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.5088Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.5087Epoch 11/15: [========================      ] 60/75 batches, loss: 0.5087Epoch 11/15: [========================      ] 61/75 batches, loss: 0.5105Epoch 11/15: [========================      ] 62/75 batches, loss: 0.5113Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.5116Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.5122Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.5117Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.5116Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.5108Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.5112Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.5108Epoch 11/15: [============================  ] 70/75 batches, loss: 0.5104Epoch 11/15: [============================  ] 71/75 batches, loss: 0.5103Epoch 11/15: [============================  ] 72/75 batches, loss: 0.5103Epoch 11/15: [============================= ] 73/75 batches, loss: 0.5108Epoch 11/15: [============================= ] 74/75 batches, loss: 0.5120Epoch 11/15: [==============================] 75/75 batches, loss: 0.5123
[2025-05-07 11:44:03,605][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5123
[2025-05-07 11:44:03,864][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.5336, Metrics: {'accuracy': 0.9130434782608695, 'f1': 0.9230769230769231, 'precision': 0.8571428571428571, 'recall': 1.0}
[2025-05-07 11:44:03,865][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-07 11:44:03,865][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 11
[2025-05-07 11:44:03,865][src.training.lm_trainer][INFO] - Training completed in 31.67 seconds
[2025-05-07 11:44:03,865][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 11:44:06,806][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9924433249370277, 'f1': 0.992430613961312, 'precision': 0.9949409780775716, 'recall': 0.9899328859060402}
[2025-05-07 11:44:06,807][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9565217391304348, 'f1': 0.9583333333333334, 'precision': 0.9583333333333334, 'recall': 0.9583333333333334}
[2025-05-07 11:44:06,807][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.6847826086956522, 'f1': 0.7786259541984732, 'precision': 0.6710526315789473, 'recall': 0.9272727272727272}
[2025-05-07 11:44:08,447][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer2/ja/ja/model.pt
[2025-05-07 11:44:08,448][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▅▇▇▇▇██
wandb:           best_val_f1 ▁▆▇▇▇▇██
wandb:         best_val_loss █▄▂▂▂▂▁▁
wandb:    best_val_precision ▇▁█▅█▅██
wandb:       best_val_recall ▁█▇█▇███
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▂▃▃▃▃▃▂▃
wandb:            train_loss █▃▂▂▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▅▆▆▆▆▇▇▆█▆
wandb:                val_f1 ▁▆▇▇▇▇▇▇▇█▆
wandb:              val_loss █▄▂▂▂▂▁▁▃▁▂
wandb:         val_precision ▇▁█▅█▅██▃█▁
wandb:            val_recall ▁▇▆▇▆▇▇▇███
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.95652
wandb:           best_val_f1 0.95833
wandb:         best_val_loss 0.51711
wandb:    best_val_precision 0.95833
wandb:       best_val_recall 0.95833
wandb:      early_stop_epoch 11
wandb:                 epoch 11
wandb:   final_test_accuracy 0.68478
wandb:         final_test_f1 0.77863
wandb:  final_test_precision 0.67105
wandb:     final_test_recall 0.92727
wandb:  final_train_accuracy 0.99244
wandb:        final_train_f1 0.99243
wandb: final_train_precision 0.99494
wandb:    final_train_recall 0.98993
wandb:    final_val_accuracy 0.95652
wandb:          final_val_f1 0.95833
wandb:   final_val_precision 0.95833
wandb:      final_val_recall 0.95833
wandb:         learning_rate 0.0001
wandb:            train_loss 0.51227
wandb:            train_time 31.67204
wandb:          val_accuracy 0.91304
wandb:                val_f1 0.92308
wandb:              val_loss 0.53362
wandb:         val_precision 0.85714
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_114313-bloz1qhn
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_114313-bloz1qhn/logs
Experiment probe_layer2_question_type_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_complexity_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/ja"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 11:44:45,491][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/ja
experiment_name: probe_layer2_complexity_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 11:44:45,491][__main__][INFO] - Normalized task: complexity
[2025-05-07 11:44:45,491][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 11:44:45,491][__main__][INFO] - Determined Task Type: regression
[2025-05-07 11:44:45,496][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-05-07 11:44:45,496][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 11:44:50,161][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 11:44:52,582][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 11:44:52,583][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:44:52,798][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:44:52,979][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:44:53,315][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 11:44:53,323][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:44:53,323][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 11:44:53,324][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:44:53,473][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:44:53,549][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:44:53,574][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 11:44:53,575][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:44:53,576][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 11:44:53,576][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:44:53,649][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:44:53,714][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:44:53,730][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 11:44:53,731][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:44:53,731][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 11:44:53,733][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 11:44:53,733][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 11:44:53,734][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 11:44:53,734][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 11:44:53,734][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 11:44:53,734][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 11:44:53,734][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-05-07 11:44:53,734][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 11:44:53,734][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-05-07 11:44:53,734][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 11:44:53,734][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 11:44:53,735][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 11:44:53,735][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 11:44:53,735][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 11:44:53,735][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-05-07 11:44:53,735][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 11:44:53,735][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-05-07 11:44:53,735][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 11:44:53,735][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 11:44:53,735][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 11:44:53,735][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 11:44:53,736][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 11:44:53,736][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-05-07 11:44:53,736][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 11:44:53,736][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-05-07 11:44:53,736][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 11:44:53,736][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 11:44:53,736][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 11:44:53,736][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 11:44:53,737][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 11:45:01,097][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 11:45:01,098][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 11:45:01,098][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 11:45:01,099][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 11:45:01,101][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 11:45:01,102][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 11:45:01,102][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 11:45:01,102][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 11:45:01,102][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 11:45:01,103][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 11:45:01,103][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4297Epoch 1/15: [                              ] 2/75 batches, loss: 0.4469Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4509Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4666Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4441Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4199Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4280Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4477Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4356Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4251Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4218Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4388Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4187Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4370Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4292Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4366Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4351Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4485Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4363Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4357Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4276Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4266Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4147Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4025Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3954Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3884Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3847Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3768Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3734Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3662Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3618Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3566Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3532Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3534Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3516Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3535Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3491Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3443Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3451Epoch 1/15: [================              ] 40/75 batches, loss: 0.3393Epoch 1/15: [================              ] 41/75 batches, loss: 0.3358Epoch 1/15: [================              ] 42/75 batches, loss: 0.3357Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3352Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3355Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3341Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3296Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3281Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3254Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3224Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3207Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3198Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3222Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3180Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3206Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3206Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3158Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3131Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3123Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3108Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3086Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3066Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3053Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3058Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3052Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3051Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3024Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3011Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2979Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2962Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2941Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2929Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2951Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2935Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2933Epoch 1/15: [==============================] 75/75 batches, loss: 0.2898
[2025-05-07 11:45:07,755][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2898
[2025-05-07 11:45:07,976][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0626, Metrics: {'mse': 0.06254742294549942, 'rmse': 0.2500948279063352, 'r2': -0.019393205642700195}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1960Epoch 2/15: [                              ] 2/75 batches, loss: 0.1780Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1800Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1881Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1979Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1855Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1852Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1801Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1897Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1965Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1882Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1877Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1869Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1843Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1891Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1878Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1865Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1838Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1786Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1739Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1714Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1696Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1694Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1667Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1629Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1668Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1652Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1637Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1646Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1644Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1629Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1670Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1647Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1630Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1646Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1623Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1618Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1613Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1606Epoch 2/15: [================              ] 40/75 batches, loss: 0.1604Epoch 2/15: [================              ] 41/75 batches, loss: 0.1591Epoch 2/15: [================              ] 42/75 batches, loss: 0.1608Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1605Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1623Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1611Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1590Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1579Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1561Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1561Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1550Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1539Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1528Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1518Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1525Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1522Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1528Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1513Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1500Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1496Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1485Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1484Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1482Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1479Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1466Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1456Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1447Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1452Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1454Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1446Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1445Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1442Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1435Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1422Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1411Epoch 2/15: [==============================] 75/75 batches, loss: 0.1411
[2025-05-07 11:45:10,679][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1411
[2025-05-07 11:45:10,940][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0599, Metrics: {'mse': 0.059586431831121445, 'rmse': 0.2441033220403226, 'r2': 0.028864800930023193}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1336Epoch 3/15: [                              ] 2/75 batches, loss: 0.1397Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1221Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1060Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1137Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1155Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1112Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1212Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1204Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1182Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1190Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1119Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1111Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1102Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1063Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1050Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1021Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1037Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1033Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1077Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1120Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1103Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1135Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1119Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1102Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1097Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1122Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1119Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1101Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1082Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1092Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1146Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1137Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1137Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1129Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1125Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1157Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1147Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1139Epoch 3/15: [================              ] 40/75 batches, loss: 0.1131Epoch 3/15: [================              ] 41/75 batches, loss: 0.1142Epoch 3/15: [================              ] 42/75 batches, loss: 0.1161Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1158Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1152Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1149Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1149Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1151Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1139Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1140Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1132Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1137Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1142Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1135Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1121Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1113Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1110Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1107Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1112Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1105Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1096Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1098Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1090Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1085Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1078Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1083Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1079Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1074Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1074Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1075Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1075Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1069Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1068Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1079Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1075Epoch 3/15: [==============================] 75/75 batches, loss: 0.1077
[2025-05-07 11:45:13,620][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1077
[2025-05-07 11:45:13,887][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0643, Metrics: {'mse': 0.06392712891101837, 'rmse': 0.252838147657782, 'r2': -0.04187953472137451}
[2025-05-07 11:45:13,887][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1379Epoch 4/15: [                              ] 2/75 batches, loss: 0.1158Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1496Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1373Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1186Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1115Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1155Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1077Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1054Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1039Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1093Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1078Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1049Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1050Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1048Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1037Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1023Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1001Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1024Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1030Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1013Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0998Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0998Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1005Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0992Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0970Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0997Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0994Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0983Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0977Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0979Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0976Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0965Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0969Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0963Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0958Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0951Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0952Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0941Epoch 4/15: [================              ] 40/75 batches, loss: 0.0942Epoch 4/15: [================              ] 41/75 batches, loss: 0.0935Epoch 4/15: [================              ] 42/75 batches, loss: 0.0936Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0943Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0935Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0933Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0939Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0942Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0941Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0936Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0927Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0926Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0927Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0925Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0920Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0917Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0916Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0918Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0924Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0924Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0922Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0928Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0922Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0925Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0925Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0924Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0917Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0920Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0917Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0921Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0920Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0916Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0916Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0919Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0925Epoch 4/15: [==============================] 75/75 batches, loss: 0.0920
[2025-05-07 11:45:16,191][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0920
[2025-05-07 11:45:16,412][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0532, Metrics: {'mse': 0.0528947152197361, 'rmse': 0.22998851106030513, 'r2': 0.1379258632659912}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0550Epoch 5/15: [                              ] 2/75 batches, loss: 0.0656Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0627Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0705Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0691Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0733Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0769Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0775Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0787Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0765Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0807Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0835Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0855Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0850Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0815Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0820Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0820Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0798Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0793Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0785Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0770Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0813Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0807Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0813Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0816Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0809Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0820Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0805Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0812Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0820Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0832Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0830Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0816Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0811Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0818Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0810Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0816Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0814Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0815Epoch 5/15: [================              ] 40/75 batches, loss: 0.0811Epoch 5/15: [================              ] 41/75 batches, loss: 0.0807Epoch 5/15: [================              ] 42/75 batches, loss: 0.0823Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0818Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0814Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0809Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0808Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0803Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0795Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0800Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0802Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0798Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0795Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0792Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0789Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0784Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0782Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0776Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0768Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0768Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0765Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0775Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0781Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0779Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0775Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0782Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0785Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0779Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0773Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0775Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0777Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0774Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0771Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0763Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0760Epoch 5/15: [==============================] 75/75 batches, loss: 0.0768
[2025-05-07 11:45:19,054][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0768
[2025-05-07 11:45:19,301][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0539, Metrics: {'mse': 0.05354180932044983, 'rmse': 0.23139103120140553, 'r2': 0.12737959623336792}
[2025-05-07 11:45:19,302][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0658Epoch 6/15: [                              ] 2/75 batches, loss: 0.0677Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0698Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0633Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0739Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0735Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0734Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0764Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0722Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0681Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0647Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0688Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0673Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0686Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0673Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0676Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0674Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0669Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0657Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0659Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0661Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0654Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0648Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0651Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0654Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0662Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0654Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0665Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0663Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0667Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0662Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0665Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0661Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0659Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0665Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0673Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0678Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0680Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0683Epoch 6/15: [================              ] 40/75 batches, loss: 0.0687Epoch 6/15: [================              ] 41/75 batches, loss: 0.0692Epoch 6/15: [================              ] 42/75 batches, loss: 0.0687Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0679Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0676Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0677Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0678Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0673Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0674Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0672Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0675Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0674Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0673Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0680Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0673Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0672Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0668Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0669Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0666Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0671Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0668Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0665Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0665Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0663Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0665Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0661Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0658Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0660Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0661Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0661Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0659Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0667Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0664Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0659Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0655Epoch 6/15: [==============================] 75/75 batches, loss: 0.0664
[2025-05-07 11:45:21,557][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0664
[2025-05-07 11:45:21,820][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0428, Metrics: {'mse': 0.042521845549345016, 'rmse': 0.20620825771376136, 'r2': 0.306982159614563}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0504Epoch 7/15: [                              ] 2/75 batches, loss: 0.0509Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0467Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0480Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0499Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0544Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0564Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0637Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0618Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0626Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0621Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0606Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0657Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0642Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0644Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0632Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0627Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0644Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0661Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0668Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0651Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0637Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0630Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0626Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0625Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0620Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0638Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0636Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0645Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0659Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0652Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0642Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0638Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0660Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0659Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0653Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0658Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0650Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0664Epoch 7/15: [================              ] 40/75 batches, loss: 0.0661Epoch 7/15: [================              ] 41/75 batches, loss: 0.0652Epoch 7/15: [================              ] 42/75 batches, loss: 0.0655Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0652Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0653Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0654Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0650Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0654Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0651Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0646Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0642Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0639Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0632Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0626Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0622Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0626Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0625Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0623Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0625Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0620Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0623Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0617Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0615Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0613Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0616Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0619Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0623Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0622Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0621Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0621Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0626Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0622Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0619Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0618Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0617Epoch 7/15: [==============================] 75/75 batches, loss: 0.0616
[2025-05-07 11:45:24,516][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0616
[2025-05-07 11:45:24,741][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0566, Metrics: {'mse': 0.05609867349267006, 'rmse': 0.23685158537081835, 'r2': 0.08570802211761475}
[2025-05-07 11:45:24,742][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0592Epoch 8/15: [                              ] 2/75 batches, loss: 0.0461Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0525Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0559Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0544Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0548Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0524Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0538Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0520Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0517Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0540Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0540Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0553Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0550Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0560Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0543Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0556Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0572Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0597Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0602Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0600Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0598Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0596Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0606Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0605Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0596Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0601Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0588Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0589Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0595Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0591Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0590Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0582Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0575Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0580Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0581Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0578Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0576Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0577Epoch 8/15: [================              ] 40/75 batches, loss: 0.0573Epoch 8/15: [================              ] 41/75 batches, loss: 0.0577Epoch 8/15: [================              ] 42/75 batches, loss: 0.0580Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0575Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0567Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0567Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0568Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0561Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0561Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0554Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0558Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0556Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0554Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0550Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0546Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0545Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0550Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0550Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0554Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0550Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0550Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0549Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0546Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0546Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0545Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0544Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0548Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0547Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0550Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0550Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0547Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0548Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0548Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0548Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0547Epoch 8/15: [==============================] 75/75 batches, loss: 0.0547
[2025-05-07 11:45:27,041][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0547
[2025-05-07 11:45:27,242][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0565, Metrics: {'mse': 0.05593235045671463, 'rmse': 0.23650021238196517, 'r2': 0.08841872215270996}
[2025-05-07 11:45:27,243][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0119Epoch 9/15: [                              ] 2/75 batches, loss: 0.0296Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0382Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0439Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0467Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0478Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0501Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0508Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0514Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0545Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0560Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0557Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0528Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0542Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0564Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0583Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0581Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0580Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0586Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0579Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0572Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0568Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0563Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0573Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0580Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0586Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0579Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0578Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0577Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0581Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0569Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0565Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0572Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0564Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0563Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0557Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0560Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0563Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0559Epoch 9/15: [================              ] 40/75 batches, loss: 0.0558Epoch 9/15: [================              ] 41/75 batches, loss: 0.0550Epoch 9/15: [================              ] 42/75 batches, loss: 0.0546Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0550Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0554Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0551Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0554Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0550Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0545Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0543Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0544Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0545Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0541Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0540Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0548Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0545Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0545Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0542Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0541Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0544Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0544Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0542Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0540Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0540Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0544Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0544Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0545Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0542Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0541Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0540Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0537Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0536Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0536Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0532Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0530Epoch 9/15: [==============================] 75/75 batches, loss: 0.0531
[2025-05-07 11:45:29,593][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0531
[2025-05-07 11:45:29,809][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0507, Metrics: {'mse': 0.0502069927752018, 'rmse': 0.22406916962224366, 'r2': 0.1817302107810974}
[2025-05-07 11:45:29,810][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0763Epoch 10/15: [                              ] 2/75 batches, loss: 0.0567Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0583Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0577Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0538Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0521Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0503Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0549Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0537Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0508Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0502Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0498Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0495Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0485Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0483Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0484Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0491Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0482Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0473Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0468Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0484Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0490Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0476Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0474Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0478Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0477Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0471Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0466Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0468Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0474Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0470Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0464Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0468Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0470Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0468Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0470Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0468Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0466Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0462Epoch 10/15: [================              ] 40/75 batches, loss: 0.0457Epoch 10/15: [================              ] 41/75 batches, loss: 0.0459Epoch 10/15: [================              ] 42/75 batches, loss: 0.0459Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0464Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0468Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0470Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0474Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0473Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0479Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0476Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0476Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0478Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0477Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0475Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0478Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0478Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0474Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0474Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0480Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0478Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0474Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0473Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0471Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0478Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0473Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0477Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0473Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0471Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0469Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0471Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0471Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0470Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0468Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0468Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0466Epoch 10/15: [==============================] 75/75 batches, loss: 0.0466
[2025-05-07 11:45:32,080][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0466
[2025-05-07 11:45:32,410][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0403, Metrics: {'mse': 0.03991001099348068, 'rmse': 0.1997749008095879, 'r2': 0.34954965114593506}
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0256Epoch 11/15: [                              ] 2/75 batches, loss: 0.0325Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0345Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0368Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0410Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0398Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0434Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0434Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0436Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0418Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0403Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0439Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0450Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0438Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0430Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0430Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0429Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0436Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0447Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0458Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0465Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0468Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0464Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0465Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0464Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0466Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0458Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0457Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0455Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0449Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0449Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0450Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0455Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0453Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0453Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0454Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0452Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0447Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0444Epoch 11/15: [================              ] 40/75 batches, loss: 0.0445Epoch 11/15: [================              ] 41/75 batches, loss: 0.0443Epoch 11/15: [================              ] 42/75 batches, loss: 0.0448Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0451Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0452Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0452Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0449Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0445Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0441Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0443Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0442Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0437Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0431Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0431Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0429Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0432Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0435Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0432Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0434Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0432Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0438Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0441Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0438Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0441Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0440Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0438Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0445Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0443Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0442Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0441Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0441Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0442Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0446Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0447Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0450Epoch 11/15: [==============================] 75/75 batches, loss: 0.0452
[2025-05-07 11:45:35,057][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0452
[2025-05-07 11:45:35,263][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0346, Metrics: {'mse': 0.034355245530605316, 'rmse': 0.1853516806792032, 'r2': 0.44008082151412964}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0226Epoch 12/15: [                              ] 2/75 batches, loss: 0.0339Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0408Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0430Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0428Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0402Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0417Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0399Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0399Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0428Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0437Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0435Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0451Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0438Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0427Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0428Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0428Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0428Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0424Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0418Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0418Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0418Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0423Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0424Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0419Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0421Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0420Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0424Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0422Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0426Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0427Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0424Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0424Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0424Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0423Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0417Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0413Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0413Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0409Epoch 12/15: [================              ] 40/75 batches, loss: 0.0409Epoch 12/15: [================              ] 41/75 batches, loss: 0.0413Epoch 12/15: [================              ] 42/75 batches, loss: 0.0411Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0407Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0405Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0404Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0401Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0407Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0410Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0406Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0404Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0407Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0409Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0413Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0412Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0416Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0413Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0409Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0412Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0411Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0413Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0416Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0415Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0416Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0416Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0414Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0410Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0408Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0405Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0401Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0402Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0401Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0400Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0401Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0402Epoch 12/15: [==============================] 75/75 batches, loss: 0.0402
[2025-05-07 11:45:37,946][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0402
[2025-05-07 11:45:38,162][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0471, Metrics: {'mse': 0.0465625636279583, 'rmse': 0.2157836037050969, 'r2': 0.24112683534622192}
[2025-05-07 11:45:38,162][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0901Epoch 13/15: [                              ] 2/75 batches, loss: 0.0676Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0572Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0503Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0492Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0445Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0449Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0498Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0508Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0506Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0501Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0492Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0481Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0466Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0469Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0452Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0462Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0451Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0453Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0454Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0464Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0461Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0459Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0455Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0458Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0460Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0455Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0449Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0449Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0447Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0451Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0448Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0442Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0435Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0438Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0443Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0443Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0444Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0452Epoch 13/15: [================              ] 40/75 batches, loss: 0.0451Epoch 13/15: [================              ] 41/75 batches, loss: 0.0450Epoch 13/15: [================              ] 42/75 batches, loss: 0.0450Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0451Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0452Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0451Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0448Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0445Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0441Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0442Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0441Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0437Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0434Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0431Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0430Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0432Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0433Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0432Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0433Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0435Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0431Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0433Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0433Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0435Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0435Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0432Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0431Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0427Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0429Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0428Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0427Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0427Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0426Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0429Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0429Epoch 13/15: [==============================] 75/75 batches, loss: 0.0433
[2025-05-07 11:45:40,463][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0433
[2025-05-07 11:45:40,672][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0351, Metrics: {'mse': 0.034623030573129654, 'rmse': 0.1860726486432911, 'r2': 0.4357163906097412}
[2025-05-07 11:45:40,673][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0370Epoch 14/15: [                              ] 2/75 batches, loss: 0.0454Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0460Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0447Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0410Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0374Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0363Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0374Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0403Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0431Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0434Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0412Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0401Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0403Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0403Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0397Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0395Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0392Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0390Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0382Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0380Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0382Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0383Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0384Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0378Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0377Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0372Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0378Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0371Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0366Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0361Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0359Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0364Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0363Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0379Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0377Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0378Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0386Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0386Epoch 14/15: [================              ] 40/75 batches, loss: 0.0387Epoch 14/15: [================              ] 41/75 batches, loss: 0.0386Epoch 14/15: [================              ] 42/75 batches, loss: 0.0382Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0382Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0379Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0390Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0386Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0385Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0385Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0383Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0391Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0398Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0397Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0399Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0395Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0394Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0391Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0390Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0390Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0389Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0388Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0387Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0385Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0387Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0387Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0384Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0388Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0387Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0386Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0387Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0386Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0384Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0384Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0380Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0379Epoch 14/15: [==============================] 75/75 batches, loss: 0.0384
[2025-05-07 11:45:42,936][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0384
[2025-05-07 11:45:43,153][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0360, Metrics: {'mse': 0.03553222119808197, 'rmse': 0.18849992360232395, 'r2': 0.42089855670928955}
[2025-05-07 11:45:43,153][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0382Epoch 15/15: [                              ] 2/75 batches, loss: 0.0346Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0311Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0302Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0303Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0281Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0278Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0281Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0273Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0277Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0273Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0267Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0304Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0293Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0282Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0292Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0293Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0298Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0300Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0298Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0298Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0295Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0300Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0292Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0289Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0295Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0291Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0293Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0288Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0293Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0294Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0297Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0307Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0311Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0307Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0308Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0309Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0308Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0309Epoch 15/15: [================              ] 40/75 batches, loss: 0.0308Epoch 15/15: [================              ] 41/75 batches, loss: 0.0304Epoch 15/15: [================              ] 42/75 batches, loss: 0.0308Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0312Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0312Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0312Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0315Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0317Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0316Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0318Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0320Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0319Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0320Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0319Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0323Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0323Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0321Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0320Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0321Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0321Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0320Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0321Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0322Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0322Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0322Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0324Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0325Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0324Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0325Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0325Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0325Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0327Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0328Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0327Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0324Epoch 15/15: [==============================] 75/75 batches, loss: 0.0323
[2025-05-07 11:45:45,427][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0323
[2025-05-07 11:45:45,632][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0363, Metrics: {'mse': 0.03570163995027542, 'rmse': 0.18894877599570584, 'r2': 0.4181373119354248}
[2025-05-07 11:45:45,633][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 11:45:45,633][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 15
[2025-05-07 11:45:45,633][src.training.lm_trainer][INFO] - Training completed in 40.87 seconds
[2025-05-07 11:45:45,633][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 11:45:48,535][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.0193499606102705, 'rmse': 0.13910413584890458, 'r2': 0.5170398950576782}
[2025-05-07 11:45:48,535][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.034355245530605316, 'rmse': 0.1853516806792032, 'r2': 0.44008082151412964}
[2025-05-07 11:45:48,535][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.027955954894423485, 'rmse': 0.16720034358344926, 'r2': 0.4629901647567749}
[2025-05-07 11:45:50,192][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/ja/ja/model.pt
[2025-05-07 11:45:50,194][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▆▃▂▁
wandb:     best_val_mse █▇▆▃▂▁
wandb:      best_val_r2 ▁▂▃▆▇█
wandb:    best_val_rmse █▇▆▃▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▁▃▃▅▃▃▄▅▆▄▆▆
wandb:       train_loss █▄▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇█▅▆▃▆▆▅▂▁▄▁▁▁
wandb:          val_mse █▇█▅▆▃▆▆▅▂▁▄▁▁▁
wandb:           val_r2 ▁▂▁▄▃▆▃▃▄▇█▅███
wandb:         val_rmse █▇█▆▆▃▆▆▅▂▁▄▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03464
wandb:     best_val_mse 0.03436
wandb:      best_val_r2 0.44008
wandb:    best_val_rmse 0.18535
wandb: early_stop_epoch 15
wandb:            epoch 15
wandb:   final_test_mse 0.02796
wandb:    final_test_r2 0.46299
wandb:  final_test_rmse 0.1672
wandb:  final_train_mse 0.01935
wandb:   final_train_r2 0.51704
wandb: final_train_rmse 0.1391
wandb:    final_val_mse 0.03436
wandb:     final_val_r2 0.44008
wandb:   final_val_rmse 0.18535
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03227
wandb:       train_time 40.87237
wandb:         val_loss 0.03627
wandb:          val_mse 0.0357
wandb:           val_r2 0.41814
wandb:         val_rmse 0.18895
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_114445-ac8c6kxc
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_114445-ac8c6kxc/logs
Experiment probe_layer2_complexity_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/ja/ja/results.json for layer 2
=======================
PROBING LAYER 6
=======================
Running experiment: probe_layer6_question_type_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_question_type_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer6/fi"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 11:46:29,619][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer6/fi
experiment_name: probe_layer6_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-07 11:46:29,619][__main__][INFO] - Normalized task: question_type
[2025-05-07 11:46:29,619][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-07 11:46:29,619][__main__][INFO] - Determined Task Type: classification
[2025-05-07 11:46:29,623][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-05-07 11:46:29,623][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 11:46:34,321][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 11:46:36,735][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 11:46:36,735][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:46:37,147][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:46:37,195][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:46:37,510][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 11:46:37,518][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:46:37,518][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 11:46:37,519][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:46:37,646][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:46:37,763][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:46:37,778][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 11:46:37,779][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:46:37,779][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 11:46:37,781][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:46:37,904][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:46:37,973][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:46:38,081][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 11:46:38,083][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:46:38,083][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 11:46:38,084][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 11:46:38,085][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:46:38,085][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:46:38,085][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:46:38,085][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:46:38,085][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-05-07 11:46:38,085][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-05-07 11:46:38,085][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 11:46:38,085][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 11:46:38,086][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:46:38,086][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:46:38,086][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:46:38,086][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:46:38,086][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-05-07 11:46:38,086][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-05-07 11:46:38,086][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 11:46:38,086][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 11:46:38,086][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:46:38,086][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:46:38,087][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:46:38,087][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:46:38,087][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-07 11:46:38,087][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-07 11:46:38,087][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 11:46:38,087][src.data.datasets][INFO] - Sample label: 0
[2025-05-07 11:46:38,087][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 11:46:38,087][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 11:46:38,087][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 11:46:38,088][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-07 11:46:38,088][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 11:46:46,473][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 11:46:46,474][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 11:46:46,475][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-07 11:46:46,475][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-07 11:46:46,480][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-07 11:46:46,481][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-07 11:46:46,481][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-07 11:46:46,481][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-07 11:46:46,481][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 11:46:46,482][__main__][INFO] - Total parameters: 394,568,839
[2025-05-07 11:46:46,482][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-07 11:46:46,483][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7086Epoch 1/15: [                              ] 2/75 batches, loss: 0.7148Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7253Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7229Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7125Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7030Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6993Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6987Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6960Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6952Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6969Epoch 1/15: [====                          ] 12/75 batches, loss: 0.6954Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.6960Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.6958Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6960Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6960Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6966Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6960Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6959Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6958Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6952Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6945Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6943Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6942Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6938Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6939Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6938Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6940Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6939Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6940Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6933Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6932Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6930Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6923Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6918Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6919Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6916Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6916Epoch 1/15: [================              ] 40/75 batches, loss: 0.6910Epoch 1/15: [================              ] 41/75 batches, loss: 0.6915Epoch 1/15: [================              ] 42/75 batches, loss: 0.6910Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6916Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6909Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6913Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6910Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6906Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6891Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6889Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6897Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6887Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6884Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6898Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6896Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6894Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6895Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6894Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6880Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6886Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6876Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6872Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6872Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6865Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6854Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6853Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6843Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6839Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6834Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6817Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6813Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6815Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6805Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6805Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6797Epoch 1/15: [==============================] 75/75 batches, loss: 0.6781
[2025-05-07 11:46:53,236][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6781
[2025-05-07 11:46:53,515][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6472, Metrics: {'accuracy': 0.746031746031746, 'f1': 0.7241379310344828, 'precision': 0.75, 'recall': 0.7}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6210Epoch 2/15: [                              ] 2/75 batches, loss: 0.6301Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6507Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6552Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6540Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6471Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6419Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6453Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6436Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6476Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6438Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6405Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6373Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6309Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6313Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6256Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6177Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6186Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6256Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6295Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6292Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6232Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6236Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6213Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6218Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6214Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6209Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6194Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6185Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6179Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6199Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6184Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6202Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6227Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6230Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6231Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6231Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6217Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6214Epoch 2/15: [================              ] 40/75 batches, loss: 0.6207Epoch 2/15: [================              ] 41/75 batches, loss: 0.6213Epoch 2/15: [================              ] 42/75 batches, loss: 0.6213Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6213Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6198Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6171Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6174Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6171Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6171Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6176Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6171Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6173Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6156Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6160Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6152Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6152Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6137Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6147Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6138Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6138Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6126Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6128Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6129Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6138Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6152Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6139Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6143Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6134Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6125Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6123Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6111Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6114Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6093Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6090Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6079Epoch 2/15: [==============================] 75/75 batches, loss: 0.6078
[2025-05-07 11:46:56,212][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6078
[2025-05-07 11:46:56,496][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.5906, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8852459016393442, 'precision': 0.8709677419354839, 'recall': 0.9}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.5314Epoch 3/15: [                              ] 2/75 batches, loss: 0.5234Epoch 3/15: [=                             ] 3/75 batches, loss: 0.5283Epoch 3/15: [=                             ] 4/75 batches, loss: 0.5641Epoch 3/15: [==                            ] 5/75 batches, loss: 0.5583Epoch 3/15: [==                            ] 6/75 batches, loss: 0.5619Epoch 3/15: [==                            ] 7/75 batches, loss: 0.5567Epoch 3/15: [===                           ] 8/75 batches, loss: 0.5607Epoch 3/15: [===                           ] 9/75 batches, loss: 0.5556Epoch 3/15: [====                          ] 10/75 batches, loss: 0.5678Epoch 3/15: [====                          ] 11/75 batches, loss: 0.5693Epoch 3/15: [====                          ] 12/75 batches, loss: 0.5666Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.5620Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.5633Epoch 3/15: [======                        ] 15/75 batches, loss: 0.5604Epoch 3/15: [======                        ] 16/75 batches, loss: 0.5583Epoch 3/15: [======                        ] 17/75 batches, loss: 0.5571Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.5507Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.5569Epoch 3/15: [========                      ] 20/75 batches, loss: 0.5513Epoch 3/15: [========                      ] 21/75 batches, loss: 0.5537Epoch 3/15: [========                      ] 22/75 batches, loss: 0.5561Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.5539Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.5588Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.5645Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.5682Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.5668Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.5676Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.5681Epoch 3/15: [============                  ] 30/75 batches, loss: 0.5695Epoch 3/15: [============                  ] 31/75 batches, loss: 0.5692Epoch 3/15: [============                  ] 32/75 batches, loss: 0.5703Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.5708Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.5684Epoch 3/15: [==============                ] 35/75 batches, loss: 0.5686Epoch 3/15: [==============                ] 36/75 batches, loss: 0.5674Epoch 3/15: [==============                ] 37/75 batches, loss: 0.5680Epoch 3/15: [===============               ] 38/75 batches, loss: 0.5674Epoch 3/15: [===============               ] 39/75 batches, loss: 0.5680Epoch 3/15: [================              ] 40/75 batches, loss: 0.5666Epoch 3/15: [================              ] 41/75 batches, loss: 0.5661Epoch 3/15: [================              ] 42/75 batches, loss: 0.5630Epoch 3/15: [=================             ] 43/75 batches, loss: 0.5613Epoch 3/15: [=================             ] 44/75 batches, loss: 0.5632Epoch 3/15: [==================            ] 45/75 batches, loss: 0.5632Epoch 3/15: [==================            ] 46/75 batches, loss: 0.5617Epoch 3/15: [==================            ] 47/75 batches, loss: 0.5622Epoch 3/15: [===================           ] 48/75 batches, loss: 0.5631Epoch 3/15: [===================           ] 49/75 batches, loss: 0.5611Epoch 3/15: [====================          ] 50/75 batches, loss: 0.5621Epoch 3/15: [====================          ] 51/75 batches, loss: 0.5617Epoch 3/15: [====================          ] 52/75 batches, loss: 0.5619Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.5629Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.5614Epoch 3/15: [======================        ] 55/75 batches, loss: 0.5625Epoch 3/15: [======================        ] 56/75 batches, loss: 0.5624Epoch 3/15: [======================        ] 57/75 batches, loss: 0.5612Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.5607Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.5629Epoch 3/15: [========================      ] 60/75 batches, loss: 0.5640Epoch 3/15: [========================      ] 61/75 batches, loss: 0.5640Epoch 3/15: [========================      ] 62/75 batches, loss: 0.5650Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.5655Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.5643Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.5641Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.5633Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.5619Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.5619Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.5617Epoch 3/15: [============================  ] 70/75 batches, loss: 0.5609Epoch 3/15: [============================  ] 71/75 batches, loss: 0.5615Epoch 3/15: [============================  ] 72/75 batches, loss: 0.5621Epoch 3/15: [============================= ] 73/75 batches, loss: 0.5622Epoch 3/15: [============================= ] 74/75 batches, loss: 0.5631Epoch 3/15: [==============================] 75/75 batches, loss: 0.5630
[2025-05-07 11:46:59,174][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.5630
[2025-05-07 11:46:59,467][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.5705, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8771929824561403, 'precision': 0.9259259259259259, 'recall': 0.8333333333333334}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.5528Epoch 4/15: [                              ] 2/75 batches, loss: 0.5177Epoch 4/15: [=                             ] 3/75 batches, loss: 0.5397Epoch 4/15: [=                             ] 4/75 batches, loss: 0.5581Epoch 4/15: [==                            ] 5/75 batches, loss: 0.5570Epoch 4/15: [==                            ] 6/75 batches, loss: 0.5715Epoch 4/15: [==                            ] 7/75 batches, loss: 0.5695Epoch 4/15: [===                           ] 8/75 batches, loss: 0.5709Epoch 4/15: [===                           ] 9/75 batches, loss: 0.5715Epoch 4/15: [====                          ] 10/75 batches, loss: 0.5702Epoch 4/15: [====                          ] 11/75 batches, loss: 0.5724Epoch 4/15: [====                          ] 12/75 batches, loss: 0.5714Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.5625Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.5645Epoch 4/15: [======                        ] 15/75 batches, loss: 0.5666Epoch 4/15: [======                        ] 16/75 batches, loss: 0.5625Epoch 4/15: [======                        ] 17/75 batches, loss: 0.5606Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.5595Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.5543Epoch 4/15: [========                      ] 20/75 batches, loss: 0.5523Epoch 4/15: [========                      ] 21/75 batches, loss: 0.5538Epoch 4/15: [========                      ] 22/75 batches, loss: 0.5564Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.5595Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.5609Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.5593Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.5599Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.5590Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.5574Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.5597Epoch 4/15: [============                  ] 30/75 batches, loss: 0.5554Epoch 4/15: [============                  ] 31/75 batches, loss: 0.5557Epoch 4/15: [============                  ] 32/75 batches, loss: 0.5556Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.5572Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.5573Epoch 4/15: [==============                ] 35/75 batches, loss: 0.5557Epoch 4/15: [==============                ] 36/75 batches, loss: 0.5523Epoch 4/15: [==============                ] 37/75 batches, loss: 0.5539Epoch 4/15: [===============               ] 38/75 batches, loss: 0.5533Epoch 4/15: [===============               ] 39/75 batches, loss: 0.5504Epoch 4/15: [================              ] 40/75 batches, loss: 0.5498Epoch 4/15: [================              ] 41/75 batches, loss: 0.5494Epoch 4/15: [================              ] 42/75 batches, loss: 0.5490Epoch 4/15: [=================             ] 43/75 batches, loss: 0.5492Epoch 4/15: [=================             ] 44/75 batches, loss: 0.5472Epoch 4/15: [==================            ] 45/75 batches, loss: 0.5469Epoch 4/15: [==================            ] 46/75 batches, loss: 0.5471Epoch 4/15: [==================            ] 47/75 batches, loss: 0.5462Epoch 4/15: [===================           ] 48/75 batches, loss: 0.5450Epoch 4/15: [===================           ] 49/75 batches, loss: 0.5479Epoch 4/15: [====================          ] 50/75 batches, loss: 0.5483Epoch 4/15: [====================          ] 51/75 batches, loss: 0.5484Epoch 4/15: [====================          ] 52/75 batches, loss: 0.5479Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.5472Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.5464Epoch 4/15: [======================        ] 55/75 batches, loss: 0.5479Epoch 4/15: [======================        ] 56/75 batches, loss: 0.5473Epoch 4/15: [======================        ] 57/75 batches, loss: 0.5467Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.5464Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.5462Epoch 4/15: [========================      ] 60/75 batches, loss: 0.5463Epoch 4/15: [========================      ] 61/75 batches, loss: 0.5474Epoch 4/15: [========================      ] 62/75 batches, loss: 0.5476Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.5475Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.5472Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.5476Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.5475Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.5473Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.5472Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.5487Epoch 4/15: [============================  ] 70/75 batches, loss: 0.5481Epoch 4/15: [============================  ] 71/75 batches, loss: 0.5474Epoch 4/15: [============================  ] 72/75 batches, loss: 0.5473Epoch 4/15: [============================= ] 73/75 batches, loss: 0.5475Epoch 4/15: [============================= ] 74/75 batches, loss: 0.5469Epoch 4/15: [==============================] 75/75 batches, loss: 0.5472
[2025-05-07 11:47:02,164][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5472
[2025-05-07 11:47:02,435][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5566, Metrics: {'accuracy': 0.9206349206349206, 'f1': 0.9152542372881356, 'precision': 0.9310344827586207, 'recall': 0.9}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.5001Epoch 5/15: [                              ] 2/75 batches, loss: 0.5223Epoch 5/15: [=                             ] 3/75 batches, loss: 0.5433Epoch 5/15: [=                             ] 4/75 batches, loss: 0.5548Epoch 5/15: [==                            ] 5/75 batches, loss: 0.5454Epoch 5/15: [==                            ] 6/75 batches, loss: 0.5487Epoch 5/15: [==                            ] 7/75 batches, loss: 0.5401Epoch 5/15: [===                           ] 8/75 batches, loss: 0.5424Epoch 5/15: [===                           ] 9/75 batches, loss: 0.5484Epoch 5/15: [====                          ] 10/75 batches, loss: 0.5449Epoch 5/15: [====                          ] 11/75 batches, loss: 0.5521Epoch 5/15: [====                          ] 12/75 batches, loss: 0.5517Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.5463Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.5388Epoch 5/15: [======                        ] 15/75 batches, loss: 0.5331Epoch 5/15: [======                        ] 16/75 batches, loss: 0.5320Epoch 5/15: [======                        ] 17/75 batches, loss: 0.5310Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.5346Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.5312Epoch 5/15: [========                      ] 20/75 batches, loss: 0.5309Epoch 5/15: [========                      ] 21/75 batches, loss: 0.5306Epoch 5/15: [========                      ] 22/75 batches, loss: 0.5329Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.5360Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.5326Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.5291Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.5303Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.5374Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.5380Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.5433Epoch 5/15: [============                  ] 30/75 batches, loss: 0.5468Epoch 5/15: [============                  ] 31/75 batches, loss: 0.5474Epoch 5/15: [============                  ] 32/75 batches, loss: 0.5479Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.5473Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.5463Epoch 5/15: [==============                ] 35/75 batches, loss: 0.5481Epoch 5/15: [==============                ] 36/75 batches, loss: 0.5506Epoch 5/15: [==============                ] 37/75 batches, loss: 0.5500Epoch 5/15: [===============               ] 38/75 batches, loss: 0.5500Epoch 5/15: [===============               ] 39/75 batches, loss: 0.5481Epoch 5/15: [================              ] 40/75 batches, loss: 0.5483Epoch 5/15: [================              ] 41/75 batches, loss: 0.5492Epoch 5/15: [================              ] 42/75 batches, loss: 0.5495Epoch 5/15: [=================             ] 43/75 batches, loss: 0.5469Epoch 5/15: [=================             ] 44/75 batches, loss: 0.5463Epoch 5/15: [==================            ] 45/75 batches, loss: 0.5474Epoch 5/15: [==================            ] 46/75 batches, loss: 0.5454Epoch 5/15: [==================            ] 47/75 batches, loss: 0.5439Epoch 5/15: [===================           ] 48/75 batches, loss: 0.5402Epoch 5/15: [===================           ] 49/75 batches, loss: 0.5410Epoch 5/15: [====================          ] 50/75 batches, loss: 0.5429Epoch 5/15: [====================          ] 51/75 batches, loss: 0.5424Epoch 5/15: [====================          ] 52/75 batches, loss: 0.5423Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.5450Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.5434Epoch 5/15: [======================        ] 55/75 batches, loss: 0.5422Epoch 5/15: [======================        ] 56/75 batches, loss: 0.5421Epoch 5/15: [======================        ] 57/75 batches, loss: 0.5426Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.5418Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.5426Epoch 5/15: [========================      ] 60/75 batches, loss: 0.5423Epoch 5/15: [========================      ] 61/75 batches, loss: 0.5411Epoch 5/15: [========================      ] 62/75 batches, loss: 0.5411Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.5421Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.5432Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.5430Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.5435Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.5440Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.5430Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.5438Epoch 5/15: [============================  ] 70/75 batches, loss: 0.5437Epoch 5/15: [============================  ] 71/75 batches, loss: 0.5431Epoch 5/15: [============================  ] 72/75 batches, loss: 0.5438Epoch 5/15: [============================= ] 73/75 batches, loss: 0.5438Epoch 5/15: [============================= ] 74/75 batches, loss: 0.5424Epoch 5/15: [==============================] 75/75 batches, loss: 0.5423
[2025-05-07 11:47:05,083][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5423
[2025-05-07 11:47:05,363][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5525, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475, 'precision': 0.9354838709677419, 'recall': 0.9666666666666667}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.5845Epoch 6/15: [                              ] 2/75 batches, loss: 0.5827Epoch 6/15: [=                             ] 3/75 batches, loss: 0.5961Epoch 6/15: [=                             ] 4/75 batches, loss: 0.6004Epoch 6/15: [==                            ] 5/75 batches, loss: 0.5714Epoch 6/15: [==                            ] 6/75 batches, loss: 0.5737Epoch 6/15: [==                            ] 7/75 batches, loss: 0.5715Epoch 6/15: [===                           ] 8/75 batches, loss: 0.5623Epoch 6/15: [===                           ] 9/75 batches, loss: 0.5587Epoch 6/15: [====                          ] 10/75 batches, loss: 0.5517Epoch 6/15: [====                          ] 11/75 batches, loss: 0.5525Epoch 6/15: [====                          ] 12/75 batches, loss: 0.5558Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.5610Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.5612Epoch 6/15: [======                        ] 15/75 batches, loss: 0.5565Epoch 6/15: [======                        ] 16/75 batches, loss: 0.5523Epoch 6/15: [======                        ] 17/75 batches, loss: 0.5462Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.5454Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.5442Epoch 6/15: [========                      ] 20/75 batches, loss: 0.5460Epoch 6/15: [========                      ] 21/75 batches, loss: 0.5494Epoch 6/15: [========                      ] 22/75 batches, loss: 0.5464Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.5458Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.5424Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.5460Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.5486Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.5465Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.5485Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.5483Epoch 6/15: [============                  ] 30/75 batches, loss: 0.5497Epoch 6/15: [============                  ] 31/75 batches, loss: 0.5488Epoch 6/15: [============                  ] 32/75 batches, loss: 0.5479Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.5446Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.5445Epoch 6/15: [==============                ] 35/75 batches, loss: 0.5442Epoch 6/15: [==============                ] 36/75 batches, loss: 0.5434Epoch 6/15: [==============                ] 37/75 batches, loss: 0.5416Epoch 6/15: [===============               ] 38/75 batches, loss: 0.5422Epoch 6/15: [===============               ] 39/75 batches, loss: 0.5428Epoch 6/15: [================              ] 40/75 batches, loss: 0.5419Epoch 6/15: [================              ] 41/75 batches, loss: 0.5405Epoch 6/15: [================              ] 42/75 batches, loss: 0.5403Epoch 6/15: [=================             ] 43/75 batches, loss: 0.5403Epoch 6/15: [=================             ] 44/75 batches, loss: 0.5411Epoch 6/15: [==================            ] 45/75 batches, loss: 0.5399Epoch 6/15: [==================            ] 46/75 batches, loss: 0.5394Epoch 6/15: [==================            ] 47/75 batches, loss: 0.5387Epoch 6/15: [===================           ] 48/75 batches, loss: 0.5389Epoch 6/15: [===================           ] 49/75 batches, loss: 0.5397Epoch 6/15: [====================          ] 50/75 batches, loss: 0.5391Epoch 6/15: [====================          ] 51/75 batches, loss: 0.5398Epoch 6/15: [====================          ] 52/75 batches, loss: 0.5384Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.5375Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.5369Epoch 6/15: [======================        ] 55/75 batches, loss: 0.5370Epoch 6/15: [======================        ] 56/75 batches, loss: 0.5379Epoch 6/15: [======================        ] 57/75 batches, loss: 0.5383Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.5371Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.5370Epoch 6/15: [========================      ] 60/75 batches, loss: 0.5357Epoch 6/15: [========================      ] 61/75 batches, loss: 0.5352Epoch 6/15: [========================      ] 62/75 batches, loss: 0.5348Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.5350Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.5359Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.5368Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.5377Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.5376Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.5372Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.5364Epoch 6/15: [============================  ] 70/75 batches, loss: 0.5364Epoch 6/15: [============================  ] 71/75 batches, loss: 0.5361Epoch 6/15: [============================  ] 72/75 batches, loss: 0.5365Epoch 6/15: [============================= ] 73/75 batches, loss: 0.5358Epoch 6/15: [============================= ] 74/75 batches, loss: 0.5359Epoch 6/15: [==============================] 75/75 batches, loss: 0.5362
[2025-05-07 11:47:08,060][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5362
[2025-05-07 11:47:08,298][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5556, Metrics: {'accuracy': 0.9047619047619048, 'f1': 0.8928571428571429, 'precision': 0.9615384615384616, 'recall': 0.8333333333333334}
[2025-05-07 11:47:08,299][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.5464Epoch 7/15: [                              ] 2/75 batches, loss: 0.5297Epoch 7/15: [=                             ] 3/75 batches, loss: 0.5412Epoch 7/15: [=                             ] 4/75 batches, loss: 0.5278Epoch 7/15: [==                            ] 5/75 batches, loss: 0.5407Epoch 7/15: [==                            ] 6/75 batches, loss: 0.5444Epoch 7/15: [==                            ] 7/75 batches, loss: 0.5531Epoch 7/15: [===                           ] 8/75 batches, loss: 0.5510Epoch 7/15: [===                           ] 9/75 batches, loss: 0.5474Epoch 7/15: [====                          ] 10/75 batches, loss: 0.5503Epoch 7/15: [====                          ] 11/75 batches, loss: 0.5402Epoch 7/15: [====                          ] 12/75 batches, loss: 0.5404Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.5449Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.5353Epoch 7/15: [======                        ] 15/75 batches, loss: 0.5354Epoch 7/15: [======                        ] 16/75 batches, loss: 0.5380Epoch 7/15: [======                        ] 17/75 batches, loss: 0.5364Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.5378Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.5424Epoch 7/15: [========                      ] 20/75 batches, loss: 0.5429Epoch 7/15: [========                      ] 21/75 batches, loss: 0.5402Epoch 7/15: [========                      ] 22/75 batches, loss: 0.5409Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.5414Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.5412Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.5443Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.5441Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.5439Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.5454Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.5451Epoch 7/15: [============                  ] 30/75 batches, loss: 0.5433Epoch 7/15: [============                  ] 31/75 batches, loss: 0.5407Epoch 7/15: [============                  ] 32/75 batches, loss: 0.5392Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.5400Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.5415Epoch 7/15: [==============                ] 35/75 batches, loss: 0.5416Epoch 7/15: [==============                ] 36/75 batches, loss: 0.5397Epoch 7/15: [==============                ] 37/75 batches, loss: 0.5393Epoch 7/15: [===============               ] 38/75 batches, loss: 0.5367Epoch 7/15: [===============               ] 39/75 batches, loss: 0.5348Epoch 7/15: [================              ] 40/75 batches, loss: 0.5327Epoch 7/15: [================              ] 41/75 batches, loss: 0.5324Epoch 7/15: [================              ] 42/75 batches, loss: 0.5330Epoch 7/15: [=================             ] 43/75 batches, loss: 0.5322Epoch 7/15: [=================             ] 44/75 batches, loss: 0.5313Epoch 7/15: [==================            ] 45/75 batches, loss: 0.5317Epoch 7/15: [==================            ] 46/75 batches, loss: 0.5298Epoch 7/15: [==================            ] 47/75 batches, loss: 0.5313Epoch 7/15: [===================           ] 48/75 batches, loss: 0.5324Epoch 7/15: [===================           ] 49/75 batches, loss: 0.5335Epoch 7/15: [====================          ] 50/75 batches, loss: 0.5340Epoch 7/15: [====================          ] 51/75 batches, loss: 0.5333Epoch 7/15: [====================          ] 52/75 batches, loss: 0.5337Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.5342Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.5332Epoch 7/15: [======================        ] 55/75 batches, loss: 0.5355Epoch 7/15: [======================        ] 56/75 batches, loss: 0.5354Epoch 7/15: [======================        ] 57/75 batches, loss: 0.5356Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.5373Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.5369Epoch 7/15: [========================      ] 60/75 batches, loss: 0.5374Epoch 7/15: [========================      ] 61/75 batches, loss: 0.5373Epoch 7/15: [========================      ] 62/75 batches, loss: 0.5369Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.5366Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.5377Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.5380Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.5367Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.5368Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.5375Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.5370Epoch 7/15: [============================  ] 70/75 batches, loss: 0.5372Epoch 7/15: [============================  ] 71/75 batches, loss: 0.5386Epoch 7/15: [============================  ] 72/75 batches, loss: 0.5384Epoch 7/15: [============================= ] 73/75 batches, loss: 0.5381Epoch 7/15: [============================= ] 74/75 batches, loss: 0.5372Epoch 7/15: [==============================] 75/75 batches, loss: 0.5361
[2025-05-07 11:47:10,621][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5361
[2025-05-07 11:47:10,855][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5479, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475, 'precision': 0.9354838709677419, 'recall': 0.9666666666666667}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.6115Epoch 8/15: [                              ] 2/75 batches, loss: 0.5536Epoch 8/15: [=                             ] 3/75 batches, loss: 0.5532Epoch 8/15: [=                             ] 4/75 batches, loss: 0.5428Epoch 8/15: [==                            ] 5/75 batches, loss: 0.5676Epoch 8/15: [==                            ] 6/75 batches, loss: 0.5599Epoch 8/15: [==                            ] 7/75 batches, loss: 0.5496Epoch 8/15: [===                           ] 8/75 batches, loss: 0.5446Epoch 8/15: [===                           ] 9/75 batches, loss: 0.5493Epoch 8/15: [====                          ] 10/75 batches, loss: 0.5449Epoch 8/15: [====                          ] 11/75 batches, loss: 0.5447Epoch 8/15: [====                          ] 12/75 batches, loss: 0.5414Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.5414Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.5418Epoch 8/15: [======                        ] 15/75 batches, loss: 0.5402Epoch 8/15: [======                        ] 16/75 batches, loss: 0.5404Epoch 8/15: [======                        ] 17/75 batches, loss: 0.5420Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.5434Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.5434Epoch 8/15: [========                      ] 20/75 batches, loss: 0.5415Epoch 8/15: [========                      ] 21/75 batches, loss: 0.5456Epoch 8/15: [========                      ] 22/75 batches, loss: 0.5432Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.5427Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.5421Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.5408Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.5395Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.5403Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.5403Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.5397Epoch 8/15: [============                  ] 30/75 batches, loss: 0.5426Epoch 8/15: [============                  ] 31/75 batches, loss: 0.5407Epoch 8/15: [============                  ] 32/75 batches, loss: 0.5408Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.5376Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.5341Epoch 8/15: [==============                ] 35/75 batches, loss: 0.5348Epoch 8/15: [==============                ] 36/75 batches, loss: 0.5335Epoch 8/15: [==============                ] 37/75 batches, loss: 0.5349Epoch 8/15: [===============               ] 38/75 batches, loss: 0.5343Epoch 8/15: [===============               ] 39/75 batches, loss: 0.5343Epoch 8/15: [================              ] 40/75 batches, loss: 0.5358Epoch 8/15: [================              ] 41/75 batches, loss: 0.5357Epoch 8/15: [================              ] 42/75 batches, loss: 0.5341Epoch 8/15: [=================             ] 43/75 batches, loss: 0.5326Epoch 8/15: [=================             ] 44/75 batches, loss: 0.5318Epoch 8/15: [==================            ] 45/75 batches, loss: 0.5300Epoch 8/15: [==================            ] 46/75 batches, loss: 0.5308Epoch 8/15: [==================            ] 47/75 batches, loss: 0.5299Epoch 8/15: [===================           ] 48/75 batches, loss: 0.5293Epoch 8/15: [===================           ] 49/75 batches, loss: 0.5280Epoch 8/15: [====================          ] 50/75 batches, loss: 0.5287Epoch 8/15: [====================          ] 51/75 batches, loss: 0.5287Epoch 8/15: [====================          ] 52/75 batches, loss: 0.5277Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.5269Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.5277Epoch 8/15: [======================        ] 55/75 batches, loss: 0.5285Epoch 8/15: [======================        ] 56/75 batches, loss: 0.5287Epoch 8/15: [======================        ] 57/75 batches, loss: 0.5271Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.5266Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.5269Epoch 8/15: [========================      ] 60/75 batches, loss: 0.5277Epoch 8/15: [========================      ] 61/75 batches, loss: 0.5280Epoch 8/15: [========================      ] 62/75 batches, loss: 0.5281Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.5284Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.5288Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.5285Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.5295Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.5299Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.5304Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.5298Epoch 8/15: [============================  ] 70/75 batches, loss: 0.5296Epoch 8/15: [============================  ] 71/75 batches, loss: 0.5300Epoch 8/15: [============================  ] 72/75 batches, loss: 0.5311Epoch 8/15: [============================= ] 73/75 batches, loss: 0.5311Epoch 8/15: [============================= ] 74/75 batches, loss: 0.5294Epoch 8/15: [==============================] 75/75 batches, loss: 0.5308
[2025-05-07 11:47:13,616][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5308
[2025-05-07 11:47:13,906][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5441, Metrics: {'accuracy': 0.9206349206349206, 'f1': 0.9122807017543859, 'precision': 0.9629629629629629, 'recall': 0.8666666666666667}
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.5526Epoch 9/15: [                              ] 2/75 batches, loss: 0.5266Epoch 9/15: [=                             ] 3/75 batches, loss: 0.5603Epoch 9/15: [=                             ] 4/75 batches, loss: 0.5611Epoch 9/15: [==                            ] 5/75 batches, loss: 0.5578Epoch 9/15: [==                            ] 6/75 batches, loss: 0.5435Epoch 9/15: [==                            ] 7/75 batches, loss: 0.5402Epoch 9/15: [===                           ] 8/75 batches, loss: 0.5455Epoch 9/15: [===                           ] 9/75 batches, loss: 0.5389Epoch 9/15: [====                          ] 10/75 batches, loss: 0.5415Epoch 9/15: [====                          ] 11/75 batches, loss: 0.5416Epoch 9/15: [====                          ] 12/75 batches, loss: 0.5438Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.5430Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.5426Epoch 9/15: [======                        ] 15/75 batches, loss: 0.5420Epoch 9/15: [======                        ] 16/75 batches, loss: 0.5428Epoch 9/15: [======                        ] 17/75 batches, loss: 0.5421Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.5432Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.5400Epoch 9/15: [========                      ] 20/75 batches, loss: 0.5372Epoch 9/15: [========                      ] 21/75 batches, loss: 0.5381Epoch 9/15: [========                      ] 22/75 batches, loss: 0.5375Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.5343Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.5359Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.5347Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.5326Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.5328Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.5296Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.5303Epoch 9/15: [============                  ] 30/75 batches, loss: 0.5296Epoch 9/15: [============                  ] 31/75 batches, loss: 0.5307Epoch 9/15: [============                  ] 32/75 batches, loss: 0.5289Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.5291Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.5270Epoch 9/15: [==============                ] 35/75 batches, loss: 0.5291Epoch 9/15: [==============                ] 36/75 batches, loss: 0.5295Epoch 9/15: [==============                ] 37/75 batches, loss: 0.5298Epoch 9/15: [===============               ] 38/75 batches, loss: 0.5278Epoch 9/15: [===============               ] 39/75 batches, loss: 0.5280Epoch 9/15: [================              ] 40/75 batches, loss: 0.5268Epoch 9/15: [================              ] 41/75 batches, loss: 0.5255Epoch 9/15: [================              ] 42/75 batches, loss: 0.5274Epoch 9/15: [=================             ] 43/75 batches, loss: 0.5269Epoch 9/15: [=================             ] 44/75 batches, loss: 0.5281Epoch 9/15: [==================            ] 45/75 batches, loss: 0.5288Epoch 9/15: [==================            ] 46/75 batches, loss: 0.5290Epoch 9/15: [==================            ] 47/75 batches, loss: 0.5281Epoch 9/15: [===================           ] 48/75 batches, loss: 0.5285Epoch 9/15: [===================           ] 49/75 batches, loss: 0.5273Epoch 9/15: [====================          ] 50/75 batches, loss: 0.5288Epoch 9/15: [====================          ] 51/75 batches, loss: 0.5288Epoch 9/15: [====================          ] 52/75 batches, loss: 0.5279Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.5292Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.5302Epoch 9/15: [======================        ] 55/75 batches, loss: 0.5298Epoch 9/15: [======================        ] 56/75 batches, loss: 0.5307Epoch 9/15: [======================        ] 57/75 batches, loss: 0.5298Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.5298Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.5288Epoch 9/15: [========================      ] 60/75 batches, loss: 0.5289Epoch 9/15: [========================      ] 61/75 batches, loss: 0.5282Epoch 9/15: [========================      ] 62/75 batches, loss: 0.5274Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.5256Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.5274Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.5279Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.5271Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.5275Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.5282Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.5281Epoch 9/15: [============================  ] 70/75 batches, loss: 0.5273Epoch 9/15: [============================  ] 71/75 batches, loss: 0.5267Epoch 9/15: [============================  ] 72/75 batches, loss: 0.5275Epoch 9/15: [============================= ] 73/75 batches, loss: 0.5270Epoch 9/15: [============================= ] 74/75 batches, loss: 0.5274Epoch 9/15: [==============================] 75/75 batches, loss: 0.5269
[2025-05-07 11:47:16,596][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5269
[2025-05-07 11:47:16,891][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.5454, Metrics: {'accuracy': 0.9206349206349206, 'f1': 0.9122807017543859, 'precision': 0.9629629629629629, 'recall': 0.8666666666666667}
[2025-05-07 11:47:16,891][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.5070Epoch 10/15: [                              ] 2/75 batches, loss: 0.5219Epoch 10/15: [=                             ] 3/75 batches, loss: 0.5167Epoch 10/15: [=                             ] 4/75 batches, loss: 0.5439Epoch 10/15: [==                            ] 5/75 batches, loss: 0.5325Epoch 10/15: [==                            ] 6/75 batches, loss: 0.5453Epoch 10/15: [==                            ] 7/75 batches, loss: 0.5457Epoch 10/15: [===                           ] 8/75 batches, loss: 0.5505Epoch 10/15: [===                           ] 9/75 batches, loss: 0.5429Epoch 10/15: [====                          ] 10/75 batches, loss: 0.5474Epoch 10/15: [====                          ] 11/75 batches, loss: 0.5423Epoch 10/15: [====                          ] 12/75 batches, loss: 0.5411Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.5489Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.5428Epoch 10/15: [======                        ] 15/75 batches, loss: 0.5403Epoch 10/15: [======                        ] 16/75 batches, loss: 0.5367Epoch 10/15: [======                        ] 17/75 batches, loss: 0.5312Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.5342Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.5329Epoch 10/15: [========                      ] 20/75 batches, loss: 0.5317Epoch 10/15: [========                      ] 21/75 batches, loss: 0.5291Epoch 10/15: [========                      ] 22/75 batches, loss: 0.5271Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.5243Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.5306Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.5313Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.5298Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.5322Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.5323Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.5334Epoch 10/15: [============                  ] 30/75 batches, loss: 0.5310Epoch 10/15: [============                  ] 31/75 batches, loss: 0.5306Epoch 10/15: [============                  ] 32/75 batches, loss: 0.5307Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.5308Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.5307Epoch 10/15: [==============                ] 35/75 batches, loss: 0.5321Epoch 10/15: [==============                ] 36/75 batches, loss: 0.5332Epoch 10/15: [==============                ] 37/75 batches, loss: 0.5334Epoch 10/15: [===============               ] 38/75 batches, loss: 0.5330Epoch 10/15: [===============               ] 39/75 batches, loss: 0.5325Epoch 10/15: [================              ] 40/75 batches, loss: 0.5317Epoch 10/15: [================              ] 41/75 batches, loss: 0.5300Epoch 10/15: [================              ] 42/75 batches, loss: 0.5296Epoch 10/15: [=================             ] 43/75 batches, loss: 0.5305Epoch 10/15: [=================             ] 44/75 batches, loss: 0.5294Epoch 10/15: [==================            ] 45/75 batches, loss: 0.5296Epoch 10/15: [==================            ] 46/75 batches, loss: 0.5316Epoch 10/15: [==================            ] 47/75 batches, loss: 0.5316Epoch 10/15: [===================           ] 48/75 batches, loss: 0.5329Epoch 10/15: [===================           ] 49/75 batches, loss: 0.5323Epoch 10/15: [====================          ] 50/75 batches, loss: 0.5323Epoch 10/15: [====================          ] 51/75 batches, loss: 0.5328Epoch 10/15: [====================          ] 52/75 batches, loss: 0.5313Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.5308Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.5295Epoch 10/15: [======================        ] 55/75 batches, loss: 0.5301Epoch 10/15: [======================        ] 56/75 batches, loss: 0.5303Epoch 10/15: [======================        ] 57/75 batches, loss: 0.5318Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.5310Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.5317Epoch 10/15: [========================      ] 60/75 batches, loss: 0.5329Epoch 10/15: [========================      ] 61/75 batches, loss: 0.5329Epoch 10/15: [========================      ] 62/75 batches, loss: 0.5327Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.5328Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.5322Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.5299Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.5296Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.5292Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.5299Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.5298Epoch 10/15: [============================  ] 70/75 batches, loss: 0.5283Epoch 10/15: [============================  ] 71/75 batches, loss: 0.5270Epoch 10/15: [============================  ] 72/75 batches, loss: 0.5260Epoch 10/15: [============================= ] 73/75 batches, loss: 0.5261Epoch 10/15: [============================= ] 74/75 batches, loss: 0.5268Epoch 10/15: [==============================] 75/75 batches, loss: 0.5289
[2025-05-07 11:47:19,178][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5289
[2025-05-07 11:47:19,436][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.5570, Metrics: {'accuracy': 0.9047619047619048, 'f1': 0.90625, 'precision': 0.8529411764705882, 'recall': 0.9666666666666667}
[2025-05-07 11:47:19,439][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.5590Epoch 11/15: [                              ] 2/75 batches, loss: 0.6185Epoch 11/15: [=                             ] 3/75 batches, loss: 0.6124Epoch 11/15: [=                             ] 4/75 batches, loss: 0.5962Epoch 11/15: [==                            ] 5/75 batches, loss: 0.5780Epoch 11/15: [==                            ] 6/75 batches, loss: 0.5632Epoch 11/15: [==                            ] 7/75 batches, loss: 0.5549Epoch 11/15: [===                           ] 8/75 batches, loss: 0.5579Epoch 11/15: [===                           ] 9/75 batches, loss: 0.5554Epoch 11/15: [====                          ] 10/75 batches, loss: 0.5555Epoch 11/15: [====                          ] 11/75 batches, loss: 0.5449Epoch 11/15: [====                          ] 12/75 batches, loss: 0.5431Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.5408Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.5405Epoch 11/15: [======                        ] 15/75 batches, loss: 0.5407Epoch 11/15: [======                        ] 16/75 batches, loss: 0.5457Epoch 11/15: [======                        ] 17/75 batches, loss: 0.5474Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.5460Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.5452Epoch 11/15: [========                      ] 20/75 batches, loss: 0.5446Epoch 11/15: [========                      ] 21/75 batches, loss: 0.5430Epoch 11/15: [========                      ] 22/75 batches, loss: 0.5430Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.5388Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.5374Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.5390Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.5390Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.5380Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.5384Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.5394Epoch 11/15: [============                  ] 30/75 batches, loss: 0.5412Epoch 11/15: [============                  ] 31/75 batches, loss: 0.5402Epoch 11/15: [============                  ] 32/75 batches, loss: 0.5406Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.5381Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.5380Epoch 11/15: [==============                ] 35/75 batches, loss: 0.5377Epoch 11/15: [==============                ] 36/75 batches, loss: 0.5376Epoch 11/15: [==============                ] 37/75 batches, loss: 0.5378Epoch 11/15: [===============               ] 38/75 batches, loss: 0.5369Epoch 11/15: [===============               ] 39/75 batches, loss: 0.5344Epoch 11/15: [================              ] 40/75 batches, loss: 0.5345Epoch 11/15: [================              ] 41/75 batches, loss: 0.5338Epoch 11/15: [================              ] 42/75 batches, loss: 0.5329Epoch 11/15: [=================             ] 43/75 batches, loss: 0.5328Epoch 11/15: [=================             ] 44/75 batches, loss: 0.5308Epoch 11/15: [==================            ] 45/75 batches, loss: 0.5318Epoch 11/15: [==================            ] 46/75 batches, loss: 0.5309Epoch 11/15: [==================            ] 47/75 batches, loss: 0.5309Epoch 11/15: [===================           ] 48/75 batches, loss: 0.5285Epoch 11/15: [===================           ] 49/75 batches, loss: 0.5283Epoch 11/15: [====================          ] 50/75 batches, loss: 0.5266Epoch 11/15: [====================          ] 51/75 batches, loss: 0.5266Epoch 11/15: [====================          ] 52/75 batches, loss: 0.5283Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.5265Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.5269Epoch 11/15: [======================        ] 55/75 batches, loss: 0.5282Epoch 11/15: [======================        ] 56/75 batches, loss: 0.5287Epoch 11/15: [======================        ] 57/75 batches, loss: 0.5286Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.5279Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.5272Epoch 11/15: [========================      ] 60/75 batches, loss: 0.5278Epoch 11/15: [========================      ] 61/75 batches, loss: 0.5260Epoch 11/15: [========================      ] 62/75 batches, loss: 0.5273Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.5273Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.5266Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.5260Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.5260Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.5267Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.5267Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.5265Epoch 11/15: [============================  ] 70/75 batches, loss: 0.5270Epoch 11/15: [============================  ] 71/75 batches, loss: 0.5264Epoch 11/15: [============================  ] 72/75 batches, loss: 0.5264Epoch 11/15: [============================= ] 73/75 batches, loss: 0.5271Epoch 11/15: [============================= ] 74/75 batches, loss: 0.5284Epoch 11/15: [==============================] 75/75 batches, loss: 0.5279
[2025-05-07 11:47:21,760][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5279
[2025-05-07 11:47:21,999][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.5452, Metrics: {'accuracy': 0.9365079365079365, 'f1': 0.9310344827586207, 'precision': 0.9642857142857143, 'recall': 0.9}
[2025-05-07 11:47:22,000][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-07 11:47:22,000][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 11
[2025-05-07 11:47:22,000][src.training.lm_trainer][INFO] - Training completed in 31.97 seconds
[2025-05-07 11:47:22,000][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 11:47:25,109][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9807531380753138, 'f1': 0.980590717299578, 'precision': 0.989778534923339, 'recall': 0.9715719063545151}
[2025-05-07 11:47:25,109][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9206349206349206, 'f1': 0.9122807017543859, 'precision': 0.9629629629629629, 'recall': 0.8666666666666667}
[2025-05-07 11:47:25,110][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.8909090909090909, 'f1': 0.8947368421052632, 'precision': 0.864406779661017, 'recall': 0.9272727272727272}
[2025-05-07 11:47:26,760][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer6/fi/fi/model.pt
[2025-05-07 11:47:26,761][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▆▆▇██▇
wandb:           best_val_f1 ▁▆▆▇██▇
wandb:         best_val_loss █▄▃▂▂▁▁
wandb:    best_val_precision ▁▅▇▇▇▇█
wandb:       best_val_recall ▁▆▅▆██▅
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▃▃▃▃▃▃▃▃
wandb:            train_loss █▅▃▂▂▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▆▆▇█▆█▇▇▆▇
wandb:                val_f1 ▁▆▆▇█▆█▇▇▇▇
wandb:              val_loss █▄▃▂▂▂▁▁▁▂▁
wandb:         val_precision ▁▅▇▇▇█▇██▄█
wandb:            val_recall ▁▆▅▆█▅█▅▅█▆
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.92063
wandb:           best_val_f1 0.91228
wandb:         best_val_loss 0.5441
wandb:    best_val_precision 0.96296
wandb:       best_val_recall 0.86667
wandb:      early_stop_epoch 11
wandb:                 epoch 11
wandb:   final_test_accuracy 0.89091
wandb:         final_test_f1 0.89474
wandb:  final_test_precision 0.86441
wandb:     final_test_recall 0.92727
wandb:  final_train_accuracy 0.98075
wandb:        final_train_f1 0.98059
wandb: final_train_precision 0.98978
wandb:    final_train_recall 0.97157
wandb:    final_val_accuracy 0.92063
wandb:          final_val_f1 0.91228
wandb:   final_val_precision 0.96296
wandb:      final_val_recall 0.86667
wandb:         learning_rate 0.0001
wandb:            train_loss 0.52787
wandb:            train_time 31.97332
wandb:          val_accuracy 0.93651
wandb:                val_f1 0.93103
wandb:              val_loss 0.54517
wandb:         val_precision 0.96429
wandb:            val_recall 0.9
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_114629-3ycty9uv
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_114629-3ycty9uv/logs
Experiment probe_layer6_question_type_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer6/fi/fi/results.json for layer 6
Running experiment: probe_layer6_complexity_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_complexity_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer6/fi"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 11:47:59,373][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer6/fi
experiment_name: probe_layer6_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 11:47:59,373][__main__][INFO] - Normalized task: complexity
[2025-05-07 11:47:59,374][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 11:47:59,374][__main__][INFO] - Determined Task Type: regression
[2025-05-07 11:47:59,378][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-05-07 11:47:59,378][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 11:48:04,222][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 11:48:06,593][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 11:48:06,594][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:48:06,860][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:48:06,960][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:48:07,319][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 11:48:07,327][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:48:07,327][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 11:48:07,329][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:48:07,523][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:48:07,689][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:48:07,723][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 11:48:07,725][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:48:07,725][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 11:48:07,726][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:48:07,810][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:48:07,924][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:48:07,939][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 11:48:07,940][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:48:07,940][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 11:48:07,941][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 11:48:07,942][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 11:48:07,942][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 11:48:07,942][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 11:48:07,942][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 11:48:07,942][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 11:48:07,943][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-05-07 11:48:07,943][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 11:48:07,943][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-05-07 11:48:07,943][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 11:48:07,943][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 11:48:07,943][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 11:48:07,943][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 11:48:07,943][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 11:48:07,943][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-05-07 11:48:07,943][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 11:48:07,943][src.data.datasets][INFO] - Sample label: 1.0
[2025-05-07 11:48:07,944][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 11:48:07,944][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 11:48:07,944][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 11:48:07,944][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 11:48:07,944][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 11:48:07,944][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-05-07 11:48:07,944][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 11:48:07,944][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-05-07 11:48:07,944][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 11:48:07,944][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 11:48:07,945][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 11:48:07,945][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 11:48:07,945][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 11:48:16,743][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 11:48:16,744][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 11:48:16,744][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-07 11:48:16,744][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 11:48:16,747][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 11:48:16,748][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 11:48:16,748][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 11:48:16,748][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 11:48:16,748][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 11:48:16,749][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 11:48:16,749][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.2458Epoch 1/15: [                              ] 2/75 batches, loss: 0.6229Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5032Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5020Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4883Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4414Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4340Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4448Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4174Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4282Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4302Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4160Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4117Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4026Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3969Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4035Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4060Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3989Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4031Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3933Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4120Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4113Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4038Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3982Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3918Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3860Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3844Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3817Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3778Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3751Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3675Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3645Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3605Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3592Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3561Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3550Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3494Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3481Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3465Epoch 1/15: [================              ] 40/75 batches, loss: 0.3437Epoch 1/15: [================              ] 41/75 batches, loss: 0.3401Epoch 1/15: [================              ] 42/75 batches, loss: 0.3393Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3358Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3389Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3421Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3372Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3348Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3303Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3284Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3268Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3238Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3203Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3190Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3175Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3155Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3148Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3145Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3146Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3141Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3108Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3078Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3053Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3020Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3015Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2990Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2974Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2954Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2923Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2917Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2898Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2888Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2862Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2838Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2816Epoch 1/15: [==============================] 75/75 batches, loss: 0.2797
[2025-05-07 11:48:25,528][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2797
[2025-05-07 11:48:25,843][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0678, Metrics: {'mse': 0.0676092579960823, 'rmse': 0.260017803229091, 'r2': -0.031249165534973145}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1996Epoch 2/15: [                              ] 2/75 batches, loss: 0.1345Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1333Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1510Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1515Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1331Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1572Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1545Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1484Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1444Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1409Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1389Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1370Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1358Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1457Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1455Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1411Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1379Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1348Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1333Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1328Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1348Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1351Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1354Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1349Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1353Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1355Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1352Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1363Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1376Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1354Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1361Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1366Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1401Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1399Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1394Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1389Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1372Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1386Epoch 2/15: [================              ] 40/75 batches, loss: 0.1375Epoch 2/15: [================              ] 41/75 batches, loss: 0.1365Epoch 2/15: [================              ] 42/75 batches, loss: 0.1352Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1334Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1321Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1322Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1337Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1325Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1325Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1337Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1323Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1343Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1343Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1348Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1341Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1332Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1363Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1344Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1335Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1335Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1329Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1321Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1316Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1304Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1299Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1289Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1287Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1278Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1275Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1267Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1264Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1261Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1255Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1256Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1251Epoch 2/15: [==============================] 75/75 batches, loss: 0.1247
[2025-05-07 11:48:28,606][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1247
[2025-05-07 11:48:28,828][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1058, Metrics: {'mse': 0.10574731230735779, 'rmse': 0.32518811833669103, 'r2': -0.6129719018936157}
[2025-05-07 11:48:28,829][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1342Epoch 3/15: [                              ] 2/75 batches, loss: 0.1234Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1151Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1008Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1001Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1000Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0970Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0900Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0838Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0840Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0863Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0832Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0917Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0941Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0943Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0930Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0926Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0911Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0929Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0922Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0927Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0922Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0919Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0970Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0970Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0963Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0950Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0943Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0956Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0943Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0933Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0932Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0933Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0925Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0923Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0920Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0908Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0915Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0901Epoch 3/15: [================              ] 40/75 batches, loss: 0.0896Epoch 3/15: [================              ] 41/75 batches, loss: 0.0901Epoch 3/15: [================              ] 42/75 batches, loss: 0.0897Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0891Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0885Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0875Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0866Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0866Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0880Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0886Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0875Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0877Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0881Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0875Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0879Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0871Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0871Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0860Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0861Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0855Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0851Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0848Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0847Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0850Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0844Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0838Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0834Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0832Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0831Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0834Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0837Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0835Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0831Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0826Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0823Epoch 3/15: [==============================] 75/75 batches, loss: 0.0826
[2025-05-07 11:48:31,181][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0826
[2025-05-07 11:48:31,418][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0787, Metrics: {'mse': 0.0786125436425209, 'rmse': 0.2803792853306408, 'r2': -0.19908320903778076}
[2025-05-07 11:48:31,418][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0430Epoch 4/15: [                              ] 2/75 batches, loss: 0.0482Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0467Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0506Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0511Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0534Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0579Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0564Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0595Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0572Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0595Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0589Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0584Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0563Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0590Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0591Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0605Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0607Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0628Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0634Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0627Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0646Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0639Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0658Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0666Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0672Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0665Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0662Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0659Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0658Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0649Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0663Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0662Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0672Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0668Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0667Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0662Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0666Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0658Epoch 4/15: [================              ] 40/75 batches, loss: 0.0653Epoch 4/15: [================              ] 41/75 batches, loss: 0.0653Epoch 4/15: [================              ] 42/75 batches, loss: 0.0651Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0653Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0644Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0644Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0646Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0653Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0650Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0652Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0649Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0654Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0661Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0658Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0659Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0655Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0656Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0651Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0648Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0648Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0648Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0651Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0645Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0649Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0647Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0647Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0646Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0645Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0643Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0643Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0638Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0639Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0638Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0642Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0640Epoch 4/15: [==============================] 75/75 batches, loss: 0.0640
[2025-05-07 11:48:33,729][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0640
[2025-05-07 11:48:33,950][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0688, Metrics: {'mse': 0.06879040598869324, 'rmse': 0.2622792519218652, 'r2': -0.049265265464782715}
[2025-05-07 11:48:33,951][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0805Epoch 5/15: [                              ] 2/75 batches, loss: 0.0560Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0556Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0578Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0562Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0623Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0635Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0647Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0620Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0580Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0560Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0563Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0563Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0571Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0576Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0578Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0570Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0577Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0581Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0577Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0600Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0602Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0603Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0591Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0598Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0595Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0587Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0591Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0594Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0588Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0581Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0588Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0579Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0579Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0592Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0591Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0589Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0590Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0586Epoch 5/15: [================              ] 40/75 batches, loss: 0.0594Epoch 5/15: [================              ] 41/75 batches, loss: 0.0590Epoch 5/15: [================              ] 42/75 batches, loss: 0.0589Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0593Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0588Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0587Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0588Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0584Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0583Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0589Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0586Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0577Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0571Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0567Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0568Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0565Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0559Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0556Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0557Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0555Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0551Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0550Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0550Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0545Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0547Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0546Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0546Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0548Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0547Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0546Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0544Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0543Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0545Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0544Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0542Epoch 5/15: [==============================] 75/75 batches, loss: 0.0542
[2025-05-07 11:48:36,271][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0542
[2025-05-07 11:48:36,569][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0611, Metrics: {'mse': 0.06103747710585594, 'rmse': 0.24705763923800442, 'r2': 0.06899070739746094}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0241Epoch 6/15: [                              ] 2/75 batches, loss: 0.0440Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0494Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0482Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0502Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0498Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0512Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0538Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0505Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0538Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0515Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0504Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0514Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0500Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0505Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0514Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0500Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0525Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0515Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0519Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0514Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0518Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0525Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0525Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0529Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0539Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0539Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0532Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0527Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0530Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0526Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0518Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0512Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0513Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0509Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0502Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0497Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0488Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0488Epoch 6/15: [================              ] 40/75 batches, loss: 0.0486Epoch 6/15: [================              ] 41/75 batches, loss: 0.0490Epoch 6/15: [================              ] 42/75 batches, loss: 0.0494Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0494Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0509Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0505Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0503Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0505Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0503Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0504Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0503Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0502Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0504Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0502Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0497Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0495Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0491Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0490Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0491Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0489Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0486Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0490Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0490Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0492Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0496Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0497Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0494Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0493Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0495Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0496Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0491Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0491Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0488Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0485Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0486Epoch 6/15: [==============================] 75/75 batches, loss: 0.0483
[2025-05-07 11:48:39,362][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0483
[2025-05-07 11:48:39,632][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0737, Metrics: {'mse': 0.07372474670410156, 'rmse': 0.2715230132126954, 'r2': -0.12452924251556396}
[2025-05-07 11:48:39,633][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0595Epoch 7/15: [                              ] 2/75 batches, loss: 0.0425Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0417Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0466Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0414Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0434Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0407Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0425Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0438Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0441Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0432Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0449Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0458Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0450Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0438Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0441Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0445Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0437Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0427Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0424Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0434Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0426Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0435Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0430Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0423Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0423Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0417Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0421Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0421Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0409Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0407Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0407Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0407Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0410Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0403Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0397Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0404Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0405Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0402Epoch 7/15: [================              ] 40/75 batches, loss: 0.0399Epoch 7/15: [================              ] 41/75 batches, loss: 0.0401Epoch 7/15: [================              ] 42/75 batches, loss: 0.0399Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0404Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0403Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0402Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0404Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0406Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0402Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0404Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0401Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0400Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0403Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0400Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0401Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0402Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0404Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0408Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0411Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0410Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0412Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0414Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0414Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0413Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0412Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0412Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0410Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0406Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0405Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0404Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0404Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0403Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0403Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0402Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0400Epoch 7/15: [==============================] 75/75 batches, loss: 0.0402
[2025-05-07 11:48:41,947][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0402
[2025-05-07 11:48:42,170][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0717, Metrics: {'mse': 0.07172590494155884, 'rmse': 0.2678169243000876, 'r2': -0.09404075145721436}
[2025-05-07 11:48:42,170][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0379Epoch 8/15: [                              ] 2/75 batches, loss: 0.0310Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0304Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0353Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0346Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0400Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0361Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0349Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0348Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0401Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0406Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0399Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0421Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0417Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0417Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0409Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0402Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0405Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0405Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0409Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0398Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0405Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0405Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0405Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0401Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0401Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0394Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0390Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0395Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0398Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0399Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0395Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0397Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0394Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0389Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0385Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0381Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0375Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0373Epoch 8/15: [================              ] 40/75 batches, loss: 0.0376Epoch 8/15: [================              ] 41/75 batches, loss: 0.0375Epoch 8/15: [================              ] 42/75 batches, loss: 0.0377Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0382Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0380Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0380Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0376Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0379Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0378Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0375Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0372Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0372Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0375Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0372Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0369Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0369Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0366Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0367Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0365Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0370Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0369Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0368Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0372Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0369Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0373Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0372Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0369Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0370Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0367Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0367Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0364Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0367Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0369Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0369Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0369Epoch 8/15: [==============================] 75/75 batches, loss: 0.0375
[2025-05-07 11:48:44,432][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0375
[2025-05-07 11:48:44,657][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0732, Metrics: {'mse': 0.07316634804010391, 'rmse': 0.2704927874086552, 'r2': -0.1160118579864502}
[2025-05-07 11:48:44,657][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0468Epoch 9/15: [                              ] 2/75 batches, loss: 0.0526Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0608Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0591Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0537Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0493Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0468Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0443Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0440Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0429Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0418Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0400Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0380Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0363Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0354Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0356Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0362Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0356Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0351Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0356Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0359Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0357Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0358Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0363Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0363Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0362Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0360Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0362Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0363Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0358Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0356Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0353Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0356Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0356Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0350Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0351Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0360Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0360Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0356Epoch 9/15: [================              ] 40/75 batches, loss: 0.0358Epoch 9/15: [================              ] 41/75 batches, loss: 0.0355Epoch 9/15: [================              ] 42/75 batches, loss: 0.0352Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0348Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0346Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0346Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0352Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0352Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0352Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0354Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0354Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0354Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0353Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0351Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0349Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0347Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0348Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0346Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0349Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0351Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0350Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0348Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0348Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0348Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0350Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0348Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0348Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0348Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0347Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0347Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0346Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0344Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0344Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0341Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0338Epoch 9/15: [==============================] 75/75 batches, loss: 0.0338
[2025-05-07 11:48:46,918][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0338
[2025-05-07 11:48:47,158][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0644, Metrics: {'mse': 0.06438174843788147, 'rmse': 0.25373558764564635, 'r2': 0.017980217933654785}
[2025-05-07 11:48:47,159][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 11:48:47,159][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 9
[2025-05-07 11:48:47,159][src.training.lm_trainer][INFO] - Training completed in 24.70 seconds
[2025-05-07 11:48:47,160][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 11:48:50,115][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.01297011412680149, 'rmse': 0.11388640887657092, 'r2': 0.3581708073616028}
[2025-05-07 11:48:50,115][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06103747710585594, 'rmse': 0.24705763923800442, 'r2': 0.06899070739746094}
[2025-05-07 11:48:50,115][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.02697366289794445, 'rmse': 0.16423660644918492, 'r2': 0.31689774990081787}
[2025-05-07 11:48:51,752][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer6/fi/fi/model.pt
[2025-05-07 11:48:51,753][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▁▄▅▆▅▅▅
wandb:       train_loss █▄▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▂█▄▂▁▃▃▃▂
wandb:          val_mse ▂█▄▂▁▃▃▃▂
wandb:           val_r2 ▇▁▅▇█▆▆▆▇
wandb:         val_rmse ▂█▄▂▁▃▃▃▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06112
wandb:     best_val_mse 0.06104
wandb:      best_val_r2 0.06899
wandb:    best_val_rmse 0.24706
wandb: early_stop_epoch 9
wandb:            epoch 9
wandb:   final_test_mse 0.02697
wandb:    final_test_r2 0.3169
wandb:  final_test_rmse 0.16424
wandb:  final_train_mse 0.01297
wandb:   final_train_r2 0.35817
wandb: final_train_rmse 0.11389
wandb:    final_val_mse 0.06104
wandb:     final_val_r2 0.06899
wandb:   final_val_rmse 0.24706
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03385
wandb:       train_time 24.69662
wandb:         val_loss 0.06441
wandb:          val_mse 0.06438
wandb:           val_r2 0.01798
wandb:         val_rmse 0.25374
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_114759-0xgsudyi
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_114759-0xgsudyi/logs
Experiment probe_layer6_complexity_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer6/fi/fi/results.json for layer 6
Running experiment: probe_layer6_question_type_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_question_type_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer6/ja"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 11:49:18,785][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer6/ja
experiment_name: probe_layer6_question_type_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-07 11:49:18,785][__main__][INFO] - Normalized task: question_type
[2025-05-07 11:49:18,786][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-07 11:49:18,786][__main__][INFO] - Determined Task Type: classification
[2025-05-07 11:49:18,790][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ja']
[2025-05-07 11:49:18,790][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 11:49:21,587][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 11:49:23,847][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 11:49:23,848][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:49:23,991][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:49:24,076][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:49:24,258][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 11:49:24,267][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:49:24,267][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 11:49:24,268][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:49:24,346][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:49:24,404][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:49:24,418][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 11:49:24,419][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:49:24,419][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 11:49:24,420][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:49:24,504][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:49:24,590][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:49:24,616][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 11:49:24,617][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:49:24,617][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 11:49:24,620][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 11:49:24,620][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:49:24,621][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:49:24,621][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:49:24,621][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:49:24,621][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-05-07 11:49:24,621][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-05-07 11:49:24,621][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 11:49:24,621][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 11:49:24,621][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:49:24,621][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:49:24,622][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:49:24,622][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:49:24,622][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-05-07 11:49:24,622][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-05-07 11:49:24,622][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 11:49:24,622][src.data.datasets][INFO] - Sample label: 0
[2025-05-07 11:49:24,622][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:49:24,622][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:49:24,622][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:49:24,622][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:49:24,623][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-05-07 11:49:24,623][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-05-07 11:49:24,623][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 11:49:24,623][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 11:49:24,623][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 11:49:24,623][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 11:49:24,623][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 11:49:24,623][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-07 11:49:24,624][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 11:49:31,316][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 11:49:31,317][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 11:49:31,317][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-07 11:49:31,317][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-07 11:49:31,323][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-07 11:49:31,323][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-07 11:49:31,324][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-07 11:49:31,324][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-07 11:49:31,324][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 11:49:31,325][__main__][INFO] - Total parameters: 394,568,839
[2025-05-07 11:49:31,325][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-07 11:49:31,326][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6992Epoch 1/15: [                              ] 2/75 batches, loss: 0.7216Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7159Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7037Epoch 1/15: [==                            ] 5/75 batches, loss: 0.6966Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6995Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6990Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6971Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6973Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6950Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6943Epoch 1/15: [====                          ] 12/75 batches, loss: 0.6933Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.6936Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.6936Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6924Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6910Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6916Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6910Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6906Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6892Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6875Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6852Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6866Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6842Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6855Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6858Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6840Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6878Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6873Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6864Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6856Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6844Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6832Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6832Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6820Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6806Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6793Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6777Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6778Epoch 1/15: [================              ] 40/75 batches, loss: 0.6776Epoch 1/15: [================              ] 41/75 batches, loss: 0.6764Epoch 1/15: [================              ] 42/75 batches, loss: 0.6765Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6752Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6745Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6737Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6737Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6718Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6695Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6685Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6684Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6674Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6667Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6659Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6644Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6625Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6609Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6610Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6609Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6594Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6588Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6565Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6557Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6559Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6552Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6524Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6528Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6527Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6523Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6513Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6509Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6494Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6491Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6487Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6471Epoch 1/15: [==============================] 75/75 batches, loss: 0.6479
[2025-05-07 11:49:37,694][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6479
[2025-05-07 11:49:37,910][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.5646, Metrics: {'accuracy': 0.9347826086956522, 'f1': 0.9387755102040817, 'precision': 0.92, 'recall': 0.9583333333333334}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.5102Epoch 2/15: [                              ] 2/75 batches, loss: 0.5409Epoch 2/15: [=                             ] 3/75 batches, loss: 0.5651Epoch 2/15: [=                             ] 4/75 batches, loss: 0.5547Epoch 2/15: [==                            ] 5/75 batches, loss: 0.5511Epoch 2/15: [==                            ] 6/75 batches, loss: 0.5658Epoch 2/15: [==                            ] 7/75 batches, loss: 0.5706Epoch 2/15: [===                           ] 8/75 batches, loss: 0.5689Epoch 2/15: [===                           ] 9/75 batches, loss: 0.5539Epoch 2/15: [====                          ] 10/75 batches, loss: 0.5560Epoch 2/15: [====                          ] 11/75 batches, loss: 0.5562Epoch 2/15: [====                          ] 12/75 batches, loss: 0.5557Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.5527Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.5550Epoch 2/15: [======                        ] 15/75 batches, loss: 0.5560Epoch 2/15: [======                        ] 16/75 batches, loss: 0.5504Epoch 2/15: [======                        ] 17/75 batches, loss: 0.5503Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.5516Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.5482Epoch 2/15: [========                      ] 20/75 batches, loss: 0.5525Epoch 2/15: [========                      ] 21/75 batches, loss: 0.5531Epoch 2/15: [========                      ] 22/75 batches, loss: 0.5515Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.5509Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.5529Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.5508Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.5513Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.5494Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.5522Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.5493Epoch 2/15: [============                  ] 30/75 batches, loss: 0.5473Epoch 2/15: [============                  ] 31/75 batches, loss: 0.5470Epoch 2/15: [============                  ] 32/75 batches, loss: 0.5488Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.5493Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.5478Epoch 2/15: [==============                ] 35/75 batches, loss: 0.5487Epoch 2/15: [==============                ] 36/75 batches, loss: 0.5486Epoch 2/15: [==============                ] 37/75 batches, loss: 0.5492Epoch 2/15: [===============               ] 38/75 batches, loss: 0.5488Epoch 2/15: [===============               ] 39/75 batches, loss: 0.5479Epoch 2/15: [================              ] 40/75 batches, loss: 0.5481Epoch 2/15: [================              ] 41/75 batches, loss: 0.5484Epoch 2/15: [================              ] 42/75 batches, loss: 0.5510Epoch 2/15: [=================             ] 43/75 batches, loss: 0.5493Epoch 2/15: [=================             ] 44/75 batches, loss: 0.5496Epoch 2/15: [==================            ] 45/75 batches, loss: 0.5513Epoch 2/15: [==================            ] 46/75 batches, loss: 0.5490Epoch 2/15: [==================            ] 47/75 batches, loss: 0.5492Epoch 2/15: [===================           ] 48/75 batches, loss: 0.5501Epoch 2/15: [===================           ] 49/75 batches, loss: 0.5504Epoch 2/15: [====================          ] 50/75 batches, loss: 0.5503Epoch 2/15: [====================          ] 51/75 batches, loss: 0.5496Epoch 2/15: [====================          ] 52/75 batches, loss: 0.5493Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.5499Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.5506Epoch 2/15: [======================        ] 55/75 batches, loss: 0.5501Epoch 2/15: [======================        ] 56/75 batches, loss: 0.5496Epoch 2/15: [======================        ] 57/75 batches, loss: 0.5485Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.5489Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.5485Epoch 2/15: [========================      ] 60/75 batches, loss: 0.5488Epoch 2/15: [========================      ] 61/75 batches, loss: 0.5498Epoch 2/15: [========================      ] 62/75 batches, loss: 0.5507Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.5512Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.5506Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.5497Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.5491Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.5485Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.5488Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.5482Epoch 2/15: [============================  ] 70/75 batches, loss: 0.5484Epoch 2/15: [============================  ] 71/75 batches, loss: 0.5479Epoch 2/15: [============================  ] 72/75 batches, loss: 0.5478Epoch 2/15: [============================= ] 73/75 batches, loss: 0.5485Epoch 2/15: [============================= ] 74/75 batches, loss: 0.5474Epoch 2/15: [==============================] 75/75 batches, loss: 0.5460
[2025-05-07 11:49:40,577][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.5460
[2025-05-07 11:49:40,777][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.5384, Metrics: {'accuracy': 0.9347826086956522, 'f1': 0.9411764705882353, 'precision': 0.8888888888888888, 'recall': 1.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.5262Epoch 3/15: [                              ] 2/75 batches, loss: 0.5277Epoch 3/15: [=                             ] 3/75 batches, loss: 0.5156Epoch 3/15: [=                             ] 4/75 batches, loss: 0.5157Epoch 3/15: [==                            ] 5/75 batches, loss: 0.5131Epoch 3/15: [==                            ] 6/75 batches, loss: 0.5141Epoch 3/15: [==                            ] 7/75 batches, loss: 0.5147Epoch 3/15: [===                           ] 8/75 batches, loss: 0.5094Epoch 3/15: [===                           ] 9/75 batches, loss: 0.5091Epoch 3/15: [====                          ] 10/75 batches, loss: 0.5005Epoch 3/15: [====                          ] 11/75 batches, loss: 0.5029Epoch 3/15: [====                          ] 12/75 batches, loss: 0.5038Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.5007Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.4999Epoch 3/15: [======                        ] 15/75 batches, loss: 0.4979Epoch 3/15: [======                        ] 16/75 batches, loss: 0.4971Epoch 3/15: [======                        ] 17/75 batches, loss: 0.4976Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.4963Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.4958Epoch 3/15: [========                      ] 20/75 batches, loss: 0.5030Epoch 3/15: [========                      ] 21/75 batches, loss: 0.5010Epoch 3/15: [========                      ] 22/75 batches, loss: 0.5006Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.5001Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.4985Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.5010Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.5020Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.5024Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.5025Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.5014Epoch 3/15: [============                  ] 30/75 batches, loss: 0.5038Epoch 3/15: [============                  ] 31/75 batches, loss: 0.5032Epoch 3/15: [============                  ] 32/75 batches, loss: 0.5030Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.5010Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.5035Epoch 3/15: [==============                ] 35/75 batches, loss: 0.5040Epoch 3/15: [==============                ] 36/75 batches, loss: 0.5061Epoch 3/15: [==============                ] 37/75 batches, loss: 0.5074Epoch 3/15: [===============               ] 38/75 batches, loss: 0.5077Epoch 3/15: [===============               ] 39/75 batches, loss: 0.5090Epoch 3/15: [================              ] 40/75 batches, loss: 0.5101Epoch 3/15: [================              ] 41/75 batches, loss: 0.5112Epoch 3/15: [================              ] 42/75 batches, loss: 0.5128Epoch 3/15: [=================             ] 43/75 batches, loss: 0.5144Epoch 3/15: [=================             ] 44/75 batches, loss: 0.5154Epoch 3/15: [==================            ] 45/75 batches, loss: 0.5167Epoch 3/15: [==================            ] 46/75 batches, loss: 0.5186Epoch 3/15: [==================            ] 47/75 batches, loss: 0.5174Epoch 3/15: [===================           ] 48/75 batches, loss: 0.5187Epoch 3/15: [===================           ] 49/75 batches, loss: 0.5200Epoch 3/15: [====================          ] 50/75 batches, loss: 0.5203Epoch 3/15: [====================          ] 51/75 batches, loss: 0.5201Epoch 3/15: [====================          ] 52/75 batches, loss: 0.5207Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.5196Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.5204Epoch 3/15: [======================        ] 55/75 batches, loss: 0.5211Epoch 3/15: [======================        ] 56/75 batches, loss: 0.5204Epoch 3/15: [======================        ] 57/75 batches, loss: 0.5199Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.5200Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.5209Epoch 3/15: [========================      ] 60/75 batches, loss: 0.5208Epoch 3/15: [========================      ] 61/75 batches, loss: 0.5214Epoch 3/15: [========================      ] 62/75 batches, loss: 0.5210Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.5214Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.5209Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.5216Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.5222Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.5226Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.5235Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.5223Epoch 3/15: [============================  ] 70/75 batches, loss: 0.5221Epoch 3/15: [============================  ] 71/75 batches, loss: 0.5220Epoch 3/15: [============================  ] 72/75 batches, loss: 0.5222Epoch 3/15: [============================= ] 73/75 batches, loss: 0.5221Epoch 3/15: [============================= ] 74/75 batches, loss: 0.5223Epoch 3/15: [==============================] 75/75 batches, loss: 0.5246
[2025-05-07 11:49:43,581][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.5246
[2025-05-07 11:49:43,862][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.5103, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6038Epoch 4/15: [                              ] 2/75 batches, loss: 0.5797Epoch 4/15: [=                             ] 3/75 batches, loss: 0.5735Epoch 4/15: [=                             ] 4/75 batches, loss: 0.5481Epoch 4/15: [==                            ] 5/75 batches, loss: 0.5572Epoch 4/15: [==                            ] 6/75 batches, loss: 0.5564Epoch 4/15: [==                            ] 7/75 batches, loss: 0.5526Epoch 4/15: [===                           ] 8/75 batches, loss: 0.5487Epoch 4/15: [===                           ] 9/75 batches, loss: 0.5439Epoch 4/15: [====                          ] 10/75 batches, loss: 0.5401Epoch 4/15: [====                          ] 11/75 batches, loss: 0.5293Epoch 4/15: [====                          ] 12/75 batches, loss: 0.5235Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.5167Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.5111Epoch 4/15: [======                        ] 15/75 batches, loss: 0.5150Epoch 4/15: [======                        ] 16/75 batches, loss: 0.5156Epoch 4/15: [======                        ] 17/75 batches, loss: 0.5151Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.5163Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.5176Epoch 4/15: [========                      ] 20/75 batches, loss: 0.5181Epoch 4/15: [========                      ] 21/75 batches, loss: 0.5210Epoch 4/15: [========                      ] 22/75 batches, loss: 0.5245Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.5227Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.5217Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.5176Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.5172Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.5176Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.5183Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.5211Epoch 4/15: [============                  ] 30/75 batches, loss: 0.5226Epoch 4/15: [============                  ] 31/75 batches, loss: 0.5214Epoch 4/15: [============                  ] 32/75 batches, loss: 0.5207Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.5174Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.5174Epoch 4/15: [==============                ] 35/75 batches, loss: 0.5161Epoch 4/15: [==============                ] 36/75 batches, loss: 0.5177Epoch 4/15: [==============                ] 37/75 batches, loss: 0.5176Epoch 4/15: [===============               ] 38/75 batches, loss: 0.5206Epoch 4/15: [===============               ] 39/75 batches, loss: 0.5220Epoch 4/15: [================              ] 40/75 batches, loss: 0.5212Epoch 4/15: [================              ] 41/75 batches, loss: 0.5214Epoch 4/15: [================              ] 42/75 batches, loss: 0.5200Epoch 4/15: [=================             ] 43/75 batches, loss: 0.5196Epoch 4/15: [=================             ] 44/75 batches, loss: 0.5166Epoch 4/15: [==================            ] 45/75 batches, loss: 0.5139Epoch 4/15: [==================            ] 46/75 batches, loss: 0.5143Epoch 4/15: [==================            ] 47/75 batches, loss: 0.5139Epoch 4/15: [===================           ] 48/75 batches, loss: 0.5147Epoch 4/15: [===================           ] 49/75 batches, loss: 0.5155Epoch 4/15: [====================          ] 50/75 batches, loss: 0.5159Epoch 4/15: [====================          ] 51/75 batches, loss: 0.5162Epoch 4/15: [====================          ] 52/75 batches, loss: 0.5170Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.5177Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.5166Epoch 4/15: [======================        ] 55/75 batches, loss: 0.5161Epoch 4/15: [======================        ] 56/75 batches, loss: 0.5172Epoch 4/15: [======================        ] 57/75 batches, loss: 0.5179Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.5182Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.5183Epoch 4/15: [========================      ] 60/75 batches, loss: 0.5176Epoch 4/15: [========================      ] 61/75 batches, loss: 0.5179Epoch 4/15: [========================      ] 62/75 batches, loss: 0.5191Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.5190Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.5196Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.5186Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.5175Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.5167Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.5174Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.5187Epoch 4/15: [============================  ] 70/75 batches, loss: 0.5183Epoch 4/15: [============================  ] 71/75 batches, loss: 0.5191Epoch 4/15: [============================  ] 72/75 batches, loss: 0.5192Epoch 4/15: [============================= ] 73/75 batches, loss: 0.5194Epoch 4/15: [============================= ] 74/75 batches, loss: 0.5176Epoch 4/15: [==============================] 75/75 batches, loss: 0.5185
[2025-05-07 11:49:46,586][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5185
[2025-05-07 11:49:46,862][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5101, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.4845Epoch 5/15: [                              ] 2/75 batches, loss: 0.5062Epoch 5/15: [=                             ] 3/75 batches, loss: 0.4988Epoch 5/15: [=                             ] 4/75 batches, loss: 0.4898Epoch 5/15: [==                            ] 5/75 batches, loss: 0.5021Epoch 5/15: [==                            ] 6/75 batches, loss: 0.4988Epoch 5/15: [==                            ] 7/75 batches, loss: 0.5030Epoch 5/15: [===                           ] 8/75 batches, loss: 0.5092Epoch 5/15: [===                           ] 9/75 batches, loss: 0.5171Epoch 5/15: [====                          ] 10/75 batches, loss: 0.5113Epoch 5/15: [====                          ] 11/75 batches, loss: 0.5109Epoch 5/15: [====                          ] 12/75 batches, loss: 0.5100Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.5141Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.5122Epoch 5/15: [======                        ] 15/75 batches, loss: 0.5135Epoch 5/15: [======                        ] 16/75 batches, loss: 0.5134Epoch 5/15: [======                        ] 17/75 batches, loss: 0.5114Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.5136Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.5172Epoch 5/15: [========                      ] 20/75 batches, loss: 0.5155Epoch 5/15: [========                      ] 21/75 batches, loss: 0.5130Epoch 5/15: [========                      ] 22/75 batches, loss: 0.5139Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.5155Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.5141Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.5130Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.5136Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.5124Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.5140Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.5140Epoch 5/15: [============                  ] 30/75 batches, loss: 0.5166Epoch 5/15: [============                  ] 31/75 batches, loss: 0.5152Epoch 5/15: [============                  ] 32/75 batches, loss: 0.5141Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.5147Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.5158Epoch 5/15: [==============                ] 35/75 batches, loss: 0.5161Epoch 5/15: [==============                ] 36/75 batches, loss: 0.5134Epoch 5/15: [==============                ] 37/75 batches, loss: 0.5125Epoch 5/15: [===============               ] 38/75 batches, loss: 0.5131Epoch 5/15: [===============               ] 39/75 batches, loss: 0.5129Epoch 5/15: [================              ] 40/75 batches, loss: 0.5136Epoch 5/15: [================              ] 41/75 batches, loss: 0.5129Epoch 5/15: [================              ] 42/75 batches, loss: 0.5140Epoch 5/15: [=================             ] 43/75 batches, loss: 0.5138Epoch 5/15: [=================             ] 44/75 batches, loss: 0.5137Epoch 5/15: [==================            ] 45/75 batches, loss: 0.5135Epoch 5/15: [==================            ] 46/75 batches, loss: 0.5136Epoch 5/15: [==================            ] 47/75 batches, loss: 0.5139Epoch 5/15: [===================           ] 48/75 batches, loss: 0.5137Epoch 5/15: [===================           ] 49/75 batches, loss: 0.5138Epoch 5/15: [====================          ] 50/75 batches, loss: 0.5131Epoch 5/15: [====================          ] 51/75 batches, loss: 0.5119Epoch 5/15: [====================          ] 52/75 batches, loss: 0.5124Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.5109Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.5103Epoch 5/15: [======================        ] 55/75 batches, loss: 0.5103Epoch 5/15: [======================        ] 56/75 batches, loss: 0.5107Epoch 5/15: [======================        ] 57/75 batches, loss: 0.5107Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.5109Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.5096Epoch 5/15: [========================      ] 60/75 batches, loss: 0.5096Epoch 5/15: [========================      ] 61/75 batches, loss: 0.5082Epoch 5/15: [========================      ] 62/75 batches, loss: 0.5089Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.5089Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.5082Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.5099Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.5092Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.5105Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.5102Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.5101Epoch 5/15: [============================  ] 70/75 batches, loss: 0.5111Epoch 5/15: [============================  ] 71/75 batches, loss: 0.5127Epoch 5/15: [============================  ] 72/75 batches, loss: 0.5139Epoch 5/15: [============================= ] 73/75 batches, loss: 0.5138Epoch 5/15: [============================= ] 74/75 batches, loss: 0.5132Epoch 5/15: [==============================] 75/75 batches, loss: 0.5134
[2025-05-07 11:49:49,552][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5134
[2025-05-07 11:49:50,030][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5109, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9795918367346939, 'precision': 0.96, 'recall': 1.0}
[2025-05-07 11:49:50,031][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.5994Epoch 6/15: [                              ] 2/75 batches, loss: 0.5880Epoch 6/15: [=                             ] 3/75 batches, loss: 0.5678Epoch 6/15: [=                             ] 4/75 batches, loss: 0.5767Epoch 6/15: [==                            ] 5/75 batches, loss: 0.5625Epoch 6/15: [==                            ] 6/75 batches, loss: 0.5414Epoch 6/15: [==                            ] 7/75 batches, loss: 0.5305Epoch 6/15: [===                           ] 8/75 batches, loss: 0.5248Epoch 6/15: [===                           ] 9/75 batches, loss: 0.5251Epoch 6/15: [====                          ] 10/75 batches, loss: 0.5267Epoch 6/15: [====                          ] 11/75 batches, loss: 0.5257Epoch 6/15: [====                          ] 12/75 batches, loss: 0.5219Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.5223Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.5215Epoch 6/15: [======                        ] 15/75 batches, loss: 0.5199Epoch 6/15: [======                        ] 16/75 batches, loss: 0.5204Epoch 6/15: [======                        ] 17/75 batches, loss: 0.5181Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.5174Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.5135Epoch 6/15: [========                      ] 20/75 batches, loss: 0.5104Epoch 6/15: [========                      ] 21/75 batches, loss: 0.5126Epoch 6/15: [========                      ] 22/75 batches, loss: 0.5135Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.5141Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.5138Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.5125Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.5115Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.5147Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.5157Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.5137Epoch 6/15: [============                  ] 30/75 batches, loss: 0.5125Epoch 6/15: [============                  ] 31/75 batches, loss: 0.5144Epoch 6/15: [============                  ] 32/75 batches, loss: 0.5157Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.5154Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.5159Epoch 6/15: [==============                ] 35/75 batches, loss: 0.5158Epoch 6/15: [==============                ] 36/75 batches, loss: 0.5168Epoch 6/15: [==============                ] 37/75 batches, loss: 0.5172Epoch 6/15: [===============               ] 38/75 batches, loss: 0.5163Epoch 6/15: [===============               ] 39/75 batches, loss: 0.5179Epoch 6/15: [================              ] 40/75 batches, loss: 0.5189Epoch 6/15: [================              ] 41/75 batches, loss: 0.5174Epoch 6/15: [================              ] 42/75 batches, loss: 0.5188Epoch 6/15: [=================             ] 43/75 batches, loss: 0.5190Epoch 6/15: [=================             ] 44/75 batches, loss: 0.5171Epoch 6/15: [==================            ] 45/75 batches, loss: 0.5164Epoch 6/15: [==================            ] 46/75 batches, loss: 0.5172Epoch 6/15: [==================            ] 47/75 batches, loss: 0.5169Epoch 6/15: [===================           ] 48/75 batches, loss: 0.5165Epoch 6/15: [===================           ] 49/75 batches, loss: 0.5166Epoch 6/15: [====================          ] 50/75 batches, loss: 0.5160Epoch 6/15: [====================          ] 51/75 batches, loss: 0.5144Epoch 6/15: [====================          ] 52/75 batches, loss: 0.5133Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.5118Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.5110Epoch 6/15: [======================        ] 55/75 batches, loss: 0.5111Epoch 6/15: [======================        ] 56/75 batches, loss: 0.5105Epoch 6/15: [======================        ] 57/75 batches, loss: 0.5108Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.5116Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.5103Epoch 6/15: [========================      ] 60/75 batches, loss: 0.5098Epoch 6/15: [========================      ] 61/75 batches, loss: 0.5101Epoch 6/15: [========================      ] 62/75 batches, loss: 0.5101Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.5100Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.5096Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.5102Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.5105Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.5100Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.5117Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.5106Epoch 6/15: [============================  ] 70/75 batches, loss: 0.5095Epoch 6/15: [============================  ] 71/75 batches, loss: 0.5095Epoch 6/15: [============================  ] 72/75 batches, loss: 0.5114Epoch 6/15: [============================= ] 73/75 batches, loss: 0.5110Epoch 6/15: [============================= ] 74/75 batches, loss: 0.5111Epoch 6/15: [==============================] 75/75 batches, loss: 0.5093
[2025-05-07 11:49:52,495][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5093
[2025-05-07 11:49:52,883][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5076, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.5047Epoch 7/15: [                              ] 2/75 batches, loss: 0.4686Epoch 7/15: [=                             ] 3/75 batches, loss: 0.4957Epoch 7/15: [=                             ] 4/75 batches, loss: 0.4804Epoch 7/15: [==                            ] 5/75 batches, loss: 0.4662Epoch 7/15: [==                            ] 6/75 batches, loss: 0.4645Epoch 7/15: [==                            ] 7/75 batches, loss: 0.4641Epoch 7/15: [===                           ] 8/75 batches, loss: 0.4690Epoch 7/15: [===                           ] 9/75 batches, loss: 0.4782Epoch 7/15: [====                          ] 10/75 batches, loss: 0.4764Epoch 7/15: [====                          ] 11/75 batches, loss: 0.4746Epoch 7/15: [====                          ] 12/75 batches, loss: 0.4774Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.4804Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.4860Epoch 7/15: [======                        ] 15/75 batches, loss: 0.4866Epoch 7/15: [======                        ] 16/75 batches, loss: 0.4852Epoch 7/15: [======                        ] 17/75 batches, loss: 0.4863Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.4860Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.4896Epoch 7/15: [========                      ] 20/75 batches, loss: 0.4939Epoch 7/15: [========                      ] 21/75 batches, loss: 0.4932Epoch 7/15: [========                      ] 22/75 batches, loss: 0.4938Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.4974Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.4967Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.4979Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.5012Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.5048Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.5058Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.5049Epoch 7/15: [============                  ] 30/75 batches, loss: 0.5035Epoch 7/15: [============                  ] 31/75 batches, loss: 0.5058Epoch 7/15: [============                  ] 32/75 batches, loss: 0.5058Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.5058Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.5079Epoch 7/15: [==============                ] 35/75 batches, loss: 0.5078Epoch 7/15: [==============                ] 36/75 batches, loss: 0.5070Epoch 7/15: [==============                ] 37/75 batches, loss: 0.5055Epoch 7/15: [===============               ] 38/75 batches, loss: 0.5076Epoch 7/15: [===============               ] 39/75 batches, loss: 0.5076Epoch 7/15: [================              ] 40/75 batches, loss: 0.5075Epoch 7/15: [================              ] 41/75 batches, loss: 0.5063Epoch 7/15: [================              ] 42/75 batches, loss: 0.5058Epoch 7/15: [=================             ] 43/75 batches, loss: 0.5070Epoch 7/15: [=================             ] 44/75 batches, loss: 0.5086Epoch 7/15: [==================            ] 45/75 batches, loss: 0.5086Epoch 7/15: [==================            ] 46/75 batches, loss: 0.5075Epoch 7/15: [==================            ] 47/75 batches, loss: 0.5075Epoch 7/15: [===================           ] 48/75 batches, loss: 0.5074Epoch 7/15: [===================           ] 49/75 batches, loss: 0.5094Epoch 7/15: [====================          ] 50/75 batches, loss: 0.5098Epoch 7/15: [====================          ] 51/75 batches, loss: 0.5104Epoch 7/15: [====================          ] 52/75 batches, loss: 0.5098Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.5093Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.5098Epoch 7/15: [======================        ] 55/75 batches, loss: 0.5103Epoch 7/15: [======================        ] 56/75 batches, loss: 0.5106Epoch 7/15: [======================        ] 57/75 batches, loss: 0.5106Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.5119Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.5114Epoch 7/15: [========================      ] 60/75 batches, loss: 0.5133Epoch 7/15: [========================      ] 61/75 batches, loss: 0.5126Epoch 7/15: [========================      ] 62/75 batches, loss: 0.5121Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.5139Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.5151Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.5163Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.5148Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.5133Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.5127Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.5122Epoch 7/15: [============================  ] 70/75 batches, loss: 0.5118Epoch 7/15: [============================  ] 71/75 batches, loss: 0.5117Epoch 7/15: [============================  ] 72/75 batches, loss: 0.5117Epoch 7/15: [============================= ] 73/75 batches, loss: 0.5114Epoch 7/15: [============================= ] 74/75 batches, loss: 0.5118Epoch 7/15: [==============================] 75/75 batches, loss: 0.5107
[2025-05-07 11:49:55,889][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5107
[2025-05-07 11:49:56,127][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5192, Metrics: {'accuracy': 0.9347826086956522, 'f1': 0.9411764705882353, 'precision': 0.8888888888888888, 'recall': 1.0}
[2025-05-07 11:49:56,128][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.5562Epoch 8/15: [                              ] 2/75 batches, loss: 0.5298Epoch 8/15: [=                             ] 3/75 batches, loss: 0.5540Epoch 8/15: [=                             ] 4/75 batches, loss: 0.5453Epoch 8/15: [==                            ] 5/75 batches, loss: 0.5552Epoch 8/15: [==                            ] 6/75 batches, loss: 0.5517Epoch 8/15: [==                            ] 7/75 batches, loss: 0.5419Epoch 8/15: [===                           ] 8/75 batches, loss: 0.5373Epoch 8/15: [===                           ] 9/75 batches, loss: 0.5235Epoch 8/15: [====                          ] 10/75 batches, loss: 0.5239Epoch 8/15: [====                          ] 11/75 batches, loss: 0.5248Epoch 8/15: [====                          ] 12/75 batches, loss: 0.5193Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.5161Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.5116Epoch 8/15: [======                        ] 15/75 batches, loss: 0.5114Epoch 8/15: [======                        ] 16/75 batches, loss: 0.5141Epoch 8/15: [======                        ] 17/75 batches, loss: 0.5135Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.5171Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.5126Epoch 8/15: [========                      ] 20/75 batches, loss: 0.5110Epoch 8/15: [========                      ] 21/75 batches, loss: 0.5118Epoch 8/15: [========                      ] 22/75 batches, loss: 0.5093Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.5083Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.5080Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.5082Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.5073Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.5090Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.5055Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.5046Epoch 8/15: [============                  ] 30/75 batches, loss: 0.5055Epoch 8/15: [============                  ] 31/75 batches, loss: 0.5077Epoch 8/15: [============                  ] 32/75 batches, loss: 0.5061Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.5082Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.5088Epoch 8/15: [==============                ] 35/75 batches, loss: 0.5097Epoch 8/15: [==============                ] 36/75 batches, loss: 0.5096Epoch 8/15: [==============                ] 37/75 batches, loss: 0.5069Epoch 8/15: [===============               ] 38/75 batches, loss: 0.5074Epoch 8/15: [===============               ] 39/75 batches, loss: 0.5055Epoch 8/15: [================              ] 40/75 batches, loss: 0.5055Epoch 8/15: [================              ] 41/75 batches, loss: 0.5055Epoch 8/15: [================              ] 42/75 batches, loss: 0.5071Epoch 8/15: [=================             ] 43/75 batches, loss: 0.5084Epoch 8/15: [=================             ] 44/75 batches, loss: 0.5083Epoch 8/15: [==================            ] 45/75 batches, loss: 0.5072Epoch 8/15: [==================            ] 46/75 batches, loss: 0.5062Epoch 8/15: [==================            ] 47/75 batches, loss: 0.5071Epoch 8/15: [===================           ] 48/75 batches, loss: 0.5086Epoch 8/15: [===================           ] 49/75 batches, loss: 0.5095Epoch 8/15: [====================          ] 50/75 batches, loss: 0.5091Epoch 8/15: [====================          ] 51/75 batches, loss: 0.5076Epoch 8/15: [====================          ] 52/75 batches, loss: 0.5089Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.5089Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.5097Epoch 8/15: [======================        ] 55/75 batches, loss: 0.5105Epoch 8/15: [======================        ] 56/75 batches, loss: 0.5099Epoch 8/15: [======================        ] 57/75 batches, loss: 0.5117Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.5109Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.5110Epoch 8/15: [========================      ] 60/75 batches, loss: 0.5106Epoch 8/15: [========================      ] 61/75 batches, loss: 0.5101Epoch 8/15: [========================      ] 62/75 batches, loss: 0.5108Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.5107Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.5102Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.5105Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.5090Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.5089Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.5092Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.5098Epoch 8/15: [============================  ] 70/75 batches, loss: 0.5094Epoch 8/15: [============================  ] 71/75 batches, loss: 0.5100Epoch 8/15: [============================  ] 72/75 batches, loss: 0.5103Epoch 8/15: [============================= ] 73/75 batches, loss: 0.5099Epoch 8/15: [============================= ] 74/75 batches, loss: 0.5099Epoch 8/15: [==============================] 75/75 batches, loss: 0.5103
[2025-05-07 11:49:58,402][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5103
[2025-05-07 11:49:58,715][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5058, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.5045Epoch 9/15: [                              ] 2/75 batches, loss: 0.5287Epoch 9/15: [=                             ] 3/75 batches, loss: 0.5361Epoch 9/15: [=                             ] 4/75 batches, loss: 0.5399Epoch 9/15: [==                            ] 5/75 batches, loss: 0.5185Epoch 9/15: [==                            ] 6/75 batches, loss: 0.5242Epoch 9/15: [==                            ] 7/75 batches, loss: 0.5246Epoch 9/15: [===                           ] 8/75 batches, loss: 0.5131Epoch 9/15: [===                           ] 9/75 batches, loss: 0.5200Epoch 9/15: [====                          ] 10/75 batches, loss: 0.5223Epoch 9/15: [====                          ] 11/75 batches, loss: 0.5098Epoch 9/15: [====                          ] 12/75 batches, loss: 0.5172Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.5144Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.5120Epoch 9/15: [======                        ] 15/75 batches, loss: 0.5114Epoch 9/15: [======                        ] 16/75 batches, loss: 0.5069Epoch 9/15: [======                        ] 17/75 batches, loss: 0.5067Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.5087Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.5085Epoch 9/15: [========                      ] 20/75 batches, loss: 0.5107Epoch 9/15: [========                      ] 21/75 batches, loss: 0.5115Epoch 9/15: [========                      ] 22/75 batches, loss: 0.5144Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.5170Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.5147Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.5161Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.5161Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.5185Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.5162Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.5191Epoch 9/15: [============                  ] 30/75 batches, loss: 0.5202Epoch 9/15: [============                  ] 31/75 batches, loss: 0.5203Epoch 9/15: [============                  ] 32/75 batches, loss: 0.5214Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.5223Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.5217Epoch 9/15: [==============                ] 35/75 batches, loss: 0.5212Epoch 9/15: [==============                ] 36/75 batches, loss: 0.5201Epoch 9/15: [==============                ] 37/75 batches, loss: 0.5178Epoch 9/15: [===============               ] 38/75 batches, loss: 0.5187Epoch 9/15: [===============               ] 39/75 batches, loss: 0.5190Epoch 9/15: [================              ] 40/75 batches, loss: 0.5174Epoch 9/15: [================              ] 41/75 batches, loss: 0.5177Epoch 9/15: [================              ] 42/75 batches, loss: 0.5151Epoch 9/15: [=================             ] 43/75 batches, loss: 0.5154Epoch 9/15: [=================             ] 44/75 batches, loss: 0.5142Epoch 9/15: [==================            ] 45/75 batches, loss: 0.5136Epoch 9/15: [==================            ] 46/75 batches, loss: 0.5135Epoch 9/15: [==================            ] 47/75 batches, loss: 0.5145Epoch 9/15: [===================           ] 48/75 batches, loss: 0.5143Epoch 9/15: [===================           ] 49/75 batches, loss: 0.5151Epoch 9/15: [====================          ] 50/75 batches, loss: 0.5144Epoch 9/15: [====================          ] 51/75 batches, loss: 0.5149Epoch 9/15: [====================          ] 52/75 batches, loss: 0.5156Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.5154Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.5160Epoch 9/15: [======================        ] 55/75 batches, loss: 0.5162Epoch 9/15: [======================        ] 56/75 batches, loss: 0.5164Epoch 9/15: [======================        ] 57/75 batches, loss: 0.5154Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.5144Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.5146Epoch 9/15: [========================      ] 60/75 batches, loss: 0.5145Epoch 9/15: [========================      ] 61/75 batches, loss: 0.5155Epoch 9/15: [========================      ] 62/75 batches, loss: 0.5153Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.5147Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.5138Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.5140Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.5125Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.5120Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.5115Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.5123Epoch 9/15: [============================  ] 70/75 batches, loss: 0.5111Epoch 9/15: [============================  ] 71/75 batches, loss: 0.5103Epoch 9/15: [============================  ] 72/75 batches, loss: 0.5100Epoch 9/15: [============================= ] 73/75 batches, loss: 0.5093Epoch 9/15: [============================= ] 74/75 batches, loss: 0.5092Epoch 9/15: [==============================] 75/75 batches, loss: 0.5093
[2025-05-07 11:50:01,497][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5093
[2025-05-07 11:50:01,748][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.5137, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.96, 'precision': 0.9230769230769231, 'recall': 1.0}
[2025-05-07 11:50:01,748][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.4399Epoch 10/15: [                              ] 2/75 batches, loss: 0.4360Epoch 10/15: [=                             ] 3/75 batches, loss: 0.4691Epoch 10/15: [=                             ] 4/75 batches, loss: 0.4778Epoch 10/15: [==                            ] 5/75 batches, loss: 0.4782Epoch 10/15: [==                            ] 6/75 batches, loss: 0.4950Epoch 10/15: [==                            ] 7/75 batches, loss: 0.5032Epoch 10/15: [===                           ] 8/75 batches, loss: 0.5093Epoch 10/15: [===                           ] 9/75 batches, loss: 0.5034Epoch 10/15: [====                          ] 10/75 batches, loss: 0.5034Epoch 10/15: [====                          ] 11/75 batches, loss: 0.5079Epoch 10/15: [====                          ] 12/75 batches, loss: 0.5136Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.5115Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.5092Epoch 10/15: [======                        ] 15/75 batches, loss: 0.5072Epoch 10/15: [======                        ] 16/75 batches, loss: 0.5056Epoch 10/15: [======                        ] 17/75 batches, loss: 0.5041Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.5016Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.5019Epoch 10/15: [========                      ] 20/75 batches, loss: 0.5045Epoch 10/15: [========                      ] 21/75 batches, loss: 0.5074Epoch 10/15: [========                      ] 22/75 batches, loss: 0.5073Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.5072Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.5080Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.5111Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.5117Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.5114Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.5084Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.5102Epoch 10/15: [============                  ] 30/75 batches, loss: 0.5146Epoch 10/15: [============                  ] 31/75 batches, loss: 0.5123Epoch 10/15: [============                  ] 32/75 batches, loss: 0.5120Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.5117Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.5136Epoch 10/15: [==============                ] 35/75 batches, loss: 0.5140Epoch 10/15: [==============                ] 36/75 batches, loss: 0.5146Epoch 10/15: [==============                ] 37/75 batches, loss: 0.5130Epoch 10/15: [===============               ] 38/75 batches, loss: 0.5128Epoch 10/15: [===============               ] 39/75 batches, loss: 0.5126Epoch 10/15: [================              ] 40/75 batches, loss: 0.5130Epoch 10/15: [================              ] 41/75 batches, loss: 0.5134Epoch 10/15: [================              ] 42/75 batches, loss: 0.5115Epoch 10/15: [=================             ] 43/75 batches, loss: 0.5103Epoch 10/15: [=================             ] 44/75 batches, loss: 0.5106Epoch 10/15: [==================            ] 45/75 batches, loss: 0.5095Epoch 10/15: [==================            ] 46/75 batches, loss: 0.5104Epoch 10/15: [==================            ] 47/75 batches, loss: 0.5103Epoch 10/15: [===================           ] 48/75 batches, loss: 0.5101Epoch 10/15: [===================           ] 49/75 batches, loss: 0.5101Epoch 10/15: [====================          ] 50/75 batches, loss: 0.5107Epoch 10/15: [====================          ] 51/75 batches, loss: 0.5106Epoch 10/15: [====================          ] 52/75 batches, loss: 0.5107Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.5115Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.5102Epoch 10/15: [======================        ] 55/75 batches, loss: 0.5101Epoch 10/15: [======================        ] 56/75 batches, loss: 0.5097Epoch 10/15: [======================        ] 57/75 batches, loss: 0.5105Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.5093Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.5088Epoch 10/15: [========================      ] 60/75 batches, loss: 0.5080Epoch 10/15: [========================      ] 61/75 batches, loss: 0.5080Epoch 10/15: [========================      ] 62/75 batches, loss: 0.5098Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.5099Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.5095Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.5094Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.5086Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.5089Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.5084Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.5073Epoch 10/15: [============================  ] 70/75 batches, loss: 0.5085Epoch 10/15: [============================  ] 71/75 batches, loss: 0.5082Epoch 10/15: [============================  ] 72/75 batches, loss: 0.5094Epoch 10/15: [============================= ] 73/75 batches, loss: 0.5084Epoch 10/15: [============================= ] 74/75 batches, loss: 0.5087Epoch 10/15: [==============================] 75/75 batches, loss: 0.5097
[2025-05-07 11:50:04,202][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5097
[2025-05-07 11:50:04,529][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.5323, Metrics: {'accuracy': 0.9347826086956522, 'f1': 0.9411764705882353, 'precision': 0.8888888888888888, 'recall': 1.0}
[2025-05-07 11:50:04,529][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.4356Epoch 11/15: [                              ] 2/75 batches, loss: 0.4981Epoch 11/15: [=                             ] 3/75 batches, loss: 0.5558Epoch 11/15: [=                             ] 4/75 batches, loss: 0.5492Epoch 11/15: [==                            ] 5/75 batches, loss: 0.5453Epoch 11/15: [==                            ] 6/75 batches, loss: 0.5306Epoch 11/15: [==                            ] 7/75 batches, loss: 0.5303Epoch 11/15: [===                           ] 8/75 batches, loss: 0.5210Epoch 11/15: [===                           ] 9/75 batches, loss: 0.5196Epoch 11/15: [====                          ] 10/75 batches, loss: 0.5156Epoch 11/15: [====                          ] 11/75 batches, loss: 0.5167Epoch 11/15: [====                          ] 12/75 batches, loss: 0.5176Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.5188Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.5250Epoch 11/15: [======                        ] 15/75 batches, loss: 0.5268Epoch 11/15: [======                        ] 16/75 batches, loss: 0.5209Epoch 11/15: [======                        ] 17/75 batches, loss: 0.5190Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.5195Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.5152Epoch 11/15: [========                      ] 20/75 batches, loss: 0.5146Epoch 11/15: [========                      ] 21/75 batches, loss: 0.5141Epoch 11/15: [========                      ] 22/75 batches, loss: 0.5148Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.5141Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.5080Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.5073Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.5107Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.5115Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.5146Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.5142Epoch 11/15: [============                  ] 30/75 batches, loss: 0.5147Epoch 11/15: [============                  ] 31/75 batches, loss: 0.5166Epoch 11/15: [============                  ] 32/75 batches, loss: 0.5162Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.5144Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.5155Epoch 11/15: [==============                ] 35/75 batches, loss: 0.5145Epoch 11/15: [==============                ] 36/75 batches, loss: 0.5136Epoch 11/15: [==============                ] 37/75 batches, loss: 0.5115Epoch 11/15: [===============               ] 38/75 batches, loss: 0.5112Epoch 11/15: [===============               ] 39/75 batches, loss: 0.5104Epoch 11/15: [================              ] 40/75 batches, loss: 0.5114Epoch 11/15: [================              ] 41/75 batches, loss: 0.5101Epoch 11/15: [================              ] 42/75 batches, loss: 0.5117Epoch 11/15: [=================             ] 43/75 batches, loss: 0.5115Epoch 11/15: [=================             ] 44/75 batches, loss: 0.5092Epoch 11/15: [==================            ] 45/75 batches, loss: 0.5091Epoch 11/15: [==================            ] 46/75 batches, loss: 0.5085Epoch 11/15: [==================            ] 47/75 batches, loss: 0.5084Epoch 11/15: [===================           ] 48/75 batches, loss: 0.5073Epoch 11/15: [===================           ] 49/75 batches, loss: 0.5073Epoch 11/15: [====================          ] 50/75 batches, loss: 0.5081Epoch 11/15: [====================          ] 51/75 batches, loss: 0.5072Epoch 11/15: [====================          ] 52/75 batches, loss: 0.5080Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.5093Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.5087Epoch 11/15: [======================        ] 55/75 batches, loss: 0.5078Epoch 11/15: [======================        ] 56/75 batches, loss: 0.5068Epoch 11/15: [======================        ] 57/75 batches, loss: 0.5080Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.5067Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.5066Epoch 11/15: [========================      ] 60/75 batches, loss: 0.5067Epoch 11/15: [========================      ] 61/75 batches, loss: 0.5089Epoch 11/15: [========================      ] 62/75 batches, loss: 0.5095Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.5098Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.5105Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.5100Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.5099Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.5091Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.5093Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.5090Epoch 11/15: [============================  ] 70/75 batches, loss: 0.5085Epoch 11/15: [============================  ] 71/75 batches, loss: 0.5084Epoch 11/15: [============================  ] 72/75 batches, loss: 0.5084Epoch 11/15: [============================= ] 73/75 batches, loss: 0.5090Epoch 11/15: [============================= ] 74/75 batches, loss: 0.5100Epoch 11/15: [==============================] 75/75 batches, loss: 0.5103
[2025-05-07 11:50:06,925][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5103
[2025-05-07 11:50:07,200][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.5126, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.96, 'precision': 0.9230769230769231, 'recall': 1.0}
[2025-05-07 11:50:07,200][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-07 11:50:07,200][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 11
[2025-05-07 11:50:07,201][src.training.lm_trainer][INFO] - Training completed in 32.74 seconds
[2025-05-07 11:50:07,201][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 11:50:10,308][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9966414777497901, 'f1': 0.9966386554621849, 'precision': 0.9983164983164983, 'recall': 0.9949664429530202}
[2025-05-07 11:50:10,309][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-07 11:50:10,309][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7065217391304348, 'f1': 0.8029197080291971, 'precision': 0.6707317073170732, 'recall': 1.0}
[2025-05-07 11:50:12,006][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer6/ja/ja/model.pt
[2025-05-07 11:50:12,008][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁████
wandb:           best_val_f1 ▁▁████
wandb:         best_val_loss █▅▂▂▁▁
wandb:    best_val_precision ▃▁████
wandb:       best_val_recall ▁█████
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▂▂▂▂▂▂▂▂
wandb:            train_loss █▃▂▁▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁██▆█▁█▃▁▃
wandb:                val_f1 ▁▁██▆█▁█▃▁▃
wandb:              val_loss █▅▂▂▂▁▃▁▂▄▂
wandb:         val_precision ▃▁██▅█▁█▃▁▃
wandb:            val_recall ▁██████████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 1
wandb:           best_val_f1 1
wandb:         best_val_loss 0.50578
wandb:    best_val_precision 1
wandb:       best_val_recall 1
wandb:      early_stop_epoch 11
wandb:                 epoch 11
wandb:   final_test_accuracy 0.70652
wandb:         final_test_f1 0.80292
wandb:  final_test_precision 0.67073
wandb:     final_test_recall 1
wandb:  final_train_accuracy 0.99664
wandb:        final_train_f1 0.99664
wandb: final_train_precision 0.99832
wandb:    final_train_recall 0.99497
wandb:    final_val_accuracy 1
wandb:          final_val_f1 1
wandb:   final_val_precision 1
wandb:      final_val_recall 1
wandb:         learning_rate 0.0001
wandb:            train_loss 0.51028
wandb:            train_time 32.7412
wandb:          val_accuracy 0.95652
wandb:                val_f1 0.96
wandb:              val_loss 0.5126
wandb:         val_precision 0.92308
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_114918-ygihad9z
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_114918-ygihad9z/logs
Experiment probe_layer6_question_type_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer6/ja/ja/results.json for layer 6
Running experiment: probe_layer6_complexity_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_complexity_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer6/ja"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 11:50:39,850][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer6/ja
experiment_name: probe_layer6_complexity_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 11:50:39,850][__main__][INFO] - Normalized task: complexity
[2025-05-07 11:50:39,850][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 11:50:39,850][__main__][INFO] - Determined Task Type: regression
[2025-05-07 11:50:39,855][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-05-07 11:50:39,855][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 11:50:43,053][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 11:50:45,304][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 11:50:45,304][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:50:45,506][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:50:45,653][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:50:45,947][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 11:50:45,955][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:50:45,956][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 11:50:45,957][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:50:46,012][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:50:46,115][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:50:46,151][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 11:50:46,153][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:50:46,153][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 11:50:46,154][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:50:46,262][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:50:46,391][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:50:46,437][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 11:50:46,439][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:50:46,439][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 11:50:46,440][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 11:50:46,441][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 11:50:46,441][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 11:50:46,441][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 11:50:46,442][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 11:50:46,442][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 11:50:46,442][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-05-07 11:50:46,442][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 11:50:46,442][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-05-07 11:50:46,442][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 11:50:46,442][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 11:50:46,442][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 11:50:46,442][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 11:50:46,443][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 11:50:46,443][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-05-07 11:50:46,443][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 11:50:46,443][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-05-07 11:50:46,443][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 11:50:46,443][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 11:50:46,443][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 11:50:46,443][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 11:50:46,443][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 11:50:46,444][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-05-07 11:50:46,444][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 11:50:46,444][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-05-07 11:50:46,444][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 11:50:46,444][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 11:50:46,444][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 11:50:46,444][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 11:50:46,445][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 11:50:54,062][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 11:50:54,063][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 11:50:54,063][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-07 11:50:54,063][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 11:50:54,066][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 11:50:54,067][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 11:50:54,067][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 11:50:54,067][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 11:50:54,067][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 11:50:54,068][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 11:50:54,068][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.3605Epoch 1/15: [                              ] 2/75 batches, loss: 0.3919Epoch 1/15: [=                             ] 3/75 batches, loss: 0.3377Epoch 1/15: [=                             ] 4/75 batches, loss: 0.3781Epoch 1/15: [==                            ] 5/75 batches, loss: 0.3841Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3807Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3849Epoch 1/15: [===                           ] 8/75 batches, loss: 0.3984Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4211Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4247Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4174Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4254Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4089Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4245Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4129Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4082Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4165Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4087Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4023Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3990Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3929Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3999Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3923Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3863Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3770Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3719Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3714Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3672Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3623Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3591Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3536Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3501Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3443Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3416Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3378Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3380Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3334Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3280Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3310Epoch 1/15: [================              ] 40/75 batches, loss: 0.3269Epoch 1/15: [================              ] 41/75 batches, loss: 0.3233Epoch 1/15: [================              ] 42/75 batches, loss: 0.3192Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3170Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3141Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3156Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3109Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3082Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3076Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3043Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3027Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3005Epoch 1/15: [====================          ] 52/75 batches, loss: 0.2976Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.2965Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.2968Epoch 1/15: [======================        ] 55/75 batches, loss: 0.2955Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2930Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2894Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2917Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2900Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2875Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2851Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2833Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2827Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2820Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2809Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2777Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2755Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2736Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2737Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2747Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2736Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2751Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2734Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2736Epoch 1/15: [==============================] 75/75 batches, loss: 0.2716
[2025-05-07 11:51:00,652][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2716
[2025-05-07 11:51:00,891][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0541, Metrics: {'mse': 0.05438226833939552, 'rmse': 0.23320006076198935, 'r2': 0.1136818528175354}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.3044Epoch 2/15: [                              ] 2/75 batches, loss: 0.2270Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1998Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1818Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1938Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1774Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1771Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1736Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1820Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1894Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1854Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1818Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1794Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1809Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1769Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1762Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1725Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1734Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1705Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1663Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1642Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1612Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1609Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1576Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1552Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1562Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1581Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1553Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1569Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1586Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1583Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1591Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1577Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1561Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1579Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1555Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1570Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1563Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1560Epoch 2/15: [================              ] 40/75 batches, loss: 0.1565Epoch 2/15: [================              ] 41/75 batches, loss: 0.1548Epoch 2/15: [================              ] 42/75 batches, loss: 0.1538Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1537Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1549Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1540Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1532Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1519Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1535Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1537Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1530Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1529Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1520Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1514Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1504Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1511Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1515Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1505Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1495Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1491Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1474Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1464Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1461Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1455Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1444Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1433Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1427Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1417Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1418Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1414Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1413Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1405Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1400Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1397Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1389Epoch 2/15: [==============================] 75/75 batches, loss: 0.1395
[2025-05-07 11:51:03,589][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1395
[2025-05-07 11:51:03,819][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0503, Metrics: {'mse': 0.04971400648355484, 'rmse': 0.2229663797157653, 'r2': 0.1897648572921753}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1533Epoch 3/15: [                              ] 2/75 batches, loss: 0.1528Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1345Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1130Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1157Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1183Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1210Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1156Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1156Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1113Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1085Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1028Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0988Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0974Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0949Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0925Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0905Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0934Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0947Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0971Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0980Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0979Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0988Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0981Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0974Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0970Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0968Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0978Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0982Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0963Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0968Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1023Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1022Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1047Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1033Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1024Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1028Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1028Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1024Epoch 3/15: [================              ] 40/75 batches, loss: 0.1029Epoch 3/15: [================              ] 41/75 batches, loss: 0.1053Epoch 3/15: [================              ] 42/75 batches, loss: 0.1061Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1048Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1048Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1054Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1045Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1046Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1042Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1054Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1044Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1049Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1046Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1042Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1029Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1023Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1018Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1026Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1025Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1032Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1023Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1020Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1014Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1013Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1003Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1007Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1003Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0996Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1007Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1009Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1011Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1008Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1003Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1015Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1011Epoch 3/15: [==============================] 75/75 batches, loss: 0.1016
[2025-05-07 11:51:06,493][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1016
[2025-05-07 11:51:06,714][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0601, Metrics: {'mse': 0.05920436605811119, 'rmse': 0.2433194732406578, 'r2': 0.03509169816970825}
[2025-05-07 11:51:06,715][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0664Epoch 4/15: [                              ] 2/75 batches, loss: 0.0892Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0973Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0815Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0755Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0713Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0742Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0701Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0760Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0822Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0866Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0874Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0854Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0862Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0851Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0859Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0860Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0858Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0877Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0887Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0898Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0888Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0871Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0875Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0880Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0863Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0898Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0907Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0900Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0889Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0878Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0867Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0860Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0868Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0870Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0866Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0857Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0852Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0848Epoch 4/15: [================              ] 40/75 batches, loss: 0.0849Epoch 4/15: [================              ] 41/75 batches, loss: 0.0852Epoch 4/15: [================              ] 42/75 batches, loss: 0.0850Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0853Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0851Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0853Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0858Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0855Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0846Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0846Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0839Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0836Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0831Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0829Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0830Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0823Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0820Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0823Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0817Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0822Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0818Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0828Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0831Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0835Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0834Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0835Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0833Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0834Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0835Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0834Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0834Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0830Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0828Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0827Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0830Epoch 4/15: [==============================] 75/75 batches, loss: 0.0836
[2025-05-07 11:51:08,999][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0836
[2025-05-07 11:51:09,229][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0496, Metrics: {'mse': 0.04892856255173683, 'rmse': 0.22119801660895794, 'r2': 0.2025659680366516}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0779Epoch 5/15: [                              ] 2/75 batches, loss: 0.0848Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0818Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0890Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0797Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0875Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0892Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0866Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0833Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0835Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0823Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0848Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0852Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0847Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0847Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0836Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0826Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0806Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0801Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0813Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0796Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0789Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0776Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0775Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0769Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0770Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0772Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0760Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0749Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0768Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0764Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0770Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0760Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0752Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0751Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0760Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0759Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0765Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0760Epoch 5/15: [================              ] 40/75 batches, loss: 0.0758Epoch 5/15: [================              ] 41/75 batches, loss: 0.0760Epoch 5/15: [================              ] 42/75 batches, loss: 0.0763Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0758Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0760Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0749Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0749Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0742Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0735Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0739Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0737Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0732Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0729Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0731Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0729Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0724Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0722Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0722Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0718Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0715Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0715Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0722Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0720Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0719Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0718Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0729Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0737Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0731Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0725Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0726Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0722Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0723Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0721Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0715Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0710Epoch 5/15: [==============================] 75/75 batches, loss: 0.0714
[2025-05-07 11:51:11,871][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0714
[2025-05-07 11:51:12,079][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0407, Metrics: {'mse': 0.040302425622940063, 'rmse': 0.20075464035219726, 'r2': 0.34315407276153564}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0799Epoch 6/15: [                              ] 2/75 batches, loss: 0.0664Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0594Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0532Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0569Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0582Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0660Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0704Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0687Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0673Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0674Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0692Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0689Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0697Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0683Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0699Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0687Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0687Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0663Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0658Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0661Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0667Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0653Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0660Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0654Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0657Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0650Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0651Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0643Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0637Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0639Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0632Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0635Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0623Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0621Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0622Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0626Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0621Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0630Epoch 6/15: [================              ] 40/75 batches, loss: 0.0633Epoch 6/15: [================              ] 41/75 batches, loss: 0.0642Epoch 6/15: [================              ] 42/75 batches, loss: 0.0643Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0638Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0633Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0636Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0634Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0633Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0634Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0634Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0632Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0635Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0635Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0634Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0631Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0635Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0632Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0626Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0627Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0630Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0624Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0620Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0615Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0612Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0617Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0612Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0608Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0607Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0608Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0607Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0610Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0612Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0607Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0605Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0606Epoch 6/15: [==============================] 75/75 batches, loss: 0.0611
[2025-05-07 11:51:14,713][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0611
[2025-05-07 11:51:15,026][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0352, Metrics: {'mse': 0.03481869772076607, 'rmse': 0.18659768948399674, 'r2': 0.43252748250961304}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0595Epoch 7/15: [                              ] 2/75 batches, loss: 0.0648Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0569Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0570Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0518Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0513Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0541Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0601Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0573Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0550Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0549Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0544Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0547Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0534Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0533Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0529Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0524Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0534Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0572Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0572Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0563Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0547Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0540Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0539Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0536Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0544Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0553Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0560Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0564Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0591Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0585Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0579Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0574Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0585Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0587Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0584Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0590Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0583Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0593Epoch 7/15: [================              ] 40/75 batches, loss: 0.0589Epoch 7/15: [================              ] 41/75 batches, loss: 0.0587Epoch 7/15: [================              ] 42/75 batches, loss: 0.0591Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0587Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0590Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0591Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0589Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0594Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0588Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0583Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0576Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0577Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0569Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0569Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0567Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0565Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0563Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0561Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0560Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0558Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0567Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0566Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0567Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0565Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0569Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0570Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0571Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0566Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0569Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0568Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0567Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0565Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0562Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0561Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0561Epoch 7/15: [==============================] 75/75 batches, loss: 0.0563
[2025-05-07 11:51:17,738][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0563
[2025-05-07 11:51:17,963][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0454, Metrics: {'mse': 0.044693537056446075, 'rmse': 0.21140846022911683, 'r2': 0.2715880870819092}
[2025-05-07 11:51:17,964][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0565Epoch 8/15: [                              ] 2/75 batches, loss: 0.0632Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0581Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0514Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0545Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0513Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0515Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0539Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0522Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0504Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0538Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0534Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0517Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0529Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0549Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0538Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0577Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0579Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0588Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0595Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0587Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0573Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0563Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0566Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0575Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0561Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0572Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0569Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0565Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0556Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0564Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0570Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0575Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0566Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0575Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0571Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0568Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0569Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0567Epoch 8/15: [================              ] 40/75 batches, loss: 0.0562Epoch 8/15: [================              ] 41/75 batches, loss: 0.0561Epoch 8/15: [================              ] 42/75 batches, loss: 0.0566Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0563Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0562Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0563Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0564Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0558Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0553Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0546Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0550Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0550Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0546Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0544Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0539Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0538Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0535Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0536Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0533Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0532Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0532Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0535Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0530Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0531Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0533Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0530Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0530Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0532Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0533Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0534Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0530Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0530Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0526Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0523Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0523Epoch 8/15: [==============================] 75/75 batches, loss: 0.0517
[2025-05-07 11:51:20,242][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0517
[2025-05-07 11:51:20,458][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0436, Metrics: {'mse': 0.04289330169558525, 'rmse': 0.2071069812816199, 'r2': 0.30092817544937134}
[2025-05-07 11:51:20,458][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0357Epoch 9/15: [                              ] 2/75 batches, loss: 0.0396Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0411Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0432Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0464Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0479Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0471Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0484Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0471Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0496Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0520Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0510Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0499Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0531Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0528Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0561Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0568Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0579Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0587Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0588Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0590Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0582Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0577Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0581Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0583Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0579Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0577Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0570Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0565Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0557Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0557Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0556Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0553Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0548Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0540Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0534Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0534Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0537Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0534Epoch 9/15: [================              ] 40/75 batches, loss: 0.0530Epoch 9/15: [================              ] 41/75 batches, loss: 0.0526Epoch 9/15: [================              ] 42/75 batches, loss: 0.0522Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0527Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0531Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0529Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0527Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0524Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0521Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0522Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0521Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0523Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0521Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0517Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0520Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0515Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0517Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0514Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0513Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0512Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0511Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0509Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0504Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0505Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0505Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0502Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0500Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0503Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0503Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0503Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0501Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0500Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0498Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0497Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0497Epoch 9/15: [==============================] 75/75 batches, loss: 0.0494
[2025-05-07 11:51:22,790][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0494
[2025-05-07 11:51:23,051][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0402, Metrics: {'mse': 0.0395747646689415, 'rmse': 0.1989340711616326, 'r2': 0.3550134301185608}
[2025-05-07 11:51:23,052][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0839Epoch 10/15: [                              ] 2/75 batches, loss: 0.0673Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0611Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0674Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0675Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0655Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0641Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0621Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0593Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0550Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0539Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0539Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0541Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0512Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0504Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0499Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0512Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0502Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0493Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0489Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0493Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0487Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0481Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0481Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0481Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0476Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0469Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0471Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0473Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0470Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0466Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0464Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0466Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0465Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0465Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0460Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0463Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0460Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0457Epoch 10/15: [================              ] 40/75 batches, loss: 0.0454Epoch 10/15: [================              ] 41/75 batches, loss: 0.0457Epoch 10/15: [================              ] 42/75 batches, loss: 0.0452Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0452Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0453Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0454Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0458Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0460Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0462Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0458Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0456Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0458Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0457Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0454Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0453Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0455Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0450Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0454Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0456Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0453Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0453Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0451Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0451Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0459Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0458Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0457Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0455Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0457Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0457Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0458Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0458Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0458Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0463Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0461Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0461Epoch 10/15: [==============================] 75/75 batches, loss: 0.0463
[2025-05-07 11:51:25,316][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0463
[2025-05-07 11:51:25,544][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0342, Metrics: {'mse': 0.033687740564346313, 'rmse': 0.18354220376890518, 'r2': 0.4509597420692444}
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0287Epoch 11/15: [                              ] 2/75 batches, loss: 0.0343Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0342Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0330Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0388Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0363Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0393Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0395Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0412Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0412Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0392Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0409Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0406Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0412Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0395Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0399Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0402Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0409Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0411Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0405Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0419Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0422Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0418Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0414Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0412Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0415Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0405Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0410Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0408Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0406Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0406Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0401Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0401Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0399Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0399Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0396Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0394Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0393Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0390Epoch 11/15: [================              ] 40/75 batches, loss: 0.0383Epoch 11/15: [================              ] 41/75 batches, loss: 0.0388Epoch 11/15: [================              ] 42/75 batches, loss: 0.0389Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0391Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0393Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0395Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0398Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0401Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0398Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0400Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0399Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0396Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0395Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0394Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0396Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0398Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0400Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0399Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0397Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0393Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0393Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0395Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0393Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0397Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0397Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0396Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0398Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0396Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0398Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0398Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0402Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0402Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0405Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0407Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0411Epoch 11/15: [==============================] 75/75 batches, loss: 0.0407
[2025-05-07 11:51:28,299][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0407
[2025-05-07 11:51:28,564][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0260, Metrics: {'mse': 0.025743644684553146, 'rmse': 0.16044826170623708, 'r2': 0.5804319977760315}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0391Epoch 12/15: [                              ] 2/75 batches, loss: 0.0437Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0440Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0431Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0436Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0418Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0414Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0388Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0394Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0389Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0384Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0378Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0394Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0380Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0367Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0378Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0380Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0375Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0377Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0379Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0374Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0374Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0376Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0372Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0378Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0381Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0380Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0392Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0395Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0399Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0399Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0395Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0399Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0397Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0391Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0387Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0389Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0392Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0390Epoch 12/15: [================              ] 40/75 batches, loss: 0.0388Epoch 12/15: [================              ] 41/75 batches, loss: 0.0387Epoch 12/15: [================              ] 42/75 batches, loss: 0.0387Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0386Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0383Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0384Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0380Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0380Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0380Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0378Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0377Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0377Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0376Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0372Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0368Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0372Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0370Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0369Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0371Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0373Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0379Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0387Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0386Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0388Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0386Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0387Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0388Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0386Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0385Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0383Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0382Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0381Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0380Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0379Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0378Epoch 12/15: [==============================] 75/75 batches, loss: 0.0378
[2025-05-07 11:51:31,254][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0378
[2025-05-07 11:51:31,620][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0416, Metrics: {'mse': 0.040835343301296234, 'rmse': 0.20207756753607323, 'r2': 0.3344686031341553}
[2025-05-07 11:51:31,621][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0608Epoch 13/15: [                              ] 2/75 batches, loss: 0.0448Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0464Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0401Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0402Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0407Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0390Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0407Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0419Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0402Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0395Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0396Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0396Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0400Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0390Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0378Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0385Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0380Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0388Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0380Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0385Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0386Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0384Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0389Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0390Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0396Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0390Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0389Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0390Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0389Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0386Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0387Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0383Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0378Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0383Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0389Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0389Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0393Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0393Epoch 13/15: [================              ] 40/75 batches, loss: 0.0394Epoch 13/15: [================              ] 41/75 batches, loss: 0.0393Epoch 13/15: [================              ] 42/75 batches, loss: 0.0391Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0388Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0387Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0389Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0390Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0386Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0384Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0383Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0382Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0380Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0382Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0378Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0376Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0378Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0377Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0378Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0375Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0374Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0374Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0371Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0373Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0376Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0376Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0373Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0371Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0369Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0370Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0370Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0372Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0372Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0373Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0374Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0374Epoch 13/15: [==============================] 75/75 batches, loss: 0.0380
[2025-05-07 11:51:34,053][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0380
[2025-05-07 11:51:34,323][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0280, Metrics: {'mse': 0.02746978960931301, 'rmse': 0.16574012673252367, 'r2': 0.5522993803024292}
[2025-05-07 11:51:34,324][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0452Epoch 14/15: [                              ] 2/75 batches, loss: 0.0357Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0457Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0418Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0425Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0414Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0406Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0404Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0435Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0439Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0433Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0418Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0419Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0432Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0426Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0422Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0414Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0405Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0397Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0395Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0398Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0400Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0395Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0392Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0385Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0386Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0387Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0394Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0387Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0382Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0378Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0375Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0374Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0370Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0379Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0378Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0377Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0377Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0375Epoch 14/15: [================              ] 40/75 batches, loss: 0.0370Epoch 14/15: [================              ] 41/75 batches, loss: 0.0368Epoch 14/15: [================              ] 42/75 batches, loss: 0.0372Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0374Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0371Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0374Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0372Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0375Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0374Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0375Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0381Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0386Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0381Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0382Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0382Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0381Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0378Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0378Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0378Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0377Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0377Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0375Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0378Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0378Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0375Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0373Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0372Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0370Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0373Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0374Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0374Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0371Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0370Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0367Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0367Epoch 14/15: [==============================] 75/75 batches, loss: 0.0367
[2025-05-07 11:51:36,644][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0367
[2025-05-07 11:51:36,911][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0323, Metrics: {'mse': 0.03163587674498558, 'rmse': 0.1778647709496897, 'r2': 0.48440080881118774}
[2025-05-07 11:51:36,911][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0165Epoch 15/15: [                              ] 2/75 batches, loss: 0.0188Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0220Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0219Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0284Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0267Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0295Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0290Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0291Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0301Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0324Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0310Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0322Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0306Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0297Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0308Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0315Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0320Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0317Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0311Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0307Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0300Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0310Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0302Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0307Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0304Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0301Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0299Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0299Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0305Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0300Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0304Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0321Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0323Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0319Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0324Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0321Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0324Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0327Epoch 15/15: [================              ] 40/75 batches, loss: 0.0328Epoch 15/15: [================              ] 41/75 batches, loss: 0.0324Epoch 15/15: [================              ] 42/75 batches, loss: 0.0328Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0327Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0327Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0329Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0333Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0337Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0337Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0337Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0337Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0337Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0336Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0337Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0338Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0338Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0335Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0332Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0332Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0331Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0333Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0333Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0335Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0334Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0336Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0337Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0338Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0340Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0338Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0339Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0339Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0340Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0341Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0340Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0339Epoch 15/15: [==============================] 75/75 batches, loss: 0.0337
[2025-05-07 11:51:39,413][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0337
[2025-05-07 11:51:39,625][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0310, Metrics: {'mse': 0.030405651777982712, 'rmse': 0.17437216457331345, 'r2': 0.5044509768486023}
[2025-05-07 11:51:39,626][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 11:51:39,626][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 15
[2025-05-07 11:51:39,626][src.training.lm_trainer][INFO] - Training completed in 42.10 seconds
[2025-05-07 11:51:39,626][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 11:51:42,624][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.01596755161881447, 'rmse': 0.1263627778216927, 'r2': 0.601462185382843}
[2025-05-07 11:51:42,624][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.025743644684553146, 'rmse': 0.16044826170623708, 'r2': 0.5804319977760315}
[2025-05-07 11:51:42,625][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.018545590341091156, 'rmse': 0.136182195389453, 'r2': 0.6437551975250244}
[2025-05-07 11:51:44,428][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer6/ja/ja/model.pt
[2025-05-07 11:51:44,430][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▇▅▃▃▁
wandb:     best_val_mse █▇▇▅▃▃▁
wandb:      best_val_r2 ▁▂▂▄▆▆█
wandb:    best_val_rmse █▇▇▅▄▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▃▁▃▅▆▄▄▅▆▇▅▆▆
wandb:       train_loss █▄▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇▆█▆▄▃▅▅▄▃▁▄▁▂▂
wandb:          val_mse ▇▆█▆▄▃▅▅▄▃▁▄▁▂▂
wandb:           val_r2 ▂▃▁▃▅▆▄▄▅▆█▅█▇▇
wandb:         val_rmse ▇▆█▆▄▃▅▅▄▃▁▅▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02598
wandb:     best_val_mse 0.02574
wandb:      best_val_r2 0.58043
wandb:    best_val_rmse 0.16045
wandb: early_stop_epoch 15
wandb:            epoch 15
wandb:   final_test_mse 0.01855
wandb:    final_test_r2 0.64376
wandb:  final_test_rmse 0.13618
wandb:  final_train_mse 0.01597
wandb:   final_train_r2 0.60146
wandb: final_train_rmse 0.12636
wandb:    final_val_mse 0.02574
wandb:     final_val_r2 0.58043
wandb:   final_val_rmse 0.16045
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03375
wandb:       train_time 42.09652
wandb:         val_loss 0.03104
wandb:          val_mse 0.03041
wandb:           val_r2 0.50445
wandb:         val_rmse 0.17437
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_115039-h5lc5y8a
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_115039-h5lc5y8a/logs
Experiment probe_layer6_complexity_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer6/ja/ja/results.json for layer 6
=======================
PROBING LAYER 10
=======================
Running experiment: probe_layer10_question_type_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_question_type_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer10/fi"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 11:52:20,272][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer10/fi
experiment_name: probe_layer10_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-07 11:52:20,272][__main__][INFO] - Normalized task: question_type
[2025-05-07 11:52:20,272][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-07 11:52:20,272][__main__][INFO] - Determined Task Type: classification
[2025-05-07 11:52:20,276][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-05-07 11:52:20,276][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 11:52:23,521][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 11:52:25,785][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 11:52:25,786][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:52:26,077][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:52:26,239][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:52:26,433][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 11:52:26,441][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:52:26,441][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 11:52:26,442][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:52:26,541][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:52:26,641][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:52:26,655][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 11:52:26,656][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:52:26,656][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 11:52:26,657][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:52:26,776][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:52:26,886][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:52:26,911][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 11:52:26,912][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:52:26,912][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 11:52:26,913][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 11:52:26,914][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:52:26,914][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:52:26,914][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:52:26,914][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:52:26,915][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-05-07 11:52:26,915][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-05-07 11:52:26,915][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 11:52:26,915][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 11:52:26,915][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:52:26,915][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:52:26,915][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:52:26,915][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:52:26,915][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-05-07 11:52:26,915][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-05-07 11:52:26,915][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 11:52:26,916][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 11:52:26,916][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:52:26,916][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:52:26,916][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:52:26,916][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:52:26,916][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-07 11:52:26,916][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-07 11:52:26,916][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 11:52:26,916][src.data.datasets][INFO] - Sample label: 0
[2025-05-07 11:52:26,916][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 11:52:26,916][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 11:52:26,917][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 11:52:26,917][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-07 11:52:26,917][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 11:52:34,768][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 11:52:34,769][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 11:52:34,769][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-07 11:52:34,769][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-07 11:52:34,775][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-07 11:52:34,775][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-07 11:52:34,775][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-07 11:52:34,775][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-07 11:52:34,775][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 11:52:34,776][__main__][INFO] - Total parameters: 394,568,839
[2025-05-07 11:52:34,776][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-07 11:52:34,777][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7029Epoch 1/15: [                              ] 2/75 batches, loss: 0.7089Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7198Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7172Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7110Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7010Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6965Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6963Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6934Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6922Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6940Epoch 1/15: [====                          ] 12/75 batches, loss: 0.6928Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.6940Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.6942Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6946Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6947Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6955Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6947Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6949Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6946Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6939Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6928Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6929Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6929Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6925Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6925Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6923Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6926Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6926Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6926Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6921Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6919Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6917Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6915Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6906Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6899Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6901Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6900Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6899Epoch 1/15: [================              ] 40/75 batches, loss: 0.6888Epoch 1/15: [================              ] 41/75 batches, loss: 0.6889Epoch 1/15: [================              ] 42/75 batches, loss: 0.6883Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6891Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6882Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6889Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6882Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6868Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6847Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6846Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6854Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6839Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6837Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6853Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6846Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6842Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6839Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6836Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6822Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6831Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6818Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6816Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6814Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6807Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6793Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6792Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6781Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6776Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6770Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6750Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6745Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6748Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6737Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6742Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6735Epoch 1/15: [==============================] 75/75 batches, loss: 0.6720
[2025-05-07 11:52:41,351][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6720
[2025-05-07 11:52:41,607][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6236, Metrics: {'accuracy': 0.8571428571428571, 'f1': 0.8421052631578947, 'precision': 0.8888888888888888, 'recall': 0.8}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.5900Epoch 2/15: [                              ] 2/75 batches, loss: 0.6106Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6379Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6387Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6388Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6336Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6300Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6314Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6306Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6365Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6337Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6315Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6277Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6207Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6224Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6179Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6112Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6121Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6180Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6193Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6184Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6127Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6135Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6100Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6112Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6122Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6118Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6114Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6102Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6093Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6113Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6104Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6119Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6147Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6150Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6151Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6153Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6140Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6140Epoch 2/15: [================              ] 40/75 batches, loss: 0.6136Epoch 2/15: [================              ] 41/75 batches, loss: 0.6140Epoch 2/15: [================              ] 42/75 batches, loss: 0.6143Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6143Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6120Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6097Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6099Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6095Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6090Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6102Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6099Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6104Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6092Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6104Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6092Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6096Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6076Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6084Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6069Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6070Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6056Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6058Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6060Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6067Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6077Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6064Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6065Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6054Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6047Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6045Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6031Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6033Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6014Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6010Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6003Epoch 2/15: [==============================] 75/75 batches, loss: 0.6004
[2025-05-07 11:52:44,343][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6004
[2025-05-07 11:52:44,581][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.5721, Metrics: {'accuracy': 0.9047619047619048, 'f1': 0.9032258064516129, 'precision': 0.875, 'recall': 0.9333333333333333}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.5262Epoch 3/15: [                              ] 2/75 batches, loss: 0.5202Epoch 3/15: [=                             ] 3/75 batches, loss: 0.5259Epoch 3/15: [=                             ] 4/75 batches, loss: 0.5627Epoch 3/15: [==                            ] 5/75 batches, loss: 0.5590Epoch 3/15: [==                            ] 6/75 batches, loss: 0.5675Epoch 3/15: [==                            ] 7/75 batches, loss: 0.5645Epoch 3/15: [===                           ] 8/75 batches, loss: 0.5677Epoch 3/15: [===                           ] 9/75 batches, loss: 0.5608Epoch 3/15: [====                          ] 10/75 batches, loss: 0.5735Epoch 3/15: [====                          ] 11/75 batches, loss: 0.5716Epoch 3/15: [====                          ] 12/75 batches, loss: 0.5676Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.5663Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.5676Epoch 3/15: [======                        ] 15/75 batches, loss: 0.5637Epoch 3/15: [======                        ] 16/75 batches, loss: 0.5614Epoch 3/15: [======                        ] 17/75 batches, loss: 0.5607Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.5545Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.5599Epoch 3/15: [========                      ] 20/75 batches, loss: 0.5536Epoch 3/15: [========                      ] 21/75 batches, loss: 0.5564Epoch 3/15: [========                      ] 22/75 batches, loss: 0.5578Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.5557Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.5605Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.5675Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.5714Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.5695Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.5698Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.5704Epoch 3/15: [============                  ] 30/75 batches, loss: 0.5722Epoch 3/15: [============                  ] 31/75 batches, loss: 0.5714Epoch 3/15: [============                  ] 32/75 batches, loss: 0.5712Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.5717Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.5697Epoch 3/15: [==============                ] 35/75 batches, loss: 0.5699Epoch 3/15: [==============                ] 36/75 batches, loss: 0.5685Epoch 3/15: [==============                ] 37/75 batches, loss: 0.5696Epoch 3/15: [===============               ] 38/75 batches, loss: 0.5695Epoch 3/15: [===============               ] 39/75 batches, loss: 0.5700Epoch 3/15: [================              ] 40/75 batches, loss: 0.5686Epoch 3/15: [================              ] 41/75 batches, loss: 0.5680Epoch 3/15: [================              ] 42/75 batches, loss: 0.5647Epoch 3/15: [=================             ] 43/75 batches, loss: 0.5628Epoch 3/15: [=================             ] 44/75 batches, loss: 0.5642Epoch 3/15: [==================            ] 45/75 batches, loss: 0.5640Epoch 3/15: [==================            ] 46/75 batches, loss: 0.5620Epoch 3/15: [==================            ] 47/75 batches, loss: 0.5628Epoch 3/15: [===================           ] 48/75 batches, loss: 0.5644Epoch 3/15: [===================           ] 49/75 batches, loss: 0.5623Epoch 3/15: [====================          ] 50/75 batches, loss: 0.5638Epoch 3/15: [====================          ] 51/75 batches, loss: 0.5636Epoch 3/15: [====================          ] 52/75 batches, loss: 0.5642Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.5651Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.5636Epoch 3/15: [======================        ] 55/75 batches, loss: 0.5645Epoch 3/15: [======================        ] 56/75 batches, loss: 0.5641Epoch 3/15: [======================        ] 57/75 batches, loss: 0.5626Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.5624Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.5632Epoch 3/15: [========================      ] 60/75 batches, loss: 0.5645Epoch 3/15: [========================      ] 61/75 batches, loss: 0.5637Epoch 3/15: [========================      ] 62/75 batches, loss: 0.5642Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.5649Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.5639Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.5634Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.5628Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.5616Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.5622Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.5620Epoch 3/15: [============================  ] 70/75 batches, loss: 0.5612Epoch 3/15: [============================  ] 71/75 batches, loss: 0.5616Epoch 3/15: [============================  ] 72/75 batches, loss: 0.5623Epoch 3/15: [============================= ] 73/75 batches, loss: 0.5624Epoch 3/15: [============================= ] 74/75 batches, loss: 0.5636Epoch 3/15: [==============================] 75/75 batches, loss: 0.5629
[2025-05-07 11:52:47,302][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.5629
[2025-05-07 11:52:47,584][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.5557, Metrics: {'accuracy': 0.9206349206349206, 'f1': 0.9152542372881356, 'precision': 0.9310344827586207, 'recall': 0.9}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.5949Epoch 4/15: [                              ] 2/75 batches, loss: 0.5355Epoch 4/15: [=                             ] 3/75 batches, loss: 0.5486Epoch 4/15: [=                             ] 4/75 batches, loss: 0.5578Epoch 4/15: [==                            ] 5/75 batches, loss: 0.5654Epoch 4/15: [==                            ] 6/75 batches, loss: 0.5753Epoch 4/15: [==                            ] 7/75 batches, loss: 0.5736Epoch 4/15: [===                           ] 8/75 batches, loss: 0.5754Epoch 4/15: [===                           ] 9/75 batches, loss: 0.5755Epoch 4/15: [====                          ] 10/75 batches, loss: 0.5736Epoch 4/15: [====                          ] 11/75 batches, loss: 0.5754Epoch 4/15: [====                          ] 12/75 batches, loss: 0.5750Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.5675Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.5681Epoch 4/15: [======                        ] 15/75 batches, loss: 0.5711Epoch 4/15: [======                        ] 16/75 batches, loss: 0.5679Epoch 4/15: [======                        ] 17/75 batches, loss: 0.5665Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.5648Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.5593Epoch 4/15: [========                      ] 20/75 batches, loss: 0.5587Epoch 4/15: [========                      ] 21/75 batches, loss: 0.5580Epoch 4/15: [========                      ] 22/75 batches, loss: 0.5602Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.5635Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.5636Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.5625Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.5628Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.5619Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.5601Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.5623Epoch 4/15: [============                  ] 30/75 batches, loss: 0.5583Epoch 4/15: [============                  ] 31/75 batches, loss: 0.5584Epoch 4/15: [============                  ] 32/75 batches, loss: 0.5587Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.5595Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.5601Epoch 4/15: [==============                ] 35/75 batches, loss: 0.5586Epoch 4/15: [==============                ] 36/75 batches, loss: 0.5548Epoch 4/15: [==============                ] 37/75 batches, loss: 0.5566Epoch 4/15: [===============               ] 38/75 batches, loss: 0.5568Epoch 4/15: [===============               ] 39/75 batches, loss: 0.5534Epoch 4/15: [================              ] 40/75 batches, loss: 0.5524Epoch 4/15: [================              ] 41/75 batches, loss: 0.5520Epoch 4/15: [================              ] 42/75 batches, loss: 0.5516Epoch 4/15: [=================             ] 43/75 batches, loss: 0.5513Epoch 4/15: [=================             ] 44/75 batches, loss: 0.5491Epoch 4/15: [==================            ] 45/75 batches, loss: 0.5491Epoch 4/15: [==================            ] 46/75 batches, loss: 0.5486Epoch 4/15: [==================            ] 47/75 batches, loss: 0.5489Epoch 4/15: [===================           ] 48/75 batches, loss: 0.5475Epoch 4/15: [===================           ] 49/75 batches, loss: 0.5493Epoch 4/15: [====================          ] 50/75 batches, loss: 0.5499Epoch 4/15: [====================          ] 51/75 batches, loss: 0.5502Epoch 4/15: [====================          ] 52/75 batches, loss: 0.5494Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.5487Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.5478Epoch 4/15: [======================        ] 55/75 batches, loss: 0.5492Epoch 4/15: [======================        ] 56/75 batches, loss: 0.5485Epoch 4/15: [======================        ] 57/75 batches, loss: 0.5482Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.5480Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.5471Epoch 4/15: [========================      ] 60/75 batches, loss: 0.5470Epoch 4/15: [========================      ] 61/75 batches, loss: 0.5478Epoch 4/15: [========================      ] 62/75 batches, loss: 0.5475Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.5473Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.5478Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.5476Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.5475Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.5471Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.5473Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.5488Epoch 4/15: [============================  ] 70/75 batches, loss: 0.5484Epoch 4/15: [============================  ] 71/75 batches, loss: 0.5480Epoch 4/15: [============================  ] 72/75 batches, loss: 0.5480Epoch 4/15: [============================= ] 73/75 batches, loss: 0.5482Epoch 4/15: [============================= ] 74/75 batches, loss: 0.5479Epoch 4/15: [==============================] 75/75 batches, loss: 0.5485
[2025-05-07 11:52:50,291][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5485
[2025-05-07 11:52:50,555][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5486, Metrics: {'accuracy': 0.9206349206349206, 'f1': 0.9122807017543859, 'precision': 0.9629629629629629, 'recall': 0.8666666666666667}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.5331Epoch 5/15: [                              ] 2/75 batches, loss: 0.5287Epoch 5/15: [=                             ] 3/75 batches, loss: 0.5461Epoch 5/15: [=                             ] 4/75 batches, loss: 0.5535Epoch 5/15: [==                            ] 5/75 batches, loss: 0.5523Epoch 5/15: [==                            ] 6/75 batches, loss: 0.5522Epoch 5/15: [==                            ] 7/75 batches, loss: 0.5442Epoch 5/15: [===                           ] 8/75 batches, loss: 0.5476Epoch 5/15: [===                           ] 9/75 batches, loss: 0.5555Epoch 5/15: [====                          ] 10/75 batches, loss: 0.5535Epoch 5/15: [====                          ] 11/75 batches, loss: 0.5587Epoch 5/15: [====                          ] 12/75 batches, loss: 0.5574Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.5507Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.5431Epoch 5/15: [======                        ] 15/75 batches, loss: 0.5370Epoch 5/15: [======                        ] 16/75 batches, loss: 0.5367Epoch 5/15: [======                        ] 17/75 batches, loss: 0.5370Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.5396Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.5358Epoch 5/15: [========                      ] 20/75 batches, loss: 0.5345Epoch 5/15: [========                      ] 21/75 batches, loss: 0.5334Epoch 5/15: [========                      ] 22/75 batches, loss: 0.5363Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.5381Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.5343Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.5307Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.5322Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.5372Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.5373Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.5429Epoch 5/15: [============                  ] 30/75 batches, loss: 0.5445Epoch 5/15: [============                  ] 31/75 batches, loss: 0.5451Epoch 5/15: [============                  ] 32/75 batches, loss: 0.5453Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.5448Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.5439Epoch 5/15: [==============                ] 35/75 batches, loss: 0.5462Epoch 5/15: [==============                ] 36/75 batches, loss: 0.5490Epoch 5/15: [==============                ] 37/75 batches, loss: 0.5484Epoch 5/15: [===============               ] 38/75 batches, loss: 0.5493Epoch 5/15: [===============               ] 39/75 batches, loss: 0.5475Epoch 5/15: [================              ] 40/75 batches, loss: 0.5482Epoch 5/15: [================              ] 41/75 batches, loss: 0.5490Epoch 5/15: [================              ] 42/75 batches, loss: 0.5486Epoch 5/15: [=================             ] 43/75 batches, loss: 0.5462Epoch 5/15: [=================             ] 44/75 batches, loss: 0.5455Epoch 5/15: [==================            ] 45/75 batches, loss: 0.5467Epoch 5/15: [==================            ] 46/75 batches, loss: 0.5451Epoch 5/15: [==================            ] 47/75 batches, loss: 0.5433Epoch 5/15: [===================           ] 48/75 batches, loss: 0.5401Epoch 5/15: [===================           ] 49/75 batches, loss: 0.5411Epoch 5/15: [====================          ] 50/75 batches, loss: 0.5427Epoch 5/15: [====================          ] 51/75 batches, loss: 0.5427Epoch 5/15: [====================          ] 52/75 batches, loss: 0.5430Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.5456Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.5443Epoch 5/15: [======================        ] 55/75 batches, loss: 0.5434Epoch 5/15: [======================        ] 56/75 batches, loss: 0.5432Epoch 5/15: [======================        ] 57/75 batches, loss: 0.5440Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.5438Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.5442Epoch 5/15: [========================      ] 60/75 batches, loss: 0.5439Epoch 5/15: [========================      ] 61/75 batches, loss: 0.5430Epoch 5/15: [========================      ] 62/75 batches, loss: 0.5428Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.5443Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.5456Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.5457Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.5458Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.5462Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.5449Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.5457Epoch 5/15: [============================  ] 70/75 batches, loss: 0.5454Epoch 5/15: [============================  ] 71/75 batches, loss: 0.5447Epoch 5/15: [============================  ] 72/75 batches, loss: 0.5455Epoch 5/15: [============================= ] 73/75 batches, loss: 0.5454Epoch 5/15: [============================= ] 74/75 batches, loss: 0.5445Epoch 5/15: [==============================] 75/75 batches, loss: 0.5445
[2025-05-07 11:52:53,185][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5445
[2025-05-07 11:52:53,438][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5467, Metrics: {'accuracy': 0.9365079365079365, 'f1': 0.9333333333333333, 'precision': 0.9333333333333333, 'recall': 0.9333333333333333}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.5864Epoch 6/15: [                              ] 2/75 batches, loss: 0.5804Epoch 6/15: [=                             ] 3/75 batches, loss: 0.5824Epoch 6/15: [=                             ] 4/75 batches, loss: 0.5892Epoch 6/15: [==                            ] 5/75 batches, loss: 0.5622Epoch 6/15: [==                            ] 6/75 batches, loss: 0.5659Epoch 6/15: [==                            ] 7/75 batches, loss: 0.5647Epoch 6/15: [===                           ] 8/75 batches, loss: 0.5534Epoch 6/15: [===                           ] 9/75 batches, loss: 0.5522Epoch 6/15: [====                          ] 10/75 batches, loss: 0.5458Epoch 6/15: [====                          ] 11/75 batches, loss: 0.5490Epoch 6/15: [====                          ] 12/75 batches, loss: 0.5497Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.5514Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.5502Epoch 6/15: [======                        ] 15/75 batches, loss: 0.5460Epoch 6/15: [======                        ] 16/75 batches, loss: 0.5432Epoch 6/15: [======                        ] 17/75 batches, loss: 0.5392Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.5387Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.5391Epoch 6/15: [========                      ] 20/75 batches, loss: 0.5399Epoch 6/15: [========                      ] 21/75 batches, loss: 0.5421Epoch 6/15: [========                      ] 22/75 batches, loss: 0.5394Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.5384Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.5376Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.5418Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.5431Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.5423Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.5444Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.5440Epoch 6/15: [============                  ] 30/75 batches, loss: 0.5465Epoch 6/15: [============                  ] 31/75 batches, loss: 0.5456Epoch 6/15: [============                  ] 32/75 batches, loss: 0.5449Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.5422Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.5419Epoch 6/15: [==============                ] 35/75 batches, loss: 0.5415Epoch 6/15: [==============                ] 36/75 batches, loss: 0.5407Epoch 6/15: [==============                ] 37/75 batches, loss: 0.5387Epoch 6/15: [===============               ] 38/75 batches, loss: 0.5405Epoch 6/15: [===============               ] 39/75 batches, loss: 0.5407Epoch 6/15: [================              ] 40/75 batches, loss: 0.5404Epoch 6/15: [================              ] 41/75 batches, loss: 0.5385Epoch 6/15: [================              ] 42/75 batches, loss: 0.5387Epoch 6/15: [=================             ] 43/75 batches, loss: 0.5390Epoch 6/15: [=================             ] 44/75 batches, loss: 0.5393Epoch 6/15: [==================            ] 45/75 batches, loss: 0.5386Epoch 6/15: [==================            ] 46/75 batches, loss: 0.5384Epoch 6/15: [==================            ] 47/75 batches, loss: 0.5378Epoch 6/15: [===================           ] 48/75 batches, loss: 0.5380Epoch 6/15: [===================           ] 49/75 batches, loss: 0.5392Epoch 6/15: [====================          ] 50/75 batches, loss: 0.5387Epoch 6/15: [====================          ] 51/75 batches, loss: 0.5395Epoch 6/15: [====================          ] 52/75 batches, loss: 0.5383Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.5373Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.5367Epoch 6/15: [======================        ] 55/75 batches, loss: 0.5361Epoch 6/15: [======================        ] 56/75 batches, loss: 0.5373Epoch 6/15: [======================        ] 57/75 batches, loss: 0.5379Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.5368Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.5365Epoch 6/15: [========================      ] 60/75 batches, loss: 0.5349Epoch 6/15: [========================      ] 61/75 batches, loss: 0.5350Epoch 6/15: [========================      ] 62/75 batches, loss: 0.5345Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.5348Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.5358Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.5366Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.5378Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.5371Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.5372Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.5367Epoch 6/15: [============================  ] 70/75 batches, loss: 0.5366Epoch 6/15: [============================  ] 71/75 batches, loss: 0.5359Epoch 6/15: [============================  ] 72/75 batches, loss: 0.5362Epoch 6/15: [============================= ] 73/75 batches, loss: 0.5354Epoch 6/15: [============================= ] 74/75 batches, loss: 0.5350Epoch 6/15: [==============================] 75/75 batches, loss: 0.5354
[2025-05-07 11:52:56,090][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5354
[2025-05-07 11:52:56,419][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5397, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9473684210526315, 'precision': 1.0, 'recall': 0.9}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.5669Epoch 7/15: [                              ] 2/75 batches, loss: 0.5370Epoch 7/15: [=                             ] 3/75 batches, loss: 0.5440Epoch 7/15: [=                             ] 4/75 batches, loss: 0.5318Epoch 7/15: [==                            ] 5/75 batches, loss: 0.5404Epoch 7/15: [==                            ] 6/75 batches, loss: 0.5429Epoch 7/15: [==                            ] 7/75 batches, loss: 0.5504Epoch 7/15: [===                           ] 8/75 batches, loss: 0.5530Epoch 7/15: [===                           ] 9/75 batches, loss: 0.5507Epoch 7/15: [====                          ] 10/75 batches, loss: 0.5533Epoch 7/15: [====                          ] 11/75 batches, loss: 0.5411Epoch 7/15: [====                          ] 12/75 batches, loss: 0.5403Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.5434Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.5340Epoch 7/15: [======                        ] 15/75 batches, loss: 0.5337Epoch 7/15: [======                        ] 16/75 batches, loss: 0.5339Epoch 7/15: [======                        ] 17/75 batches, loss: 0.5332Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.5352Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.5411Epoch 7/15: [========                      ] 20/75 batches, loss: 0.5417Epoch 7/15: [========                      ] 21/75 batches, loss: 0.5390Epoch 7/15: [========                      ] 22/75 batches, loss: 0.5400Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.5402Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.5398Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.5443Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.5437Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.5437Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.5426Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.5415Epoch 7/15: [============                  ] 30/75 batches, loss: 0.5393Epoch 7/15: [============                  ] 31/75 batches, loss: 0.5367Epoch 7/15: [============                  ] 32/75 batches, loss: 0.5352Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.5368Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.5386Epoch 7/15: [==============                ] 35/75 batches, loss: 0.5393Epoch 7/15: [==============                ] 36/75 batches, loss: 0.5371Epoch 7/15: [==============                ] 37/75 batches, loss: 0.5363Epoch 7/15: [===============               ] 38/75 batches, loss: 0.5346Epoch 7/15: [===============               ] 39/75 batches, loss: 0.5326Epoch 7/15: [================              ] 40/75 batches, loss: 0.5302Epoch 7/15: [================              ] 41/75 batches, loss: 0.5306Epoch 7/15: [================              ] 42/75 batches, loss: 0.5309Epoch 7/15: [=================             ] 43/75 batches, loss: 0.5298Epoch 7/15: [=================             ] 44/75 batches, loss: 0.5291Epoch 7/15: [==================            ] 45/75 batches, loss: 0.5298Epoch 7/15: [==================            ] 46/75 batches, loss: 0.5282Epoch 7/15: [==================            ] 47/75 batches, loss: 0.5296Epoch 7/15: [===================           ] 48/75 batches, loss: 0.5306Epoch 7/15: [===================           ] 49/75 batches, loss: 0.5317Epoch 7/15: [====================          ] 50/75 batches, loss: 0.5324Epoch 7/15: [====================          ] 51/75 batches, loss: 0.5319Epoch 7/15: [====================          ] 52/75 batches, loss: 0.5329Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.5342Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.5332Epoch 7/15: [======================        ] 55/75 batches, loss: 0.5349Epoch 7/15: [======================        ] 56/75 batches, loss: 0.5345Epoch 7/15: [======================        ] 57/75 batches, loss: 0.5345Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.5364Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.5362Epoch 7/15: [========================      ] 60/75 batches, loss: 0.5367Epoch 7/15: [========================      ] 61/75 batches, loss: 0.5364Epoch 7/15: [========================      ] 62/75 batches, loss: 0.5361Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.5368Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.5372Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.5375Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.5362Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.5368Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.5364Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.5360Epoch 7/15: [============================  ] 70/75 batches, loss: 0.5365Epoch 7/15: [============================  ] 71/75 batches, loss: 0.5374Epoch 7/15: [============================  ] 72/75 batches, loss: 0.5372Epoch 7/15: [============================= ] 73/75 batches, loss: 0.5370Epoch 7/15: [============================= ] 74/75 batches, loss: 0.5360Epoch 7/15: [==============================] 75/75 batches, loss: 0.5349
[2025-05-07 11:52:59,110][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5349
[2025-05-07 11:52:59,356][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5426, Metrics: {'accuracy': 0.9365079365079365, 'f1': 0.9310344827586207, 'precision': 0.9642857142857143, 'recall': 0.9}
[2025-05-07 11:52:59,357][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.6457Epoch 8/15: [                              ] 2/75 batches, loss: 0.5646Epoch 8/15: [=                             ] 3/75 batches, loss: 0.5668Epoch 8/15: [=                             ] 4/75 batches, loss: 0.5572Epoch 8/15: [==                            ] 5/75 batches, loss: 0.5727Epoch 8/15: [==                            ] 6/75 batches, loss: 0.5634Epoch 8/15: [==                            ] 7/75 batches, loss: 0.5526Epoch 8/15: [===                           ] 8/75 batches, loss: 0.5489Epoch 8/15: [===                           ] 9/75 batches, loss: 0.5504Epoch 8/15: [====                          ] 10/75 batches, loss: 0.5461Epoch 8/15: [====                          ] 11/75 batches, loss: 0.5460Epoch 8/15: [====                          ] 12/75 batches, loss: 0.5427Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.5428Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.5417Epoch 8/15: [======                        ] 15/75 batches, loss: 0.5385Epoch 8/15: [======                        ] 16/75 batches, loss: 0.5391Epoch 8/15: [======                        ] 17/75 batches, loss: 0.5421Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.5426Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.5428Epoch 8/15: [========                      ] 20/75 batches, loss: 0.5401Epoch 8/15: [========                      ] 21/75 batches, loss: 0.5441Epoch 8/15: [========                      ] 22/75 batches, loss: 0.5414Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.5411Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.5392Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.5379Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.5365Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.5380Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.5387Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.5385Epoch 8/15: [============                  ] 30/75 batches, loss: 0.5420Epoch 8/15: [============                  ] 31/75 batches, loss: 0.5401Epoch 8/15: [============                  ] 32/75 batches, loss: 0.5400Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.5372Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.5345Epoch 8/15: [==============                ] 35/75 batches, loss: 0.5350Epoch 8/15: [==============                ] 36/75 batches, loss: 0.5344Epoch 8/15: [==============                ] 37/75 batches, loss: 0.5359Epoch 8/15: [===============               ] 38/75 batches, loss: 0.5352Epoch 8/15: [===============               ] 39/75 batches, loss: 0.5359Epoch 8/15: [================              ] 40/75 batches, loss: 0.5366Epoch 8/15: [================              ] 41/75 batches, loss: 0.5355Epoch 8/15: [================              ] 42/75 batches, loss: 0.5346Epoch 8/15: [=================             ] 43/75 batches, loss: 0.5330Epoch 8/15: [=================             ] 44/75 batches, loss: 0.5319Epoch 8/15: [==================            ] 45/75 batches, loss: 0.5297Epoch 8/15: [==================            ] 46/75 batches, loss: 0.5300Epoch 8/15: [==================            ] 47/75 batches, loss: 0.5293Epoch 8/15: [===================           ] 48/75 batches, loss: 0.5290Epoch 8/15: [===================           ] 49/75 batches, loss: 0.5276Epoch 8/15: [====================          ] 50/75 batches, loss: 0.5283Epoch 8/15: [====================          ] 51/75 batches, loss: 0.5284Epoch 8/15: [====================          ] 52/75 batches, loss: 0.5275Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.5273Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.5282Epoch 8/15: [======================        ] 55/75 batches, loss: 0.5288Epoch 8/15: [======================        ] 56/75 batches, loss: 0.5293Epoch 8/15: [======================        ] 57/75 batches, loss: 0.5281Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.5285Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.5291Epoch 8/15: [========================      ] 60/75 batches, loss: 0.5300Epoch 8/15: [========================      ] 61/75 batches, loss: 0.5311Epoch 8/15: [========================      ] 62/75 batches, loss: 0.5310Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.5312Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.5315Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.5308Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.5318Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.5323Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.5334Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.5329Epoch 8/15: [============================  ] 70/75 batches, loss: 0.5324Epoch 8/15: [============================  ] 71/75 batches, loss: 0.5329Epoch 8/15: [============================  ] 72/75 batches, loss: 0.5340Epoch 8/15: [============================= ] 73/75 batches, loss: 0.5345Epoch 8/15: [============================= ] 74/75 batches, loss: 0.5329Epoch 8/15: [==============================] 75/75 batches, loss: 0.5342
[2025-05-07 11:53:01,674][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5342
[2025-05-07 11:53:01,981][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5472, Metrics: {'accuracy': 0.9206349206349206, 'f1': 0.9152542372881356, 'precision': 0.9310344827586207, 'recall': 0.9}
[2025-05-07 11:53:01,982][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.5608Epoch 9/15: [                              ] 2/75 batches, loss: 0.5319Epoch 9/15: [=                             ] 3/75 batches, loss: 0.5713Epoch 9/15: [=                             ] 4/75 batches, loss: 0.5674Epoch 9/15: [==                            ] 5/75 batches, loss: 0.5653Epoch 9/15: [==                            ] 6/75 batches, loss: 0.5491Epoch 9/15: [==                            ] 7/75 batches, loss: 0.5461Epoch 9/15: [===                           ] 8/75 batches, loss: 0.5520Epoch 9/15: [===                           ] 9/75 batches, loss: 0.5458Epoch 9/15: [====                          ] 10/75 batches, loss: 0.5466Epoch 9/15: [====                          ] 11/75 batches, loss: 0.5468Epoch 9/15: [====                          ] 12/75 batches, loss: 0.5473Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.5464Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.5458Epoch 9/15: [======                        ] 15/75 batches, loss: 0.5450Epoch 9/15: [======                        ] 16/75 batches, loss: 0.5455Epoch 9/15: [======                        ] 17/75 batches, loss: 0.5445Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.5478Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.5449Epoch 9/15: [========                      ] 20/75 batches, loss: 0.5423Epoch 9/15: [========                      ] 21/75 batches, loss: 0.5429Epoch 9/15: [========                      ] 22/75 batches, loss: 0.5411Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.5378Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.5376Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.5383Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.5362Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.5361Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.5326Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.5333Epoch 9/15: [============                  ] 30/75 batches, loss: 0.5325Epoch 9/15: [============                  ] 31/75 batches, loss: 0.5343Epoch 9/15: [============                  ] 32/75 batches, loss: 0.5331Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.5335Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.5314Epoch 9/15: [==============                ] 35/75 batches, loss: 0.5337Epoch 9/15: [==============                ] 36/75 batches, loss: 0.5340Epoch 9/15: [==============                ] 37/75 batches, loss: 0.5343Epoch 9/15: [===============               ] 38/75 batches, loss: 0.5325Epoch 9/15: [===============               ] 39/75 batches, loss: 0.5326Epoch 9/15: [================              ] 40/75 batches, loss: 0.5313Epoch 9/15: [================              ] 41/75 batches, loss: 0.5303Epoch 9/15: [================              ] 42/75 batches, loss: 0.5321Epoch 9/15: [=================             ] 43/75 batches, loss: 0.5319Epoch 9/15: [=================             ] 44/75 batches, loss: 0.5341Epoch 9/15: [==================            ] 45/75 batches, loss: 0.5346Epoch 9/15: [==================            ] 46/75 batches, loss: 0.5347Epoch 9/15: [==================            ] 47/75 batches, loss: 0.5335Epoch 9/15: [===================           ] 48/75 batches, loss: 0.5345Epoch 9/15: [===================           ] 49/75 batches, loss: 0.5331Epoch 9/15: [====================          ] 50/75 batches, loss: 0.5345Epoch 9/15: [====================          ] 51/75 batches, loss: 0.5345Epoch 9/15: [====================          ] 52/75 batches, loss: 0.5344Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.5356Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.5368Epoch 9/15: [======================        ] 55/75 batches, loss: 0.5366Epoch 9/15: [======================        ] 56/75 batches, loss: 0.5378Epoch 9/15: [======================        ] 57/75 batches, loss: 0.5372Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.5369Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.5360Epoch 9/15: [========================      ] 60/75 batches, loss: 0.5356Epoch 9/15: [========================      ] 61/75 batches, loss: 0.5351Epoch 9/15: [========================      ] 62/75 batches, loss: 0.5342Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.5323Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.5333Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.5335Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.5325Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.5331Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.5333Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.5326Epoch 9/15: [============================  ] 70/75 batches, loss: 0.5326Epoch 9/15: [============================  ] 71/75 batches, loss: 0.5318Epoch 9/15: [============================  ] 72/75 batches, loss: 0.5322Epoch 9/15: [============================= ] 73/75 batches, loss: 0.5313Epoch 9/15: [============================= ] 74/75 batches, loss: 0.5317Epoch 9/15: [==============================] 75/75 batches, loss: 0.5310
[2025-05-07 11:53:04,320][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5310
[2025-05-07 11:53:04,587][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.5462, Metrics: {'accuracy': 0.9206349206349206, 'f1': 0.9152542372881356, 'precision': 0.9310344827586207, 'recall': 0.9}
[2025-05-07 11:53:04,588][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-07 11:53:04,588][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 9
[2025-05-07 11:53:04,588][src.training.lm_trainer][INFO] - Training completed in 26.40 seconds
[2025-05-07 11:53:04,589][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 11:53:07,670][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9757322175732217, 'f1': 0.9753191489361702, 'precision': 0.9930675909878682, 'recall': 0.9581939799331104}
[2025-05-07 11:53:07,671][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9473684210526315, 'precision': 1.0, 'recall': 0.9}
[2025-05-07 11:53:07,671][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9, 'f1': 0.9009009009009009, 'precision': 0.8928571428571429, 'recall': 0.9090909090909091}
[2025-05-07 11:53:09,318][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer10/fi/fi/model.pt
[2025-05-07 11:53:09,320][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▄▆▆▇█
wandb:           best_val_f1 ▁▅▆▆▇█
wandb:         best_val_loss █▄▂▂▂▁
wandb:    best_val_precision ▂▁▄▆▄█
wandb:       best_val_recall ▁█▆▅█▆
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▂▃▃▃▃▃
wandb:            train_loss █▄▃▂▂▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▄▆▆▇█▇▆▆
wandb:                val_f1 ▁▅▆▆▇█▇▆▆
wandb:              val_loss █▄▂▂▂▁▁▂▂
wandb:         val_precision ▂▁▄▆▄█▆▄▄
wandb:            val_recall ▁█▆▅█▆▆▆▆
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.95238
wandb:           best_val_f1 0.94737
wandb:         best_val_loss 0.53969
wandb:    best_val_precision 1
wandb:       best_val_recall 0.9
wandb:      early_stop_epoch 9
wandb:                 epoch 9
wandb:   final_test_accuracy 0.9
wandb:         final_test_f1 0.9009
wandb:  final_test_precision 0.89286
wandb:     final_test_recall 0.90909
wandb:  final_train_accuracy 0.97573
wandb:        final_train_f1 0.97532
wandb: final_train_precision 0.99307
wandb:    final_train_recall 0.95819
wandb:    final_val_accuracy 0.95238
wandb:          final_val_f1 0.94737
wandb:   final_val_precision 1
wandb:      final_val_recall 0.9
wandb:         learning_rate 0.0001
wandb:            train_loss 0.53096
wandb:            train_time 26.39826
wandb:          val_accuracy 0.92063
wandb:                val_f1 0.91525
wandb:              val_loss 0.54618
wandb:         val_precision 0.93103
wandb:            val_recall 0.9
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_115220-cf0v96z1
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_115220-cf0v96z1/logs
Experiment probe_layer10_question_type_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer10/fi/fi/results.json for layer 10
Running experiment: probe_layer10_complexity_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_complexity_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer10/fi"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 11:53:48,593][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer10/fi
experiment_name: probe_layer10_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 11:53:48,593][__main__][INFO] - Normalized task: complexity
[2025-05-07 11:53:48,594][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 11:53:48,594][__main__][INFO] - Determined Task Type: regression
[2025-05-07 11:53:48,598][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-05-07 11:53:48,598][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 11:53:53,271][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 11:53:55,626][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 11:53:55,627][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:53:55,887][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:53:55,938][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:53:56,266][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 11:53:56,277][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:53:56,278][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 11:53:56,279][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:53:56,416][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:53:56,545][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:53:56,610][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 11:53:56,612][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:53:56,612][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 11:53:56,613][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:53:56,708][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:53:56,812][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:53:56,823][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 11:53:56,825][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:53:56,825][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 11:53:56,836][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 11:53:56,836][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 11:53:56,836][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 11:53:56,836][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 11:53:56,836][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 11:53:56,837][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 11:53:56,837][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-05-07 11:53:56,837][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 11:53:56,837][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-05-07 11:53:56,837][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 11:53:56,837][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 11:53:56,837][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 11:53:56,837][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 11:53:56,837][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 11:53:56,838][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-05-07 11:53:56,838][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 11:53:56,838][src.data.datasets][INFO] - Sample label: 1.0
[2025-05-07 11:53:56,838][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 11:53:56,838][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 11:53:56,838][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 11:53:56,838][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 11:53:56,838][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 11:53:56,838][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-05-07 11:53:56,839][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 11:53:56,839][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-05-07 11:53:56,839][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 11:53:56,839][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 11:53:56,839][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 11:53:56,839][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 11:53:56,839][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 11:54:03,999][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 11:54:04,000][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 11:54:04,000][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-07 11:54:04,000][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 11:54:04,003][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 11:54:04,003][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 11:54:04,003][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 11:54:04,004][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 11:54:04,004][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 11:54:04,004][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 11:54:04,005][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5803Epoch 1/15: [                              ] 2/75 batches, loss: 0.8218Epoch 1/15: [=                             ] 3/75 batches, loss: 0.8205Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7954Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7287Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6436Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6142Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6178Epoch 1/15: [===                           ] 9/75 batches, loss: 0.5858Epoch 1/15: [====                          ] 10/75 batches, loss: 0.5980Epoch 1/15: [====                          ] 11/75 batches, loss: 0.5667Epoch 1/15: [====                          ] 12/75 batches, loss: 0.5385Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.5298Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.5199Epoch 1/15: [======                        ] 15/75 batches, loss: 0.5040Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4937Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4965Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4919Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4811Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4758Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4810Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4815Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4716Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4654Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4606Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4535Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4568Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4552Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4466Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4446Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4401Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4399Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4328Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4314Epoch 1/15: [==============                ] 35/75 batches, loss: 0.4277Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4319Epoch 1/15: [==============                ] 37/75 batches, loss: 0.4258Epoch 1/15: [===============               ] 38/75 batches, loss: 0.4244Epoch 1/15: [===============               ] 39/75 batches, loss: 0.4205Epoch 1/15: [================              ] 40/75 batches, loss: 0.4198Epoch 1/15: [================              ] 41/75 batches, loss: 0.4188Epoch 1/15: [================              ] 42/75 batches, loss: 0.4164Epoch 1/15: [=================             ] 43/75 batches, loss: 0.4125Epoch 1/15: [=================             ] 44/75 batches, loss: 0.4115Epoch 1/15: [==================            ] 45/75 batches, loss: 0.4109Epoch 1/15: [==================            ] 46/75 batches, loss: 0.4039Epoch 1/15: [==================            ] 47/75 batches, loss: 0.4001Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3956Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3913Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3902Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3869Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3838Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3821Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3822Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3809Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3770Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3742Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3736Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3722Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3687Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3646Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3615Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3581Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3561Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3534Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3511Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3477Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3447Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3439Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3412Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3380Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3360Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3338Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3305Epoch 1/15: [==============================] 75/75 batches, loss: 0.3277
[2025-05-07 11:54:10,534][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3277
[2025-05-07 11:54:10,834][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0612, Metrics: {'mse': 0.061193760484457016, 'rmse': 0.2473737263422634, 'r2': 0.06660687923431396}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2044Epoch 2/15: [                              ] 2/75 batches, loss: 0.2003Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2020Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2103Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2063Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1833Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1853Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1904Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1791Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1814Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1816Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1778Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1742Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1753Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1734Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1682Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1643Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1652Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1671Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1639Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1657Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1640Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1649Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1623Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1648Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1657Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1732Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1718Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1745Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1767Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1761Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1768Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1745Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1770Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1761Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1742Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1719Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1703Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1703Epoch 2/15: [================              ] 40/75 batches, loss: 0.1684Epoch 2/15: [================              ] 41/75 batches, loss: 0.1677Epoch 2/15: [================              ] 42/75 batches, loss: 0.1657Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1645Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1631Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1636Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1650Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1643Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1639Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1648Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1640Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1631Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1625Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1624Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1610Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1618Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1630Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1612Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1594Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1578Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1565Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1551Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1537Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1531Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1519Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1520Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1518Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1517Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1512Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1495Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1485Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1488Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1478Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1477Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1473Epoch 2/15: [==============================] 75/75 batches, loss: 0.1460
[2025-05-07 11:54:13,521][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1460
[2025-05-07 11:54:13,861][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1010, Metrics: {'mse': 0.10118767619132996, 'rmse': 0.31810010404168365, 'r2': -0.5434232950210571}
[2025-05-07 11:54:13,862][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1475Epoch 3/15: [                              ] 2/75 batches, loss: 0.1178Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1336Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1139Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1092Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1064Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1032Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1031Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1024Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1073Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1040Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1020Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1049Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1062Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1062Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1053Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1065Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1083Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1049Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1032Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1017Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0991Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0980Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0992Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0988Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0987Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0981Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0986Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0995Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0977Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0965Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1002Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0989Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0983Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0971Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0975Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0967Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0972Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0958Epoch 3/15: [================              ] 40/75 batches, loss: 0.0965Epoch 3/15: [================              ] 41/75 batches, loss: 0.0989Epoch 3/15: [================              ] 42/75 batches, loss: 0.0982Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0982Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0972Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0961Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0960Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0967Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0987Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0985Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0975Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0971Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0971Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0969Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0964Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0961Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0959Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0955Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0956Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0960Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0959Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0956Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0952Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0950Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0946Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0940Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0936Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0935Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0942Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0949Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0946Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0943Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0938Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0938Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0938Epoch 3/15: [==============================] 75/75 batches, loss: 0.0945
[2025-05-07 11:54:16,208][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0945
[2025-05-07 11:54:16,515][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0841, Metrics: {'mse': 0.08416565507650375, 'rmse': 0.2901131763234889, 'r2': -0.28378522396087646}
[2025-05-07 11:54:16,515][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0600Epoch 4/15: [                              ] 2/75 batches, loss: 0.1047Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0954Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1074Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1005Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1053Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1033Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1000Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1049Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0984Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1001Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0996Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0962Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0921Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0960Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0977Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0966Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0965Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0975Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0953Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0958Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0987Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0961Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0965Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0971Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0950Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0947Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0941Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0935Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0920Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0906Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0908Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0904Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0905Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0900Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0906Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0893Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0888Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0876Epoch 4/15: [================              ] 40/75 batches, loss: 0.0872Epoch 4/15: [================              ] 41/75 batches, loss: 0.0871Epoch 4/15: [================              ] 42/75 batches, loss: 0.0864Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0864Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0853Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0851Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0854Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0850Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0850Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0857Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0860Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0861Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0861Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0855Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0851Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0848Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0850Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0840Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0837Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0836Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0830Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0834Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0826Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0828Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0826Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0824Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0822Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0817Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0823Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0823Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0819Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0813Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0810Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0809Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0801Epoch 4/15: [==============================] 75/75 batches, loss: 0.0796
[2025-05-07 11:54:18,891][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0796
[2025-05-07 11:54:19,117][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0819, Metrics: {'mse': 0.08196916431188583, 'rmse': 0.2863025747559491, 'r2': -0.2502819299697876}
[2025-05-07 11:54:19,117][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0627Epoch 5/15: [                              ] 2/75 batches, loss: 0.0578Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0656Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0627Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0555Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0604Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0623Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0640Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0619Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0592Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0574Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0566Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0544Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0542Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0542Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0547Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0551Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0549Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0563Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0556Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0585Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0576Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0584Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0592Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0602Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0592Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0591Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0593Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0595Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0591Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0589Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0594Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0593Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0590Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0601Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0603Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0603Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0600Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0596Epoch 5/15: [================              ] 40/75 batches, loss: 0.0603Epoch 5/15: [================              ] 41/75 batches, loss: 0.0600Epoch 5/15: [================              ] 42/75 batches, loss: 0.0594Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0603Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0602Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0600Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0608Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0605Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0604Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0612Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0608Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0603Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0597Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0595Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0592Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0587Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0590Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0586Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0587Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0584Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0579Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0576Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0574Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0575Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0576Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0577Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0575Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0576Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0577Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0573Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0573Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0571Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0581Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0579Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0574Epoch 5/15: [==============================] 75/75 batches, loss: 0.0572
[2025-05-07 11:54:21,390][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0572
[2025-05-07 11:54:21,637][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0704, Metrics: {'mse': 0.07036285102367401, 'rmse': 0.2652599687545673, 'r2': -0.0732499361038208}
[2025-05-07 11:54:21,638][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 11:54:21,638][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-07 11:54:21,638][src.training.lm_trainer][INFO] - Training completed in 14.27 seconds
[2025-05-07 11:54:21,638][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 11:54:24,700][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.02627338469028473, 'rmse': 0.16209066811597986, 'r2': -0.30014467239379883}
[2025-05-07 11:54:24,701][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.061193760484457016, 'rmse': 0.2473737263422634, 'r2': 0.06660687923431396}
[2025-05-07 11:54:24,701][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04039514437317848, 'rmse': 0.20098543323628826, 'r2': -0.022998332977294922}
[2025-05-07 11:54:26,344][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer10/fi/fi/model.pt
[2025-05-07 11:54:26,345][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▁▃▃
wandb:       train_loss █▃▂▂▁
wandb:       train_time ▁
wandb:         val_loss ▁█▅▅▃
wandb:          val_mse ▁█▅▅▃
wandb:           val_r2 █▁▄▄▆
wandb:         val_rmse ▁█▅▅▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06124
wandb:     best_val_mse 0.06119
wandb:      best_val_r2 0.06661
wandb:    best_val_rmse 0.24737
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.0404
wandb:    final_test_r2 -0.023
wandb:  final_test_rmse 0.20099
wandb:  final_train_mse 0.02627
wandb:   final_train_r2 -0.30014
wandb: final_train_rmse 0.16209
wandb:    final_val_mse 0.06119
wandb:     final_val_r2 0.06661
wandb:   final_val_rmse 0.24737
wandb:    learning_rate 0.0001
wandb:       train_loss 0.05719
wandb:       train_time 14.27006
wandb:         val_loss 0.07038
wandb:          val_mse 0.07036
wandb:           val_r2 -0.07325
wandb:         val_rmse 0.26526
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_115348-3gg63hcg
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_115348-3gg63hcg/logs
Experiment probe_layer10_complexity_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer10/fi/fi/results.json for layer 10
Running experiment: probe_layer10_question_type_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_question_type_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer10/ja"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 11:54:55,937][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer10/ja
experiment_name: probe_layer10_question_type_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-07 11:54:55,937][__main__][INFO] - Normalized task: question_type
[2025-05-07 11:54:55,937][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-07 11:54:55,937][__main__][INFO] - Determined Task Type: classification
[2025-05-07 11:54:55,941][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ja']
[2025-05-07 11:54:55,942][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 11:54:59,332][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 11:55:01,636][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 11:55:01,636][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:55:01,942][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:55:02,058][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:55:02,364][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 11:55:02,374][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:55:02,375][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 11:55:02,387][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:55:02,471][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:55:02,617][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:55:02,654][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 11:55:02,655][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:55:02,655][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 11:55:02,656][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:55:02,705][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:55:02,824][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:55:02,870][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 11:55:02,872][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:55:02,872][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 11:55:02,874][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 11:55:02,874][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:55:02,874][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:55:02,874][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:55:02,874][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:55:02,874][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-05-07 11:55:02,875][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-05-07 11:55:02,875][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 11:55:02,875][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 11:55:02,875][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:55:02,875][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:55:02,875][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:55:02,875][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:55:02,875][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-05-07 11:55:02,875][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-05-07 11:55:02,875][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 11:55:02,875][src.data.datasets][INFO] - Sample label: 0
[2025-05-07 11:55:02,876][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:55:02,876][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:55:02,876][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:55:02,876][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:55:02,876][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-05-07 11:55:02,876][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-05-07 11:55:02,876][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 11:55:02,876][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 11:55:02,876][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 11:55:02,876][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 11:55:02,877][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 11:55:02,877][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-07 11:55:02,877][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 11:55:12,742][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 11:55:12,742][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 11:55:12,743][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-07 11:55:12,743][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-07 11:55:12,748][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-07 11:55:12,749][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-07 11:55:12,749][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-07 11:55:12,749][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-07 11:55:12,749][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 11:55:12,750][__main__][INFO] - Total parameters: 394,568,839
[2025-05-07 11:55:12,750][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-07 11:55:12,751][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6863Epoch 1/15: [                              ] 2/75 batches, loss: 0.7203Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7145Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7021Epoch 1/15: [==                            ] 5/75 batches, loss: 0.6922Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6951Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6963Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6943Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6945Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6908Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6902Epoch 1/15: [====                          ] 12/75 batches, loss: 0.6890Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.6893Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.6888Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6867Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6846Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6855Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6848Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6838Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6810Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6784Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6741Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6762Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6728Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6741Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6744Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6717Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6759Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6756Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6736Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6721Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6698Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6679Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6681Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6667Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6646Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6628Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6613Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6612Epoch 1/15: [================              ] 40/75 batches, loss: 0.6607Epoch 1/15: [================              ] 41/75 batches, loss: 0.6594Epoch 1/15: [================              ] 42/75 batches, loss: 0.6593Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6575Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6559Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6551Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6550Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6530Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6502Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6491Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6488Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6473Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6462Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6451Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6434Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6411Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6392Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6394Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6390Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6376Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6367Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6340Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6324Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6325Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6314Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6286Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6288Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6287Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6281Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6266Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6263Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6247Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6240Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6237Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6221Epoch 1/15: [==============================] 75/75 batches, loss: 0.6227
[2025-05-07 11:55:20,825][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6227
[2025-05-07 11:55:21,057][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.5219, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.9565217391304348, 'precision': 1.0, 'recall': 0.9166666666666666}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.4773Epoch 2/15: [                              ] 2/75 batches, loss: 0.5159Epoch 2/15: [=                             ] 3/75 batches, loss: 0.5381Epoch 2/15: [=                             ] 4/75 batches, loss: 0.5229Epoch 2/15: [==                            ] 5/75 batches, loss: 0.5226Epoch 2/15: [==                            ] 6/75 batches, loss: 0.5372Epoch 2/15: [==                            ] 7/75 batches, loss: 0.5386Epoch 2/15: [===                           ] 8/75 batches, loss: 0.5359Epoch 2/15: [===                           ] 9/75 batches, loss: 0.5229Epoch 2/15: [====                          ] 10/75 batches, loss: 0.5233Epoch 2/15: [====                          ] 11/75 batches, loss: 0.5251Epoch 2/15: [====                          ] 12/75 batches, loss: 0.5252Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.5219Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.5245Epoch 2/15: [======                        ] 15/75 batches, loss: 0.5260Epoch 2/15: [======                        ] 16/75 batches, loss: 0.5223Epoch 2/15: [======                        ] 17/75 batches, loss: 0.5216Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.5233Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.5202Epoch 2/15: [========                      ] 20/75 batches, loss: 0.5244Epoch 2/15: [========                      ] 21/75 batches, loss: 0.5260Epoch 2/15: [========                      ] 22/75 batches, loss: 0.5253Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.5247Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.5258Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.5247Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.5251Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.5240Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.5276Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.5247Epoch 2/15: [============                  ] 30/75 batches, loss: 0.5231Epoch 2/15: [============                  ] 31/75 batches, loss: 0.5236Epoch 2/15: [============                  ] 32/75 batches, loss: 0.5258Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.5269Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.5245Epoch 2/15: [==============                ] 35/75 batches, loss: 0.5257Epoch 2/15: [==============                ] 36/75 batches, loss: 0.5255Epoch 2/15: [==============                ] 37/75 batches, loss: 0.5266Epoch 2/15: [===============               ] 38/75 batches, loss: 0.5260Epoch 2/15: [===============               ] 39/75 batches, loss: 0.5249Epoch 2/15: [================              ] 40/75 batches, loss: 0.5253Epoch 2/15: [================              ] 41/75 batches, loss: 0.5249Epoch 2/15: [================              ] 42/75 batches, loss: 0.5271Epoch 2/15: [=================             ] 43/75 batches, loss: 0.5253Epoch 2/15: [=================             ] 44/75 batches, loss: 0.5261Epoch 2/15: [==================            ] 45/75 batches, loss: 0.5275Epoch 2/15: [==================            ] 46/75 batches, loss: 0.5256Epoch 2/15: [==================            ] 47/75 batches, loss: 0.5261Epoch 2/15: [===================           ] 48/75 batches, loss: 0.5271Epoch 2/15: [===================           ] 49/75 batches, loss: 0.5275Epoch 2/15: [====================          ] 50/75 batches, loss: 0.5280Epoch 2/15: [====================          ] 51/75 batches, loss: 0.5274Epoch 2/15: [====================          ] 52/75 batches, loss: 0.5273Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.5275Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.5282Epoch 2/15: [======================        ] 55/75 batches, loss: 0.5279Epoch 2/15: [======================        ] 56/75 batches, loss: 0.5278Epoch 2/15: [======================        ] 57/75 batches, loss: 0.5272Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.5278Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.5277Epoch 2/15: [========================      ] 60/75 batches, loss: 0.5279Epoch 2/15: [========================      ] 61/75 batches, loss: 0.5291Epoch 2/15: [========================      ] 62/75 batches, loss: 0.5300Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.5306Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.5304Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.5294Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.5291Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.5284Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.5288Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.5284Epoch 2/15: [============================  ] 70/75 batches, loss: 0.5288Epoch 2/15: [============================  ] 71/75 batches, loss: 0.5282Epoch 2/15: [============================  ] 72/75 batches, loss: 0.5282Epoch 2/15: [============================= ] 73/75 batches, loss: 0.5289Epoch 2/15: [============================= ] 74/75 batches, loss: 0.5280Epoch 2/15: [==============================] 75/75 batches, loss: 0.5268
[2025-05-07 11:55:23,749][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.5268
[2025-05-07 11:55:24,048][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.5038, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9787234042553191, 'precision': 1.0, 'recall': 0.9583333333333334}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.5124Epoch 3/15: [                              ] 2/75 batches, loss: 0.5170Epoch 3/15: [=                             ] 3/75 batches, loss: 0.5067Epoch 3/15: [=                             ] 4/75 batches, loss: 0.5101Epoch 3/15: [==                            ] 5/75 batches, loss: 0.5037Epoch 3/15: [==                            ] 6/75 batches, loss: 0.5078Epoch 3/15: [==                            ] 7/75 batches, loss: 0.5076Epoch 3/15: [===                           ] 8/75 batches, loss: 0.5033Epoch 3/15: [===                           ] 9/75 batches, loss: 0.5017Epoch 3/15: [====                          ] 10/75 batches, loss: 0.4938Epoch 3/15: [====                          ] 11/75 batches, loss: 0.4949Epoch 3/15: [====                          ] 12/75 batches, loss: 0.4958Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.4933Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.4931Epoch 3/15: [======                        ] 15/75 batches, loss: 0.4911Epoch 3/15: [======                        ] 16/75 batches, loss: 0.4913Epoch 3/15: [======                        ] 17/75 batches, loss: 0.4910Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.4892Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.4891Epoch 3/15: [========                      ] 20/75 batches, loss: 0.4955Epoch 3/15: [========                      ] 21/75 batches, loss: 0.4941Epoch 3/15: [========                      ] 22/75 batches, loss: 0.4944Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.4931Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.4918Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.4936Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.4945Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.4950Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.4946Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.4933Epoch 3/15: [============                  ] 30/75 batches, loss: 0.4957Epoch 3/15: [============                  ] 31/75 batches, loss: 0.4953Epoch 3/15: [============                  ] 32/75 batches, loss: 0.4957Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.4939Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.4966Epoch 3/15: [==============                ] 35/75 batches, loss: 0.4967Epoch 3/15: [==============                ] 36/75 batches, loss: 0.4990Epoch 3/15: [==============                ] 37/75 batches, loss: 0.5006Epoch 3/15: [===============               ] 38/75 batches, loss: 0.5008Epoch 3/15: [===============               ] 39/75 batches, loss: 0.5023Epoch 3/15: [================              ] 40/75 batches, loss: 0.5030Epoch 3/15: [================              ] 41/75 batches, loss: 0.5040Epoch 3/15: [================              ] 42/75 batches, loss: 0.5053Epoch 3/15: [=================             ] 43/75 batches, loss: 0.5066Epoch 3/15: [=================             ] 44/75 batches, loss: 0.5078Epoch 3/15: [==================            ] 45/75 batches, loss: 0.5090Epoch 3/15: [==================            ] 46/75 batches, loss: 0.5110Epoch 3/15: [==================            ] 47/75 batches, loss: 0.5100Epoch 3/15: [===================           ] 48/75 batches, loss: 0.5114Epoch 3/15: [===================           ] 49/75 batches, loss: 0.5129Epoch 3/15: [====================          ] 50/75 batches, loss: 0.5131Epoch 3/15: [====================          ] 51/75 batches, loss: 0.5132Epoch 3/15: [====================          ] 52/75 batches, loss: 0.5136Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.5129Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.5137Epoch 3/15: [======================        ] 55/75 batches, loss: 0.5147Epoch 3/15: [======================        ] 56/75 batches, loss: 0.5143Epoch 3/15: [======================        ] 57/75 batches, loss: 0.5134Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.5132Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.5142Epoch 3/15: [========================      ] 60/75 batches, loss: 0.5141Epoch 3/15: [========================      ] 61/75 batches, loss: 0.5148Epoch 3/15: [========================      ] 62/75 batches, loss: 0.5143Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.5146Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.5145Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.5146Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.5153Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.5155Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.5164Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.5152Epoch 3/15: [============================  ] 70/75 batches, loss: 0.5152Epoch 3/15: [============================  ] 71/75 batches, loss: 0.5151Epoch 3/15: [============================  ] 72/75 batches, loss: 0.5153Epoch 3/15: [============================= ] 73/75 batches, loss: 0.5149Epoch 3/15: [============================= ] 74/75 batches, loss: 0.5148Epoch 3/15: [==============================] 75/75 batches, loss: 0.5172
[2025-05-07 11:55:26,795][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.5172
[2025-05-07 11:55:27,159][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.5014, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9787234042553191, 'precision': 1.0, 'recall': 0.9583333333333334}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6031Epoch 4/15: [                              ] 2/75 batches, loss: 0.5681Epoch 4/15: [=                             ] 3/75 batches, loss: 0.5565Epoch 4/15: [=                             ] 4/75 batches, loss: 0.5330Epoch 4/15: [==                            ] 5/75 batches, loss: 0.5451Epoch 4/15: [==                            ] 6/75 batches, loss: 0.5464Epoch 4/15: [==                            ] 7/75 batches, loss: 0.5436Epoch 4/15: [===                           ] 8/75 batches, loss: 0.5419Epoch 4/15: [===                           ] 9/75 batches, loss: 0.5366Epoch 4/15: [====                          ] 10/75 batches, loss: 0.5339Epoch 4/15: [====                          ] 11/75 batches, loss: 0.5227Epoch 4/15: [====                          ] 12/75 batches, loss: 0.5174Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.5109Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.5057Epoch 4/15: [======                        ] 15/75 batches, loss: 0.5092Epoch 4/15: [======                        ] 16/75 batches, loss: 0.5096Epoch 4/15: [======                        ] 17/75 batches, loss: 0.5091Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.5118Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.5141Epoch 4/15: [========                      ] 20/75 batches, loss: 0.5146Epoch 4/15: [========                      ] 21/75 batches, loss: 0.5176Epoch 4/15: [========                      ] 22/75 batches, loss: 0.5203Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.5189Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.5165Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.5123Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.5120Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.5128Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.5137Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.5159Epoch 4/15: [============                  ] 30/75 batches, loss: 0.5174Epoch 4/15: [============                  ] 31/75 batches, loss: 0.5167Epoch 4/15: [============                  ] 32/75 batches, loss: 0.5156Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.5124Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.5133Epoch 4/15: [==============                ] 35/75 batches, loss: 0.5122Epoch 4/15: [==============                ] 36/75 batches, loss: 0.5139Epoch 4/15: [==============                ] 37/75 batches, loss: 0.5138Epoch 4/15: [===============               ] 38/75 batches, loss: 0.5154Epoch 4/15: [===============               ] 39/75 batches, loss: 0.5163Epoch 4/15: [================              ] 40/75 batches, loss: 0.5157Epoch 4/15: [================              ] 41/75 batches, loss: 0.5163Epoch 4/15: [================              ] 42/75 batches, loss: 0.5150Epoch 4/15: [=================             ] 43/75 batches, loss: 0.5148Epoch 4/15: [=================             ] 44/75 batches, loss: 0.5119Epoch 4/15: [==================            ] 45/75 batches, loss: 0.5093Epoch 4/15: [==================            ] 46/75 batches, loss: 0.5093Epoch 4/15: [==================            ] 47/75 batches, loss: 0.5088Epoch 4/15: [===================           ] 48/75 batches, loss: 0.5097Epoch 4/15: [===================           ] 49/75 batches, loss: 0.5105Epoch 4/15: [====================          ] 50/75 batches, loss: 0.5116Epoch 4/15: [====================          ] 51/75 batches, loss: 0.5119Epoch 4/15: [====================          ] 52/75 batches, loss: 0.5131Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.5138Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.5128Epoch 4/15: [======================        ] 55/75 batches, loss: 0.5122Epoch 4/15: [======================        ] 56/75 batches, loss: 0.5133Epoch 4/15: [======================        ] 57/75 batches, loss: 0.5144Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.5147Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.5146Epoch 4/15: [========================      ] 60/75 batches, loss: 0.5141Epoch 4/15: [========================      ] 61/75 batches, loss: 0.5145Epoch 4/15: [========================      ] 62/75 batches, loss: 0.5156Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.5155Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.5160Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.5151Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.5139Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.5131Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.5139Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.5148Epoch 4/15: [============================  ] 70/75 batches, loss: 0.5143Epoch 4/15: [============================  ] 71/75 batches, loss: 0.5152Epoch 4/15: [============================  ] 72/75 batches, loss: 0.5151Epoch 4/15: [============================= ] 73/75 batches, loss: 0.5153Epoch 4/15: [============================= ] 74/75 batches, loss: 0.5135Epoch 4/15: [==============================] 75/75 batches, loss: 0.5145
[2025-05-07 11:55:29,851][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5145
[2025-05-07 11:55:30,324][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.4957, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.4820Epoch 5/15: [                              ] 2/75 batches, loss: 0.5088Epoch 5/15: [=                             ] 3/75 batches, loss: 0.5000Epoch 5/15: [=                             ] 4/75 batches, loss: 0.4892Epoch 5/15: [==                            ] 5/75 batches, loss: 0.5016Epoch 5/15: [==                            ] 6/75 batches, loss: 0.4986Epoch 5/15: [==                            ] 7/75 batches, loss: 0.5030Epoch 5/15: [===                           ] 8/75 batches, loss: 0.5094Epoch 5/15: [===                           ] 9/75 batches, loss: 0.5176Epoch 5/15: [====                          ] 10/75 batches, loss: 0.5117Epoch 5/15: [====                          ] 11/75 batches, loss: 0.5109Epoch 5/15: [====                          ] 12/75 batches, loss: 0.5086Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.5118Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.5097Epoch 5/15: [======                        ] 15/75 batches, loss: 0.5110Epoch 5/15: [======                        ] 16/75 batches, loss: 0.5109Epoch 5/15: [======                        ] 17/75 batches, loss: 0.5081Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.5105Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.5151Epoch 5/15: [========                      ] 20/75 batches, loss: 0.5134Epoch 5/15: [========                      ] 21/75 batches, loss: 0.5107Epoch 5/15: [========                      ] 22/75 batches, loss: 0.5118Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.5126Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.5113Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.5105Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.5112Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.5101Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.5110Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.5108Epoch 5/15: [============                  ] 30/75 batches, loss: 0.5136Epoch 5/15: [============                  ] 31/75 batches, loss: 0.5119Epoch 5/15: [============                  ] 32/75 batches, loss: 0.5111Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.5116Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.5128Epoch 5/15: [==============                ] 35/75 batches, loss: 0.5126Epoch 5/15: [==============                ] 36/75 batches, loss: 0.5098Epoch 5/15: [==============                ] 37/75 batches, loss: 0.5090Epoch 5/15: [===============               ] 38/75 batches, loss: 0.5096Epoch 5/15: [===============               ] 39/75 batches, loss: 0.5097Epoch 5/15: [================              ] 40/75 batches, loss: 0.5107Epoch 5/15: [================              ] 41/75 batches, loss: 0.5100Epoch 5/15: [================              ] 42/75 batches, loss: 0.5110Epoch 5/15: [=================             ] 43/75 batches, loss: 0.5115Epoch 5/15: [=================             ] 44/75 batches, loss: 0.5114Epoch 5/15: [==================            ] 45/75 batches, loss: 0.5113Epoch 5/15: [==================            ] 46/75 batches, loss: 0.5117Epoch 5/15: [==================            ] 47/75 batches, loss: 0.5121Epoch 5/15: [===================           ] 48/75 batches, loss: 0.5115Epoch 5/15: [===================           ] 49/75 batches, loss: 0.5117Epoch 5/15: [====================          ] 50/75 batches, loss: 0.5111Epoch 5/15: [====================          ] 51/75 batches, loss: 0.5097Epoch 5/15: [====================          ] 52/75 batches, loss: 0.5101Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.5087Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.5082Epoch 5/15: [======================        ] 55/75 batches, loss: 0.5081Epoch 5/15: [======================        ] 56/75 batches, loss: 0.5085Epoch 5/15: [======================        ] 57/75 batches, loss: 0.5087Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.5087Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.5074Epoch 5/15: [========================      ] 60/75 batches, loss: 0.5073Epoch 5/15: [========================      ] 61/75 batches, loss: 0.5058Epoch 5/15: [========================      ] 62/75 batches, loss: 0.5061Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.5061Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.5050Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.5069Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.5061Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.5072Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.5069Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.5072Epoch 5/15: [============================  ] 70/75 batches, loss: 0.5082Epoch 5/15: [============================  ] 71/75 batches, loss: 0.5098Epoch 5/15: [============================  ] 72/75 batches, loss: 0.5111Epoch 5/15: [============================= ] 73/75 batches, loss: 0.5112Epoch 5/15: [============================= ] 74/75 batches, loss: 0.5105Epoch 5/15: [==============================] 75/75 batches, loss: 0.5108
[2025-05-07 11:55:33,050][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5108
[2025-05-07 11:55:33,473][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.4993, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9787234042553191, 'precision': 1.0, 'recall': 0.9583333333333334}
[2025-05-07 11:55:33,474][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.5983Epoch 6/15: [                              ] 2/75 batches, loss: 0.5971Epoch 6/15: [=                             ] 3/75 batches, loss: 0.5747Epoch 6/15: [=                             ] 4/75 batches, loss: 0.5806Epoch 6/15: [==                            ] 5/75 batches, loss: 0.5651Epoch 6/15: [==                            ] 6/75 batches, loss: 0.5432Epoch 6/15: [==                            ] 7/75 batches, loss: 0.5312Epoch 6/15: [===                           ] 8/75 batches, loss: 0.5250Epoch 6/15: [===                           ] 9/75 batches, loss: 0.5252Epoch 6/15: [====                          ] 10/75 batches, loss: 0.5258Epoch 6/15: [====                          ] 11/75 batches, loss: 0.5257Epoch 6/15: [====                          ] 12/75 batches, loss: 0.5219Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.5223Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.5211Epoch 6/15: [======                        ] 15/75 batches, loss: 0.5202Epoch 6/15: [======                        ] 16/75 batches, loss: 0.5207Epoch 6/15: [======                        ] 17/75 batches, loss: 0.5183Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.5174Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.5132Epoch 6/15: [========                      ] 20/75 batches, loss: 0.5092Epoch 6/15: [========                      ] 21/75 batches, loss: 0.5121Epoch 6/15: [========                      ] 22/75 batches, loss: 0.5141Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.5147Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.5143Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.5130Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.5118Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.5134Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.5133Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.5116Epoch 6/15: [============                  ] 30/75 batches, loss: 0.5105Epoch 6/15: [============                  ] 31/75 batches, loss: 0.5126Epoch 6/15: [============                  ] 32/75 batches, loss: 0.5138Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.5129Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.5134Epoch 6/15: [==============                ] 35/75 batches, loss: 0.5131Epoch 6/15: [==============                ] 36/75 batches, loss: 0.5143Epoch 6/15: [==============                ] 37/75 batches, loss: 0.5149Epoch 6/15: [===============               ] 38/75 batches, loss: 0.5141Epoch 6/15: [===============               ] 39/75 batches, loss: 0.5157Epoch 6/15: [================              ] 40/75 batches, loss: 0.5166Epoch 6/15: [================              ] 41/75 batches, loss: 0.5145Epoch 6/15: [================              ] 42/75 batches, loss: 0.5160Epoch 6/15: [=================             ] 43/75 batches, loss: 0.5162Epoch 6/15: [=================             ] 44/75 batches, loss: 0.5143Epoch 6/15: [==================            ] 45/75 batches, loss: 0.5136Epoch 6/15: [==================            ] 46/75 batches, loss: 0.5145Epoch 6/15: [==================            ] 47/75 batches, loss: 0.5143Epoch 6/15: [===================           ] 48/75 batches, loss: 0.5140Epoch 6/15: [===================           ] 49/75 batches, loss: 0.5141Epoch 6/15: [====================          ] 50/75 batches, loss: 0.5134Epoch 6/15: [====================          ] 51/75 batches, loss: 0.5118Epoch 6/15: [====================          ] 52/75 batches, loss: 0.5107Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.5094Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.5084Epoch 6/15: [======================        ] 55/75 batches, loss: 0.5083Epoch 6/15: [======================        ] 56/75 batches, loss: 0.5079Epoch 6/15: [======================        ] 57/75 batches, loss: 0.5082Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.5089Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.5077Epoch 6/15: [========================      ] 60/75 batches, loss: 0.5072Epoch 6/15: [========================      ] 61/75 batches, loss: 0.5075Epoch 6/15: [========================      ] 62/75 batches, loss: 0.5078Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.5077Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.5073Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.5080Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.5083Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.5079Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.5096Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.5085Epoch 6/15: [============================  ] 70/75 batches, loss: 0.5076Epoch 6/15: [============================  ] 71/75 batches, loss: 0.5076Epoch 6/15: [============================  ] 72/75 batches, loss: 0.5095Epoch 6/15: [============================= ] 73/75 batches, loss: 0.5091Epoch 6/15: [============================= ] 74/75 batches, loss: 0.5093Epoch 6/15: [==============================] 75/75 batches, loss: 0.5078
[2025-05-07 11:55:35,807][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5078
[2025-05-07 11:55:36,059][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.4967, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-07 11:55:36,060][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.5088Epoch 7/15: [                              ] 2/75 batches, loss: 0.4710Epoch 7/15: [=                             ] 3/75 batches, loss: 0.4904Epoch 7/15: [=                             ] 4/75 batches, loss: 0.4796Epoch 7/15: [==                            ] 5/75 batches, loss: 0.4654Epoch 7/15: [==                            ] 6/75 batches, loss: 0.4657Epoch 7/15: [==                            ] 7/75 batches, loss: 0.4662Epoch 7/15: [===                           ] 8/75 batches, loss: 0.4709Epoch 7/15: [===                           ] 9/75 batches, loss: 0.4800Epoch 7/15: [====                          ] 10/75 batches, loss: 0.4777Epoch 7/15: [====                          ] 11/75 batches, loss: 0.4770Epoch 7/15: [====                          ] 12/75 batches, loss: 0.4813Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.4819Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.4875Epoch 7/15: [======                        ] 15/75 batches, loss: 0.4873Epoch 7/15: [======                        ] 16/75 batches, loss: 0.4870Epoch 7/15: [======                        ] 17/75 batches, loss: 0.4880Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.4876Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.4909Epoch 7/15: [========                      ] 20/75 batches, loss: 0.4951Epoch 7/15: [========                      ] 21/75 batches, loss: 0.4952Epoch 7/15: [========                      ] 22/75 batches, loss: 0.4955Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.4991Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.4985Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.5003Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.5031Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.5067Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.5075Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.5071Epoch 7/15: [============                  ] 30/75 batches, loss: 0.5055Epoch 7/15: [============                  ] 31/75 batches, loss: 0.5077Epoch 7/15: [============                  ] 32/75 batches, loss: 0.5087Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.5086Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.5106Epoch 7/15: [==============                ] 35/75 batches, loss: 0.5105Epoch 7/15: [==============                ] 36/75 batches, loss: 0.5097Epoch 7/15: [==============                ] 37/75 batches, loss: 0.5076Epoch 7/15: [===============               ] 38/75 batches, loss: 0.5088Epoch 7/15: [===============               ] 39/75 batches, loss: 0.5086Epoch 7/15: [================              ] 40/75 batches, loss: 0.5090Epoch 7/15: [================              ] 41/75 batches, loss: 0.5083Epoch 7/15: [================              ] 42/75 batches, loss: 0.5082Epoch 7/15: [=================             ] 43/75 batches, loss: 0.5093Epoch 7/15: [=================             ] 44/75 batches, loss: 0.5108Epoch 7/15: [==================            ] 45/75 batches, loss: 0.5107Epoch 7/15: [==================            ] 46/75 batches, loss: 0.5095Epoch 7/15: [==================            ] 47/75 batches, loss: 0.5094Epoch 7/15: [===================           ] 48/75 batches, loss: 0.5093Epoch 7/15: [===================           ] 49/75 batches, loss: 0.5107Epoch 7/15: [====================          ] 50/75 batches, loss: 0.5110Epoch 7/15: [====================          ] 51/75 batches, loss: 0.5113Epoch 7/15: [====================          ] 52/75 batches, loss: 0.5107Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.5103Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.5108Epoch 7/15: [======================        ] 55/75 batches, loss: 0.5110Epoch 7/15: [======================        ] 56/75 batches, loss: 0.5113Epoch 7/15: [======================        ] 57/75 batches, loss: 0.5113Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.5128Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.5123Epoch 7/15: [========================      ] 60/75 batches, loss: 0.5141Epoch 7/15: [========================      ] 61/75 batches, loss: 0.5132Epoch 7/15: [========================      ] 62/75 batches, loss: 0.5123Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.5141Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.5148Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.5157Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.5144Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.5134Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.5126Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.5121Epoch 7/15: [============================  ] 70/75 batches, loss: 0.5114Epoch 7/15: [============================  ] 71/75 batches, loss: 0.5113Epoch 7/15: [============================  ] 72/75 batches, loss: 0.5111Epoch 7/15: [============================= ] 73/75 batches, loss: 0.5107Epoch 7/15: [============================= ] 74/75 batches, loss: 0.5109Epoch 7/15: [==============================] 75/75 batches, loss: 0.5098
[2025-05-07 11:55:38,333][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5098
[2025-05-07 11:55:38,544][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.4935, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.5541Epoch 8/15: [                              ] 2/75 batches, loss: 0.5288Epoch 8/15: [=                             ] 3/75 batches, loss: 0.5441Epoch 8/15: [=                             ] 4/75 batches, loss: 0.5340Epoch 8/15: [==                            ] 5/75 batches, loss: 0.5470Epoch 8/15: [==                            ] 6/75 batches, loss: 0.5469Epoch 8/15: [==                            ] 7/75 batches, loss: 0.5379Epoch 8/15: [===                           ] 8/75 batches, loss: 0.5340Epoch 8/15: [===                           ] 9/75 batches, loss: 0.5201Epoch 8/15: [====                          ] 10/75 batches, loss: 0.5208Epoch 8/15: [====                          ] 11/75 batches, loss: 0.5214Epoch 8/15: [====                          ] 12/75 batches, loss: 0.5159Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.5116Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.5060Epoch 8/15: [======                        ] 15/75 batches, loss: 0.5061Epoch 8/15: [======                        ] 16/75 batches, loss: 0.5094Epoch 8/15: [======                        ] 17/75 batches, loss: 0.5091Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.5134Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.5095Epoch 8/15: [========                      ] 20/75 batches, loss: 0.5080Epoch 8/15: [========                      ] 21/75 batches, loss: 0.5093Epoch 8/15: [========                      ] 22/75 batches, loss: 0.5069Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.5058Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.5056Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.5056Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.5046Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.5064Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.5029Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.5033Epoch 8/15: [============                  ] 30/75 batches, loss: 0.5041Epoch 8/15: [============                  ] 31/75 batches, loss: 0.5064Epoch 8/15: [============                  ] 32/75 batches, loss: 0.5049Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.5083Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.5095Epoch 8/15: [==============                ] 35/75 batches, loss: 0.5093Epoch 8/15: [==============                ] 36/75 batches, loss: 0.5092Epoch 8/15: [==============                ] 37/75 batches, loss: 0.5065Epoch 8/15: [===============               ] 38/75 batches, loss: 0.5077Epoch 8/15: [===============               ] 39/75 batches, loss: 0.5059Epoch 8/15: [================              ] 40/75 batches, loss: 0.5059Epoch 8/15: [================              ] 41/75 batches, loss: 0.5058Epoch 8/15: [================              ] 42/75 batches, loss: 0.5074Epoch 8/15: [=================             ] 43/75 batches, loss: 0.5086Epoch 8/15: [=================             ] 44/75 batches, loss: 0.5091Epoch 8/15: [==================            ] 45/75 batches, loss: 0.5079Epoch 8/15: [==================            ] 46/75 batches, loss: 0.5067Epoch 8/15: [==================            ] 47/75 batches, loss: 0.5077Epoch 8/15: [===================           ] 48/75 batches, loss: 0.5091Epoch 8/15: [===================           ] 49/75 batches, loss: 0.5100Epoch 8/15: [====================          ] 50/75 batches, loss: 0.5098Epoch 8/15: [====================          ] 51/75 batches, loss: 0.5083Epoch 8/15: [====================          ] 52/75 batches, loss: 0.5100Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.5099Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.5107Epoch 8/15: [======================        ] 55/75 batches, loss: 0.5116Epoch 8/15: [======================        ] 56/75 batches, loss: 0.5110Epoch 8/15: [======================        ] 57/75 batches, loss: 0.5121Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.5112Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.5111Epoch 8/15: [========================      ] 60/75 batches, loss: 0.5112Epoch 8/15: [========================      ] 61/75 batches, loss: 0.5112Epoch 8/15: [========================      ] 62/75 batches, loss: 0.5119Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.5118Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.5113Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.5115Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.5100Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.5099Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.5101Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.5107Epoch 8/15: [============================  ] 70/75 batches, loss: 0.5103Epoch 8/15: [============================  ] 71/75 batches, loss: 0.5108Epoch 8/15: [============================  ] 72/75 batches, loss: 0.5114Epoch 8/15: [============================= ] 73/75 batches, loss: 0.5110Epoch 8/15: [============================= ] 74/75 batches, loss: 0.5109Epoch 8/15: [==============================] 75/75 batches, loss: 0.5112
[2025-05-07 11:55:41,431][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5112
[2025-05-07 11:55:41,914][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.4938, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-07 11:55:41,915][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.5034Epoch 9/15: [                              ] 2/75 batches, loss: 0.5292Epoch 9/15: [=                             ] 3/75 batches, loss: 0.5365Epoch 9/15: [=                             ] 4/75 batches, loss: 0.5402Epoch 9/15: [==                            ] 5/75 batches, loss: 0.5192Epoch 9/15: [==                            ] 6/75 batches, loss: 0.5251Epoch 9/15: [==                            ] 7/75 batches, loss: 0.5254Epoch 9/15: [===                           ] 8/75 batches, loss: 0.5145Epoch 9/15: [===                           ] 9/75 batches, loss: 0.5227Epoch 9/15: [====                          ] 10/75 batches, loss: 0.5220Epoch 9/15: [====                          ] 11/75 batches, loss: 0.5099Epoch 9/15: [====                          ] 12/75 batches, loss: 0.5173Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.5146Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.5130Epoch 9/15: [======                        ] 15/75 batches, loss: 0.5128Epoch 9/15: [======                        ] 16/75 batches, loss: 0.5081Epoch 9/15: [======                        ] 17/75 batches, loss: 0.5078Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.5093Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.5090Epoch 9/15: [========                      ] 20/75 batches, loss: 0.5111Epoch 9/15: [========                      ] 21/75 batches, loss: 0.5118Epoch 9/15: [========                      ] 22/75 batches, loss: 0.5147Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.5173Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.5148Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.5162Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.5172Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.5199Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.5177Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.5205Epoch 9/15: [============                  ] 30/75 batches, loss: 0.5216Epoch 9/15: [============                  ] 31/75 batches, loss: 0.5205Epoch 9/15: [============                  ] 32/75 batches, loss: 0.5220Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.5232Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.5227Epoch 9/15: [==============                ] 35/75 batches, loss: 0.5221Epoch 9/15: [==============                ] 36/75 batches, loss: 0.5210Epoch 9/15: [==============                ] 37/75 batches, loss: 0.5186Epoch 9/15: [===============               ] 38/75 batches, loss: 0.5188Epoch 9/15: [===============               ] 39/75 batches, loss: 0.5191Epoch 9/15: [================              ] 40/75 batches, loss: 0.5180Epoch 9/15: [================              ] 41/75 batches, loss: 0.5182Epoch 9/15: [================              ] 42/75 batches, loss: 0.5158Epoch 9/15: [=================             ] 43/75 batches, loss: 0.5160Epoch 9/15: [=================             ] 44/75 batches, loss: 0.5154Epoch 9/15: [==================            ] 45/75 batches, loss: 0.5146Epoch 9/15: [==================            ] 46/75 batches, loss: 0.5144Epoch 9/15: [==================            ] 47/75 batches, loss: 0.5152Epoch 9/15: [===================           ] 48/75 batches, loss: 0.5150Epoch 9/15: [===================           ] 49/75 batches, loss: 0.5158Epoch 9/15: [====================          ] 50/75 batches, loss: 0.5155Epoch 9/15: [====================          ] 51/75 batches, loss: 0.5157Epoch 9/15: [====================          ] 52/75 batches, loss: 0.5164Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.5166Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.5170Epoch 9/15: [======================        ] 55/75 batches, loss: 0.5172Epoch 9/15: [======================        ] 56/75 batches, loss: 0.5174Epoch 9/15: [======================        ] 57/75 batches, loss: 0.5163Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.5156Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.5158Epoch 9/15: [========================      ] 60/75 batches, loss: 0.5156Epoch 9/15: [========================      ] 61/75 batches, loss: 0.5166Epoch 9/15: [========================      ] 62/75 batches, loss: 0.5164Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.5158Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.5149Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.5151Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.5135Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.5130Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.5125Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.5128Epoch 9/15: [============================  ] 70/75 batches, loss: 0.5117Epoch 9/15: [============================  ] 71/75 batches, loss: 0.5109Epoch 9/15: [============================  ] 72/75 batches, loss: 0.5105Epoch 9/15: [============================= ] 73/75 batches, loss: 0.5099Epoch 9/15: [============================= ] 74/75 batches, loss: 0.5097Epoch 9/15: [==============================] 75/75 batches, loss: 0.5092
[2025-05-07 11:55:44,349][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5092
[2025-05-07 11:55:44,645][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.4936, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-07 11:55:44,645][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.4414Epoch 10/15: [                              ] 2/75 batches, loss: 0.4408Epoch 10/15: [=                             ] 3/75 batches, loss: 0.4622Epoch 10/15: [=                             ] 4/75 batches, loss: 0.4725Epoch 10/15: [==                            ] 5/75 batches, loss: 0.4739Epoch 10/15: [==                            ] 6/75 batches, loss: 0.4965Epoch 10/15: [==                            ] 7/75 batches, loss: 0.5049Epoch 10/15: [===                           ] 8/75 batches, loss: 0.5107Epoch 10/15: [===                           ] 9/75 batches, loss: 0.5046Epoch 10/15: [====                          ] 10/75 batches, loss: 0.5045Epoch 10/15: [====                          ] 11/75 batches, loss: 0.5118Epoch 10/15: [====                          ] 12/75 batches, loss: 0.5171Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.5138Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.5114Epoch 10/15: [======                        ] 15/75 batches, loss: 0.5093Epoch 10/15: [======                        ] 16/75 batches, loss: 0.5075Epoch 10/15: [======                        ] 17/75 batches, loss: 0.5059Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.5043Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.5044Epoch 10/15: [========                      ] 20/75 batches, loss: 0.5079Epoch 10/15: [========                      ] 21/75 batches, loss: 0.5100Epoch 10/15: [========                      ] 22/75 batches, loss: 0.5099Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.5096Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.5103Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.5132Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.5146Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.5142Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.5104Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.5119Epoch 10/15: [============                  ] 30/75 batches, loss: 0.5151Epoch 10/15: [============                  ] 31/75 batches, loss: 0.5124Epoch 10/15: [============                  ] 32/75 batches, loss: 0.5121Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.5118Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.5137Epoch 10/15: [==============                ] 35/75 batches, loss: 0.5147Epoch 10/15: [==============                ] 36/75 batches, loss: 0.5151Epoch 10/15: [==============                ] 37/75 batches, loss: 0.5139Epoch 10/15: [===============               ] 38/75 batches, loss: 0.5136Epoch 10/15: [===============               ] 39/75 batches, loss: 0.5134Epoch 10/15: [================              ] 40/75 batches, loss: 0.5137Epoch 10/15: [================              ] 41/75 batches, loss: 0.5141Epoch 10/15: [================              ] 42/75 batches, loss: 0.5122Epoch 10/15: [=================             ] 43/75 batches, loss: 0.5109Epoch 10/15: [=================             ] 44/75 batches, loss: 0.5113Epoch 10/15: [==================            ] 45/75 batches, loss: 0.5104Epoch 10/15: [==================            ] 46/75 batches, loss: 0.5113Epoch 10/15: [==================            ] 47/75 batches, loss: 0.5111Epoch 10/15: [===================           ] 48/75 batches, loss: 0.5110Epoch 10/15: [===================           ] 49/75 batches, loss: 0.5109Epoch 10/15: [====================          ] 50/75 batches, loss: 0.5119Epoch 10/15: [====================          ] 51/75 batches, loss: 0.5123Epoch 10/15: [====================          ] 52/75 batches, loss: 0.5126Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.5135Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.5120Epoch 10/15: [======================        ] 55/75 batches, loss: 0.5118Epoch 10/15: [======================        ] 56/75 batches, loss: 0.5116Epoch 10/15: [======================        ] 57/75 batches, loss: 0.5123Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.5106Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.5100Epoch 10/15: [========================      ] 60/75 batches, loss: 0.5092Epoch 10/15: [========================      ] 61/75 batches, loss: 0.5091Epoch 10/15: [========================      ] 62/75 batches, loss: 0.5109Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.5108Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.5103Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.5102Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.5094Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.5097Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.5092Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.5082Epoch 10/15: [============================  ] 70/75 batches, loss: 0.5092Epoch 10/15: [============================  ] 71/75 batches, loss: 0.5091Epoch 10/15: [============================  ] 72/75 batches, loss: 0.5103Epoch 10/15: [============================= ] 73/75 batches, loss: 0.5093Epoch 10/15: [============================= ] 74/75 batches, loss: 0.5095Epoch 10/15: [==============================] 75/75 batches, loss: 0.5105
[2025-05-07 11:55:47,002][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5105
[2025-05-07 11:55:47,336][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.4932, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.4320Epoch 11/15: [                              ] 2/75 batches, loss: 0.4742Epoch 11/15: [=                             ] 3/75 batches, loss: 0.5290Epoch 11/15: [=                             ] 4/75 batches, loss: 0.5305Epoch 11/15: [==                            ] 5/75 batches, loss: 0.5298Epoch 11/15: [==                            ] 6/75 batches, loss: 0.5175Epoch 11/15: [==                            ] 7/75 batches, loss: 0.5157Epoch 11/15: [===                           ] 8/75 batches, loss: 0.5083Epoch 11/15: [===                           ] 9/75 batches, loss: 0.5077Epoch 11/15: [====                          ] 10/75 batches, loss: 0.5049Epoch 11/15: [====                          ] 11/75 batches, loss: 0.5069Epoch 11/15: [====                          ] 12/75 batches, loss: 0.5089Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.5126Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.5178Epoch 11/15: [======                        ] 15/75 batches, loss: 0.5200Epoch 11/15: [======                        ] 16/75 batches, loss: 0.5146Epoch 11/15: [======                        ] 17/75 batches, loss: 0.5117Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.5126Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.5072Epoch 11/15: [========                      ] 20/75 batches, loss: 0.5070Epoch 11/15: [========                      ] 21/75 batches, loss: 0.5072Epoch 11/15: [========                      ] 22/75 batches, loss: 0.5090Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.5079Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.5018Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.5011Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.5051Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.5063Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.5097Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.5095Epoch 11/15: [============                  ] 30/75 batches, loss: 0.5101Epoch 11/15: [============                  ] 31/75 batches, loss: 0.5124Epoch 11/15: [============                  ] 32/75 batches, loss: 0.5121Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.5104Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.5116Epoch 11/15: [==============                ] 35/75 batches, loss: 0.5107Epoch 11/15: [==============                ] 36/75 batches, loss: 0.5099Epoch 11/15: [==============                ] 37/75 batches, loss: 0.5079Epoch 11/15: [===============               ] 38/75 batches, loss: 0.5078Epoch 11/15: [===============               ] 39/75 batches, loss: 0.5072Epoch 11/15: [================              ] 40/75 batches, loss: 0.5082Epoch 11/15: [================              ] 41/75 batches, loss: 0.5076Epoch 11/15: [================              ] 42/75 batches, loss: 0.5092Epoch 11/15: [=================             ] 43/75 batches, loss: 0.5095Epoch 11/15: [=================             ] 44/75 batches, loss: 0.5072Epoch 11/15: [==================            ] 45/75 batches, loss: 0.5072Epoch 11/15: [==================            ] 46/75 batches, loss: 0.5066Epoch 11/15: [==================            ] 47/75 batches, loss: 0.5066Epoch 11/15: [===================           ] 48/75 batches, loss: 0.5055Epoch 11/15: [===================           ] 49/75 batches, loss: 0.5055Epoch 11/15: [====================          ] 50/75 batches, loss: 0.5064Epoch 11/15: [====================          ] 51/75 batches, loss: 0.5055Epoch 11/15: [====================          ] 52/75 batches, loss: 0.5064Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.5076Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.5071Epoch 11/15: [======================        ] 55/75 batches, loss: 0.5062Epoch 11/15: [======================        ] 56/75 batches, loss: 0.5053Epoch 11/15: [======================        ] 57/75 batches, loss: 0.5065Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.5052Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.5052Epoch 11/15: [========================      ] 60/75 batches, loss: 0.5053Epoch 11/15: [========================      ] 61/75 batches, loss: 0.5075Epoch 11/15: [========================      ] 62/75 batches, loss: 0.5082Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.5085Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.5092Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.5087Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.5087Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.5079Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.5081Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.5077Epoch 11/15: [============================  ] 70/75 batches, loss: 0.5074Epoch 11/15: [============================  ] 71/75 batches, loss: 0.5070Epoch 11/15: [============================  ] 72/75 batches, loss: 0.5070Epoch 11/15: [============================= ] 73/75 batches, loss: 0.5076Epoch 11/15: [============================= ] 74/75 batches, loss: 0.5083Epoch 11/15: [==============================] 75/75 batches, loss: 0.5086
[2025-05-07 11:55:50,034][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5086
[2025-05-07 11:55:50,298][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.4933, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-07 11:55:50,299][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.4758Epoch 12/15: [                              ] 2/75 batches, loss: 0.4302Epoch 12/15: [=                             ] 3/75 batches, loss: 0.4520Epoch 12/15: [=                             ] 4/75 batches, loss: 0.4713Epoch 12/15: [==                            ] 5/75 batches, loss: 0.4872Epoch 12/15: [==                            ] 6/75 batches, loss: 0.4820Epoch 12/15: [==                            ] 7/75 batches, loss: 0.5055Epoch 12/15: [===                           ] 8/75 batches, loss: 0.5023Epoch 12/15: [===                           ] 9/75 batches, loss: 0.5051Epoch 12/15: [====                          ] 10/75 batches, loss: 0.5073Epoch 12/15: [====                          ] 11/75 batches, loss: 0.5069Epoch 12/15: [====                          ] 12/75 batches, loss: 0.5133Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.5143Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.5103Epoch 12/15: [======                        ] 15/75 batches, loss: 0.5149Epoch 12/15: [======                        ] 16/75 batches, loss: 0.5142Epoch 12/15: [======                        ] 17/75 batches, loss: 0.5112Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.5082Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.5092Epoch 12/15: [========                      ] 20/75 batches, loss: 0.5081Epoch 12/15: [========                      ] 21/75 batches, loss: 0.5079Epoch 12/15: [========                      ] 22/75 batches, loss: 0.5062Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.5072Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.5093Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.5101Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.5071Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.5087Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.5077Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.5075Epoch 12/15: [============                  ] 30/75 batches, loss: 0.5076Epoch 12/15: [============                  ] 31/75 batches, loss: 0.5122Epoch 12/15: [============                  ] 32/75 batches, loss: 0.5112Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.5117Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.5101Epoch 12/15: [==============                ] 35/75 batches, loss: 0.5112Epoch 12/15: [==============                ] 36/75 batches, loss: 0.5084Epoch 12/15: [==============                ] 37/75 batches, loss: 0.5072Epoch 12/15: [===============               ] 38/75 batches, loss: 0.5090Epoch 12/15: [===============               ] 39/75 batches, loss: 0.5089Epoch 12/15: [================              ] 40/75 batches, loss: 0.5093Epoch 12/15: [================              ] 41/75 batches, loss: 0.5092Epoch 12/15: [================              ] 42/75 batches, loss: 0.5090Epoch 12/15: [=================             ] 43/75 batches, loss: 0.5089Epoch 12/15: [=================             ] 44/75 batches, loss: 0.5084Epoch 12/15: [==================            ] 45/75 batches, loss: 0.5067Epoch 12/15: [==================            ] 46/75 batches, loss: 0.5046Epoch 12/15: [==================            ] 47/75 batches, loss: 0.5046Epoch 12/15: [===================           ] 48/75 batches, loss: 0.5041Epoch 12/15: [===================           ] 49/75 batches, loss: 0.5084Epoch 12/15: [====================          ] 50/75 batches, loss: 0.5085Epoch 12/15: [====================          ] 51/75 batches, loss: 0.5080Epoch 12/15: [====================          ] 52/75 batches, loss: 0.5088Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.5093Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.5097Epoch 12/15: [======================        ] 55/75 batches, loss: 0.5079Epoch 12/15: [======================        ] 56/75 batches, loss: 0.5078Epoch 12/15: [======================        ] 57/75 batches, loss: 0.5074Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.5065Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.5057Epoch 12/15: [========================      ] 60/75 batches, loss: 0.5074Epoch 12/15: [========================      ] 61/75 batches, loss: 0.5085Epoch 12/15: [========================      ] 62/75 batches, loss: 0.5088Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.5083Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.5075Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.5082Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.5085Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.5082Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.5071Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.5081Epoch 12/15: [============================  ] 70/75 batches, loss: 0.5087Epoch 12/15: [============================  ] 71/75 batches, loss: 0.5097Epoch 12/15: [============================  ] 72/75 batches, loss: 0.5086Epoch 12/15: [============================= ] 73/75 batches, loss: 0.5085Epoch 12/15: [============================= ] 74/75 batches, loss: 0.5088Epoch 12/15: [==============================] 75/75 batches, loss: 0.5091
[2025-05-07 11:55:52,603][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.5091
[2025-05-07 11:55:52,886][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.4935, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-07 11:55:52,887][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.5033Epoch 13/15: [                              ] 2/75 batches, loss: 0.5153Epoch 13/15: [=                             ] 3/75 batches, loss: 0.4798Epoch 13/15: [=                             ] 4/75 batches, loss: 0.4699Epoch 13/15: [==                            ] 5/75 batches, loss: 0.4887Epoch 13/15: [==                            ] 6/75 batches, loss: 0.4990Epoch 13/15: [==                            ] 7/75 batches, loss: 0.5133Epoch 13/15: [===                           ] 8/75 batches, loss: 0.5120Epoch 13/15: [===                           ] 9/75 batches, loss: 0.5058Epoch 13/15: [====                          ] 10/75 batches, loss: 0.5127Epoch 13/15: [====                          ] 11/75 batches, loss: 0.5099Epoch 13/15: [====                          ] 12/75 batches, loss: 0.5078Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.5111Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.5157Epoch 13/15: [======                        ] 15/75 batches, loss: 0.5133Epoch 13/15: [======                        ] 16/75 batches, loss: 0.5126Epoch 13/15: [======                        ] 17/75 batches, loss: 0.5162Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.5182Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.5212Epoch 13/15: [========                      ] 20/75 batches, loss: 0.5191Epoch 13/15: [========                      ] 21/75 batches, loss: 0.5182Epoch 13/15: [========                      ] 22/75 batches, loss: 0.5186Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.5200Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.5203Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.5180Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.5202Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.5191Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.5177Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.5180Epoch 13/15: [============                  ] 30/75 batches, loss: 0.5167Epoch 13/15: [============                  ] 31/75 batches, loss: 0.5155Epoch 13/15: [============                  ] 32/75 batches, loss: 0.5152Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.5170Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.5145Epoch 13/15: [==============                ] 35/75 batches, loss: 0.5149Epoch 13/15: [==============                ] 36/75 batches, loss: 0.5113Epoch 13/15: [==============                ] 37/75 batches, loss: 0.5066Epoch 13/15: [===============               ] 38/75 batches, loss: 0.5059Epoch 13/15: [===============               ] 39/75 batches, loss: 0.5058Epoch 13/15: [================              ] 40/75 batches, loss: 0.5076Epoch 13/15: [================              ] 41/75 batches, loss: 0.5057Epoch 13/15: [================              ] 42/75 batches, loss: 0.5068Epoch 13/15: [=================             ] 43/75 batches, loss: 0.5073Epoch 13/15: [=================             ] 44/75 batches, loss: 0.5061Epoch 13/15: [==================            ] 45/75 batches, loss: 0.5066Epoch 13/15: [==================            ] 46/75 batches, loss: 0.5081Epoch 13/15: [==================            ] 47/75 batches, loss: 0.5080Epoch 13/15: [===================           ] 48/75 batches, loss: 0.5105Epoch 13/15: [===================           ] 49/75 batches, loss: 0.5109Epoch 13/15: [====================          ] 50/75 batches, loss: 0.5098Epoch 13/15: [====================          ] 51/75 batches, loss: 0.5096Epoch 13/15: [====================          ] 52/75 batches, loss: 0.5104Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.5107Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.5093Epoch 13/15: [======================        ] 55/75 batches, loss: 0.5087Epoch 13/15: [======================        ] 56/75 batches, loss: 0.5078Epoch 13/15: [======================        ] 57/75 batches, loss: 0.5069Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.5068Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.5084Epoch 13/15: [========================      ] 60/75 batches, loss: 0.5071Epoch 13/15: [========================      ] 61/75 batches, loss: 0.5068Epoch 13/15: [========================      ] 62/75 batches, loss: 0.5064Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.5075Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.5074Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.5066Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.5080Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.5076Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.5082Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.5088Epoch 13/15: [============================  ] 70/75 batches, loss: 0.5081Epoch 13/15: [============================  ] 71/75 batches, loss: 0.5080Epoch 13/15: [============================  ] 72/75 batches, loss: 0.5082Epoch 13/15: [============================= ] 73/75 batches, loss: 0.5085Epoch 13/15: [============================= ] 74/75 batches, loss: 0.5075Epoch 13/15: [==============================] 75/75 batches, loss: 0.5063
[2025-05-07 11:55:55,220][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.5063
[2025-05-07 11:55:55,529][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.4933, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-07 11:55:55,530][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-07 11:55:55,530][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 13
[2025-05-07 11:55:55,530][src.training.lm_trainer][INFO] - Training completed in 37.60 seconds
[2025-05-07 11:55:55,530][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 11:55:58,485][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9974811083123426, 'f1': 0.9974768713204374, 'precision': 1.0, 'recall': 0.9949664429530202}
[2025-05-07 11:55:58,485][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-07 11:55:58,485][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.8260869565217391, 'f1': 0.8666666666666667, 'precision': 0.8, 'recall': 0.9454545454545454}
[2025-05-07 11:56:00,116][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer10/ja/ja/model.pt
[2025-05-07 11:56:00,118][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▅▅███
wandb:           best_val_f1 ▁▅▅███
wandb:         best_val_loss █▄▃▂▁▁
wandb:    best_val_precision ▁▁▁▁▁▁
wandb:       best_val_recall ▁▄▄███
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▂▂▂▂▂▂▂▂▂▂
wandb:            train_loss █▂▂▁▁▁▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▅▅█▅████████
wandb:                val_f1 ▁▅▅█▅████████
wandb:              val_loss █▄▃▂▂▂▁▁▁▁▁▁▁
wandb:         val_precision ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val_recall ▁▄▄█▄████████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 1
wandb:           best_val_f1 1
wandb:         best_val_loss 0.4932
wandb:    best_val_precision 1
wandb:       best_val_recall 1
wandb:      early_stop_epoch 13
wandb:                 epoch 13
wandb:   final_test_accuracy 0.82609
wandb:         final_test_f1 0.86667
wandb:  final_test_precision 0.8
wandb:     final_test_recall 0.94545
wandb:  final_train_accuracy 0.99748
wandb:        final_train_f1 0.99748
wandb: final_train_precision 1
wandb:    final_train_recall 0.99497
wandb:    final_val_accuracy 1
wandb:          final_val_f1 1
wandb:   final_val_precision 1
wandb:      final_val_recall 1
wandb:         learning_rate 0.0001
wandb:            train_loss 0.50635
wandb:            train_time 37.60482
wandb:          val_accuracy 1
wandb:                val_f1 1
wandb:              val_loss 0.4933
wandb:         val_precision 1
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_115455-a91cqu0e
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_115455-a91cqu0e/logs
Experiment probe_layer10_question_type_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer10/ja/ja/results.json for layer 10
Running experiment: probe_layer10_complexity_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_complexity_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer10/ja"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 11:56:35,650][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer10/ja
experiment_name: probe_layer10_complexity_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 11:56:35,650][__main__][INFO] - Normalized task: complexity
[2025-05-07 11:56:35,650][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 11:56:35,650][__main__][INFO] - Determined Task Type: regression
[2025-05-07 11:56:35,654][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-05-07 11:56:35,654][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 11:56:40,360][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 11:56:42,756][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 11:56:42,756][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:56:43,099][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:56:43,208][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:56:43,588][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 11:56:43,597][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:56:43,597][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 11:56:43,598][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:56:43,730][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:56:43,854][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:56:43,869][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 11:56:43,870][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:56:43,870][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 11:56:43,871][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:56:43,952][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:56:44,027][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:56:44,054][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 11:56:44,055][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:56:44,055][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 11:56:44,056][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 11:56:44,058][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 11:56:44,058][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 11:56:44,058][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 11:56:44,058][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 11:56:44,058][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 11:56:44,058][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-05-07 11:56:44,058][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 11:56:44,058][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-05-07 11:56:44,059][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 11:56:44,059][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 11:56:44,059][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 11:56:44,059][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 11:56:44,059][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 11:56:44,059][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-05-07 11:56:44,059][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 11:56:44,059][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-05-07 11:56:44,059][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 11:56:44,060][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 11:56:44,060][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 11:56:44,060][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 11:56:44,060][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 11:56:44,060][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-05-07 11:56:44,060][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 11:56:44,060][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-05-07 11:56:44,060][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 11:56:44,060][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 11:56:44,061][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 11:56:44,061][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 11:56:44,061][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 11:56:52,301][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 11:56:52,303][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 11:56:52,303][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-07 11:56:52,303][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 11:56:52,306][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 11:56:52,306][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 11:56:52,307][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 11:56:52,307][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 11:56:52,307][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 11:56:52,308][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 11:56:52,308][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4343Epoch 1/15: [                              ] 2/75 batches, loss: 0.6946Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7173Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7049Epoch 1/15: [==                            ] 5/75 batches, loss: 0.6385Epoch 1/15: [==                            ] 6/75 batches, loss: 0.5866Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6025Epoch 1/15: [===                           ] 8/75 batches, loss: 0.5890Epoch 1/15: [===                           ] 9/75 batches, loss: 0.5883Epoch 1/15: [====                          ] 10/75 batches, loss: 0.5908Epoch 1/15: [====                          ] 11/75 batches, loss: 0.5580Epoch 1/15: [====                          ] 12/75 batches, loss: 0.5373Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.5307Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.5238Epoch 1/15: [======                        ] 15/75 batches, loss: 0.5135Epoch 1/15: [======                        ] 16/75 batches, loss: 0.5088Epoch 1/15: [======                        ] 17/75 batches, loss: 0.5050Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4945Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4882Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4884Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4806Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4766Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4748Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4637Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4562Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4474Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4486Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4428Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4381Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4330Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4266Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4223Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4131Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4108Epoch 1/15: [==============                ] 35/75 batches, loss: 0.4129Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4083Epoch 1/15: [==============                ] 37/75 batches, loss: 0.4022Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3960Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3939Epoch 1/15: [================              ] 40/75 batches, loss: 0.3950Epoch 1/15: [================              ] 41/75 batches, loss: 0.3885Epoch 1/15: [================              ] 42/75 batches, loss: 0.3838Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3785Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3755Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3758Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3711Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3674Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3653Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3614Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3572Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3558Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3536Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3502Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3496Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3488Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3487Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3452Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3456Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3428Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3388Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3346Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3315Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3303Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3281Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3267Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3241Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3215Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3184Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3174Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3151Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3139Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3138Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3122Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3120Epoch 1/15: [==============================] 75/75 batches, loss: 0.3099
[2025-05-07 11:56:59,624][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3099
[2025-05-07 11:56:59,822][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0531, Metrics: {'mse': 0.053425367921590805, 'rmse': 0.23113928251509047, 'r2': 0.12927734851837158}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1865Epoch 2/15: [                              ] 2/75 batches, loss: 0.1906Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1922Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1982Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1954Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1775Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1667Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1671Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1763Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1794Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1726Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1671Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1670Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1675Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1684Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1625Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1633Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1599Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1589Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1579Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1558Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1539Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1527Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1492Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1451Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1457Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1492Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1470Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1466Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1462Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1475Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1499Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1499Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1500Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1524Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1504Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1491Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1492Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1483Epoch 2/15: [================              ] 40/75 batches, loss: 0.1484Epoch 2/15: [================              ] 41/75 batches, loss: 0.1471Epoch 2/15: [================              ] 42/75 batches, loss: 0.1463Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1474Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1482Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1479Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1468Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1451Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1465Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1467Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1450Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1444Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1442Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1453Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1452Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1467Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1459Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1461Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1450Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1465Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1449Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1440Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1432Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1428Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1424Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1412Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1396Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1384Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1382Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1370Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1369Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1366Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1366Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1362Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1352Epoch 2/15: [==============================] 75/75 batches, loss: 0.1343
[2025-05-07 11:57:02,579][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1343
[2025-05-07 11:57:02,795][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0482, Metrics: {'mse': 0.04800271987915039, 'rmse': 0.21909523016065502, 'r2': 0.2176552414894104}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1070Epoch 3/15: [                              ] 2/75 batches, loss: 0.1299Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1368Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1202Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1243Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1248Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1167Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1196Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1132Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1087Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1112Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1137Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1102Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1109Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1089Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1072Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1072Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1114Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1092Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1089Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1085Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1075Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1081Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1084Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1070Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1058Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1045Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1064Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1055Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1046Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1039Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1050Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1056Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1057Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1038Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1044Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1060Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1051Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1048Epoch 3/15: [================              ] 40/75 batches, loss: 0.1055Epoch 3/15: [================              ] 41/75 batches, loss: 0.1076Epoch 3/15: [================              ] 42/75 batches, loss: 0.1076Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1065Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1062Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1077Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1068Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1064Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1067Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1078Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1069Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1063Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1068Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1064Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1056Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1053Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1047Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1049Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1047Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1047Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1043Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1038Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1028Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1024Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1022Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1021Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1016Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1019Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1036Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1043Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1047Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1047Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1042Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1050Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1047Epoch 3/15: [==============================] 75/75 batches, loss: 0.1049
[2025-05-07 11:57:05,491][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1049
[2025-05-07 11:57:05,722][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0684, Metrics: {'mse': 0.06787975132465363, 'rmse': 0.26053742787679013, 'r2': -0.10629904270172119}
[2025-05-07 11:57:05,723][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0545Epoch 4/15: [                              ] 2/75 batches, loss: 0.1069Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1224Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1017Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0940Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0867Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0884Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0816Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0950Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1021Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1062Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1040Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1029Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1044Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1057Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1057Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1046Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1023Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1029Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1006Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1002Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1009Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0993Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0976Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0980Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0977Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1005Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1027Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1010Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0990Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0971Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0959Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0950Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0954Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0945Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0938Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0932Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0927Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0919Epoch 4/15: [================              ] 40/75 batches, loss: 0.0949Epoch 4/15: [================              ] 41/75 batches, loss: 0.0943Epoch 4/15: [================              ] 42/75 batches, loss: 0.0933Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0937Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0941Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0945Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0941Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0940Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0929Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0933Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0923Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0919Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0920Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0914Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0915Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0909Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0900Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0895Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0891Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0903Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0900Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0902Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0904Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0902Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0899Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0898Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0901Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0904Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0908Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0908Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0908Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0904Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0895Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0892Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0889Epoch 4/15: [==============================] 75/75 batches, loss: 0.0889
[2025-05-07 11:57:08,018][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0889
[2025-05-07 11:57:08,224][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0536, Metrics: {'mse': 0.05328254774212837, 'rmse': 0.23083012745767909, 'r2': 0.13160502910614014}
[2025-05-07 11:57:08,225][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0694Epoch 5/15: [                              ] 2/75 batches, loss: 0.0777Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0657Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0726Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0654Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0687Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0739Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0864Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0833Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0836Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0838Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0822Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0837Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0845Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0837Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0813Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0819Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0810Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0818Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0835Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0827Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0831Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0821Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0806Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0793Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0794Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0793Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0794Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0798Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0804Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0796Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0805Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0801Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0807Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0815Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0826Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0827Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0832Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0845Epoch 5/15: [================              ] 40/75 batches, loss: 0.0844Epoch 5/15: [================              ] 41/75 batches, loss: 0.0835Epoch 5/15: [================              ] 42/75 batches, loss: 0.0845Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0836Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0830Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0821Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0823Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0818Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0809Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0817Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0817Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0816Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0814Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0809Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0803Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0799Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0794Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0790Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0791Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0789Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0793Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0797Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0793Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0793Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0790Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0805Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0808Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0804Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0796Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0795Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0793Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0790Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0788Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0784Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0783Epoch 5/15: [==============================] 75/75 batches, loss: 0.0783
[2025-05-07 11:57:10,560][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0783
[2025-05-07 11:57:10,844][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0553, Metrics: {'mse': 0.05494534969329834, 'rmse': 0.23440424418789507, 'r2': 0.10450482368469238}
[2025-05-07 11:57:10,845][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0798Epoch 6/15: [                              ] 2/75 batches, loss: 0.0746Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0709Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0666Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0621Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0619Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0669Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0689Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0677Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0693Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0686Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0698Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0682Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0692Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0680Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0681Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0682Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0690Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0679Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0673Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0674Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0674Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0668Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0683Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0666Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0681Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0676Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0676Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0670Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0667Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0668Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0664Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0662Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0658Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0661Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0671Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0678Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0672Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0674Epoch 6/15: [================              ] 40/75 batches, loss: 0.0675Epoch 6/15: [================              ] 41/75 batches, loss: 0.0679Epoch 6/15: [================              ] 42/75 batches, loss: 0.0676Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0674Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0673Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0675Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0673Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0670Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0670Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0668Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0669Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0670Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0671Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0674Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0671Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0673Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0670Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0664Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0664Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0661Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0659Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0652Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0650Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0650Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0652Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0648Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0641Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0642Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0644Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0648Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0647Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0647Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0645Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0652Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0650Epoch 6/15: [==============================] 75/75 batches, loss: 0.0647
[2025-05-07 11:57:13,158][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0647
[2025-05-07 11:57:13,366][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0530, Metrics: {'mse': 0.05266478657722473, 'rmse': 0.2294880968094527, 'r2': 0.14167320728302002}
[2025-05-07 11:57:13,367][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 11:57:13,367][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-07 11:57:13,367][src.training.lm_trainer][INFO] - Training completed in 16.97 seconds
[2025-05-07 11:57:13,367][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 11:57:16,270][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.027074914425611496, 'rmse': 0.16454456668517345, 'r2': 0.32423096895217896}
[2025-05-07 11:57:16,271][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.04800271987915039, 'rmse': 0.21909523016065502, 'r2': 0.2176552414894104}
[2025-05-07 11:57:16,271][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.05011143535375595, 'rmse': 0.22385583609492057, 'r2': 0.037402451038360596}
[2025-05-07 11:57:17,949][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer10/ja/ja/model.pt
[2025-05-07 11:57:17,951][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▄▅▁▄▃
wandb:       train_loss █▃▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▃▁█▃▃▃
wandb:          val_mse ▃▁█▃▃▃
wandb:           val_r2 ▆█▁▆▆▆
wandb:         val_rmse ▃▁█▃▄▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04821
wandb:     best_val_mse 0.048
wandb:      best_val_r2 0.21766
wandb:    best_val_rmse 0.2191
wandb: early_stop_epoch 6
wandb:            epoch 6
wandb:   final_test_mse 0.05011
wandb:    final_test_r2 0.0374
wandb:  final_test_rmse 0.22386
wandb:  final_train_mse 0.02707
wandb:   final_train_r2 0.32423
wandb: final_train_rmse 0.16454
wandb:    final_val_mse 0.048
wandb:     final_val_r2 0.21766
wandb:   final_val_rmse 0.2191
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06467
wandb:       train_time 16.97032
wandb:         val_loss 0.053
wandb:          val_mse 0.05266
wandb:           val_r2 0.14167
wandb:         val_rmse 0.22949
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_115635-gw707lru
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_115635-gw707lru/logs
Experiment probe_layer10_complexity_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer10/ja/ja/results.json for layer 10
Running control probing experiments...
=======================
PROBING LAYER 2 (CONTROL EXPERIMENTS)
=======================
Running experiment: probe_layer2_question_type_control1_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_question_type_control1_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control1/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 11:57:47,797][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control1/layer2/fi
experiment_name: probe_layer2_question_type_control1_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-07 11:57:47,797][__main__][INFO] - Normalized task: question_type
[2025-05-07 11:57:47,797][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-07 11:57:47,797][__main__][INFO] - Determined Task Type: classification
[2025-05-07 11:57:47,801][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-05-07 11:57:47,801][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 11:57:51,843][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 11:57:54,107][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 11:57:54,108][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:57:54,303][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-05-07 11:57:54,399][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-05-07 11:57:54,742][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 11:57:54,758][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:57:54,761][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 11:57:54,763][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:57:54,907][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:57:54,986][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:57:55,079][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 11:57:55,080][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:57:55,080][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 11:57:55,082][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:57:55,153][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:57:55,262][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:57:55,275][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 11:57:55,277][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:57:55,277][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 11:57:55,278][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 11:57:55,279][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:57:55,279][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:57:55,279][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:57:55,279][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:57:55,279][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-05-07 11:57:55,279][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-05-07 11:57:55,280][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 11:57:55,280][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 11:57:55,280][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:57:55,280][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:57:55,280][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:57:55,280][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:57:55,280][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-05-07 11:57:55,280][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-05-07 11:57:55,280][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 11:57:55,280][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 11:57:55,281][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:57:55,281][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:57:55,281][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:57:55,281][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:57:55,281][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-07 11:57:55,281][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-07 11:57:55,281][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 11:57:55,281][src.data.datasets][INFO] - Sample label: 0
[2025-05-07 11:57:55,281][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 11:57:55,281][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 11:57:55,282][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 11:57:55,282][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-07 11:57:55,282][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 11:58:03,753][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 11:58:03,755][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 11:58:03,755][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 11:58:03,755][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-07 11:58:03,761][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-07 11:58:03,762][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-07 11:58:03,762][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-07 11:58:03,762][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-07 11:58:03,762][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 11:58:03,763][__main__][INFO] - Total parameters: 394,568,839
[2025-05-07 11:58:03,763][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-07 11:58:03,764][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6669Epoch 1/15: [                              ] 2/75 batches, loss: 0.6651Epoch 1/15: [=                             ] 3/75 batches, loss: 0.6917Epoch 1/15: [=                             ] 4/75 batches, loss: 0.6834Epoch 1/15: [==                            ] 5/75 batches, loss: 0.6911Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6692Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6729Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6783Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6866Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6857Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6790Epoch 1/15: [====                          ] 12/75 batches, loss: 0.6801Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.6805Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.6800Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6892Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6888Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6927Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6962Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6973Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6990Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6983Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6979Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6983Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6983Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6979Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6981Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6980Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6979Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6978Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6974Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6975Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6974Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6972Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6971Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6971Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6969Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6968Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6966Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6966Epoch 1/15: [================              ] 40/75 batches, loss: 0.6965Epoch 1/15: [================              ] 41/75 batches, loss: 0.6965Epoch 1/15: [================              ] 42/75 batches, loss: 0.6964Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6963Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6963Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6962Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6963Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6963Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6962Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6961Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6960Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6959Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6959Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6958Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6958Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6957Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6957Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6956Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6956Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6955Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6955Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6955Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6955Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6954Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6954Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6953Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6953Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6952Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6952Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6952Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6952Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6952Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6951Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6951Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6951Epoch 1/15: [==============================] 75/75 batches, loss: 0.6951
[2025-05-07 11:58:10,583][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6951
[2025-05-07 11:58:10,811][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6901Epoch 2/15: [                              ] 2/75 batches, loss: 0.6918Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6921Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6919Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6922Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6923Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6924Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6928Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6930Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6931Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6928Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6928Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6929Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6930Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6929Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6930Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6930Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6929Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6927Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6928Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6928Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6927Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6927Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6928Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6928Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6927Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6930Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6930Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6930Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6929Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6929Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6929Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6929Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6930Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6929Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6929Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6929Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6929Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6929Epoch 2/15: [================              ] 40/75 batches, loss: 0.6930Epoch 2/15: [================              ] 41/75 batches, loss: 0.6929Epoch 2/15: [================              ] 42/75 batches, loss: 0.6929Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6930Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6929Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6929Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6929Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6929Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6929Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6929Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6929Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6929Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6928Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6928Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6928Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6928Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6928Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6932Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6932Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6932Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6932Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6932Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6932Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6932Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6932Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6933Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6932Epoch 2/15: [==============================] 75/75 batches, loss: 0.6932
[2025-05-07 11:58:13,546][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6932
[2025-05-07 11:58:13,828][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 11:58:13,829][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6931Epoch 3/15: [                              ] 2/75 batches, loss: 0.6929Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6929Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6931Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6932Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6931Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6932Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6932Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6932Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6932Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6930Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6930Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6931Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6930Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6930Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6929Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6929Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6929Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6930Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6928Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6928Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6927Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6927Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6927Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6928Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6928Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6928Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6928Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6928Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6928Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6928Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6928Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6928Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6928Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6925Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6926Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6927Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6926Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6927Epoch 3/15: [================              ] 40/75 batches, loss: 0.6926Epoch 3/15: [================              ] 41/75 batches, loss: 0.6926Epoch 3/15: [================              ] 42/75 batches, loss: 0.6927Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6927Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6927Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6927Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6928Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6928Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6928Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6927Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6927Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6928Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6928Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6928Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6928Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6928Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6928Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6928Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6928Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6928Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6928Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6928Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6928Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6928Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6929Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6930Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6930Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6930Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6930Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6930Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6930Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6930Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6930Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6930Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 3/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-07 11:58:16,097][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6931
[2025-05-07 11:58:16,426][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 11:58:16,426][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6935Epoch 4/15: [                              ] 2/75 batches, loss: 0.6932Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6937Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6937Epoch 4/15: [==                            ] 5/75 batches, loss: 0.6936Epoch 4/15: [==                            ] 6/75 batches, loss: 0.6935Epoch 4/15: [==                            ] 7/75 batches, loss: 0.6933Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6934Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6934Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6933Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6935Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6935Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6935Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6935Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6944Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6945Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6944Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6945Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6945Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6944Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6943Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6942Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6942Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6942Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6942Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6941Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6941Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6940Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6940Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6940Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6940Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6939Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6939Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6939Epoch 4/15: [==============                ] 35/75 batches, loss: 0.6939Epoch 4/15: [==============                ] 36/75 batches, loss: 0.6939Epoch 4/15: [==============                ] 37/75 batches, loss: 0.6938Epoch 4/15: [===============               ] 38/75 batches, loss: 0.6938Epoch 4/15: [===============               ] 39/75 batches, loss: 0.6938Epoch 4/15: [================              ] 40/75 batches, loss: 0.6938Epoch 4/15: [================              ] 41/75 batches, loss: 0.6938Epoch 4/15: [================              ] 42/75 batches, loss: 0.6938Epoch 4/15: [=================             ] 43/75 batches, loss: 0.6938Epoch 4/15: [=================             ] 44/75 batches, loss: 0.6937Epoch 4/15: [==================            ] 45/75 batches, loss: 0.6937Epoch 4/15: [==================            ] 46/75 batches, loss: 0.6937Epoch 4/15: [==================            ] 47/75 batches, loss: 0.6937Epoch 4/15: [===================           ] 48/75 batches, loss: 0.6937Epoch 4/15: [===================           ] 49/75 batches, loss: 0.6936Epoch 4/15: [====================          ] 50/75 batches, loss: 0.6936Epoch 4/15: [====================          ] 51/75 batches, loss: 0.6936Epoch 4/15: [====================          ] 52/75 batches, loss: 0.6936Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.6936Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.6936Epoch 4/15: [======================        ] 55/75 batches, loss: 0.6936Epoch 4/15: [======================        ] 56/75 batches, loss: 0.6936Epoch 4/15: [======================        ] 57/75 batches, loss: 0.6936Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.6935Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.6935Epoch 4/15: [========================      ] 60/75 batches, loss: 0.6935Epoch 4/15: [========================      ] 61/75 batches, loss: 0.6935Epoch 4/15: [========================      ] 62/75 batches, loss: 0.6935Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.6935Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.6935Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.6935Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.6935Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.6935Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.6935Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.6935Epoch 4/15: [============================  ] 70/75 batches, loss: 0.6935Epoch 4/15: [============================  ] 71/75 batches, loss: 0.6935Epoch 4/15: [============================  ] 72/75 batches, loss: 0.6935Epoch 4/15: [============================= ] 73/75 batches, loss: 0.6935Epoch 4/15: [============================= ] 74/75 batches, loss: 0.6935Epoch 4/15: [==============================] 75/75 batches, loss: 0.6935
[2025-05-07 11:58:18,746][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6935
[2025-05-07 11:58:18,984][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 11:58:18,985][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-07 11:58:18,985][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 4
[2025-05-07 11:58:18,985][src.training.lm_trainer][INFO] - Training completed in 11.55 seconds
[2025-05-07 11:58:18,985][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 11:58:22,010][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.499581589958159, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 11:58:22,011][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 11:58:22,011][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 11:58:23,666][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control1/layer2/fi/fi/model.pt
[2025-05-07 11:58:23,667][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁
wandb:           best_val_f1 ▁
wandb:         best_val_loss ▁
wandb:    best_val_precision ▁
wandb:       best_val_recall ▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▃▃▆▆██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁
wandb:            train_loss █▂▁▂
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁
wandb:                val_f1 ▁▁▁▁
wandb:              val_loss ▁▂█▁
wandb:         val_precision ▁▁▁▁
wandb:            val_recall ▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.52381
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69315
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 4
wandb:                 epoch 4
wandb:   final_test_accuracy 0.5
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.49958
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.52381
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69346
wandb:            train_time 11.55156
wandb:          val_accuracy 0.52381
wandb:                val_f1 0
wandb:              val_loss 0.69315
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_115747-s8vgd3q2
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_115747-s8vgd3q2/logs
Experiment probe_layer2_question_type_control1_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control1/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_question_type_control2_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_question_type_control2_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control2/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 11:58:54,127][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control2/layer2/fi
experiment_name: probe_layer2_question_type_control2_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-07 11:58:54,127][__main__][INFO] - Normalized task: question_type
[2025-05-07 11:58:54,127][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-07 11:58:54,127][__main__][INFO] - Determined Task Type: classification
[2025-05-07 11:58:54,131][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-05-07 11:58:54,131][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 11:58:57,466][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 11:58:59,705][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 11:58:59,705][src.data.datasets][INFO] - Loading 'control_question_type_seed2' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:58:59,980][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-05-07 11:59:00,095][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-05-07 11:59:00,360][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 11:59:00,369][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:59:00,369][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 11:59:00,370][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:59:00,456][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:59:00,508][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:59:00,539][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 11:59:00,540][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:59:00,540][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 11:59:00,548][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 11:59:00,623][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:59:00,690][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 11:59:00,703][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 11:59:00,704][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 11:59:00,704][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 11:59:00,705][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 11:59:00,706][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:59:00,706][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:59:00,706][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:59:00,706][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:59:00,706][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-05-07 11:59:00,706][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-05-07 11:59:00,706][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 11:59:00,706][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 11:59:00,706][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:59:00,706][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:59:00,707][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:59:00,707][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:59:00,707][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-05-07 11:59:00,707][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-05-07 11:59:00,707][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 11:59:00,707][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 11:59:00,707][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 11:59:00,707][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 11:59:00,707][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 11:59:00,707][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 11:59:00,708][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-07 11:59:00,708][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-07 11:59:00,708][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 11:59:00,708][src.data.datasets][INFO] - Sample label: 0
[2025-05-07 11:59:00,708][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 11:59:00,708][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 11:59:00,708][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 11:59:00,708][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-07 11:59:00,709][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 11:59:08,937][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 11:59:08,937][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 11:59:08,938][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 11:59:08,938][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-07 11:59:08,943][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-07 11:59:08,944][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-07 11:59:08,944][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-07 11:59:08,944][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-07 11:59:08,944][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 11:59:08,945][__main__][INFO] - Total parameters: 394,568,839
[2025-05-07 11:59:08,945][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-07 11:59:08,946][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7175Epoch 1/15: [                              ] 2/75 batches, loss: 0.6998Epoch 1/15: [=                             ] 3/75 batches, loss: 0.6963Epoch 1/15: [=                             ] 4/75 batches, loss: 0.6967Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7009Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6987Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6954Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6947Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6957Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6957Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6941Epoch 1/15: [====                          ] 12/75 batches, loss: 0.6926Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.6911Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.6899Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6900Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6903Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6905Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6902Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6905Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6905Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6887Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6895Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6873Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6874Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6868Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6875Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6883Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6889Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6891Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6878Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6873Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6876Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6871Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6873Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6880Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6876Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6879Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6888Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6885Epoch 1/15: [================              ] 40/75 batches, loss: 0.6886Epoch 1/15: [================              ] 41/75 batches, loss: 0.6883Epoch 1/15: [================              ] 42/75 batches, loss: 0.6884Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6884Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6886Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6882Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6898Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6895Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6894Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6895Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6891Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6893Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6901Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6901Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6903Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6899Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6899Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6903Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6903Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6905Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6905Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6905Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6906Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6900Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6898Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6904Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6909Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6903Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6904Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6901Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6904Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6904Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6905Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6905Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6906Epoch 1/15: [==============================] 75/75 batches, loss: 0.6906
[2025-05-07 11:59:15,843][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6906
[2025-05-07 11:59:16,161][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6935, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.7098Epoch 2/15: [                              ] 2/75 batches, loss: 0.7013Epoch 2/15: [=                             ] 3/75 batches, loss: 0.7095Epoch 2/15: [=                             ] 4/75 batches, loss: 0.7087Epoch 2/15: [==                            ] 5/75 batches, loss: 0.7056Epoch 2/15: [==                            ] 6/75 batches, loss: 0.7033Epoch 2/15: [==                            ] 7/75 batches, loss: 0.7019Epoch 2/15: [===                           ] 8/75 batches, loss: 0.7021Epoch 2/15: [===                           ] 9/75 batches, loss: 0.7021Epoch 2/15: [====                          ] 10/75 batches, loss: 0.7015Epoch 2/15: [====                          ] 11/75 batches, loss: 0.7011Epoch 2/15: [====                          ] 12/75 batches, loss: 0.7004Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6998Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6993Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6989Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6985Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6982Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6980Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6978Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6975Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6973Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6971Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6969Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6968Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6966Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6965Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6963Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6962Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6961Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6960Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6959Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6958Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6958Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6957Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6956Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6955Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6955Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6954Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6954Epoch 2/15: [================              ] 40/75 batches, loss: 0.6953Epoch 2/15: [================              ] 41/75 batches, loss: 0.6953Epoch 2/15: [================              ] 42/75 batches, loss: 0.6952Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6952Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6951Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6951Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6950Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6950Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6949Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6949Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6949Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6948Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6948Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6948Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6947Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6947Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6947Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6947Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6946Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6946Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6946Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6946Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6945Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6945Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6945Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6945Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6945Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6945Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6944Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6944Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6944Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6944Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6944Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6944Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6943Epoch 2/15: [==============================] 75/75 batches, loss: 0.6943
[2025-05-07 11:59:18,889][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6943
[2025-05-07 11:59:19,151][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6929Epoch 3/15: [                              ] 2/75 batches, loss: 0.6928Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6928Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6929Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6929Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6930Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6931Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6931Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6931Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6931Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6931Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6931Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6931Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6931Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6931Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6931Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6931Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6931Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6931Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6931Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6931Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6932Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6932Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6932Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6932Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6932Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6932Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6932Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6932Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6932Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6932Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6932Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6932Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6932Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6932Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6932Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6932Epoch 3/15: [================              ] 40/75 batches, loss: 0.6932Epoch 3/15: [================              ] 41/75 batches, loss: 0.6932Epoch 3/15: [================              ] 42/75 batches, loss: 0.6932Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6932Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6932Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6932Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6932Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6932Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6932Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6932Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6932Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6932Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6932Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6932Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6932Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6932Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6932Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6932Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6932Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6932Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6932Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6932Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6932Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6932Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6932Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6932Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6932Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6932Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6932Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6932Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6932Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6932Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6932Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6932Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 3/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-07 11:59:21,886][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6931
[2025-05-07 11:59:22,192][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6934Epoch 4/15: [                              ] 2/75 batches, loss: 0.6933Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6933Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6933Epoch 4/15: [==                            ] 5/75 batches, loss: 0.6932Epoch 4/15: [==                            ] 6/75 batches, loss: 0.6932Epoch 4/15: [==                            ] 7/75 batches, loss: 0.6931Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6932Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6932Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6931Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6932Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6932Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6932Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6932Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6931Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6931Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6931Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6931Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6931Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6931Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6932Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6932Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6932Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6932Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6932Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6932Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6932Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6932Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6932Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6932Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6932Epoch 4/15: [==============                ] 35/75 batches, loss: 0.6932Epoch 4/15: [==============                ] 36/75 batches, loss: 0.6932Epoch 4/15: [==============                ] 37/75 batches, loss: 0.6932Epoch 4/15: [===============               ] 38/75 batches, loss: 0.6932Epoch 4/15: [===============               ] 39/75 batches, loss: 0.6932Epoch 4/15: [================              ] 40/75 batches, loss: 0.6931Epoch 4/15: [================              ] 41/75 batches, loss: 0.6931Epoch 4/15: [================              ] 42/75 batches, loss: 0.6931Epoch 4/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 4/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 4/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 4/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 4/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 4/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 4/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 4/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 4/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 4/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 4/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 4/15: [============================  ] 71/75 batches, loss: 0.6932Epoch 4/15: [============================  ] 72/75 batches, loss: 0.6932Epoch 4/15: [============================= ] 73/75 batches, loss: 0.6932Epoch 4/15: [============================= ] 74/75 batches, loss: 0.6932Epoch 4/15: [==============================] 75/75 batches, loss: 0.6932
[2025-05-07 11:59:24,888][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6932
[2025-05-07 11:59:25,181][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 11:59:25,182][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.6930Epoch 5/15: [                              ] 2/75 batches, loss: 0.6931Epoch 5/15: [=                             ] 3/75 batches, loss: 0.6931Epoch 5/15: [=                             ] 4/75 batches, loss: 0.6931Epoch 5/15: [==                            ] 5/75 batches, loss: 0.6930Epoch 5/15: [==                            ] 6/75 batches, loss: 0.6929Epoch 5/15: [==                            ] 7/75 batches, loss: 0.6929Epoch 5/15: [===                           ] 8/75 batches, loss: 0.6930Epoch 5/15: [===                           ] 9/75 batches, loss: 0.6930Epoch 5/15: [====                          ] 10/75 batches, loss: 0.6930Epoch 5/15: [====                          ] 11/75 batches, loss: 0.6930Epoch 5/15: [====                          ] 12/75 batches, loss: 0.6930Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.6930Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.6931Epoch 5/15: [======                        ] 15/75 batches, loss: 0.6930Epoch 5/15: [======                        ] 16/75 batches, loss: 0.6930Epoch 5/15: [======                        ] 17/75 batches, loss: 0.6930Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.6930Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 5/15: [========                      ] 20/75 batches, loss: 0.6931Epoch 5/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 5/15: [========                      ] 22/75 batches, loss: 0.6931Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.6931Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.6931Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.6931Epoch 5/15: [============                  ] 30/75 batches, loss: 0.6931Epoch 5/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 5/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 5/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 5/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 5/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 5/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 5/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 5/15: [================              ] 40/75 batches, loss: 0.6931Epoch 5/15: [================              ] 41/75 batches, loss: 0.6931Epoch 5/15: [================              ] 42/75 batches, loss: 0.6931Epoch 5/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 5/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 5/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 5/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 5/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 5/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 5/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 5/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 5/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 5/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 5/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 5/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 5/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 5/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 5/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 5/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 5/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 5/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 5/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 5/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 5/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 5/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-07 11:59:27,446][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6931
[2025-05-07 11:59:27,745][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 11:59:27,745][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.6930Epoch 6/15: [                              ] 2/75 batches, loss: 0.6930Epoch 6/15: [=                             ] 3/75 batches, loss: 0.6931Epoch 6/15: [=                             ] 4/75 batches, loss: 0.6931Epoch 6/15: [==                            ] 5/75 batches, loss: 0.6932Epoch 6/15: [==                            ] 6/75 batches, loss: 0.6932Epoch 6/15: [==                            ] 7/75 batches, loss: 0.6932Epoch 6/15: [===                           ] 8/75 batches, loss: 0.6932Epoch 6/15: [===                           ] 9/75 batches, loss: 0.6932Epoch 6/15: [====                          ] 10/75 batches, loss: 0.6932Epoch 6/15: [====                          ] 11/75 batches, loss: 0.6932Epoch 6/15: [====                          ] 12/75 batches, loss: 0.6932Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.6932Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.6932Epoch 6/15: [======                        ] 15/75 batches, loss: 0.6932Epoch 6/15: [======                        ] 16/75 batches, loss: 0.6932Epoch 6/15: [======                        ] 17/75 batches, loss: 0.6932Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.6932Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.6932Epoch 6/15: [========                      ] 20/75 batches, loss: 0.6932Epoch 6/15: [========                      ] 21/75 batches, loss: 0.6932Epoch 6/15: [========                      ] 22/75 batches, loss: 0.6932Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.6932Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.6932Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.6932Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.6932Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.6932Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.6932Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.6932Epoch 6/15: [============                  ] 30/75 batches, loss: 0.6932Epoch 6/15: [============                  ] 31/75 batches, loss: 0.6932Epoch 6/15: [============                  ] 32/75 batches, loss: 0.6932Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.6932Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.6932Epoch 6/15: [==============                ] 35/75 batches, loss: 0.6932Epoch 6/15: [==============                ] 36/75 batches, loss: 0.6932Epoch 6/15: [==============                ] 37/75 batches, loss: 0.6932Epoch 6/15: [===============               ] 38/75 batches, loss: 0.6932Epoch 6/15: [===============               ] 39/75 batches, loss: 0.6932Epoch 6/15: [================              ] 40/75 batches, loss: 0.6932Epoch 6/15: [================              ] 41/75 batches, loss: 0.6932Epoch 6/15: [================              ] 42/75 batches, loss: 0.6931Epoch 6/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 6/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 6/15: [==================            ] 45/75 batches, loss: 0.6932Epoch 6/15: [==================            ] 46/75 batches, loss: 0.6932Epoch 6/15: [==================            ] 47/75 batches, loss: 0.6932Epoch 6/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 6/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 6/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 6/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 6/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 6/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 6/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 6/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 6/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 6/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 6/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 6/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 6/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 6/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 6/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 6/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 6/15: [==============================] 75/75 batches, loss: 0.6930
[2025-05-07 11:59:30,135][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6930
[2025-05-07 11:59:30,404][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6933, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 11:59:30,405][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-07 11:59:30,405][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-07 11:59:30,405][src.training.lm_trainer][INFO] - Training completed in 17.59 seconds
[2025-05-07 11:59:30,405][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 11:59:33,437][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.499581589958159, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 11:59:33,438][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 11:59:33,438][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 11:59:35,065][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control2/layer2/fi/fi/model.pt
[2025-05-07 11:59:35,066][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▁
wandb:           best_val_f1 ▁▁▁
wandb:         best_val_loss █▁▁
wandb:    best_val_precision ▁▁▁
wandb:       best_val_recall ▁▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁▁
wandb:            train_loss ▁█▆▆▆▆
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁▁▁
wandb:                val_f1 ▁▁▁▁▁▁
wandb:              val_loss █▁▁▁▂▃
wandb:         val_precision ▁▁▁▁▁▁
wandb:            val_recall ▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.52381
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69319
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 6
wandb:                 epoch 6
wandb:   final_test_accuracy 0.5
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.49958
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.52381
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69304
wandb:            train_time 17.5887
wandb:          val_accuracy 0.52381
wandb:                val_f1 0
wandb:              val_loss 0.69329
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_115854-m49x7o24
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_115854-m49x7o24/logs
Experiment probe_layer2_question_type_control2_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control2/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_question_type_control3_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_question_type_control3_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control3/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 12:00:04,057][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control3/layer2/fi
experiment_name: probe_layer2_question_type_control3_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-07 12:00:04,057][__main__][INFO] - Normalized task: question_type
[2025-05-07 12:00:04,057][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-07 12:00:04,057][__main__][INFO] - Determined Task Type: classification
[2025-05-07 12:00:04,061][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-05-07 12:00:04,061][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 12:00:07,728][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 12:00:10,044][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 12:00:10,045][src.data.datasets][INFO] - Loading 'control_question_type_seed3' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:00:10,269][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:49:56 2025).
[2025-05-07 12:00:10,345][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:49:56 2025).
[2025-05-07 12:00:10,613][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 12:00:10,622][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:00:10,622][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 12:00:10,624][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:00:10,735][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:00:10,861][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:00:10,890][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 12:00:10,891][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:00:10,891][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 12:00:10,903][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:00:10,980][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:00:11,034][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:00:11,053][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 12:00:11,054][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:00:11,055][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 12:00:11,057][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 12:00:11,058][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 12:00:11,058][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 12:00:11,058][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 12:00:11,058][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 12:00:11,058][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-05-07 12:00:11,058][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-05-07 12:00:11,058][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 12:00:11,058][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 12:00:11,059][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 12:00:11,059][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 12:00:11,059][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 12:00:11,059][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 12:00:11,059][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-05-07 12:00:11,059][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-05-07 12:00:11,059][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 12:00:11,059][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 12:00:11,059][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 12:00:11,059][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 12:00:11,060][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 12:00:11,060][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 12:00:11,060][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-07 12:00:11,060][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-07 12:00:11,060][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 12:00:11,060][src.data.datasets][INFO] - Sample label: 0
[2025-05-07 12:00:11,060][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 12:00:11,060][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 12:00:11,060][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 12:00:11,061][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-07 12:00:11,061][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 12:00:18,742][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 12:00:18,743][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 12:00:18,743][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 12:00:18,743][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-07 12:00:18,749][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-07 12:00:18,750][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-07 12:00:18,750][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-07 12:00:18,750][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-07 12:00:18,750][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 12:00:18,751][__main__][INFO] - Total parameters: 394,568,839
[2025-05-07 12:00:18,751][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-07 12:00:18,752][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7396Epoch 1/15: [                              ] 2/75 batches, loss: 0.7470Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7376Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7295Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7203Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7165Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7129Epoch 1/15: [===                           ] 8/75 batches, loss: 0.7111Epoch 1/15: [===                           ] 9/75 batches, loss: 0.7095Epoch 1/15: [====                          ] 10/75 batches, loss: 0.7078Epoch 1/15: [====                          ] 11/75 batches, loss: 0.7062Epoch 1/15: [====                          ] 12/75 batches, loss: 0.7049Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.7039Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.7031Epoch 1/15: [======                        ] 15/75 batches, loss: 0.7023Epoch 1/15: [======                        ] 16/75 batches, loss: 0.7018Epoch 1/15: [======                        ] 17/75 batches, loss: 0.7013Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.7009Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.7006Epoch 1/15: [========                      ] 20/75 batches, loss: 0.7002Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6997Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6995Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6992Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6989Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6987Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6984Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6982Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6980Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6978Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6977Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6977Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6975Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6974Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6973Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6972Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6970Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6969Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6968Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6967Epoch 1/15: [================              ] 40/75 batches, loss: 0.6966Epoch 1/15: [================              ] 41/75 batches, loss: 0.6965Epoch 1/15: [================              ] 42/75 batches, loss: 0.6965Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6964Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6963Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6963Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6962Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6962Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6961Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6961Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6960Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6960Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6959Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6959Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6958Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6958Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6957Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6957Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6956Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6956Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6955Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6955Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6955Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6954Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6954Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6954Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6953Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6953Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6952Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6952Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6952Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6951Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6951Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6951Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6951Epoch 1/15: [==============================] 75/75 batches, loss: 0.6950
[2025-05-07 12:00:25,714][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6950
[2025-05-07 12:00:26,042][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6919Epoch 2/15: [                              ] 2/75 batches, loss: 0.6925Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6930Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6931Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6931Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6931Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6930Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6930Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6929Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6929Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6928Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6928Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6929Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6929Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6930Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6930Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6930Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6930Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6929Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6930Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6930Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6930Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6931Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6930Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6930Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6930Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6931Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6930Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6930Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 2/15: [================              ] 40/75 batches, loss: 0.6931Epoch 2/15: [================              ] 41/75 batches, loss: 0.6931Epoch 2/15: [================              ] 42/75 batches, loss: 0.6931Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6930Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 2/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-07 12:00:28,798][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6931
[2025-05-07 12:00:29,033][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:00:29,034][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6927Epoch 3/15: [                              ] 2/75 batches, loss: 0.6931Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6931Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6932Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6931Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6931Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6932Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6931Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6932Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6931Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6930Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6931Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6929Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6930Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6930Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6930Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6929Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6930Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6930Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6928Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6928Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6927Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6927Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6929Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6927Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6926Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6925Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6926Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6926Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6926Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6926Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6929Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6932Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6932Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6933Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6933Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6933Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6932Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6933Epoch 3/15: [================              ] 40/75 batches, loss: 0.6932Epoch 3/15: [================              ] 41/75 batches, loss: 0.6932Epoch 3/15: [================              ] 42/75 batches, loss: 0.6932Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6932Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6932Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6932Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6932Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6932Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6932Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6932Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6932Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6932Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6932Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6932Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6932Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6932Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6932Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6932Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6932Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6932Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6932Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6932Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6932Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6932Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6932Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6932Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6932Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6932Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6932Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6932Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6932Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6932Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6932Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6932Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6932Epoch 3/15: [==============================] 75/75 batches, loss: 0.6932
[2025-05-07 12:00:31,314][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6932
[2025-05-07 12:00:31,619][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:00:31,620][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6931Epoch 4/15: [                              ] 2/75 batches, loss: 0.6932Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6931Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6931Epoch 4/15: [==                            ] 5/75 batches, loss: 0.6931Epoch 4/15: [==                            ] 6/75 batches, loss: 0.6931Epoch 4/15: [==                            ] 7/75 batches, loss: 0.6931Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6930Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6931Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6931Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6931Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6931Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6932Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6932Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6933Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6933Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6932Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6933Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6932Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6932Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6932Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6932Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6932Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6932Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6932Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6931Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6931Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6931Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6932Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6932Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6932Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6932Epoch 4/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 4/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 4/15: [==============                ] 37/75 batches, loss: 0.6932Epoch 4/15: [===============               ] 38/75 batches, loss: 0.6932Epoch 4/15: [===============               ] 39/75 batches, loss: 0.6932Epoch 4/15: [================              ] 40/75 batches, loss: 0.6932Epoch 4/15: [================              ] 41/75 batches, loss: 0.6932Epoch 4/15: [================              ] 42/75 batches, loss: 0.6932Epoch 4/15: [=================             ] 43/75 batches, loss: 0.6932Epoch 4/15: [=================             ] 44/75 batches, loss: 0.6932Epoch 4/15: [==================            ] 45/75 batches, loss: 0.6932Epoch 4/15: [==================            ] 46/75 batches, loss: 0.6932Epoch 4/15: [==================            ] 47/75 batches, loss: 0.6932Epoch 4/15: [===================           ] 48/75 batches, loss: 0.6932Epoch 4/15: [===================           ] 49/75 batches, loss: 0.6932Epoch 4/15: [====================          ] 50/75 batches, loss: 0.6932Epoch 4/15: [====================          ] 51/75 batches, loss: 0.6932Epoch 4/15: [====================          ] 52/75 batches, loss: 0.6932Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.6932Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.6932Epoch 4/15: [======================        ] 55/75 batches, loss: 0.6932Epoch 4/15: [======================        ] 56/75 batches, loss: 0.6932Epoch 4/15: [======================        ] 57/75 batches, loss: 0.6932Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.6932Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.6932Epoch 4/15: [========================      ] 60/75 batches, loss: 0.6932Epoch 4/15: [========================      ] 61/75 batches, loss: 0.6932Epoch 4/15: [========================      ] 62/75 batches, loss: 0.6932Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.6932Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.6932Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.6932Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.6932Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.6932Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.6932Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.6932Epoch 4/15: [============================  ] 70/75 batches, loss: 0.6932Epoch 4/15: [============================  ] 71/75 batches, loss: 0.6932Epoch 4/15: [============================  ] 72/75 batches, loss: 0.6932Epoch 4/15: [============================= ] 73/75 batches, loss: 0.6932Epoch 4/15: [============================= ] 74/75 batches, loss: 0.6932Epoch 4/15: [==============================] 75/75 batches, loss: 0.6932
[2025-05-07 12:00:33,941][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6932
[2025-05-07 12:00:34,186][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:00:34,187][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-07 12:00:34,187][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 4
[2025-05-07 12:00:34,187][src.training.lm_trainer][INFO] - Training completed in 11.76 seconds
[2025-05-07 12:00:34,187][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 12:00:37,090][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.499581589958159, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:00:37,091][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:00:37,091][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:00:38,708][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control3/layer2/fi/fi/model.pt
[2025-05-07 12:00:38,709][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁
wandb:           best_val_f1 ▁
wandb:         best_val_loss ▁
wandb:    best_val_precision ▁
wandb:       best_val_recall ▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▃▃▆▆██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁
wandb:            train_loss █▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁
wandb:                val_f1 ▁▁▁▁
wandb:              val_loss ▁█▅▅
wandb:         val_precision ▁▁▁▁
wandb:            val_recall ▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.52381
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69319
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 4
wandb:                 epoch 4
wandb:   final_test_accuracy 0.5
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.49958
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.52381
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69317
wandb:            train_time 11.75961
wandb:          val_accuracy 0.52381
wandb:                val_f1 0
wandb:              val_loss 0.69321
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_120004-aghiw0nj
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_120004-aghiw0nj/logs
Experiment probe_layer2_question_type_control3_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control3/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_complexity_control1_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_control1_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 12:01:07,967][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/fi
experiment_name: probe_layer2_complexity_control1_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 12:01:07,967][__main__][INFO] - Normalized task: complexity
[2025-05-07 12:01:07,967][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 12:01:07,967][__main__][INFO] - Determined Task Type: regression
[2025-05-07 12:01:07,972][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-05-07 12:01:07,972][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 12:01:11,619][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 12:01:13,956][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 12:01:13,957][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:01:14,212][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-07 12:01:14,329][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-07 12:01:14,602][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 12:01:14,611][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:01:14,611][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 12:01:14,613][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:01:14,680][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:01:14,802][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:01:14,826][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 12:01:14,827][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:01:14,827][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 12:01:14,828][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:01:14,860][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:01:14,973][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:01:15,031][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 12:01:15,032][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:01:15,033][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 12:01:15,034][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 12:01:15,035][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 12:01:15,035][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 12:01:15,035][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 12:01:15,035][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 12:01:15,035][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 12:01:15,035][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-05-07 12:01:15,035][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 12:01:15,035][src.data.datasets][INFO] - Sample label: 0.21079079806804657
[2025-05-07 12:01:15,036][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 12:01:15,036][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 12:01:15,036][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 12:01:15,036][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 12:01:15,036][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 12:01:15,036][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-05-07 12:01:15,036][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 12:01:15,036][src.data.datasets][INFO] - Sample label: 1.0
[2025-05-07 12:01:15,036][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 12:01:15,036][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 12:01:15,036][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 12:01:15,037][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 12:01:15,037][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 12:01:15,037][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-05-07 12:01:15,037][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 12:01:15,037][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-05-07 12:01:15,037][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 12:01:15,037][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 12:01:15,037][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 12:01:15,038][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 12:01:15,038][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 12:01:23,117][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 12:01:23,118][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 12:01:23,118][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 12:01:23,118][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 12:01:23,121][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 12:01:23,122][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 12:01:23,122][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 12:01:23,122][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 12:01:23,122][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 12:01:23,123][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 12:01:23,123][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5536Epoch 1/15: [                              ] 2/75 batches, loss: 0.6171Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5359Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5046Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4800Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4533Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4279Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4599Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4717Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4517Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4345Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4359Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4182Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4213Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4168Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4267Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4161Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4132Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4112Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4055Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4182Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4214Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4152Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4031Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3960Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3914Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3850Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3797Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3788Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3822Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3775Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3735Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3709Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3682Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3644Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3639Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3602Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3580Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3552Epoch 1/15: [================              ] 40/75 batches, loss: 0.3503Epoch 1/15: [================              ] 41/75 batches, loss: 0.3465Epoch 1/15: [================              ] 42/75 batches, loss: 0.3426Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3388Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3364Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3368Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3326Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3328Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3287Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3260Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3241Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3252Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3238Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3200Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3208Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3187Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3175Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3186Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3174Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3156Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3135Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3100Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3089Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3068Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3048Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3031Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3014Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2991Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2962Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2960Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2957Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2933Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2918Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2887Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2863Epoch 1/15: [==============================] 75/75 batches, loss: 0.2860
[2025-05-07 12:01:30,016][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2860
[2025-05-07 12:01:30,322][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1289, Metrics: {'mse': 0.12874649465084076, 'rmse': 0.358812617741964, 'r2': -0.9637801647186279}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1691Epoch 2/15: [                              ] 2/75 batches, loss: 0.1455Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1139Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1468Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1375Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1385Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1443Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1390Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1363Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1410Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1340Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1310Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1289Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1243Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1250Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1221Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1244Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1224Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1221Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1225Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1218Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1229Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1258Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1234Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1249Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1312Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1307Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1293Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1316Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1324Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1304Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1288Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1270Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1292Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1296Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1294Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1292Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1272Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1267Epoch 2/15: [================              ] 40/75 batches, loss: 0.1260Epoch 2/15: [================              ] 41/75 batches, loss: 0.1252Epoch 2/15: [================              ] 42/75 batches, loss: 0.1254Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1250Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1230Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1228Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1243Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1241Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1237Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1262Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1251Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1236Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1241Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1248Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1244Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1240Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1251Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1246Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1246Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1241Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1237Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1244Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1241Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1236Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1233Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1227Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1228Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1224Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1224Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1222Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1217Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1211Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1216Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1224Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1216Epoch 2/15: [==============================] 75/75 batches, loss: 0.1207
[2025-05-07 12:01:33,059][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1207
[2025-05-07 12:01:33,321][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1390, Metrics: {'mse': 0.1390034407377243, 'rmse': 0.37283165200626983, 'r2': -1.120229959487915}
[2025-05-07 12:01:33,322][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1243Epoch 3/15: [                              ] 2/75 batches, loss: 0.1004Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1071Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0951Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0904Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0939Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0938Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0890Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0885Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0924Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0925Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0912Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0943Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0948Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0946Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0957Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0975Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0975Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0956Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0935Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0929Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0933Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0932Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0976Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0963Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0953Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0944Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0933Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0946Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0935Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0936Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0937Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0942Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0948Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0940Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0937Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0920Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0918Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0922Epoch 3/15: [================              ] 40/75 batches, loss: 0.0922Epoch 3/15: [================              ] 41/75 batches, loss: 0.0920Epoch 3/15: [================              ] 42/75 batches, loss: 0.0915Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0904Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0911Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0910Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0910Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0907Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0911Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0912Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0906Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0898Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0902Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0900Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0910Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0901Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0899Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0889Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0887Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0880Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0874Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0868Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0868Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0881Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0884Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0881Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0877Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0877Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0874Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0871Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0878Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0874Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0875Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0869Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0869Epoch 3/15: [==============================] 75/75 batches, loss: 0.0876
[2025-05-07 12:01:35,628][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0876
[2025-05-07 12:01:35,942][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1180, Metrics: {'mse': 0.11801812052726746, 'rmse': 0.34353765518101137, 'r2': -0.8001394271850586}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0845Epoch 4/15: [                              ] 2/75 batches, loss: 0.0895Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0782Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0892Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0833Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0860Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0802Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0824Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0811Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0776Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0788Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0792Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0763Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0731Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0769Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0778Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0780Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0778Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0773Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0771Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0768Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0776Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0786Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0792Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0825Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0815Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0811Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0805Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0807Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0801Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0798Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0794Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0787Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0774Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0769Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0764Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0756Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0762Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0754Epoch 4/15: [================              ] 40/75 batches, loss: 0.0766Epoch 4/15: [================              ] 41/75 batches, loss: 0.0777Epoch 4/15: [================              ] 42/75 batches, loss: 0.0775Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0776Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0770Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0770Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0764Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0759Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0759Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0759Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0750Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0747Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0749Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0751Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0758Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0757Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0758Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0754Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0754Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0754Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0759Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0757Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0755Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0751Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0748Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0744Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0738Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0735Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0737Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0743Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0740Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0741Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0738Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0736Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0738Epoch 4/15: [==============================] 75/75 batches, loss: 0.0741
[2025-05-07 12:01:38,637][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0741
[2025-05-07 12:01:38,875][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0943, Metrics: {'mse': 0.09424945712089539, 'rmse': 0.30700074449566955, 'r2': -0.4375941753387451}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1691Epoch 5/15: [                              ] 2/75 batches, loss: 0.1240Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1089Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1029Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0940Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0845Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0869Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0814Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0758Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0731Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0722Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0712Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0712Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0705Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0753Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0753Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0747Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0757Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0739Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0730Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0727Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0709Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0724Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0726Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0758Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0765Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0758Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0757Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0752Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0732Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0722Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0738Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0730Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0726Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0729Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0723Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0717Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0718Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0714Epoch 5/15: [================              ] 40/75 batches, loss: 0.0709Epoch 5/15: [================              ] 41/75 batches, loss: 0.0712Epoch 5/15: [================              ] 42/75 batches, loss: 0.0705Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0697Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0696Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0692Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0688Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0684Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0679Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0705Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0703Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0699Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0697Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0694Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0693Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0691Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0685Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0681Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0674Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0670Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0670Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0668Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0665Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0668Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0664Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0666Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0668Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0665Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0665Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0662Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0665Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0667Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0664Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0661Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0658Epoch 5/15: [==============================] 75/75 batches, loss: 0.0656
[2025-05-07 12:01:41,519][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0656
[2025-05-07 12:01:41,780][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1023, Metrics: {'mse': 0.10231486707925797, 'rmse': 0.3198669521523878, 'r2': -0.5606164932250977}
[2025-05-07 12:01:41,780][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0338Epoch 6/15: [                              ] 2/75 batches, loss: 0.0348Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0472Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0484Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0503Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0478Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0462Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0471Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0471Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0478Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0485Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0478Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0502Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0488Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0516Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0536Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0526Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0536Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0528Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0527Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0517Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0507Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0510Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0518Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0511Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0532Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0539Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0535Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0528Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0520Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0516Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0513Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0510Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0512Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0517Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0510Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0506Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0506Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0527Epoch 6/15: [================              ] 40/75 batches, loss: 0.0524Epoch 6/15: [================              ] 41/75 batches, loss: 0.0535Epoch 6/15: [================              ] 42/75 batches, loss: 0.0540Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0541Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0549Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0557Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0555Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0560Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0556Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0552Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0551Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0557Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0561Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0557Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0557Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0559Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0556Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0549Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0551Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0549Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0549Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0552Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0550Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0549Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0549Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0552Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0549Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0544Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0546Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0546Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0546Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0542Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0538Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0537Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0537Epoch 6/15: [==============================] 75/75 batches, loss: 0.0536
[2025-05-07 12:01:44,071][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0536
[2025-05-07 12:01:44,344][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.1080, Metrics: {'mse': 0.10800161957740784, 'rmse': 0.328635998602417, 'r2': -0.6473568677902222}
[2025-05-07 12:01:44,345][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0466Epoch 7/15: [                              ] 2/75 batches, loss: 0.0412Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0408Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0422Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0419Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0425Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0447Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0453Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0477Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0488Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0504Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0491Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0484Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0464Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0458Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0456Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0463Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0475Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0462Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0464Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0457Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0453Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0444Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0454Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0454Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0456Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0461Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0472Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0468Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0466Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0461Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0461Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0456Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0454Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0451Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0447Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0457Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0456Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0452Epoch 7/15: [================              ] 40/75 batches, loss: 0.0450Epoch 7/15: [================              ] 41/75 batches, loss: 0.0452Epoch 7/15: [================              ] 42/75 batches, loss: 0.0455Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0460Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0467Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0463Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0460Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0471Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0476Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0477Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0474Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0474Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0477Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0474Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0473Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0479Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0479Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0476Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0476Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0475Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0481Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0480Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0477Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0475Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0475Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0476Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0475Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0477Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0475Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0482Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0482Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0480Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0477Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0474Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0470Epoch 7/15: [==============================] 75/75 batches, loss: 0.0471
[2025-05-07 12:01:46,670][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0471
[2025-05-07 12:01:46,956][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1063, Metrics: {'mse': 0.10637115687131882, 'rmse': 0.3261459134671456, 'r2': -0.6224874258041382}
[2025-05-07 12:01:46,957][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0403Epoch 8/15: [                              ] 2/75 batches, loss: 0.0392Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0415Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0404Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0393Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0434Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0430Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0428Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0427Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0409Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0401Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0419Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0408Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0393Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0397Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0412Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0411Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0409Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0408Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0402Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0405Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0420Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0416Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0424Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0427Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0419Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0414Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0411Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0400Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0399Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0397Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0399Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0400Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0398Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0400Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0400Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0395Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0399Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0407Epoch 8/15: [================              ] 40/75 batches, loss: 0.0412Epoch 8/15: [================              ] 41/75 batches, loss: 0.0414Epoch 8/15: [================              ] 42/75 batches, loss: 0.0428Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0428Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0435Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0435Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0430Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0433Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0429Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0425Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0423Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0422Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0419Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0420Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0419Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0426Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0426Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0425Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0427Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0428Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0439Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0434Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0433Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0431Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0431Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0426Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0424Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0426Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0424Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0423Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0422Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0420Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0419Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0416Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0415Epoch 8/15: [==============================] 75/75 batches, loss: 0.0416
[2025-05-07 12:01:49,307][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0416
[2025-05-07 12:01:49,549][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.1084, Metrics: {'mse': 0.10843678563833237, 'rmse': 0.3292974121342777, 'r2': -0.6539945602416992}
[2025-05-07 12:01:49,549][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 12:01:49,550][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-07 12:01:49,550][src.training.lm_trainer][INFO] - Training completed in 22.64 seconds
[2025-05-07 12:01:49,550][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 12:01:52,500][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.021398909389972687, 'rmse': 0.14628366070745114, 'r2': -0.058930158615112305}
[2025-05-07 12:01:52,500][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.09424945712089539, 'rmse': 0.30700074449566955, 'r2': -0.4375941753387451}
[2025-05-07 12:01:52,500][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04391805827617645, 'rmse': 0.20956635769172602, 'r2': -0.11221551895141602}
[2025-05-07 12:01:54,165][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/fi/fi/model.pt
[2025-05-07 12:01:54,167][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▁
wandb:     best_val_mse █▆▁
wandb:      best_val_r2 ▁▃█
wandb:    best_val_rmse █▆▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▃▅▄▄▄
wandb:       train_loss █▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆█▅▁▂▃▃▃
wandb:          val_mse ▆█▅▁▂▃▃▃
wandb:           val_r2 ▃▁▄█▇▆▆▆
wandb:         val_rmse ▇█▅▁▂▃▃▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.09431
wandb:     best_val_mse 0.09425
wandb:      best_val_r2 -0.43759
wandb:    best_val_rmse 0.307
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.04392
wandb:    final_test_r2 -0.11222
wandb:  final_test_rmse 0.20957
wandb:  final_train_mse 0.0214
wandb:   final_train_r2 -0.05893
wandb: final_train_rmse 0.14628
wandb:    final_val_mse 0.09425
wandb:     final_val_r2 -0.43759
wandb:   final_val_rmse 0.307
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04159
wandb:       train_time 22.64293
wandb:         val_loss 0.10839
wandb:          val_mse 0.10844
wandb:           val_r2 -0.65399
wandb:         val_rmse 0.3293
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_120108-k3cetg8f
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_120108-k3cetg8f/logs
Experiment probe_layer2_complexity_control1_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_complexity_control2_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_control2_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 12:02:24,675][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/fi
experiment_name: probe_layer2_complexity_control2_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 12:02:24,675][__main__][INFO] - Normalized task: complexity
[2025-05-07 12:02:24,675][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 12:02:24,675][__main__][INFO] - Determined Task Type: regression
[2025-05-07 12:02:24,680][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-05-07 12:02:24,680][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 12:02:28,216][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 12:02:30,749][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 12:02:30,749][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:02:31,000][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-07 12:02:31,100][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-07 12:02:31,363][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 12:02:31,372][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:02:31,372][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 12:02:31,373][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:02:31,483][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:02:31,584][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:02:31,597][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 12:02:31,598][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:02:31,598][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 12:02:31,599][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:02:31,688][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:02:31,791][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:02:31,848][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 12:02:31,849][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:02:31,849][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 12:02:31,850][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 12:02:31,851][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 12:02:31,851][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 12:02:31,851][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 12:02:31,851][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 12:02:31,851][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 12:02:31,852][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-05-07 12:02:31,852][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 12:02:31,852][src.data.datasets][INFO] - Sample label: 0.6458609104156494
[2025-05-07 12:02:31,852][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 12:02:31,852][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 12:02:31,852][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 12:02:31,852][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 12:02:31,852][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 12:02:31,852][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-05-07 12:02:31,853][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 12:02:31,853][src.data.datasets][INFO] - Sample label: 1.0
[2025-05-07 12:02:31,853][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 12:02:31,853][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 12:02:31,853][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 12:02:31,853][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 12:02:31,853][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 12:02:31,853][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-05-07 12:02:31,853][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 12:02:31,853][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-05-07 12:02:31,854][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 12:02:31,854][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 12:02:31,854][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 12:02:31,854][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 12:02:31,854][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 12:02:39,346][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 12:02:39,347][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 12:02:39,347][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 12:02:39,347][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 12:02:39,350][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 12:02:39,351][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 12:02:39,351][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 12:02:39,351][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 12:02:39,351][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 12:02:39,352][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 12:02:39,352][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5716Epoch 1/15: [                              ] 2/75 batches, loss: 0.6299Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5288Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4743Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4784Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4445Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4170Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4617Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4723Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4528Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4417Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4357Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4217Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4262Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4235Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4419Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4324Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4269Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4214Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4167Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4245Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4270Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4193Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4070Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4009Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3945Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3881Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3825Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3823Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3812Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3729Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3686Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3650Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3620Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3582Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3596Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3553Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3499Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3495Epoch 1/15: [================              ] 40/75 batches, loss: 0.3449Epoch 1/15: [================              ] 41/75 batches, loss: 0.3407Epoch 1/15: [================              ] 42/75 batches, loss: 0.3368Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3347Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3321Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3356Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3316Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3321Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3277Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3253Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3243Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3235Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3221Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3190Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3202Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3181Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3165Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3178Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3182Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3158Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3133Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3097Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3081Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3065Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3050Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3033Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3022Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2999Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2964Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2978Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2972Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2951Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2926Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2906Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2882Epoch 1/15: [==============================] 75/75 batches, loss: 0.2864
[2025-05-07 12:02:46,203][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2864
[2025-05-07 12:02:46,478][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1313, Metrics: {'mse': 0.13113075494766235, 'rmse': 0.36211980745005146, 'r2': -1.0001473426818848}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1747Epoch 2/15: [                              ] 2/75 batches, loss: 0.1361Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1081Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1267Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1302Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1294Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1430Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1408Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1386Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1431Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1392Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1321Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1331Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1289Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1318Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1317Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1303Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1304Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1274Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1280Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1296Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1311Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1322Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1305Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1305Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1339Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1370Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1370Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1383Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1400Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1381Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1361Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1346Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1361Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1393Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1373Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1377Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1371Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1364Epoch 2/15: [================              ] 40/75 batches, loss: 0.1355Epoch 2/15: [================              ] 41/75 batches, loss: 0.1358Epoch 2/15: [================              ] 42/75 batches, loss: 0.1348Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1344Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1326Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1306Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1338Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1340Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1335Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1353Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1339Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1332Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1330Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1335Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1332Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1322Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1338Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1327Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1318Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1316Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1305Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1308Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1305Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1300Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1298Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1290Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1289Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1286Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1284Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1281Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1271Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1269Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1269Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1266Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1267Epoch 2/15: [==============================] 75/75 batches, loss: 0.1268
[2025-05-07 12:02:49,178][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1268
[2025-05-07 12:02:49,499][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1284, Metrics: {'mse': 0.12829945981502533, 'rmse': 0.35818913972233346, 'r2': -0.9569613933563232}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.0950Epoch 3/15: [                              ] 2/75 batches, loss: 0.0857Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0807Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0792Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0776Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0776Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0752Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0728Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0695Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0770Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0745Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0740Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0789Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0798Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0795Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0793Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0838Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0838Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0857Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0841Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0845Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0854Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0832Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0927Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0917Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0909Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0901Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0907Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0918Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0902Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0900Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0907Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0908Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0906Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0899Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0888Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0875Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0872Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0871Epoch 3/15: [================              ] 40/75 batches, loss: 0.0873Epoch 3/15: [================              ] 41/75 batches, loss: 0.0872Epoch 3/15: [================              ] 42/75 batches, loss: 0.0865Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0857Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0866Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0866Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0867Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0872Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0876Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0880Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0879Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0875Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0876Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0868Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0878Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0870Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0863Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0861Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0857Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0852Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0844Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0837Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0838Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0840Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0846Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0842Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0836Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0837Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0834Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0839Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0841Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0842Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0835Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0832Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0830Epoch 3/15: [==============================] 75/75 batches, loss: 0.0839
[2025-05-07 12:02:52,180][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0839
[2025-05-07 12:02:52,483][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1134, Metrics: {'mse': 0.11337482929229736, 'rmse': 0.33671178965444226, 'r2': -0.729314923286438}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0697Epoch 4/15: [                              ] 2/75 batches, loss: 0.0637Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0706Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0789Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0726Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0731Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0757Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0761Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0777Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0740Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0771Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0799Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0756Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0734Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0749Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0733Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0721Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0718Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0700Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0725Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0719Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0724Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0733Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0735Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0765Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0774Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0770Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0755Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0756Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0747Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0752Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0747Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0735Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0727Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0740Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0741Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0735Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0740Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0731Epoch 4/15: [================              ] 40/75 batches, loss: 0.0726Epoch 4/15: [================              ] 41/75 batches, loss: 0.0729Epoch 4/15: [================              ] 42/75 batches, loss: 0.0727Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0726Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0715Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0716Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0713Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0710Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0715Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0717Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0714Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0711Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0714Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0713Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0708Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0708Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0723Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0717Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0724Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0723Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0727Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0730Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0726Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0722Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0722Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0724Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0720Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0719Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0719Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0716Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0716Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0713Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0707Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0706Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0710Epoch 4/15: [==============================] 75/75 batches, loss: 0.0712
[2025-05-07 12:02:55,207][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0712
[2025-05-07 12:02:55,430][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1129, Metrics: {'mse': 0.11288510262966156, 'rmse': 0.3359837832837495, 'r2': -0.7218451499938965}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0637Epoch 5/15: [                              ] 2/75 batches, loss: 0.0736Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0954Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0924Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0887Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0797Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0770Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0742Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0720Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0695Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0690Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0696Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0697Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0712Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0700Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0687Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0685Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0681Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0671Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0662Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0671Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0656Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0657Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0655Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0680Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0677Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0671Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0671Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0668Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0658Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0651Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0658Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0653Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0653Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0666Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0661Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0669Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0668Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0671Epoch 5/15: [================              ] 40/75 batches, loss: 0.0675Epoch 5/15: [================              ] 41/75 batches, loss: 0.0674Epoch 5/15: [================              ] 42/75 batches, loss: 0.0669Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0673Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0671Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0663Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0668Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0660Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0653Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0668Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0664Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0657Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0652Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0650Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0650Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0647Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0644Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0640Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0638Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0633Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0631Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0626Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0627Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0627Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0627Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0628Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0628Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0625Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0623Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0620Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0622Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0625Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0622Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0620Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0621Epoch 5/15: [==============================] 75/75 batches, loss: 0.0616
[2025-05-07 12:02:58,123][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0616
[2025-05-07 12:02:58,401][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1117, Metrics: {'mse': 0.11177487671375275, 'rmse': 0.3343274991886739, 'r2': -0.7049107551574707}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0454Epoch 6/15: [                              ] 2/75 batches, loss: 0.0551Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0530Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0478Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0605Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0567Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0565Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0573Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0521Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0508Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0511Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0506Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0525Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0522Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0536Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0544Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0529Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0578Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0567Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0557Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0547Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0533Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0540Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0541Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0549Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0540Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0537Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0530Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0521Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0526Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0524Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0520Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0525Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0522Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0525Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0519Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0520Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0516Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0539Epoch 6/15: [================              ] 40/75 batches, loss: 0.0540Epoch 6/15: [================              ] 41/75 batches, loss: 0.0535Epoch 6/15: [================              ] 42/75 batches, loss: 0.0534Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0534Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0535Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0547Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0543Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0540Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0534Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0528Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0525Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0530Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0531Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0531Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0528Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0530Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0529Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0530Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0530Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0529Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0534Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0540Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0539Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0541Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0541Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0541Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0539Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0538Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0538Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0537Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0534Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0534Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0532Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0528Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0527Epoch 6/15: [==============================] 75/75 batches, loss: 0.0524
[2025-05-07 12:03:01,213][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0524
[2025-05-07 12:03:01,563][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.1115, Metrics: {'mse': 0.11156716197729111, 'rmse': 0.3340167091288864, 'r2': -0.7017425298690796}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0374Epoch 7/15: [                              ] 2/75 batches, loss: 0.0453Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0428Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0439Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0436Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0442Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0465Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0442Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0444Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0438Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0449Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0449Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0478Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0487Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0497Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0491Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0490Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0504Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0500Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0492Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0497Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0493Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0487Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0490Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0486Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0491Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0491Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0487Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0490Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0480Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0475Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0469Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0464Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0465Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0459Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0456Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0465Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0463Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0460Epoch 7/15: [================              ] 40/75 batches, loss: 0.0460Epoch 7/15: [================              ] 41/75 batches, loss: 0.0461Epoch 7/15: [================              ] 42/75 batches, loss: 0.0463Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0462Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0461Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0454Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0453Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0455Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0453Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0454Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0452Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0456Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0462Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0465Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0470Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0474Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0475Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0476Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0481Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0481Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0488Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0485Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0480Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0483Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0483Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0485Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0486Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0486Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0482Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0483Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0485Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0483Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0482Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0481Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0477Epoch 7/15: [==============================] 75/75 batches, loss: 0.0478
[2025-05-07 12:03:04,361][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0478
[2025-05-07 12:03:04,764][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1065, Metrics: {'mse': 0.10654313117265701, 'rmse': 0.32640945325259346, 'r2': -0.6251105070114136}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0380Epoch 8/15: [                              ] 2/75 batches, loss: 0.0302Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0327Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0322Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0307Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0331Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0371Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0382Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0384Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0385Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0388Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0380Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0393Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0384Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0383Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0389Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0391Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0395Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0411Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0433Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0435Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0433Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0425Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0427Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0437Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0430Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0438Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0432Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0427Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0437Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0434Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0433Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0430Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0431Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0431Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0426Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0427Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0423Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0423Epoch 8/15: [================              ] 40/75 batches, loss: 0.0424Epoch 8/15: [================              ] 41/75 batches, loss: 0.0421Epoch 8/15: [================              ] 42/75 batches, loss: 0.0419Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0417Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0421Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0426Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0426Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0430Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0426Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0428Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0426Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0434Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0437Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0436Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0432Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0431Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0436Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0435Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0431Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0432Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0437Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0434Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0435Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0439Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0440Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0439Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0436Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0434Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0432Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0431Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0428Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0427Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0424Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0420Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0422Epoch 8/15: [==============================] 75/75 batches, loss: 0.0422
[2025-05-07 12:03:07,476][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0422
[2025-05-07 12:03:07,711][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.1174, Metrics: {'mse': 0.11751455068588257, 'rmse': 0.3428039537197355, 'r2': -0.7924584150314331}
[2025-05-07 12:03:07,711][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0224Epoch 9/15: [                              ] 2/75 batches, loss: 0.0270Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0268Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0246Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0221Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0262Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0253Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0288Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0306Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0297Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0304Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0300Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0300Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0299Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0300Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0308Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0310Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0322Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0340Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0346Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0350Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0345Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0343Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0348Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0350Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0350Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0345Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0342Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0353Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0355Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0358Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0355Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0356Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0363Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0362Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0366Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0367Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0362Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0363Epoch 9/15: [================              ] 40/75 batches, loss: 0.0369Epoch 9/15: [================              ] 41/75 batches, loss: 0.0373Epoch 9/15: [================              ] 42/75 batches, loss: 0.0371Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0376Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0377Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0377Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0381Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0379Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0381Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0379Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0377Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0380Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0380Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0384Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0385Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0385Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0383Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0384Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0382Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0382Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0379Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0378Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0387Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0386Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0387Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0386Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0387Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0387Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0386Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0388Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0387Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0386Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0385Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0384Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0385Epoch 9/15: [==============================] 75/75 batches, loss: 0.0381
[2025-05-07 12:03:10,098][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0381
[2025-05-07 12:03:10,360][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.1113, Metrics: {'mse': 0.11136011779308319, 'rmse': 0.33370663432584496, 'r2': -0.6985844373703003}
[2025-05-07 12:03:10,361][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0433Epoch 10/15: [                              ] 2/75 batches, loss: 0.0406Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0343Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0341Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0356Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0414Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0400Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0433Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0435Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0443Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0441Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0428Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0415Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0435Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0428Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0430Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0420Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0416Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0412Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0405Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0397Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0398Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0394Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0394Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0387Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0385Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0392Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0392Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0401Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0399Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0396Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0400Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0406Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0400Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0406Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0405Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0399Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0404Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0401Epoch 10/15: [================              ] 40/75 batches, loss: 0.0404Epoch 10/15: [================              ] 41/75 batches, loss: 0.0410Epoch 10/15: [================              ] 42/75 batches, loss: 0.0407Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0411Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0416Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0413Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0412Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0411Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0414Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0411Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0416Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0414Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0411Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0408Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0407Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0404Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0403Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0400Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0397Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0397Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0398Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0397Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0394Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0394Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0392Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0392Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0394Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0394Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0396Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0396Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0394Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0393Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0395Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0397Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0404Epoch 10/15: [==============================] 75/75 batches, loss: 0.0404
[2025-05-07 12:03:12,804][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0404
[2025-05-07 12:03:13,214][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.1209, Metrics: {'mse': 0.1209455356001854, 'rmse': 0.3477722467365465, 'r2': -0.8447915315628052}
[2025-05-07 12:03:13,215][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0249Epoch 11/15: [                              ] 2/75 batches, loss: 0.0271Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0269Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0275Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0291Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0285Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0297Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0331Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0352Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0364Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0356Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0349Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0344Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0342Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0341Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0340Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0358Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0366Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0359Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0359Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0362Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0364Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0364Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0365Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0371Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0372Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0373Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0369Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0364Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0368Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0363Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0373Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0372Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0368Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0366Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0366Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0363Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0360Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0361Epoch 11/15: [================              ] 40/75 batches, loss: 0.0364Epoch 11/15: [================              ] 41/75 batches, loss: 0.0362Epoch 11/15: [================              ] 42/75 batches, loss: 0.0358Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0358Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0361Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0359Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0357Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0355Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0354Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0353Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0353Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0350Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0351Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0352Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0351Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0355Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0352Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0353Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0352Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0352Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0348Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0348Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0346Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0343Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0341Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0342Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0339Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0339Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0338Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0340Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0339Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0339Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0340Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0341Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0340Epoch 11/15: [==============================] 75/75 batches, loss: 0.0339
[2025-05-07 12:03:15,598][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0339
[2025-05-07 12:03:15,886][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.1097, Metrics: {'mse': 0.10973445326089859, 'rmse': 0.33126191036836483, 'r2': -0.6737879514694214}
[2025-05-07 12:03:15,887][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 12:03:15,887][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 11
[2025-05-07 12:03:15,887][src.training.lm_trainer][INFO] - Training completed in 32.93 seconds
[2025-05-07 12:03:15,887][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 12:03:18,971][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.022454697638750076, 'rmse': 0.14984891604129166, 'r2': -0.1111760139465332}
[2025-05-07 12:03:18,971][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.10654313117265701, 'rmse': 0.32640945325259346, 'r2': -0.6251105070114136}
[2025-05-07 12:03:18,971][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.048581503331661224, 'rmse': 0.22041212156245224, 'r2': -0.23031628131866455}
[2025-05-07 12:03:20,640][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/fi/fi/model.pt
[2025-05-07 12:03:20,641][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▃▃▂▂▁
wandb:     best_val_mse █▇▃▃▂▂▁
wandb:      best_val_r2 ▁▂▆▆▇▇█
wandb:    best_val_rmse █▇▃▃▃▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▁▃▃▃▃▃▂▃▂
wandb:       train_loss █▄▂▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▃▃▂▂▁▄▂▅▂
wandb:          val_mse █▇▃▃▂▂▁▄▂▅▂
wandb:           val_r2 ▁▂▆▆▇▇█▅▇▄▇
wandb:         val_rmse █▇▃▃▃▂▁▄▂▅▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.10652
wandb:     best_val_mse 0.10654
wandb:      best_val_r2 -0.62511
wandb:    best_val_rmse 0.32641
wandb: early_stop_epoch 11
wandb:            epoch 11
wandb:   final_test_mse 0.04858
wandb:    final_test_r2 -0.23032
wandb:  final_test_rmse 0.22041
wandb:  final_train_mse 0.02245
wandb:   final_train_r2 -0.11118
wandb: final_train_rmse 0.14985
wandb:    final_val_mse 0.10654
wandb:     final_val_r2 -0.62511
wandb:   final_val_rmse 0.32641
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03387
wandb:       train_time 32.93036
wandb:         val_loss 0.10966
wandb:          val_mse 0.10973
wandb:           val_r2 -0.67379
wandb:         val_rmse 0.33126
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_120224-61865dcb
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_120224-61865dcb/logs
Experiment probe_layer2_complexity_control2_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_complexity_control3_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_control3_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 12:03:52,754][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/fi
experiment_name: probe_layer2_complexity_control3_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 12:03:52,754][__main__][INFO] - Normalized task: complexity
[2025-05-07 12:03:52,754][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 12:03:52,754][__main__][INFO] - Determined Task Type: regression
[2025-05-07 12:03:52,762][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-05-07 12:03:52,762][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 12:03:56,645][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 12:03:58,898][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 12:03:58,898][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:03:59,375][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-07 12:03:59,541][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-07 12:03:59,839][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 12:03:59,848][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:03:59,848][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 12:03:59,849][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:04:00,076][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:04:00,154][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:04:00,166][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 12:04:00,167][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:04:00,167][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 12:04:00,168][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:04:00,290][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:04:00,436][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:04:00,485][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 12:04:00,487][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:04:00,487][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 12:04:00,488][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 12:04:00,488][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 12:04:00,488][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 12:04:00,488][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 12:04:00,488][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 12:04:00,488][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 12:04:00,489][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-05-07 12:04:00,489][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 12:04:00,489][src.data.datasets][INFO] - Sample label: 0.3422375023365021
[2025-05-07 12:04:00,489][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 12:04:00,489][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 12:04:00,489][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 12:04:00,489][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 12:04:00,489][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 12:04:00,489][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-05-07 12:04:00,490][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 12:04:00,490][src.data.datasets][INFO] - Sample label: 1.0
[2025-05-07 12:04:00,490][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 12:04:00,490][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 12:04:00,490][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 12:04:00,490][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 12:04:00,490][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 12:04:00,490][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-05-07 12:04:00,490][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 12:04:00,490][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-05-07 12:04:00,491][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 12:04:00,491][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 12:04:00,491][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 12:04:00,491][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 12:04:00,491][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 12:04:08,579][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 12:04:08,580][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 12:04:08,580][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 12:04:08,580][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 12:04:08,583][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 12:04:08,584][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 12:04:08,584][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 12:04:08,584][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 12:04:08,584][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 12:04:08,585][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 12:04:08,585][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5435Epoch 1/15: [                              ] 2/75 batches, loss: 0.6127Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5197Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4734Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4630Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4445Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4205Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4493Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4549Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4424Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4303Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4265Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4073Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4140Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4040Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4171Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4059Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4014Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3993Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3937Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4038Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4076Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3986Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3873Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3810Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3744Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3685Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3661Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3656Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3651Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3602Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3560Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3537Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3507Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3462Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3477Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3434Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3392Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3389Epoch 1/15: [================              ] 40/75 batches, loss: 0.3341Epoch 1/15: [================              ] 41/75 batches, loss: 0.3297Epoch 1/15: [================              ] 42/75 batches, loss: 0.3266Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3221Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3213Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3241Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3202Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3213Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3175Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3147Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3127Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3118Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3103Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3078Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3082Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3064Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3045Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3083Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3073Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3046Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3027Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2994Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2977Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2962Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2942Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2925Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2925Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2894Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2863Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2856Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2841Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2820Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2809Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2780Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2757Epoch 1/15: [==============================] 75/75 batches, loss: 0.2738
[2025-05-07 12:04:15,334][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2738
[2025-05-07 12:04:15,644][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1275, Metrics: {'mse': 0.12730590999126434, 'rmse': 0.35679953754351246, 'r2': -0.9418067932128906}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1484Epoch 2/15: [                              ] 2/75 batches, loss: 0.1140Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1008Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1268Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1332Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1300Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1449Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1426Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1478Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1554Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1487Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1443Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1407Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1363Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1361Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1349Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1356Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1383Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1362Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1342Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1337Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1361Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1354Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1335Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1328Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1349Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1360Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1363Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1387Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1400Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1389Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1379Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1376Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1389Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1401Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1388Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1366Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1351Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1361Epoch 2/15: [================              ] 40/75 batches, loss: 0.1344Epoch 2/15: [================              ] 41/75 batches, loss: 0.1331Epoch 2/15: [================              ] 42/75 batches, loss: 0.1313Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1312Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1294Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1288Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1304Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1294Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1289Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1326Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1311Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1306Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1311Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1310Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1306Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1295Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1299Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1297Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1291Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1280Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1266Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1258Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1256Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1246Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1249Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1244Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1242Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1245Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1249Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1244Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1235Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1232Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1240Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1239Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1230Epoch 2/15: [==============================] 75/75 batches, loss: 0.1229
[2025-05-07 12:04:18,419][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1229
[2025-05-07 12:04:18,758][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1435, Metrics: {'mse': 0.14352861046791077, 'rmse': 0.3788516998350552, 'r2': -1.1892528533935547}
[2025-05-07 12:04:18,758][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.0910Epoch 3/15: [                              ] 2/75 batches, loss: 0.0784Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0883Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0975Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0882Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0865Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0832Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0799Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0790Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0859Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0846Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0876Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0907Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0934Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0926Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0925Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0919Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0897Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0893Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0898Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0900Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0905Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0883Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0952Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0948Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0935Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0923Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0919Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0933Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0935Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0929Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0921Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0924Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0932Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0924Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0921Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0911Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0910Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0916Epoch 3/15: [================              ] 40/75 batches, loss: 0.0914Epoch 3/15: [================              ] 41/75 batches, loss: 0.0907Epoch 3/15: [================              ] 42/75 batches, loss: 0.0897Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0890Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0908Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0906Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0901Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0908Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0911Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0910Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0906Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0899Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0896Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0889Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0890Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0884Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0873Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0866Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0868Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0862Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0855Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0848Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0851Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0853Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0862Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0861Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0853Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0855Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0848Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0846Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0848Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0847Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0845Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0840Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0844Epoch 3/15: [==============================] 75/75 batches, loss: 0.0849
[2025-05-07 12:04:21,155][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0849
[2025-05-07 12:04:21,399][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1145, Metrics: {'mse': 0.11447889357805252, 'rmse': 0.3383472972820272, 'r2': -0.7461552619934082}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1423Epoch 4/15: [                              ] 2/75 batches, loss: 0.0987Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0834Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0849Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0771Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0788Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0817Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0792Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0809Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0782Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0863Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0903Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0858Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0838Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0873Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0879Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0858Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0831Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0820Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0811Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0810Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0831Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0824Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0834Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0846Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0844Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0838Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0828Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0826Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0821Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0819Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0813Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0800Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0800Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0798Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0790Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0779Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0791Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0783Epoch 4/15: [================              ] 40/75 batches, loss: 0.0778Epoch 4/15: [================              ] 41/75 batches, loss: 0.0782Epoch 4/15: [================              ] 42/75 batches, loss: 0.0773Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0783Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0784Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0782Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0775Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0771Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0772Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0772Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0768Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0760Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0763Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0762Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0760Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0755Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0758Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0750Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0747Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0748Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0745Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0749Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0743Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0741Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0742Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0740Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0738Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0735Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0734Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0732Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0734Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0731Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0731Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0728Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0727Epoch 4/15: [==============================] 75/75 batches, loss: 0.0722
[2025-05-07 12:04:24,181][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0722
[2025-05-07 12:04:24,542][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1093, Metrics: {'mse': 0.10927031189203262, 'rmse': 0.3305606024498876, 'r2': -0.6667084693908691}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0926Epoch 5/15: [                              ] 2/75 batches, loss: 0.0898Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1047Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0954Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0868Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0826Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0826Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0802Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0757Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0724Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0741Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0744Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0767Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0757Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0775Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0753Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0748Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0752Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0750Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0738Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0739Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0737Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0733Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0731Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0747Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0749Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0743Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0730Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0715Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0702Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0706Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0706Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0699Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0699Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0703Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0696Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0698Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0692Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0686Epoch 5/15: [================              ] 40/75 batches, loss: 0.0681Epoch 5/15: [================              ] 41/75 batches, loss: 0.0680Epoch 5/15: [================              ] 42/75 batches, loss: 0.0681Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0673Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0667Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0666Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0668Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0659Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0656Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0660Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0655Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0648Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0646Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0644Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0645Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0642Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0638Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0637Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0631Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0632Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0634Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0630Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0637Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0643Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0644Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0640Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0638Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0633Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0630Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0626Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0633Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0636Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0633Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0629Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0628Epoch 5/15: [==============================] 75/75 batches, loss: 0.0622
[2025-05-07 12:04:27,276][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0622
[2025-05-07 12:04:27,565][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1089, Metrics: {'mse': 0.10895277559757233, 'rmse': 0.330079953340963, 'r2': -0.6618649959564209}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0663Epoch 6/15: [                              ] 2/75 batches, loss: 0.0501Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0472Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0435Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0446Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0439Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0430Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0482Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0461Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0454Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0467Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0480Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0547Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0537Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0538Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0532Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0510Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0532Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0519Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0510Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0499Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0492Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0504Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0513Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0524Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0525Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0528Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0519Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0513Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0520Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0517Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0511Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0503Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0498Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0500Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0498Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0492Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0492Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0515Epoch 6/15: [================              ] 40/75 batches, loss: 0.0514Epoch 6/15: [================              ] 41/75 batches, loss: 0.0514Epoch 6/15: [================              ] 42/75 batches, loss: 0.0510Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0505Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0505Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0513Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0509Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0514Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0511Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0514Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0514Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0516Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0513Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0520Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0519Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0514Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0515Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0516Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0515Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0513Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0522Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0525Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0526Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0523Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0525Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0525Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0524Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0527Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0531Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0528Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0525Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0527Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0525Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0526Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0525Epoch 6/15: [==============================] 75/75 batches, loss: 0.0523
[2025-05-07 12:04:30,207][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0523
[2025-05-07 12:04:30,446][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.1025, Metrics: {'mse': 0.10245323926210403, 'rmse': 0.3200831755373969, 'r2': -0.5627270936965942}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0665Epoch 7/15: [                              ] 2/75 batches, loss: 0.0633Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0578Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0582Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0528Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0522Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0524Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0516Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0506Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0530Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0509Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0491Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0489Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0482Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0479Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0496Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0499Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0521Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0505Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0519Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0518Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0521Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0520Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0533Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0532Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0531Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0535Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0533Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0526Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0517Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0507Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0513Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0503Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0506Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0500Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0503Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0505Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0513Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0508Epoch 7/15: [================              ] 40/75 batches, loss: 0.0510Epoch 7/15: [================              ] 41/75 batches, loss: 0.0510Epoch 7/15: [================              ] 42/75 batches, loss: 0.0518Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0515Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0513Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0509Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0503Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0502Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0495Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0493Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0491Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0491Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0494Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0491Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0492Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0498Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0498Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0497Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0496Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0493Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0499Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0495Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0491Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0498Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0496Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0494Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0496Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0492Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0489Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0487Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0487Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0485Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0481Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0479Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0476Epoch 7/15: [==============================] 75/75 batches, loss: 0.0473
[2025-05-07 12:04:33,186][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0473
[2025-05-07 12:04:33,470][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1122, Metrics: {'mse': 0.11220899224281311, 'rmse': 0.33497610697303937, 'r2': -0.7115323543548584}
[2025-05-07 12:04:33,471][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0433Epoch 8/15: [                              ] 2/75 batches, loss: 0.0413Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0429Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0406Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0368Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0395Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0411Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0413Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0426Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0412Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0403Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0393Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0389Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0421Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0413Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0407Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0407Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0402Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0415Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0411Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0416Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0424Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0416Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0427Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0424Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0418Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0413Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0412Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0405Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0411Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0423Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0427Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0429Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0423Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0423Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0418Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0413Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0410Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0412Epoch 8/15: [================              ] 40/75 batches, loss: 0.0414Epoch 8/15: [================              ] 41/75 batches, loss: 0.0414Epoch 8/15: [================              ] 42/75 batches, loss: 0.0412Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0411Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0411Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0416Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0423Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0426Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0424Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0423Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0424Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0427Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0431Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0427Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0425Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0426Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0424Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0425Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0422Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0426Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0427Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0426Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0424Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0422Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0427Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0428Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0428Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0429Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0427Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0426Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0422Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0422Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0421Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0422Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0420Epoch 8/15: [==============================] 75/75 batches, loss: 0.0427
[2025-05-07 12:04:35,786][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0427
[2025-05-07 12:04:36,089][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.1121, Metrics: {'mse': 0.1121562272310257, 'rmse': 0.3348973383456872, 'r2': -0.7107274532318115}
[2025-05-07 12:04:36,090][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0314Epoch 9/15: [                              ] 2/75 batches, loss: 0.0521Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0447Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0428Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0464Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0461Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0454Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0431Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0436Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0434Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0436Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0419Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0404Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0419Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0427Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0433Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0429Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0424Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0427Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0422Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0412Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0423Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0417Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0426Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0426Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0417Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0408Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0400Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0400Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0401Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0396Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0394Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0391Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0398Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0393Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0388Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0392Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0389Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0387Epoch 9/15: [================              ] 40/75 batches, loss: 0.0388Epoch 9/15: [================              ] 41/75 batches, loss: 0.0390Epoch 9/15: [================              ] 42/75 batches, loss: 0.0392Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0389Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0389Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0392Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0389Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0389Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0388Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0389Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0388Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0388Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0389Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0387Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0384Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0384Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0387Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0387Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0385Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0382Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0383Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0381Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0383Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0383Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0382Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0383Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0382Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0381Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0378Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0380Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0381Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0384Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0383Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0383Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0382Epoch 9/15: [==============================] 75/75 batches, loss: 0.0381
[2025-05-07 12:04:38,404][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0381
[2025-05-07 12:04:38,649][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.1097, Metrics: {'mse': 0.10973675549030304, 'rmse': 0.3312653852884467, 'r2': -0.6738231182098389}
[2025-05-07 12:04:38,650][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0427Epoch 10/15: [                              ] 2/75 batches, loss: 0.0324Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0283Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0303Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0330Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0370Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0397Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0398Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0385Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0382Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0382Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0392Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0392Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0415Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0423Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0422Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0424Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0433Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0437Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0450Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0440Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0436Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0434Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0430Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0423Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0420Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0415Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0436Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0433Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0427Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0425Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0423Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0419Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0415Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0413Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0422Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0419Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0415Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0413Epoch 10/15: [================              ] 40/75 batches, loss: 0.0411Epoch 10/15: [================              ] 41/75 batches, loss: 0.0412Epoch 10/15: [================              ] 42/75 batches, loss: 0.0407Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0408Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0410Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0410Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0410Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0412Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0412Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0412Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0413Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0417Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0414Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0417Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0415Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0419Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0419Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0420Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0416Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0419Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0416Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0413Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0410Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0408Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0407Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0406Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0406Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0404Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0404Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0403Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0403Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0402Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0407Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0406Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0406Epoch 10/15: [==============================] 75/75 batches, loss: 0.0404
[2025-05-07 12:04:40,990][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0404
[2025-05-07 12:04:41,319][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.1119, Metrics: {'mse': 0.1119619607925415, 'rmse': 0.3346071738509823, 'r2': -0.7077643871307373}
[2025-05-07 12:04:41,319][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 12:04:41,319][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 12:04:41,320][src.training.lm_trainer][INFO] - Training completed in 29.11 seconds
[2025-05-07 12:04:41,320][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 12:04:44,277][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.021991433575749397, 'rmse': 0.14829508952001547, 'r2': -0.08825135231018066}
[2025-05-07 12:04:44,278][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.10245323926210403, 'rmse': 0.3200831755373969, 'r2': -0.5627270936965942}
[2025-05-07 12:04:44,278][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04661376774311066, 'rmse': 0.21590221801341147, 'r2': -0.18048369884490967}
[2025-05-07 12:04:45,978][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/fi/fi/model.pt
[2025-05-07 12:04:45,979][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▃▃▁
wandb:     best_val_mse █▄▃▃▁
wandb:      best_val_r2 ▁▅▆▆█
wandb:    best_val_rmse █▄▃▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▄▄▄▄▄▄▄
wandb:       train_loss █▄▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▅█▃▂▂▁▃▃▂▃
wandb:          val_mse ▅█▃▂▂▁▃▃▂▃
wandb:           val_r2 ▄▁▆▇▇█▆▆▇▆
wandb:         val_rmse ▅█▃▂▂▁▃▃▂▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.10247
wandb:     best_val_mse 0.10245
wandb:      best_val_r2 -0.56273
wandb:    best_val_rmse 0.32008
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.04661
wandb:    final_test_r2 -0.18048
wandb:  final_test_rmse 0.2159
wandb:  final_train_mse 0.02199
wandb:   final_train_r2 -0.08825
wandb: final_train_rmse 0.1483
wandb:    final_val_mse 0.10245
wandb:     final_val_r2 -0.56273
wandb:   final_val_rmse 0.32008
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04042
wandb:       train_time 29.11203
wandb:         val_loss 0.11192
wandb:          val_mse 0.11196
wandb:           val_r2 -0.70776
wandb:         val_rmse 0.33461
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_120352-kswmn8ot
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_120352-kswmn8ot/logs
Experiment probe_layer2_complexity_control3_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_question_type_control1_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_question_type_control1_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control1/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 12:05:19,812][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control1/layer2/ja
experiment_name: probe_layer2_question_type_control1_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-07 12:05:19,812][__main__][INFO] - Normalized task: question_type
[2025-05-07 12:05:19,812][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-07 12:05:19,812][__main__][INFO] - Determined Task Type: classification
[2025-05-07 12:05:19,816][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ja']
[2025-05-07 12:05:19,816][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 12:05:23,437][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 12:05:25,828][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 12:05:25,829][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:05:26,004][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-05-07 12:05:26,081][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-05-07 12:05:26,375][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 12:05:26,383][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:05:26,383][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 12:05:26,384][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:05:26,473][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:05:26,653][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:05:26,693][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 12:05:26,694][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:05:26,695][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 12:05:26,696][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:05:26,788][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:05:26,872][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:05:26,894][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 12:05:26,896][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:05:26,896][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 12:05:26,907][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 12:05:26,908][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 12:05:26,908][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 12:05:26,908][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 12:05:26,908][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 12:05:26,909][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-05-07 12:05:26,909][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-05-07 12:05:26,909][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 12:05:26,909][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 12:05:26,909][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 12:05:26,909][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 12:05:26,909][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 12:05:26,909][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 12:05:26,909][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-05-07 12:05:26,909][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-05-07 12:05:26,909][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 12:05:26,910][src.data.datasets][INFO] - Sample label: 0
[2025-05-07 12:05:26,910][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 12:05:26,910][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 12:05:26,910][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 12:05:26,910][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 12:05:26,910][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-05-07 12:05:26,910][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-05-07 12:05:26,910][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 12:05:26,910][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 12:05:26,910][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 12:05:26,910][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 12:05:26,911][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 12:05:26,911][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-07 12:05:26,911][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 12:05:34,523][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 12:05:34,524][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 12:05:34,524][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 12:05:34,524][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-07 12:05:34,530][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-07 12:05:34,530][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-07 12:05:34,530][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-07 12:05:34,530][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-07 12:05:34,531][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 12:05:34,531][__main__][INFO] - Total parameters: 394,568,839
[2025-05-07 12:05:34,531][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-07 12:05:34,532][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7072Epoch 1/15: [                              ] 2/75 batches, loss: 0.7142Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7126Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7022Epoch 1/15: [==                            ] 5/75 batches, loss: 0.6979Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6974Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6973Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6972Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6946Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6952Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6955Epoch 1/15: [====                          ] 12/75 batches, loss: 0.6949Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.6936Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.6931Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6927Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6923Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6912Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6915Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6911Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6911Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6895Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6909Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6905Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6907Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6917Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6917Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6911Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6922Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6918Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6908Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6918Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6927Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6935Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6942Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6949Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6952Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6950Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6949Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6948Epoch 1/15: [================              ] 40/75 batches, loss: 0.6947Epoch 1/15: [================              ] 41/75 batches, loss: 0.6948Epoch 1/15: [================              ] 42/75 batches, loss: 0.6947Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6946Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6945Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6946Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6946Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6946Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6945Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6945Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6945Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6944Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6944Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6943Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6943Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6943Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6943Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6943Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6943Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6942Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6942Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6942Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6941Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6941Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6941Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6941Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6941Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6941Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6941Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6941Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6941Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6940Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6940Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6941Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6940Epoch 1/15: [==============================] 75/75 batches, loss: 0.6940
[2025-05-07 12:05:41,403][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6940
[2025-05-07 12:05:41,645][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6929, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6928Epoch 2/15: [                              ] 2/75 batches, loss: 0.6933Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6932Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6931Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6931Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6929Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6930Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6929Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6930Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6931Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6931Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6931Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6930Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6930Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6931Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6930Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6930Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6931Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6932Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6931Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6931Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6931Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6930Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6930Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6930Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6930Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6930Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6930Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6930Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6930Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6930Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6930Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6929Epoch 2/15: [================              ] 40/75 batches, loss: 0.6929Epoch 2/15: [================              ] 41/75 batches, loss: 0.6929Epoch 2/15: [================              ] 42/75 batches, loss: 0.6929Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6929Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6929Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6929Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6929Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6929Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6929Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6929Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6929Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6929Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6929Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6929Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6930Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6929Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6929Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6929Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6929Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6930Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6930Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6930Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6930Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6930Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6930Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6930Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6930Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6930Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6930Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6930Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6930Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 2/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-07 12:05:44,328][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6931
[2025-05-07 12:05:44,578][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6928, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6928Epoch 3/15: [                              ] 2/75 batches, loss: 0.6928Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6936Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6933Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6934Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6933Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6933Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6932Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6933Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6933Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6933Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6933Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6934Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6934Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6934Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6933Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6933Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6933Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6932Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6932Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6932Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6932Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6932Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6932Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6932Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6932Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6932Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6932Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6932Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6932Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6932Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6932Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6932Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6932Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6932Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6932Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6933Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6932Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6932Epoch 3/15: [================              ] 40/75 batches, loss: 0.6932Epoch 3/15: [================              ] 41/75 batches, loss: 0.6932Epoch 3/15: [================              ] 42/75 batches, loss: 0.6932Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6932Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6932Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6932Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6932Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6932Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6932Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6930Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6930Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6930Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6930Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6930Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6930Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6930Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6930Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6930Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6930Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 3/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-07 12:05:47,332][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6931
[2025-05-07 12:05:47,641][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6928, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6937Epoch 4/15: [                              ] 2/75 batches, loss: 0.6938Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6933Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6933Epoch 4/15: [==                            ] 5/75 batches, loss: 0.6932Epoch 4/15: [==                            ] 6/75 batches, loss: 0.6932Epoch 4/15: [==                            ] 7/75 batches, loss: 0.6933Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6932Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6933Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6932Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6931Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6931Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6930Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6930Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6930Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6931Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6931Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6930Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6930Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6931Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6931Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6931Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6931Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6931Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6931Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 4/15: [==============                ] 35/75 batches, loss: 0.6930Epoch 4/15: [==============                ] 36/75 batches, loss: 0.6930Epoch 4/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 4/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 4/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 4/15: [================              ] 40/75 batches, loss: 0.6931Epoch 4/15: [================              ] 41/75 batches, loss: 0.6931Epoch 4/15: [================              ] 42/75 batches, loss: 0.6931Epoch 4/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 4/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 46/75 batches, loss: 0.6930Epoch 4/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 4/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 4/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.6930Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.6930Epoch 4/15: [======================        ] 55/75 batches, loss: 0.6930Epoch 4/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 4/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 4/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 4/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 4/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.6930Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 4/15: [============================  ] 70/75 batches, loss: 0.6930Epoch 4/15: [============================  ] 71/75 batches, loss: 0.6930Epoch 4/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 4/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 4/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 4/15: [==============================] 75/75 batches, loss: 0.6930
[2025-05-07 12:05:50,322][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6930
[2025-05-07 12:05:50,598][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6926, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.6930Epoch 5/15: [                              ] 2/75 batches, loss: 0.6919Epoch 5/15: [=                             ] 3/75 batches, loss: 0.6921Epoch 5/15: [=                             ] 4/75 batches, loss: 0.6923Epoch 5/15: [==                            ] 5/75 batches, loss: 0.6922Epoch 5/15: [==                            ] 6/75 batches, loss: 0.6922Epoch 5/15: [==                            ] 7/75 batches, loss: 0.6920Epoch 5/15: [===                           ] 8/75 batches, loss: 0.6920Epoch 5/15: [===                           ] 9/75 batches, loss: 0.6914Epoch 5/15: [====                          ] 10/75 batches, loss: 0.6913Epoch 5/15: [====                          ] 11/75 batches, loss: 0.6918Epoch 5/15: [====                          ] 12/75 batches, loss: 0.6920Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.6918Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.6917Epoch 5/15: [======                        ] 15/75 batches, loss: 0.6921Epoch 5/15: [======                        ] 16/75 batches, loss: 0.6921Epoch 5/15: [======                        ] 17/75 batches, loss: 0.6918Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.6921Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.6926Epoch 5/15: [========                      ] 20/75 batches, loss: 0.6928Epoch 5/15: [========                      ] 21/75 batches, loss: 0.6932Epoch 5/15: [========                      ] 22/75 batches, loss: 0.6933Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.6935Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.6936Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.6939Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.6938Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.6938Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.6939Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.6938Epoch 5/15: [============                  ] 30/75 batches, loss: 0.6937Epoch 5/15: [============                  ] 31/75 batches, loss: 0.6937Epoch 5/15: [============                  ] 32/75 batches, loss: 0.6937Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.6937Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.6936Epoch 5/15: [==============                ] 35/75 batches, loss: 0.6936Epoch 5/15: [==============                ] 36/75 batches, loss: 0.6936Epoch 5/15: [==============                ] 37/75 batches, loss: 0.6936Epoch 5/15: [===============               ] 38/75 batches, loss: 0.6936Epoch 5/15: [===============               ] 39/75 batches, loss: 0.6936Epoch 5/15: [================              ] 40/75 batches, loss: 0.6936Epoch 5/15: [================              ] 41/75 batches, loss: 0.6936Epoch 5/15: [================              ] 42/75 batches, loss: 0.6936Epoch 5/15: [=================             ] 43/75 batches, loss: 0.6936Epoch 5/15: [=================             ] 44/75 batches, loss: 0.6936Epoch 5/15: [==================            ] 45/75 batches, loss: 0.6936Epoch 5/15: [==================            ] 46/75 batches, loss: 0.6936Epoch 5/15: [==================            ] 47/75 batches, loss: 0.6936Epoch 5/15: [===================           ] 48/75 batches, loss: 0.6936Epoch 5/15: [===================           ] 49/75 batches, loss: 0.6935Epoch 5/15: [====================          ] 50/75 batches, loss: 0.6935Epoch 5/15: [====================          ] 51/75 batches, loss: 0.6935Epoch 5/15: [====================          ] 52/75 batches, loss: 0.6935Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.6935Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.6935Epoch 5/15: [======================        ] 55/75 batches, loss: 0.6935Epoch 5/15: [======================        ] 56/75 batches, loss: 0.6935Epoch 5/15: [======================        ] 57/75 batches, loss: 0.6935Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.6935Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.6934Epoch 5/15: [========================      ] 60/75 batches, loss: 0.6934Epoch 5/15: [========================      ] 61/75 batches, loss: 0.6934Epoch 5/15: [========================      ] 62/75 batches, loss: 0.6934Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.6934Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.6934Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.6934Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.6934Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.6934Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.6934Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.6934Epoch 5/15: [============================  ] 70/75 batches, loss: 0.6934Epoch 5/15: [============================  ] 71/75 batches, loss: 0.6934Epoch 5/15: [============================  ] 72/75 batches, loss: 0.6934Epoch 5/15: [============================= ] 73/75 batches, loss: 0.6934Epoch 5/15: [============================= ] 74/75 batches, loss: 0.6934Epoch 5/15: [==============================] 75/75 batches, loss: 0.6934
[2025-05-07 12:05:53,242][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6934
[2025-05-07 12:05:53,510][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:05:53,511][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.6929Epoch 6/15: [                              ] 2/75 batches, loss: 0.6933Epoch 6/15: [=                             ] 3/75 batches, loss: 0.6935Epoch 6/15: [=                             ] 4/75 batches, loss: 0.6935Epoch 6/15: [==                            ] 5/75 batches, loss: 0.6934Epoch 6/15: [==                            ] 6/75 batches, loss: 0.6934Epoch 6/15: [==                            ] 7/75 batches, loss: 0.6933Epoch 6/15: [===                           ] 8/75 batches, loss: 0.6933Epoch 6/15: [===                           ] 9/75 batches, loss: 0.6933Epoch 6/15: [====                          ] 10/75 batches, loss: 0.6932Epoch 6/15: [====                          ] 11/75 batches, loss: 0.6932Epoch 6/15: [====                          ] 12/75 batches, loss: 0.6932Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.6932Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.6932Epoch 6/15: [======                        ] 15/75 batches, loss: 0.6931Epoch 6/15: [======                        ] 16/75 batches, loss: 0.6931Epoch 6/15: [======                        ] 17/75 batches, loss: 0.6931Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.6931Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 6/15: [========                      ] 20/75 batches, loss: 0.6931Epoch 6/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 6/15: [========                      ] 22/75 batches, loss: 0.6931Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.6931Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.6931Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.6931Epoch 6/15: [============                  ] 30/75 batches, loss: 0.6931Epoch 6/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 6/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 6/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 6/15: [==============                ] 36/75 batches, loss: 0.6930Epoch 6/15: [==============                ] 37/75 batches, loss: 0.6930Epoch 6/15: [===============               ] 38/75 batches, loss: 0.6930Epoch 6/15: [===============               ] 39/75 batches, loss: 0.6930Epoch 6/15: [================              ] 40/75 batches, loss: 0.6930Epoch 6/15: [================              ] 41/75 batches, loss: 0.6930Epoch 6/15: [================              ] 42/75 batches, loss: 0.6930Epoch 6/15: [=================             ] 43/75 batches, loss: 0.6930Epoch 6/15: [=================             ] 44/75 batches, loss: 0.6930Epoch 6/15: [==================            ] 45/75 batches, loss: 0.6930Epoch 6/15: [==================            ] 46/75 batches, loss: 0.6930Epoch 6/15: [==================            ] 47/75 batches, loss: 0.6930Epoch 6/15: [===================           ] 48/75 batches, loss: 0.6930Epoch 6/15: [===================           ] 49/75 batches, loss: 0.6930Epoch 6/15: [====================          ] 50/75 batches, loss: 0.6930Epoch 6/15: [====================          ] 51/75 batches, loss: 0.6930Epoch 6/15: [====================          ] 52/75 batches, loss: 0.6930Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.6930Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.6930Epoch 6/15: [======================        ] 55/75 batches, loss: 0.6930Epoch 6/15: [======================        ] 56/75 batches, loss: 0.6929Epoch 6/15: [======================        ] 57/75 batches, loss: 0.6929Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.6930Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.6930Epoch 6/15: [========================      ] 60/75 batches, loss: 0.6930Epoch 6/15: [========================      ] 61/75 batches, loss: 0.6930Epoch 6/15: [========================      ] 62/75 batches, loss: 0.6930Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.6930Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.6930Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.6930Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.6930Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.6930Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.6930Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.6930Epoch 6/15: [============================  ] 70/75 batches, loss: 0.6930Epoch 6/15: [============================  ] 71/75 batches, loss: 0.6930Epoch 6/15: [============================  ] 72/75 batches, loss: 0.6930Epoch 6/15: [============================= ] 73/75 batches, loss: 0.6930Epoch 6/15: [============================= ] 74/75 batches, loss: 0.6930Epoch 6/15: [==============================] 75/75 batches, loss: 0.6930
[2025-05-07 12:05:55,838][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6930
[2025-05-07 12:05:56,128][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6927, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:05:56,129][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.6918Epoch 7/15: [                              ] 2/75 batches, loss: 0.6918Epoch 7/15: [=                             ] 3/75 batches, loss: 0.6930Epoch 7/15: [=                             ] 4/75 batches, loss: 0.6929Epoch 7/15: [==                            ] 5/75 batches, loss: 0.6927Epoch 7/15: [==                            ] 6/75 batches, loss: 0.6928Epoch 7/15: [==                            ] 7/75 batches, loss: 0.6929Epoch 7/15: [===                           ] 8/75 batches, loss: 0.6926Epoch 7/15: [===                           ] 9/75 batches, loss: 0.6926Epoch 7/15: [====                          ] 10/75 batches, loss: 0.6926Epoch 7/15: [====                          ] 11/75 batches, loss: 0.6927Epoch 7/15: [====                          ] 12/75 batches, loss: 0.6926Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.6928Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.6927Epoch 7/15: [======                        ] 15/75 batches, loss: 0.6927Epoch 7/15: [======                        ] 16/75 batches, loss: 0.6928Epoch 7/15: [======                        ] 17/75 batches, loss: 0.6925Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.6925Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.6923Epoch 7/15: [========                      ] 20/75 batches, loss: 0.6924Epoch 7/15: [========                      ] 21/75 batches, loss: 0.6924Epoch 7/15: [========                      ] 22/75 batches, loss: 0.6924Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.6924Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.6926Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.6927Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.6928Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.6928Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.6929Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.6929Epoch 7/15: [============                  ] 30/75 batches, loss: 0.6930Epoch 7/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 7/15: [============                  ] 32/75 batches, loss: 0.6930Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.6929Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.6928Epoch 7/15: [==============                ] 35/75 batches, loss: 0.6928Epoch 7/15: [==============                ] 36/75 batches, loss: 0.6928Epoch 7/15: [==============                ] 37/75 batches, loss: 0.6928Epoch 7/15: [===============               ] 38/75 batches, loss: 0.6928Epoch 7/15: [===============               ] 39/75 batches, loss: 0.6930Epoch 7/15: [================              ] 40/75 batches, loss: 0.6931Epoch 7/15: [================              ] 41/75 batches, loss: 0.6931Epoch 7/15: [================              ] 42/75 batches, loss: 0.6932Epoch 7/15: [=================             ] 43/75 batches, loss: 0.6932Epoch 7/15: [=================             ] 44/75 batches, loss: 0.6933Epoch 7/15: [==================            ] 45/75 batches, loss: 0.6933Epoch 7/15: [==================            ] 46/75 batches, loss: 0.6934Epoch 7/15: [==================            ] 47/75 batches, loss: 0.6934Epoch 7/15: [===================           ] 48/75 batches, loss: 0.6934Epoch 7/15: [===================           ] 49/75 batches, loss: 0.6933Epoch 7/15: [====================          ] 50/75 batches, loss: 0.6933Epoch 7/15: [====================          ] 51/75 batches, loss: 0.6933Epoch 7/15: [====================          ] 52/75 batches, loss: 0.6933Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.6933Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.6933Epoch 7/15: [======================        ] 55/75 batches, loss: 0.6933Epoch 7/15: [======================        ] 56/75 batches, loss: 0.6933Epoch 7/15: [======================        ] 57/75 batches, loss: 0.6933Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.6933Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.6933Epoch 7/15: [========================      ] 60/75 batches, loss: 0.6933Epoch 7/15: [========================      ] 61/75 batches, loss: 0.6933Epoch 7/15: [========================      ] 62/75 batches, loss: 0.6933Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.6933Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.6933Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.6933Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.6933Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.6932Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.6933Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.6933Epoch 7/15: [============================  ] 70/75 batches, loss: 0.6933Epoch 7/15: [============================  ] 71/75 batches, loss: 0.6932Epoch 7/15: [============================  ] 72/75 batches, loss: 0.6932Epoch 7/15: [============================= ] 73/75 batches, loss: 0.6932Epoch 7/15: [============================= ] 74/75 batches, loss: 0.6932Epoch 7/15: [==============================] 75/75 batches, loss: 0.6932
[2025-05-07 12:05:58,411][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.6932
[2025-05-07 12:05:58,640][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:05:58,641][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-07 12:05:58,641][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 7
[2025-05-07 12:05:58,641][src.training.lm_trainer][INFO] - Training completed in 20.39 seconds
[2025-05-07 12:05:58,641][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 12:06:01,664][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.4995801847187238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:06:01,664][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:06:01,664][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.40217391304347827, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:06:03,314][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control1/layer2/ja/ja/model.pt
[2025-05-07 12:06:03,317][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▁▁
wandb:           best_val_f1 ▁▁▁▁
wandb:         best_val_loss █▆▅▁
wandb:    best_val_precision ▁▁▁▁
wandb:       best_val_recall ▁▁▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁▁▁
wandb:            train_loss █▂▁▁▃▁▂
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁▁▁▁
wandb:                val_f1 ▁▁▁▁▁▁▁
wandb:              val_loss ▇▅▄▁█▃█
wandb:         val_precision ▁▁▁▁▁▁▁
wandb:            val_recall ▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.47826
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69255
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 7
wandb:                 epoch 7
wandb:   final_test_accuracy 0.40217
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.49958
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.47826
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69324
wandb:            train_time 20.39032
wandb:          val_accuracy 0.47826
wandb:                val_f1 0
wandb:              val_loss 0.693
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_120519-lak2hus1
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_120519-lak2hus1/logs
Experiment probe_layer2_question_type_control1_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control1/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_question_type_control2_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_question_type_control2_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control2/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 12:06:32,441][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control2/layer2/ja
experiment_name: probe_layer2_question_type_control2_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-07 12:06:32,441][__main__][INFO] - Normalized task: question_type
[2025-05-07 12:06:32,441][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-07 12:06:32,441][__main__][INFO] - Determined Task Type: classification
[2025-05-07 12:06:32,445][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ja']
[2025-05-07 12:06:32,445][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 12:06:36,357][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 12:06:38,820][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 12:06:38,821][src.data.datasets][INFO] - Loading 'control_question_type_seed2' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:06:38,960][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-05-07 12:06:39,026][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-05-07 12:06:39,245][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 12:06:39,253][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:06:39,254][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 12:06:39,256][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:06:39,372][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:06:39,471][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:06:39,509][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 12:06:39,510][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:06:39,510][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 12:06:39,511][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:06:39,616][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:06:39,680][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:06:39,726][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 12:06:39,727][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:06:39,728][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 12:06:39,728][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 12:06:39,729][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 12:06:39,729][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 12:06:39,730][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 12:06:39,730][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 12:06:39,730][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-05-07 12:06:39,730][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-05-07 12:06:39,730][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 12:06:39,730][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 12:06:39,730][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 12:06:39,730][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 12:06:39,730][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 12:06:39,730][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 12:06:39,731][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-05-07 12:06:39,731][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-05-07 12:06:39,731][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 12:06:39,731][src.data.datasets][INFO] - Sample label: 0
[2025-05-07 12:06:39,731][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 12:06:39,731][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 12:06:39,731][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 12:06:39,731][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 12:06:39,731][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-05-07 12:06:39,731][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-05-07 12:06:39,732][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 12:06:39,732][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 12:06:39,732][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 12:06:39,732][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 12:06:39,732][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 12:06:39,732][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-07 12:06:39,732][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 12:06:47,254][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 12:06:47,255][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 12:06:47,256][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 12:06:47,256][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-07 12:06:47,261][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-07 12:06:47,262][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-07 12:06:47,262][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-07 12:06:47,262][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-07 12:06:47,262][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 12:06:47,263][__main__][INFO] - Total parameters: 394,568,839
[2025-05-07 12:06:47,263][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-07 12:06:47,264][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7789Epoch 1/15: [                              ] 2/75 batches, loss: 0.7335Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7191Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7085Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7057Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7092Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7065Epoch 1/15: [===                           ] 8/75 batches, loss: 0.7060Epoch 1/15: [===                           ] 9/75 batches, loss: 0.7047Epoch 1/15: [====                          ] 10/75 batches, loss: 0.7026Epoch 1/15: [====                          ] 11/75 batches, loss: 0.7027Epoch 1/15: [====                          ] 12/75 batches, loss: 0.7025Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.7017Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.7009Epoch 1/15: [======                        ] 15/75 batches, loss: 0.7002Epoch 1/15: [======                        ] 16/75 batches, loss: 0.7001Epoch 1/15: [======                        ] 17/75 batches, loss: 0.7000Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6994Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6993Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6990Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6989Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6987Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6983Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6980Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6978Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6977Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6975Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6974Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6973Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6972Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6971Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6970Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6969Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6968Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6967Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6967Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6966Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6965Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6964Epoch 1/15: [================              ] 40/75 batches, loss: 0.6963Epoch 1/15: [================              ] 41/75 batches, loss: 0.6962Epoch 1/15: [================              ] 42/75 batches, loss: 0.6961Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6960Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6959Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6959Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6958Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6958Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6957Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6956Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6956Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6955Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6955Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6954Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6954Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6953Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6953Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6953Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6952Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6952Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6952Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6952Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6951Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6951Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6951Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6950Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6950Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6950Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6950Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6949Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6949Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6948Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6948Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6948Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6948Epoch 1/15: [==============================] 75/75 batches, loss: 0.6948
[2025-05-07 12:06:54,503][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6948
[2025-05-07 12:06:54,723][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6938Epoch 2/15: [                              ] 2/75 batches, loss: 0.6935Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6932Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6930Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6929Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6927Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6928Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6928Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6929Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6929Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6931Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6931Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6931Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6930Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6931Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6931Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6931Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6931Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6931Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6930Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6930Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6930Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6930Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6930Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6930Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6930Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6930Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6930Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6930Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6930Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6930Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 2/15: [================              ] 40/75 batches, loss: 0.6931Epoch 2/15: [================              ] 41/75 batches, loss: 0.6931Epoch 2/15: [================              ] 42/75 batches, loss: 0.6931Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6932Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6932Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6932Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6932Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6932Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6932Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6932Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6932Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6932Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6932Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6932Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6932Epoch 2/15: [==============================] 75/75 batches, loss: 0.6932
[2025-05-07 12:06:57,570][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6932
[2025-05-07 12:06:57,833][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:06:57,834][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6945Epoch 3/15: [                              ] 2/75 batches, loss: 0.6937Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6937Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6936Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6935Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6936Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6935Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6935Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6934Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6933Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6932Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6931Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6931Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6931Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6931Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6931Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6931Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6930Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6930Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6931Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6931Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6931Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6931Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6931Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6931Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 3/15: [================              ] 40/75 batches, loss: 0.6931Epoch 3/15: [================              ] 41/75 batches, loss: 0.6931Epoch 3/15: [================              ] 42/75 batches, loss: 0.6931Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 3/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-07 12:07:00,161][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6931
[2025-05-07 12:07:00,393][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:07:00,394][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6937Epoch 4/15: [                              ] 2/75 batches, loss: 0.6931Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6930Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6928Epoch 4/15: [==                            ] 5/75 batches, loss: 0.6930Epoch 4/15: [==                            ] 6/75 batches, loss: 0.6931Epoch 4/15: [==                            ] 7/75 batches, loss: 0.6931Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6929Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6929Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6928Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6929Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6928Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6929Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6929Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6929Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6929Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6930Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6929Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6929Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6928Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6929Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6928Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6928Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6928Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6929Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6928Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6928Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6928Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6928Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6929Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6932Epoch 4/15: [==============                ] 35/75 batches, loss: 0.6932Epoch 4/15: [==============                ] 36/75 batches, loss: 0.6933Epoch 4/15: [==============                ] 37/75 batches, loss: 0.6933Epoch 4/15: [===============               ] 38/75 batches, loss: 0.6933Epoch 4/15: [===============               ] 39/75 batches, loss: 0.6933Epoch 4/15: [================              ] 40/75 batches, loss: 0.6933Epoch 4/15: [================              ] 41/75 batches, loss: 0.6933Epoch 4/15: [================              ] 42/75 batches, loss: 0.6933Epoch 4/15: [=================             ] 43/75 batches, loss: 0.6933Epoch 4/15: [=================             ] 44/75 batches, loss: 0.6933Epoch 4/15: [==================            ] 45/75 batches, loss: 0.6933Epoch 4/15: [==================            ] 46/75 batches, loss: 0.6933Epoch 4/15: [==================            ] 47/75 batches, loss: 0.6933Epoch 4/15: [===================           ] 48/75 batches, loss: 0.6933Epoch 4/15: [===================           ] 49/75 batches, loss: 0.6933Epoch 4/15: [====================          ] 50/75 batches, loss: 0.6933Epoch 4/15: [====================          ] 51/75 batches, loss: 0.6933Epoch 4/15: [====================          ] 52/75 batches, loss: 0.6933Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.6933Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.6933Epoch 4/15: [======================        ] 55/75 batches, loss: 0.6933Epoch 4/15: [======================        ] 56/75 batches, loss: 0.6933Epoch 4/15: [======================        ] 57/75 batches, loss: 0.6933Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.6933Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.6933Epoch 4/15: [========================      ] 60/75 batches, loss: 0.6932Epoch 4/15: [========================      ] 61/75 batches, loss: 0.6932Epoch 4/15: [========================      ] 62/75 batches, loss: 0.6932Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.6933Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.6932Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.6932Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.6932Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.6932Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.6932Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.6932Epoch 4/15: [============================  ] 70/75 batches, loss: 0.6932Epoch 4/15: [============================  ] 71/75 batches, loss: 0.6932Epoch 4/15: [============================  ] 72/75 batches, loss: 0.6932Epoch 4/15: [============================= ] 73/75 batches, loss: 0.6932Epoch 4/15: [============================= ] 74/75 batches, loss: 0.6932Epoch 4/15: [==============================] 75/75 batches, loss: 0.6932
[2025-05-07 12:07:02,677][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6932
[2025-05-07 12:07:02,879][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:07:02,880][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-07 12:07:02,880][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 4
[2025-05-07 12:07:02,880][src.training.lm_trainer][INFO] - Training completed in 11.55 seconds
[2025-05-07 12:07:02,880][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 12:07:05,973][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.4995801847187238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:07:05,973][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:07:05,974][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.40217391304347827, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:07:07,778][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control2/layer2/ja/ja/model.pt
[2025-05-07 12:07:07,779][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁
wandb:           best_val_f1 ▁
wandb:         best_val_loss ▁
wandb:    best_val_precision ▁
wandb:       best_val_recall ▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▃▃▆▆██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁
wandb:            train_loss █▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁
wandb:                val_f1 ▁▁▁▁
wandb:              val_loss ▁▃▅█
wandb:         val_precision ▁▁▁▁
wandb:            val_recall ▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.47826
wandb:           best_val_f1 0
wandb:         best_val_loss 0.6931
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 4
wandb:                 epoch 4
wandb:   final_test_accuracy 0.40217
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.49958
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.47826
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69321
wandb:            train_time 11.54761
wandb:          val_accuracy 0.47826
wandb:                val_f1 0
wandb:              val_loss 0.69314
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_120632-mziuilcx
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_120632-mziuilcx/logs
Experiment probe_layer2_question_type_control2_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control2/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_question_type_control3_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_question_type_control3_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control3/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 12:07:40,840][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control3/layer2/ja
experiment_name: probe_layer2_question_type_control3_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-07 12:07:40,840][__main__][INFO] - Normalized task: question_type
[2025-05-07 12:07:40,840][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-07 12:07:40,840][__main__][INFO] - Determined Task Type: classification
[2025-05-07 12:07:40,844][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ja']
[2025-05-07 12:07:40,844][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 12:07:44,599][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 12:07:46,933][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 12:07:46,934][src.data.datasets][INFO] - Loading 'control_question_type_seed3' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:07:47,197][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:49:56 2025).
[2025-05-07 12:07:47,376][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:49:56 2025).
[2025-05-07 12:07:47,610][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 12:07:47,618][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:07:47,619][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 12:07:47,620][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:07:47,681][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:07:47,772][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:07:47,849][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 12:07:47,850][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:07:47,850][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 12:07:47,851][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:07:47,913][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:07:47,974][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:07:48,018][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 12:07:48,020][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:07:48,020][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 12:07:48,043][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 12:07:48,043][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 12:07:48,043][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 12:07:48,043][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 12:07:48,044][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 12:07:48,044][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-05-07 12:07:48,044][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-05-07 12:07:48,044][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 12:07:48,044][src.data.datasets][INFO] - Sample label: 0
[2025-05-07 12:07:48,044][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 12:07:48,044][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 12:07:48,044][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 12:07:48,044][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 12:07:48,045][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-05-07 12:07:48,045][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-05-07 12:07:48,045][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 12:07:48,045][src.data.datasets][INFO] - Sample label: 0
[2025-05-07 12:07:48,045][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 12:07:48,045][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 12:07:48,045][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 12:07:48,045][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 12:07:48,045][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-05-07 12:07:48,045][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-05-07 12:07:48,045][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 12:07:48,046][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 12:07:48,046][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 12:07:48,046][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 12:07:48,046][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 12:07:48,046][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-07 12:07:48,046][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 12:07:56,236][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 12:07:56,237][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 12:07:56,237][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 12:07:56,237][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-07 12:07:56,243][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-07 12:07:56,243][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-07 12:07:56,244][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-07 12:07:56,244][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-07 12:07:56,244][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 12:07:56,245][__main__][INFO] - Total parameters: 394,568,839
[2025-05-07 12:07:56,245][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-07 12:07:56,245][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6533Epoch 1/15: [                              ] 2/75 batches, loss: 0.7378Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7303Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7083Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7085Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7062Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7086Epoch 1/15: [===                           ] 8/75 batches, loss: 0.7082Epoch 1/15: [===                           ] 9/75 batches, loss: 0.7061Epoch 1/15: [====                          ] 10/75 batches, loss: 0.7051Epoch 1/15: [====                          ] 11/75 batches, loss: 0.7053Epoch 1/15: [====                          ] 12/75 batches, loss: 0.7047Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.7041Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.7034Epoch 1/15: [======                        ] 15/75 batches, loss: 0.7025Epoch 1/15: [======                        ] 16/75 batches, loss: 0.7018Epoch 1/15: [======                        ] 17/75 batches, loss: 0.7012Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.7009Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.7004Epoch 1/15: [========                      ] 20/75 batches, loss: 0.7001Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6997Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6993Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6991Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6988Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6986Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6984Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6982Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6979Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6978Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6976Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6974Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6973Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6972Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6971Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6970Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6970Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6969Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6969Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6968Epoch 1/15: [================              ] 40/75 batches, loss: 0.6968Epoch 1/15: [================              ] 41/75 batches, loss: 0.6967Epoch 1/15: [================              ] 42/75 batches, loss: 0.6966Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6966Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6965Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6963Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6962Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6962Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6961Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6960Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6960Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6959Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6959Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6958Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6957Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6957Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6957Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6956Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6956Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6955Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6956Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6955Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6955Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6955Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6954Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6954Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6954Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6953Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6952Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6952Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6952Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6951Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6951Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6950Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6950Epoch 1/15: [==============================] 75/75 batches, loss: 0.6950
[2025-05-07 12:08:02,697][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6950
[2025-05-07 12:08:02,983][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6947Epoch 2/15: [                              ] 2/75 batches, loss: 0.6941Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6935Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6934Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6935Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6933Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6933Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6932Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6929Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6929Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6929Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6929Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6930Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6929Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6930Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6930Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6930Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6930Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6929Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6928Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6927Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6927Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6927Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6927Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6927Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6926Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6925Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6926Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6927Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6927Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6928Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6928Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6928Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6928Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6928Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6929Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6929Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6929Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6928Epoch 2/15: [================              ] 40/75 batches, loss: 0.6929Epoch 2/15: [================              ] 41/75 batches, loss: 0.6929Epoch 2/15: [================              ] 42/75 batches, loss: 0.6928Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6928Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6928Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6929Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6928Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6927Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6927Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6927Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6927Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6926Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6926Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6927Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6927Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6928Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6928Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6929Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6930Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6929Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6928Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6929Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6929Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6932Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6933Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6933Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6933Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6932Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6932Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6932Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6932Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6932Epoch 2/15: [==============================] 75/75 batches, loss: 0.6932
[2025-05-07 12:08:05,641][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6932
[2025-05-07 12:08:05,918][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6926Epoch 3/15: [                              ] 2/75 batches, loss: 0.6927Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6929Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6929Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6927Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6927Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6929Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6928Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6925Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6928Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6926Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6927Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6927Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6927Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6928Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6928Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6928Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6929Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6930Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6930Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6931Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6931Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6930Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6930Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6930Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6929Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6930Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6930Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6930Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6930Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6930Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6930Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 3/15: [================              ] 40/75 batches, loss: 0.6931Epoch 3/15: [================              ] 41/75 batches, loss: 0.6930Epoch 3/15: [================              ] 42/75 batches, loss: 0.6930Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6930Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6932Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6932Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6932Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6932Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6932Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 3/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-07 12:08:08,910][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6931
[2025-05-07 12:08:09,140][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:08:09,140][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6917Epoch 4/15: [                              ] 2/75 batches, loss: 0.6917Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6911Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6914Epoch 4/15: [==                            ] 5/75 batches, loss: 0.6918Epoch 4/15: [==                            ] 6/75 batches, loss: 0.6914Epoch 4/15: [==                            ] 7/75 batches, loss: 0.6912Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6907Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6911Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6909Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6916Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6915Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6914Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6911Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6902Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6904Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6896Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6917Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6920Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6924Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6928Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6929Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6934Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6938Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6937Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6936Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6938Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6936Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6936Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6936Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6935Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6935Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6936Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6935Epoch 4/15: [==============                ] 35/75 batches, loss: 0.6935Epoch 4/15: [==============                ] 36/75 batches, loss: 0.6935Epoch 4/15: [==============                ] 37/75 batches, loss: 0.6935Epoch 4/15: [===============               ] 38/75 batches, loss: 0.6935Epoch 4/15: [===============               ] 39/75 batches, loss: 0.6935Epoch 4/15: [================              ] 40/75 batches, loss: 0.6935Epoch 4/15: [================              ] 41/75 batches, loss: 0.6935Epoch 4/15: [================              ] 42/75 batches, loss: 0.6935Epoch 4/15: [=================             ] 43/75 batches, loss: 0.6935Epoch 4/15: [=================             ] 44/75 batches, loss: 0.6935Epoch 4/15: [==================            ] 45/75 batches, loss: 0.6934Epoch 4/15: [==================            ] 46/75 batches, loss: 0.6934Epoch 4/15: [==================            ] 47/75 batches, loss: 0.6934Epoch 4/15: [===================           ] 48/75 batches, loss: 0.6934Epoch 4/15: [===================           ] 49/75 batches, loss: 0.6934Epoch 4/15: [====================          ] 50/75 batches, loss: 0.6934Epoch 4/15: [====================          ] 51/75 batches, loss: 0.6934Epoch 4/15: [====================          ] 52/75 batches, loss: 0.6934Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.6934Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.6934Epoch 4/15: [======================        ] 55/75 batches, loss: 0.6933Epoch 4/15: [======================        ] 56/75 batches, loss: 0.6933Epoch 4/15: [======================        ] 57/75 batches, loss: 0.6933Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.6933Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.6933Epoch 4/15: [========================      ] 60/75 batches, loss: 0.6933Epoch 4/15: [========================      ] 61/75 batches, loss: 0.6933Epoch 4/15: [========================      ] 62/75 batches, loss: 0.6933Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.6933Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.6933Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.6933Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.6933Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.6933Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.6933Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.6933Epoch 4/15: [============================  ] 70/75 batches, loss: 0.6933Epoch 4/15: [============================  ] 71/75 batches, loss: 0.6933Epoch 4/15: [============================  ] 72/75 batches, loss: 0.6933Epoch 4/15: [============================= ] 73/75 batches, loss: 0.6933Epoch 4/15: [============================= ] 74/75 batches, loss: 0.6933Epoch 4/15: [==============================] 75/75 batches, loss: 0.6933
[2025-05-07 12:08:11,435][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6933
[2025-05-07 12:08:11,668][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:08:11,669][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.6930Epoch 5/15: [                              ] 2/75 batches, loss: 0.6930Epoch 5/15: [=                             ] 3/75 batches, loss: 0.6927Epoch 5/15: [=                             ] 4/75 batches, loss: 0.6927Epoch 5/15: [==                            ] 5/75 batches, loss: 0.6929Epoch 5/15: [==                            ] 6/75 batches, loss: 0.6929Epoch 5/15: [==                            ] 7/75 batches, loss: 0.6930Epoch 5/15: [===                           ] 8/75 batches, loss: 0.6929Epoch 5/15: [===                           ] 9/75 batches, loss: 0.6928Epoch 5/15: [====                          ] 10/75 batches, loss: 0.6928Epoch 5/15: [====                          ] 11/75 batches, loss: 0.6928Epoch 5/15: [====                          ] 12/75 batches, loss: 0.6928Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.6928Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.6929Epoch 5/15: [======                        ] 15/75 batches, loss: 0.6929Epoch 5/15: [======                        ] 16/75 batches, loss: 0.6930Epoch 5/15: [======                        ] 17/75 batches, loss: 0.6929Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.6929Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.6930Epoch 5/15: [========                      ] 20/75 batches, loss: 0.6930Epoch 5/15: [========                      ] 21/75 batches, loss: 0.6930Epoch 5/15: [========                      ] 22/75 batches, loss: 0.6930Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.6931Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.6930Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.6930Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.6931Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.6931Epoch 5/15: [============                  ] 30/75 batches, loss: 0.6931Epoch 5/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 5/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 5/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 5/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 5/15: [==============                ] 37/75 batches, loss: 0.6930Epoch 5/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 5/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 5/15: [================              ] 40/75 batches, loss: 0.6931Epoch 5/15: [================              ] 41/75 batches, loss: 0.6931Epoch 5/15: [================              ] 42/75 batches, loss: 0.6931Epoch 5/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 5/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 5/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 5/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 5/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 5/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 5/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 5/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 5/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 5/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 5/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 5/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 5/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 5/15: [========================      ] 60/75 batches, loss: 0.6930Epoch 5/15: [========================      ] 61/75 batches, loss: 0.6930Epoch 5/15: [========================      ] 62/75 batches, loss: 0.6930Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.6930Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.6930Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.6930Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.6930Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.6930Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 5/15: [============================  ] 70/75 batches, loss: 0.6930Epoch 5/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 5/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 5/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 5/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 5/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-07 12:08:14,010][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6931
[2025-05-07 12:08:14,272][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.6938Epoch 6/15: [                              ] 2/75 batches, loss: 0.6938Epoch 6/15: [=                             ] 3/75 batches, loss: 0.6935Epoch 6/15: [=                             ] 4/75 batches, loss: 0.6936Epoch 6/15: [==                            ] 5/75 batches, loss: 0.6936Epoch 6/15: [==                            ] 6/75 batches, loss: 0.6934Epoch 6/15: [==                            ] 7/75 batches, loss: 0.6934Epoch 6/15: [===                           ] 8/75 batches, loss: 0.6933Epoch 6/15: [===                           ] 9/75 batches, loss: 0.6933Epoch 6/15: [====                          ] 10/75 batches, loss: 0.6932Epoch 6/15: [====                          ] 11/75 batches, loss: 0.6932Epoch 6/15: [====                          ] 12/75 batches, loss: 0.6932Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.6931Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.6931Epoch 6/15: [======                        ] 15/75 batches, loss: 0.6929Epoch 6/15: [======                        ] 16/75 batches, loss: 0.6930Epoch 6/15: [======                        ] 17/75 batches, loss: 0.6930Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.6930Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.6930Epoch 6/15: [========                      ] 20/75 batches, loss: 0.6930Epoch 6/15: [========                      ] 21/75 batches, loss: 0.6930Epoch 6/15: [========                      ] 22/75 batches, loss: 0.6931Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.6931Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.6932Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.6931Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.6930Epoch 6/15: [============                  ] 30/75 batches, loss: 0.6930Epoch 6/15: [============                  ] 31/75 batches, loss: 0.6930Epoch 6/15: [============                  ] 32/75 batches, loss: 0.6930Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.6930Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.6930Epoch 6/15: [==============                ] 35/75 batches, loss: 0.6930Epoch 6/15: [==============                ] 36/75 batches, loss: 0.6930Epoch 6/15: [==============                ] 37/75 batches, loss: 0.6930Epoch 6/15: [===============               ] 38/75 batches, loss: 0.6930Epoch 6/15: [===============               ] 39/75 batches, loss: 0.6930Epoch 6/15: [================              ] 40/75 batches, loss: 0.6930Epoch 6/15: [================              ] 41/75 batches, loss: 0.6930Epoch 6/15: [================              ] 42/75 batches, loss: 0.6930Epoch 6/15: [=================             ] 43/75 batches, loss: 0.6930Epoch 6/15: [=================             ] 44/75 batches, loss: 0.6929Epoch 6/15: [==================            ] 45/75 batches, loss: 0.6929Epoch 6/15: [==================            ] 46/75 batches, loss: 0.6929Epoch 6/15: [==================            ] 47/75 batches, loss: 0.6929Epoch 6/15: [===================           ] 48/75 batches, loss: 0.6929Epoch 6/15: [===================           ] 49/75 batches, loss: 0.6930Epoch 6/15: [====================          ] 50/75 batches, loss: 0.6930Epoch 6/15: [====================          ] 51/75 batches, loss: 0.6930Epoch 6/15: [====================          ] 52/75 batches, loss: 0.6930Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.6930Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.6930Epoch 6/15: [======================        ] 55/75 batches, loss: 0.6930Epoch 6/15: [======================        ] 56/75 batches, loss: 0.6930Epoch 6/15: [======================        ] 57/75 batches, loss: 0.6930Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.6930Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.6930Epoch 6/15: [========================      ] 60/75 batches, loss: 0.6930Epoch 6/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 6/15: [========================      ] 62/75 batches, loss: 0.6930Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.6930Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.6930Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.6930Epoch 6/15: [============================  ] 70/75 batches, loss: 0.6930Epoch 6/15: [============================  ] 71/75 batches, loss: 0.6930Epoch 6/15: [============================  ] 72/75 batches, loss: 0.6930Epoch 6/15: [============================= ] 73/75 batches, loss: 0.6930Epoch 6/15: [============================= ] 74/75 batches, loss: 0.6930Epoch 6/15: [==============================] 75/75 batches, loss: 0.6930
[2025-05-07 12:08:16,931][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6930
[2025-05-07 12:08:17,194][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6929, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.6943Epoch 7/15: [                              ] 2/75 batches, loss: 0.6942Epoch 7/15: [=                             ] 3/75 batches, loss: 0.6933Epoch 7/15: [=                             ] 4/75 batches, loss: 0.6920Epoch 7/15: [==                            ] 5/75 batches, loss: 0.6924Epoch 7/15: [==                            ] 6/75 batches, loss: 0.6930Epoch 7/15: [==                            ] 7/75 batches, loss: 0.6926Epoch 7/15: [===                           ] 8/75 batches, loss: 0.6925Epoch 7/15: [===                           ] 9/75 batches, loss: 0.6925Epoch 7/15: [====                          ] 10/75 batches, loss: 0.6924Epoch 7/15: [====                          ] 11/75 batches, loss: 0.6922Epoch 7/15: [====                          ] 12/75 batches, loss: 0.6920Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.6922Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.6921Epoch 7/15: [======                        ] 15/75 batches, loss: 0.6923Epoch 7/15: [======                        ] 16/75 batches, loss: 0.6916Epoch 7/15: [======                        ] 17/75 batches, loss: 0.6916Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.6919Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.6922Epoch 7/15: [========                      ] 20/75 batches, loss: 0.6920Epoch 7/15: [========                      ] 21/75 batches, loss: 0.6917Epoch 7/15: [========                      ] 22/75 batches, loss: 0.6914Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.6914Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.6918Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.6910Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.6913Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.6917Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.6916Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.6913Epoch 7/15: [============                  ] 30/75 batches, loss: 0.6916Epoch 7/15: [============                  ] 31/75 batches, loss: 0.6913Epoch 7/15: [============                  ] 32/75 batches, loss: 0.6922Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.6924Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.6932Epoch 7/15: [==============                ] 35/75 batches, loss: 0.6927Epoch 7/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 7/15: [==============                ] 37/75 batches, loss: 0.6927Epoch 7/15: [===============               ] 38/75 batches, loss: 0.6937Epoch 7/15: [===============               ] 39/75 batches, loss: 0.6930Epoch 7/15: [================              ] 40/75 batches, loss: 0.6934Epoch 7/15: [================              ] 41/75 batches, loss: 0.6923Epoch 7/15: [================              ] 42/75 batches, loss: 0.6928Epoch 7/15: [=================             ] 43/75 batches, loss: 0.6926Epoch 7/15: [=================             ] 44/75 batches, loss: 0.6923Epoch 7/15: [==================            ] 45/75 batches, loss: 0.6932Epoch 7/15: [==================            ] 46/75 batches, loss: 0.6928Epoch 7/15: [==================            ] 47/75 batches, loss: 0.6928Epoch 7/15: [===================           ] 48/75 batches, loss: 0.6923Epoch 7/15: [===================           ] 49/75 batches, loss: 0.6923Epoch 7/15: [====================          ] 50/75 batches, loss: 0.6930Epoch 7/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 7/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.6932Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.6934Epoch 7/15: [======================        ] 55/75 batches, loss: 0.6932Epoch 7/15: [======================        ] 56/75 batches, loss: 0.6933Epoch 7/15: [======================        ] 57/75 batches, loss: 0.6932Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.6933Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.6934Epoch 7/15: [========================      ] 60/75 batches, loss: 0.6935Epoch 7/15: [========================      ] 61/75 batches, loss: 0.6933Epoch 7/15: [========================      ] 62/75 batches, loss: 0.6933Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.6933Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.6933Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.6934Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.6933Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.6933Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.6933Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.6933Epoch 7/15: [============================  ] 70/75 batches, loss: 0.6933Epoch 7/15: [============================  ] 71/75 batches, loss: 0.6933Epoch 7/15: [============================  ] 72/75 batches, loss: 0.6933Epoch 7/15: [============================= ] 73/75 batches, loss: 0.6933Epoch 7/15: [============================= ] 74/75 batches, loss: 0.6933Epoch 7/15: [==============================] 75/75 batches, loss: 0.6933
[2025-05-07 12:08:20,001][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.6933
[2025-05-07 12:08:20,284][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.6927, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.6917Epoch 8/15: [                              ] 2/75 batches, loss: 0.6921Epoch 8/15: [=                             ] 3/75 batches, loss: 0.6923Epoch 8/15: [=                             ] 4/75 batches, loss: 0.6928Epoch 8/15: [==                            ] 5/75 batches, loss: 0.6927Epoch 8/15: [==                            ] 6/75 batches, loss: 0.6922Epoch 8/15: [==                            ] 7/75 batches, loss: 0.6919Epoch 8/15: [===                           ] 8/75 batches, loss: 0.6918Epoch 8/15: [===                           ] 9/75 batches, loss: 0.6915Epoch 8/15: [====                          ] 10/75 batches, loss: 0.6915Epoch 8/15: [====                          ] 11/75 batches, loss: 0.6914Epoch 8/15: [====                          ] 12/75 batches, loss: 0.6917Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.6910Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.6911Epoch 8/15: [======                        ] 15/75 batches, loss: 0.6905Epoch 8/15: [======                        ] 16/75 batches, loss: 0.6902Epoch 8/15: [======                        ] 17/75 batches, loss: 0.6902Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.6899Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.6888Epoch 8/15: [========                      ] 20/75 batches, loss: 0.6896Epoch 8/15: [========                      ] 21/75 batches, loss: 0.6894Epoch 8/15: [========                      ] 22/75 batches, loss: 0.6895Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.6877Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.6900Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.6915Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.6905Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.6915Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.6917Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.6915Epoch 8/15: [============                  ] 30/75 batches, loss: 0.6916Epoch 8/15: [============                  ] 31/75 batches, loss: 0.6920Epoch 8/15: [============                  ] 32/75 batches, loss: 0.6917Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.6916Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.6918Epoch 8/15: [==============                ] 35/75 batches, loss: 0.6918Epoch 8/15: [==============                ] 36/75 batches, loss: 0.6919Epoch 8/15: [==============                ] 37/75 batches, loss: 0.6919Epoch 8/15: [===============               ] 38/75 batches, loss: 0.6919Epoch 8/15: [===============               ] 39/75 batches, loss: 0.6920Epoch 8/15: [================              ] 40/75 batches, loss: 0.6920Epoch 8/15: [================              ] 41/75 batches, loss: 0.6920Epoch 8/15: [================              ] 42/75 batches, loss: 0.6920Epoch 8/15: [=================             ] 43/75 batches, loss: 0.6920Epoch 8/15: [=================             ] 44/75 batches, loss: 0.6921Epoch 8/15: [==================            ] 45/75 batches, loss: 0.6921Epoch 8/15: [==================            ] 46/75 batches, loss: 0.6921Epoch 8/15: [==================            ] 47/75 batches, loss: 0.6921Epoch 8/15: [===================           ] 48/75 batches, loss: 0.6921Epoch 8/15: [===================           ] 49/75 batches, loss: 0.6921Epoch 8/15: [====================          ] 50/75 batches, loss: 0.6921Epoch 8/15: [====================          ] 51/75 batches, loss: 0.6922Epoch 8/15: [====================          ] 52/75 batches, loss: 0.6922Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.6922Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.6922Epoch 8/15: [======================        ] 55/75 batches, loss: 0.6922Epoch 8/15: [======================        ] 56/75 batches, loss: 0.6922Epoch 8/15: [======================        ] 57/75 batches, loss: 0.6922Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.6922Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.6923Epoch 8/15: [========================      ] 60/75 batches, loss: 0.6923Epoch 8/15: [========================      ] 61/75 batches, loss: 0.6923Epoch 8/15: [========================      ] 62/75 batches, loss: 0.6923Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.6923Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.6923Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.6923Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.6924Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.6924Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.6924Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.6924Epoch 8/15: [============================  ] 70/75 batches, loss: 0.6924Epoch 8/15: [============================  ] 71/75 batches, loss: 0.6924Epoch 8/15: [============================  ] 72/75 batches, loss: 0.6924Epoch 8/15: [============================= ] 73/75 batches, loss: 0.6924Epoch 8/15: [============================= ] 74/75 batches, loss: 0.6924Epoch 8/15: [==============================] 75/75 batches, loss: 0.6924
[2025-05-07 12:08:22,960][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.6924
[2025-05-07 12:08:23,190][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:08:23,190][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.6926Epoch 9/15: [                              ] 2/75 batches, loss: 0.6928Epoch 9/15: [=                             ] 3/75 batches, loss: 0.6928Epoch 9/15: [=                             ] 4/75 batches, loss: 0.6930Epoch 9/15: [==                            ] 5/75 batches, loss: 0.6930Epoch 9/15: [==                            ] 6/75 batches, loss: 0.6930Epoch 9/15: [==                            ] 7/75 batches, loss: 0.6929Epoch 9/15: [===                           ] 8/75 batches, loss: 0.6929Epoch 9/15: [===                           ] 9/75 batches, loss: 0.6928Epoch 9/15: [====                          ] 10/75 batches, loss: 0.6928Epoch 9/15: [====                          ] 11/75 batches, loss: 0.6929Epoch 9/15: [====                          ] 12/75 batches, loss: 0.6929Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.6929Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.6930Epoch 9/15: [======                        ] 15/75 batches, loss: 0.6930Epoch 9/15: [======                        ] 16/75 batches, loss: 0.6929Epoch 9/15: [======                        ] 17/75 batches, loss: 0.6929Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.6929Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.6930Epoch 9/15: [========                      ] 20/75 batches, loss: 0.6930Epoch 9/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 9/15: [========                      ] 22/75 batches, loss: 0.6930Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.6930Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.6930Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.6930Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.6930Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.6930Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.6930Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.6930Epoch 9/15: [============                  ] 30/75 batches, loss: 0.6930Epoch 9/15: [============                  ] 31/75 batches, loss: 0.6930Epoch 9/15: [============                  ] 32/75 batches, loss: 0.6930Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.6930Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.6930Epoch 9/15: [==============                ] 35/75 batches, loss: 0.6929Epoch 9/15: [==============                ] 36/75 batches, loss: 0.6930Epoch 9/15: [==============                ] 37/75 batches, loss: 0.6930Epoch 9/15: [===============               ] 38/75 batches, loss: 0.6930Epoch 9/15: [===============               ] 39/75 batches, loss: 0.6930Epoch 9/15: [================              ] 40/75 batches, loss: 0.6930Epoch 9/15: [================              ] 41/75 batches, loss: 0.6930Epoch 9/15: [================              ] 42/75 batches, loss: 0.6930Epoch 9/15: [=================             ] 43/75 batches, loss: 0.6930Epoch 9/15: [=================             ] 44/75 batches, loss: 0.6930Epoch 9/15: [==================            ] 45/75 batches, loss: 0.6930Epoch 9/15: [==================            ] 46/75 batches, loss: 0.6930Epoch 9/15: [==================            ] 47/75 batches, loss: 0.6930Epoch 9/15: [===================           ] 48/75 batches, loss: 0.6930Epoch 9/15: [===================           ] 49/75 batches, loss: 0.6930Epoch 9/15: [====================          ] 50/75 batches, loss: 0.6930Epoch 9/15: [====================          ] 51/75 batches, loss: 0.6930Epoch 9/15: [====================          ] 52/75 batches, loss: 0.6930Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.6929Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.6929Epoch 9/15: [======================        ] 55/75 batches, loss: 0.6929Epoch 9/15: [======================        ] 56/75 batches, loss: 0.6929Epoch 9/15: [======================        ] 57/75 batches, loss: 0.6929Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.6928Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.6928Epoch 9/15: [========================      ] 60/75 batches, loss: 0.6927Epoch 9/15: [========================      ] 61/75 batches, loss: 0.6927Epoch 9/15: [========================      ] 62/75 batches, loss: 0.6927Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.6928Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.6927Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.6927Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.6927Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.6926Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.6925Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.6926Epoch 9/15: [============================  ] 70/75 batches, loss: 0.6925Epoch 9/15: [============================  ] 71/75 batches, loss: 0.6923Epoch 9/15: [============================  ] 72/75 batches, loss: 0.6921Epoch 9/15: [============================= ] 73/75 batches, loss: 0.6919Epoch 9/15: [============================= ] 74/75 batches, loss: 0.6915Epoch 9/15: [==============================] 75/75 batches, loss: 0.6915
[2025-05-07 12:08:25,506][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.6915
[2025-05-07 12:08:25,772][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.6862, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.2, 'precision': 0.5, 'recall': 0.125}
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.6144Epoch 10/15: [                              ] 2/75 batches, loss: 0.6310Epoch 10/15: [=                             ] 3/75 batches, loss: 0.6679Epoch 10/15: [=                             ] 4/75 batches, loss: 0.6640Epoch 10/15: [==                            ] 5/75 batches, loss: 0.6636Epoch 10/15: [==                            ] 6/75 batches, loss: 0.6874Epoch 10/15: [==                            ] 7/75 batches, loss: 0.6841Epoch 10/15: [===                           ] 8/75 batches, loss: 0.6908Epoch 10/15: [===                           ] 9/75 batches, loss: 0.6913Epoch 10/15: [====                          ] 10/75 batches, loss: 0.6911Epoch 10/15: [====                          ] 11/75 batches, loss: 0.6911Epoch 10/15: [====                          ] 12/75 batches, loss: 0.6911Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.6911Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.6912Epoch 10/15: [======                        ] 15/75 batches, loss: 0.6913Epoch 10/15: [======                        ] 16/75 batches, loss: 0.6915Epoch 10/15: [======                        ] 17/75 batches, loss: 0.6916Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.6916Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.6917Epoch 10/15: [========                      ] 20/75 batches, loss: 0.6918Epoch 10/15: [========                      ] 21/75 batches, loss: 0.6919Epoch 10/15: [========                      ] 22/75 batches, loss: 0.6919Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.6920Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.6920Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.6921Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.6921Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.6921Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.6922Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.6922Epoch 10/15: [============                  ] 30/75 batches, loss: 0.6923Epoch 10/15: [============                  ] 31/75 batches, loss: 0.6923Epoch 10/15: [============                  ] 32/75 batches, loss: 0.6923Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.6923Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.6924Epoch 10/15: [==============                ] 35/75 batches, loss: 0.6924Epoch 10/15: [==============                ] 36/75 batches, loss: 0.6924Epoch 10/15: [==============                ] 37/75 batches, loss: 0.6924Epoch 10/15: [===============               ] 38/75 batches, loss: 0.6924Epoch 10/15: [===============               ] 39/75 batches, loss: 0.6925Epoch 10/15: [================              ] 40/75 batches, loss: 0.6925Epoch 10/15: [================              ] 41/75 batches, loss: 0.6925Epoch 10/15: [================              ] 42/75 batches, loss: 0.6925Epoch 10/15: [=================             ] 43/75 batches, loss: 0.6925Epoch 10/15: [=================             ] 44/75 batches, loss: 0.6925Epoch 10/15: [==================            ] 45/75 batches, loss: 0.6925Epoch 10/15: [==================            ] 46/75 batches, loss: 0.6926Epoch 10/15: [==================            ] 47/75 batches, loss: 0.6926Epoch 10/15: [===================           ] 48/75 batches, loss: 0.6926Epoch 10/15: [===================           ] 49/75 batches, loss: 0.6926Epoch 10/15: [====================          ] 50/75 batches, loss: 0.6926Epoch 10/15: [====================          ] 51/75 batches, loss: 0.6926Epoch 10/15: [====================          ] 52/75 batches, loss: 0.6926Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.6926Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.6926Epoch 10/15: [======================        ] 55/75 batches, loss: 0.6927Epoch 10/15: [======================        ] 56/75 batches, loss: 0.6927Epoch 10/15: [======================        ] 57/75 batches, loss: 0.6927Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.6927Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.6927Epoch 10/15: [========================      ] 60/75 batches, loss: 0.6927Epoch 10/15: [========================      ] 61/75 batches, loss: 0.6927Epoch 10/15: [========================      ] 62/75 batches, loss: 0.6927Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.6927Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.6927Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.6927Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.6927Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.6927Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.6927Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.6928Epoch 10/15: [============================  ] 70/75 batches, loss: 0.6928Epoch 10/15: [============================  ] 71/75 batches, loss: 0.6928Epoch 10/15: [============================  ] 72/75 batches, loss: 0.6928Epoch 10/15: [============================= ] 73/75 batches, loss: 0.6928Epoch 10/15: [============================= ] 74/75 batches, loss: 0.6928Epoch 10/15: [==============================] 75/75 batches, loss: 0.6928
[2025-05-07 12:08:28,588][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.6928
[2025-05-07 12:08:28,820][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:08:28,820][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.6931Epoch 11/15: [                              ] 2/75 batches, loss: 0.6932Epoch 11/15: [=                             ] 3/75 batches, loss: 0.6931Epoch 11/15: [=                             ] 4/75 batches, loss: 0.6931Epoch 11/15: [==                            ] 5/75 batches, loss: 0.6931Epoch 11/15: [==                            ] 6/75 batches, loss: 0.6931Epoch 11/15: [==                            ] 7/75 batches, loss: 0.6931Epoch 11/15: [===                           ] 8/75 batches, loss: 0.6931Epoch 11/15: [===                           ] 9/75 batches, loss: 0.6931Epoch 11/15: [====                          ] 10/75 batches, loss: 0.6931Epoch 11/15: [====                          ] 11/75 batches, loss: 0.6931Epoch 11/15: [====                          ] 12/75 batches, loss: 0.6931Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.6931Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.6931Epoch 11/15: [======                        ] 15/75 batches, loss: 0.6931Epoch 11/15: [======                        ] 16/75 batches, loss: 0.6931Epoch 11/15: [======                        ] 17/75 batches, loss: 0.6931Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.6931Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 11/15: [========                      ] 20/75 batches, loss: 0.6931Epoch 11/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 11/15: [========                      ] 22/75 batches, loss: 0.6931Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.6931Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.6931Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.6931Epoch 11/15: [============                  ] 30/75 batches, loss: 0.6931Epoch 11/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 11/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 11/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 11/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 11/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 11/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 11/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 11/15: [================              ] 40/75 batches, loss: 0.6931Epoch 11/15: [================              ] 41/75 batches, loss: 0.6931Epoch 11/15: [================              ] 42/75 batches, loss: 0.6931Epoch 11/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 11/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 11/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 11/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 11/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 11/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 11/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 11/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 11/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 11/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 11/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 11/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 11/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 11/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 11/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 11/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 11/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 11/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 11/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 11/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 11/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 11/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-07 12:08:31,107][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.6931
[2025-05-07 12:08:31,330][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:08:31,331][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.6931Epoch 12/15: [                              ] 2/75 batches, loss: 0.6931Epoch 12/15: [=                             ] 3/75 batches, loss: 0.6931Epoch 12/15: [=                             ] 4/75 batches, loss: 0.6931Epoch 12/15: [==                            ] 5/75 batches, loss: 0.6931Epoch 12/15: [==                            ] 6/75 batches, loss: 0.6931Epoch 12/15: [==                            ] 7/75 batches, loss: 0.6931Epoch 12/15: [===                           ] 8/75 batches, loss: 0.6931Epoch 12/15: [===                           ] 9/75 batches, loss: 0.6931Epoch 12/15: [====                          ] 10/75 batches, loss: 0.6931Epoch 12/15: [====                          ] 11/75 batches, loss: 0.6931Epoch 12/15: [====                          ] 12/75 batches, loss: 0.6931Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.6931Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.6931Epoch 12/15: [======                        ] 15/75 batches, loss: 0.6931Epoch 12/15: [======                        ] 16/75 batches, loss: 0.6931Epoch 12/15: [======                        ] 17/75 batches, loss: 0.6931Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.6931Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 12/15: [========                      ] 20/75 batches, loss: 0.6931Epoch 12/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 12/15: [========                      ] 22/75 batches, loss: 0.6931Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.6931Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.6931Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.6931Epoch 12/15: [============                  ] 30/75 batches, loss: 0.6931Epoch 12/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 12/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 12/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 12/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 12/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 12/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 12/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 12/15: [================              ] 40/75 batches, loss: 0.6931Epoch 12/15: [================              ] 41/75 batches, loss: 0.6931Epoch 12/15: [================              ] 42/75 batches, loss: 0.6931Epoch 12/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 12/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 12/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 12/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 12/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 12/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 12/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 12/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 12/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 12/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 12/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 12/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 12/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 12/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 12/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 12/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 12/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 12/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 12/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 12/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 12/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 12/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-07 12:08:33,663][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.6931
[2025-05-07 12:08:33,881][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:08:33,881][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-07 12:08:33,881][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 12
[2025-05-07 12:08:33,882][src.training.lm_trainer][INFO] - Training completed in 34.22 seconds
[2025-05-07 12:08:33,882][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 12:08:36,832][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5499580184718724, 'f1': 0.3110539845758355, 'precision': 0.6648351648351648, 'recall': 0.20302013422818793}
[2025-05-07 12:08:36,832][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.4782608695652174, 'f1': 0.2, 'precision': 0.5, 'recall': 0.125}
[2025-05-07 12:08:36,832][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.3804347826086957, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 12:08:38,679][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control3/layer2/ja/ja/model.pt
[2025-05-07 12:08:38,681][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▁▁▁▁
wandb:           best_val_f1 ▁▁▁▁▁█
wandb:         best_val_loss █████▁
wandb:    best_val_precision ▁▁▁▁▁█
wandb:       best_val_recall ▁▁▁▁▁█
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁▁▁▁▁▁▁▁
wandb:            train_loss █▄▄▅▄▄▅▃▁▄▄▄
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                val_f1 ▁▁▁▁▁▁▁▁█▁▁▁
wandb:              val_loss ████████▁███
wandb:         val_precision ▁▁▁▁▁▁▁▁█▁▁▁
wandb:            val_recall ▁▁▁▁▁▁▁▁█▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.47826
wandb:           best_val_f1 0.2
wandb:         best_val_loss 0.68615
wandb:    best_val_precision 0.5
wandb:       best_val_recall 0.125
wandb:      early_stop_epoch 12
wandb:                 epoch 12
wandb:   final_test_accuracy 0.38043
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.54996
wandb:        final_train_f1 0.31105
wandb: final_train_precision 0.66484
wandb:    final_train_recall 0.20302
wandb:    final_val_accuracy 0.47826
wandb:          final_val_f1 0.2
wandb:   final_val_precision 0.5
wandb:      final_val_recall 0.125
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69314
wandb:            train_time 34.22032
wandb:          val_accuracy 0.47826
wandb:                val_f1 0
wandb:              val_loss 0.69315
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_120740-rqp6prjw
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_120740-rqp6prjw/logs
Experiment probe_layer2_question_type_control3_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control3/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_complexity_control1_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_control1_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 12:09:11,404][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/ja
experiment_name: probe_layer2_complexity_control1_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 12:09:11,404][__main__][INFO] - Normalized task: complexity
[2025-05-07 12:09:11,404][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 12:09:11,404][__main__][INFO] - Determined Task Type: regression
[2025-05-07 12:09:11,426][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-05-07 12:09:11,426][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 12:09:14,966][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 12:09:17,285][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 12:09:17,285][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:09:17,498][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-07 12:09:17,542][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-07 12:09:17,881][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 12:09:17,891][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:09:17,892][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 12:09:17,894][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:09:17,987][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:09:18,124][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:09:18,138][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 12:09:18,139][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:09:18,139][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 12:09:18,142][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 12:09:18,233][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:09:18,356][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 12:09:18,371][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 12:09:18,372][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 12:09:18,373][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 12:09:18,373][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 12:09:18,374][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 12:09:18,374][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 12:09:18,374][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 12:09:18,374][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 12:09:18,374][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 12:09:18,375][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-05-07 12:09:18,375][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 12:09:18,375][src.data.datasets][INFO] - Sample label: 0.5826417803764343
[2025-05-07 12:09:18,375][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 12:09:18,375][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 12:09:18,375][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 12:09:18,375][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 12:09:18,375][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 12:09:18,375][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-05-07 12:09:18,376][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 12:09:18,376][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-05-07 12:09:18,376][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 12:09:18,376][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 12:09:18,376][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 12:09:18,376][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 12:09:18,376][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 12:09:18,376][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-05-07 12:09:18,376][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 12:09:18,376][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-05-07 12:09:18,376][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 12:09:18,377][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 12:09:18,377][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 12:09:18,377][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 12:09:18,377][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 12:09:26,307][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 12:09:26,308][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 12:09:26,308][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 12:09:26,308][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 12:09:26,311][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 12:09:26,311][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 12:09:26,311][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 12:09:26,312][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 12:09:26,312][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 12:09:26,313][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 12:09:26,313][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4611Epoch 1/15: [                              ] 2/75 batches, loss: 0.5168Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4971Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4820Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4501Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4213Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4048Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4202Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3981Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3889Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3764Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3864Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3682Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3841Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3834Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3989Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3949Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4127Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4067Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4091Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4001Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4028Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3907Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3805Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3813Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3789Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3728Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3676Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3669Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3615Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3559Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3511Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3518Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3502Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3486Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3516Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3485Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3452Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3449Epoch 1/15: [================              ] 40/75 batches, loss: 0.3412Epoch 1/15: [================              ] 41/75 batches, loss: 0.3375Epoch 1/15: [================              ] 42/75 batches, loss: 0.3370Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3343Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3345Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3336Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3288Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3248Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3208Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3182Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3155Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3144Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3171Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3132Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3152Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3151Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3120Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3088Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3071Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3052Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3047Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3041Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3036Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3050Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3050Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3036Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3021Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3018Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2999Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2987Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2960Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2947Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2941Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2919Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2913Epoch 1/15: [==============================] 75/75 batches, loss: 0.2891
[2025-05-07 12:09:32,950][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2891
[2025-05-07 12:09:33,185][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0808, Metrics: {'mse': 0.08104854077100754, 'rmse': 0.2846902540850451, 'r2': -0.3209230899810791}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1608Epoch 2/15: [                              ] 2/75 batches, loss: 0.2141Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2375Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2447Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2356Epoch 2/15: [==                            ] 6/75 batches, loss: 0.2223Epoch 2/15: [==                            ] 7/75 batches, loss: 0.2340Epoch 2/15: [===                           ] 8/75 batches, loss: 0.2207Epoch 2/15: [===                           ] 9/75 batches, loss: 0.2347Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2376Epoch 2/15: [====                          ] 11/75 batches, loss: 0.2251Epoch 2/15: [====                          ] 12/75 batches, loss: 0.2242Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.2222Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.2169Epoch 2/15: [======                        ] 15/75 batches, loss: 0.2166Epoch 2/15: [======                        ] 16/75 batches, loss: 0.2128Epoch 2/15: [======                        ] 17/75 batches, loss: 0.2103Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.2104Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.2069Epoch 2/15: [========                      ] 20/75 batches, loss: 0.2033Epoch 2/15: [========                      ] 21/75 batches, loss: 0.2002Epoch 2/15: [========                      ] 22/75 batches, loss: 0.2001Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1960Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1940Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1892Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1916Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1925Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1928Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1935Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1904Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1877Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1886Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1877Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1863Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1842Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1823Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1840Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1818Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1792Epoch 2/15: [================              ] 40/75 batches, loss: 0.1790Epoch 2/15: [================              ] 41/75 batches, loss: 0.1777Epoch 2/15: [================              ] 42/75 batches, loss: 0.1771Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1755Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1771Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1764Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1746Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1738Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1721Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1721Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1712Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1710Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1706Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1701Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1696Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1681Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1690Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1680Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1675Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1662Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1651Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1649Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1660Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1657Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1645Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1626Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1611Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1635Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1635Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1622Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1618Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1614Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1609Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1600Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1587Epoch 2/15: [==============================] 75/75 batches, loss: 0.1598
[2025-05-07 12:09:35,957][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1598
[2025-05-07 12:09:36,173][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0872, Metrics: {'mse': 0.08705375343561172, 'rmse': 0.29504873061176135, 'r2': -0.4187955856323242}
[2025-05-07 12:09:36,174][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1935Epoch 3/15: [                              ] 2/75 batches, loss: 0.1772Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1512Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1368Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1318Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1385Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1301Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1316Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1329Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1269Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1245Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1207Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1273Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1218Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1215Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1212Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1169Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1179Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1181Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1200Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1233Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1219Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1234Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1217Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1196Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1200Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1239Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1245Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1234Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1230Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1237Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1282Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1286Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1278Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1269Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1267Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1297Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1278Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1272Epoch 3/15: [================              ] 40/75 batches, loss: 0.1257Epoch 3/15: [================              ] 41/75 batches, loss: 0.1257Epoch 3/15: [================              ] 42/75 batches, loss: 0.1256Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1245Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1231Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1228Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1216Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1212Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1197Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1188Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1185Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1180Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1180Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1173Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1168Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1160Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1149Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1142Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1154Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1152Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1146Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1149Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1147Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1141Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1137Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1153Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1145Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1141Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1144Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1141Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1136Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1129Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1131Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1135Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1133Epoch 3/15: [==============================] 75/75 batches, loss: 0.1139
[2025-05-07 12:09:38,504][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1139
[2025-05-07 12:09:38,688][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0747, Metrics: {'mse': 0.07468772679567337, 'rmse': 0.273290553798834, 'r2': -0.21725499629974365}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1173Epoch 4/15: [                              ] 2/75 batches, loss: 0.1120Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1367Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1318Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1153Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1151Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1142Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1173Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1140Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1083Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1151Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1126Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1108Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1080Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1031Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1031Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1039Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1034Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1066Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1057Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1027Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1029Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1013Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1025Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1018Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1009Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1025Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1009Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1008Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1006Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1010Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1010Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1008Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0997Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0996Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0998Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0988Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0984Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0978Epoch 4/15: [================              ] 40/75 batches, loss: 0.0983Epoch 4/15: [================              ] 41/75 batches, loss: 0.0976Epoch 4/15: [================              ] 42/75 batches, loss: 0.0983Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0991Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0984Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0978Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0984Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0984Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0993Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0980Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0973Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0972Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0968Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0972Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0963Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0959Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0963Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0957Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0962Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0956Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0951Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0948Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0940Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0938Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0935Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0933Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0937Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0940Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0941Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0946Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0942Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0938Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0941Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0940Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0954Epoch 4/15: [==============================] 75/75 batches, loss: 0.0956
[2025-05-07 12:09:41,718][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0956
[2025-05-07 12:09:41,977][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0740, Metrics: {'mse': 0.07396923750638962, 'rmse': 0.2719728617093801, 'r2': -0.20554518699645996}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1075Epoch 5/15: [                              ] 2/75 batches, loss: 0.1029Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0835Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0857Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0864Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0939Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0927Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0873Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0892Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0901Epoch 5/15: [====                          ] 11/75 batches, loss: 0.1010Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0998Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0965Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0971Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0959Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0939Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0930Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0922Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0908Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0899Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0885Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0895Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0923Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0909Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0928Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0920Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0926Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0913Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0923Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0912Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0904Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0895Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0893Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0889Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0902Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0889Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0888Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0884Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0874Epoch 5/15: [================              ] 40/75 batches, loss: 0.0866Epoch 5/15: [================              ] 41/75 batches, loss: 0.0867Epoch 5/15: [================              ] 42/75 batches, loss: 0.0885Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0879Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0872Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0865Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0869Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0867Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0864Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0862Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0865Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0860Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0858Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0863slurmstepd: error: *** JOB 64463355 ON k28i22 CANCELLED AT 2025-05-07T12:09:44 DUE TO TIME LIMIT ***
